{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-Work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /home/mist861/anaconda3/lib/python3.11/site-packages (2.4.1)\n",
      "Requirement already satisfied: torchvision in /home/mist861/anaconda3/lib/python3.11/site-packages (0.19.1)\n",
      "Requirement already satisfied: keras in /home/mist861/anaconda3/lib/python3.11/site-packages (3.5.0)\n",
      "Collecting tensorflow[and-cuda]\n",
      "  Using cached tensorflow-2.17.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.2 kB)\n",
      "Requirement already satisfied: filelock in /home/mist861/anaconda3/lib/python3.11/site-packages (from torch) (3.16.0)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /home/mist861/anaconda3/lib/python3.11/site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: sympy in /home/mist861/anaconda3/lib/python3.11/site-packages (from torch) (1.11.1)\n",
      "Requirement already satisfied: networkx in /home/mist861/anaconda3/lib/python3.11/site-packages (from torch) (3.1)\n",
      "Requirement already satisfied: jinja2 in /home/mist861/anaconda3/lib/python3.11/site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /home/mist861/anaconda3/lib/python3.11/site-packages (from torch) (2024.6.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /home/mist861/anaconda3/lib/python3.11/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /home/mist861/anaconda3/lib/python3.11/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /home/mist861/anaconda3/lib/python3.11/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /home/mist861/anaconda3/lib/python3.11/site-packages (from torch) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /home/mist861/anaconda3/lib/python3.11/site-packages (from torch) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /home/mist861/anaconda3/lib/python3.11/site-packages (from torch) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /home/mist861/anaconda3/lib/python3.11/site-packages (from torch) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /home/mist861/anaconda3/lib/python3.11/site-packages (from torch) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /home/mist861/anaconda3/lib/python3.11/site-packages (from torch) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /home/mist861/anaconda3/lib/python3.11/site-packages (from torch) (2.20.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /home/mist861/anaconda3/lib/python3.11/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: triton==3.0.0 in /home/mist861/anaconda3/lib/python3.11/site-packages (from torch) (3.0.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /home/mist861/anaconda3/lib/python3.11/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch) (12.3.101)\n",
      "Requirement already satisfied: numpy in /home/mist861/anaconda3/lib/python3.11/site-packages (from torchvision) (1.26.4)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /home/mist861/anaconda3/lib/python3.11/site-packages (from torchvision) (9.4.0)\n",
      "Requirement already satisfied: absl-py in /home/mist861/anaconda3/lib/python3.11/site-packages (from keras) (2.1.0)\n",
      "Requirement already satisfied: rich in /home/mist861/anaconda3/lib/python3.11/site-packages (from keras) (13.7.1)\n",
      "Requirement already satisfied: namex in /home/mist861/anaconda3/lib/python3.11/site-packages (from keras) (0.0.8)\n",
      "Requirement already satisfied: h5py in /home/mist861/anaconda3/lib/python3.11/site-packages (from keras) (3.11.0)\n",
      "Requirement already satisfied: optree in /home/mist861/anaconda3/lib/python3.11/site-packages (from keras) (0.11.0)\n",
      "Requirement already satisfied: ml-dtypes in /home/mist861/anaconda3/lib/python3.11/site-packages (from keras) (0.3.2)\n",
      "Requirement already satisfied: packaging in /home/mist861/anaconda3/lib/python3.11/site-packages (from keras) (23.2)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /home/mist861/anaconda3/lib/python3.11/site-packages (from tensorflow[and-cuda]) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in /home/mist861/anaconda3/lib/python3.11/site-packages (from tensorflow[and-cuda]) (24.3.25)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /home/mist861/anaconda3/lib/python3.11/site-packages (from tensorflow[and-cuda]) (0.5.4)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /home/mist861/anaconda3/lib/python3.11/site-packages (from tensorflow[and-cuda]) (0.2.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /home/mist861/anaconda3/lib/python3.11/site-packages (from tensorflow[and-cuda]) (18.1.1)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /home/mist861/anaconda3/lib/python3.11/site-packages (from tensorflow[and-cuda]) (3.3.0)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /home/mist861/anaconda3/lib/python3.11/site-packages (from tensorflow[and-cuda]) (4.21.12)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /home/mist861/anaconda3/lib/python3.11/site-packages (from tensorflow[and-cuda]) (2.32.3)\n",
      "Requirement already satisfied: setuptools in /home/mist861/anaconda3/lib/python3.11/site-packages (from tensorflow[and-cuda]) (70.0.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /home/mist861/anaconda3/lib/python3.11/site-packages (from tensorflow[and-cuda]) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /home/mist861/anaconda3/lib/python3.11/site-packages (from tensorflow[and-cuda]) (2.4.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /home/mist861/anaconda3/lib/python3.11/site-packages (from tensorflow[and-cuda]) (1.14.1)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /home/mist861/anaconda3/lib/python3.11/site-packages (from tensorflow[and-cuda]) (1.64.1)\n",
      "Requirement already satisfied: tensorboard<2.18,>=2.17 in /home/mist861/anaconda3/lib/python3.11/site-packages (from tensorflow[and-cuda]) (2.17.0)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /home/mist861/anaconda3/lib/python3.11/site-packages (from tensorflow[and-cuda]) (0.37.0)\n",
      "INFO: pip is looking at multiple versions of tensorflow[and-cuda] to determine which version is compatible with other requirements. This could take a while.\n",
      "  Downloading tensorflow-2.16.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.2 kB)\n",
      "Collecting tensorboard<2.17,>=2.16 (from tensorflow[and-cuda])\n",
      "  Downloading tensorboard-2.16.2-py3-none-any.whl.metadata (1.6 kB)\n",
      "Collecting tensorflow[and-cuda]\n",
      "  Downloading tensorflow-2.16.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.3 kB)\n",
      "  Downloading tensorflow-2.15.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.2 kB)\n",
      "Collecting tensorboard<2.16,>=2.15 (from tensorflow[and-cuda])\n",
      "  Downloading tensorboard-2.15.2-py3-none-any.whl.metadata (1.7 kB)\n",
      "Collecting tensorflow-estimator<2.16,>=2.15.0 (from tensorflow[and-cuda])\n",
      "  Downloading tensorflow_estimator-2.15.0-py2.py3-none-any.whl.metadata (1.3 kB)\n",
      "Collecting keras\n",
      "  Downloading keras-2.15.0-py3-none-any.whl.metadata (2.4 kB)\n",
      "Collecting tensorflow[and-cuda]\n",
      "  Downloading tensorflow-2.15.0.post1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.2 kB)\n",
      "Collecting ml-dtypes (from keras)\n",
      "  Downloading ml_dtypes-0.2.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (20 kB)\n",
      "Collecting tensorflow[and-cuda]\n",
      "  Downloading tensorflow-2.15.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.4 kB)\n",
      "  Downloading tensorflow-2.14.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.1 kB)\n",
      "Collecting tensorboard<2.15,>=2.14 (from tensorflow[and-cuda])\n",
      "  Downloading tensorboard-2.14.1-py3-none-any.whl.metadata (1.7 kB)\n",
      "Collecting tensorflow-estimator<2.15,>=2.14.0 (from tensorflow[and-cuda])\n",
      "  Downloading tensorflow_estimator-2.14.0-py2.py3-none-any.whl.metadata (1.3 kB)\n",
      "Collecting keras\n",
      "  Downloading keras-2.14.0-py3-none-any.whl.metadata (2.4 kB)\n",
      "Collecting nvidia-cuda-runtime-cu11==11.8.89 (from tensorflow[and-cuda])\n",
      "  Downloading nvidia_cuda_runtime_cu11-11.8.89-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cublas-cu11==11.11.3.6 (from tensorflow[and-cuda])\n",
      "  Downloading nvidia_cublas_cu11-11.11.3.6-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cufft-cu11==10.9.0.58 (from tensorflow[and-cuda])\n",
      "  Downloading nvidia_cufft_cu11-10.9.0.58-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cudnn-cu11==8.7.0.84 (from tensorflow[and-cuda])\n",
      "  Downloading nvidia_cudnn_cu11-8.7.0.84-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-curand-cu11==10.3.0.86 (from tensorflow[and-cuda])\n",
      "  Downloading nvidia_curand_cu11-10.3.0.86-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cusolver-cu11==11.4.1.48 (from tensorflow[and-cuda])\n",
      "  Downloading nvidia_cusolver_cu11-11.4.1.48-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cusparse-cu11==11.7.5.86 (from tensorflow[and-cuda])\n",
      "  Downloading nvidia_cusparse_cu11-11.7.5.86-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-nccl-cu11==2.16.5 (from tensorflow[and-cuda])\n",
      "  Downloading nvidia_nccl_cu11-2.16.5-py3-none-manylinux1_x86_64.whl.metadata (1.8 kB)\n",
      "Collecting nvidia-cuda-cupti-cu11==11.8.87 (from tensorflow[and-cuda])\n",
      "  Downloading nvidia_cuda_cupti_cu11-11.8.87-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cuda-nvcc-cu11==11.8.89 (from tensorflow[and-cuda])\n",
      "  Downloading nvidia_cuda_nvcc_cu11-11.8.89-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting tensorflow[and-cuda]\n",
      "  Downloading tensorflow-2.14.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.1 kB)\n",
      "INFO: pip is still looking at multiple versions of tensorflow[and-cuda] to determine which version is compatible with other requirements. This could take a while.\n",
      "  Downloading tensorflow-2.13.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.4 kB)\n",
      "\u001b[33mWARNING: tensorflow 2.13.1 does not provide the extra 'and-cuda'\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting gast<=0.4.0,>=0.2.1 (from tensorflow[and-cuda])\n",
      "  Downloading gast-0.4.0-py3-none-any.whl.metadata (1.1 kB)\n",
      "Collecting keras\n",
      "  Downloading keras-2.13.1-py3-none-any.whl.metadata (2.4 kB)\n",
      "Collecting numpy (from torchvision)\n",
      "  Downloading numpy-1.24.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.6 kB)\n",
      "Collecting tensorboard<2.14,>=2.13 (from tensorflow[and-cuda])\n",
      "  Downloading tensorboard-2.13.0-py3-none-any.whl.metadata (1.8 kB)\n",
      "Collecting tensorflow-estimator<2.14,>=2.13.0 (from tensorflow[and-cuda])\n",
      "  Downloading tensorflow_estimator-2.13.0-py2.py3-none-any.whl.metadata (1.3 kB)\n",
      "Collecting tensorflow[and-cuda]\n",
      "  Downloading tensorflow-2.13.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.4 kB)\n",
      "\u001b[33mWARNING: tensorflow 2.13.0 does not provide the extra 'and-cuda'\u001b[0m\u001b[33m\n",
      "\u001b[0m  Downloading tensorflow-2.12.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.4 kB)\n",
      "\u001b[33mWARNING: tensorflow 2.12.1 does not provide the extra 'and-cuda'\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting jax>=0.3.15 (from tensorflow[and-cuda])\n",
      "  Downloading jax-0.4.34-py3-none-any.whl.metadata (22 kB)\n",
      "Collecting keras\n",
      "  Downloading keras-2.12.0-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Collecting tensorboard<2.13,>=2.12 (from tensorflow[and-cuda])\n",
      "  Downloading tensorboard-2.12.3-py3-none-any.whl.metadata (1.8 kB)\n",
      "Collecting tensorflow-estimator<2.13,>=2.12.0 (from tensorflow[and-cuda])\n",
      "  Downloading tensorflow_estimator-2.12.0-py2.py3-none-any.whl.metadata (1.3 kB)\n",
      "Collecting tensorflow[and-cuda]\n",
      "  Downloading tensorflow-2.12.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.4 kB)\n",
      "\u001b[33mWARNING: tensorflow 2.12.0 does not provide the extra 'and-cuda'\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting numpy (from torchvision)\n",
      "  Downloading numpy-1.23.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.3 kB)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /home/mist861/anaconda3/lib/python3.11/site-packages (from astunparse>=1.6.0->tensorflow[and-cuda]) (0.38.4)\n",
      "Collecting jaxlib<=0.4.34,>=0.4.34 (from jax>=0.3.15->tensorflow[and-cuda])\n",
      "  Downloading jaxlib-0.4.34-cp311-cp311-manylinux2014_x86_64.whl.metadata (983 bytes)\n",
      "INFO: pip is looking at multiple versions of jax to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting jax>=0.3.15 (from tensorflow[and-cuda])\n",
      "  Downloading jax-0.4.33-py3-none-any.whl.metadata (22 kB)\n",
      "Collecting jaxlib<=0.4.33,>=0.4.33 (from jax>=0.3.15->tensorflow[and-cuda])\n",
      "  Downloading jaxlib-0.4.33-cp311-cp311-manylinux2014_x86_64.whl.metadata (983 bytes)\n",
      "Collecting jax>=0.3.15 (from tensorflow[and-cuda])\n",
      "  Downloading jax-0.4.31-py3-none-any.whl.metadata (22 kB)\n",
      "Collecting jaxlib<=0.4.31,>=0.4.30 (from jax>=0.3.15->tensorflow[and-cuda])\n",
      "  Downloading jaxlib-0.4.31-cp311-cp311-manylinux2014_x86_64.whl.metadata (983 bytes)\n",
      "Collecting jax>=0.3.15 (from tensorflow[and-cuda])\n",
      "  Downloading jax-0.4.30-py3-none-any.whl.metadata (22 kB)\n",
      "Collecting jaxlib<=0.4.30,>=0.4.27 (from jax>=0.3.15->tensorflow[and-cuda])\n",
      "  Downloading jaxlib-0.4.30-cp311-cp311-manylinux2014_x86_64.whl.metadata (1.0 kB)\n",
      "Requirement already satisfied: scipy>=1.9 in /home/mist861/anaconda3/lib/python3.11/site-packages (from jax>=0.3.15->tensorflow[and-cuda]) (1.10.1)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /home/mist861/anaconda3/lib/python3.11/site-packages (from tensorboard<2.13,>=2.12->tensorflow[and-cuda]) (2.34.0)\n",
      "Collecting google-auth-oauthlib<1.1,>=0.5 (from tensorboard<2.13,>=2.12->tensorflow[and-cuda])\n",
      "  Downloading google_auth_oauthlib-1.0.0-py2.py3-none-any.whl.metadata (2.7 kB)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /home/mist861/anaconda3/lib/python3.11/site-packages (from tensorboard<2.13,>=2.12->tensorflow[and-cuda]) (3.4.1)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /home/mist861/anaconda3/lib/python3.11/site-packages (from tensorboard<2.13,>=2.12->tensorflow[and-cuda]) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /home/mist861/anaconda3/lib/python3.11/site-packages (from tensorboard<2.13,>=2.12->tensorflow[and-cuda]) (3.0.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/mist861/anaconda3/lib/python3.11/site-packages (from jinja2->torch) (2.1.5)\n",
      "Requirement already satisfied: mpmath>=0.19 in /home/mist861/anaconda3/lib/python3.11/site-packages (from sympy->torch) (1.3.0)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /home/mist861/anaconda3/lib/python3.11/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow[and-cuda]) (5.3.3)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /home/mist861/anaconda3/lib/python3.11/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow[and-cuda]) (0.2.8)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /home/mist861/anaconda3/lib/python3.11/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow[and-cuda]) (4.9)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /home/mist861/anaconda3/lib/python3.11/site-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow[and-cuda]) (2.0.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/mist861/anaconda3/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorflow[and-cuda]) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/mist861/anaconda3/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorflow[and-cuda]) (2.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/mist861/anaconda3/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorflow[and-cuda]) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/mist861/anaconda3/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorflow[and-cuda]) (2024.6.2)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /home/mist861/anaconda3/lib/python3.11/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow[and-cuda]) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /home/mist861/anaconda3/lib/python3.11/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow[and-cuda]) (3.2.2)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /home/mist861/anaconda3/lib/python3.11/site-packages (from rich->keras) (2.2.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /home/mist861/anaconda3/lib/python3.11/site-packages (from rich->keras) (2.18.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in /home/mist861/anaconda3/lib/python3.11/site-packages (from markdown-it-py>=2.2.0->rich->keras) (0.1.0)\n",
      "Downloading keras-2.12.0-py2.py3-none-any.whl (1.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading gast-0.4.0-py3-none-any.whl (9.8 kB)\n",
      "Downloading jax-0.4.30-py3-none-any.whl (2.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m18.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading numpy-1.23.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.1/17.1 MB\u001b[0m \u001b[31m28.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading tensorboard-2.12.3-py3-none-any.whl (5.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m33.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tensorflow_estimator-2.12.0-py2.py3-none-any.whl (440 kB)\n",
      "Downloading tensorflow-2.12.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (586.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m586.0/586.0 MB\u001b[0m \u001b[31m39.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading google_auth_oauthlib-1.0.0-py2.py3-none-any.whl (18 kB)\n",
      "Downloading jaxlib-0.4.30-cp311-cp311-manylinux2014_x86_64.whl (79.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.6/79.6 MB\u001b[0m \u001b[31m40.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: tensorflow-estimator, numpy, keras, gast, jaxlib, google-auth-oauthlib, tensorboard, jax, tensorflow\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.26.4\n",
      "    Uninstalling numpy-1.26.4:\n",
      "      Successfully uninstalled numpy-1.26.4\n",
      "  Attempting uninstall: keras\n",
      "    Found existing installation: keras 3.5.0\n",
      "    Uninstalling keras-3.5.0:\n",
      "      Successfully uninstalled keras-3.5.0\n",
      "  Attempting uninstall: gast\n",
      "    Found existing installation: gast 0.5.4\n",
      "    Uninstalling gast-0.5.4:\n",
      "      Successfully uninstalled gast-0.5.4\n",
      "  Attempting uninstall: tensorboard\n",
      "    Found existing installation: tensorboard 2.17.0\n",
      "    Uninstalling tensorboard-2.17.0:\n",
      "      Successfully uninstalled tensorboard-2.17.0\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "flair 0.13.1 requires pytorch-revgrad>=0.2.0, which is not installed.\n",
      "tables 3.8.0 requires blosc2~=2.0.0, which is not installed.\n",
      "tables 3.8.0 requires cython>=0.29.21, which is not installed.\n",
      "semantic-router 0.0.53 requires numpy<2.0.0,>=1.25.2, but you have numpy 1.23.5 which is incompatible.\n",
      "tf-keras 2.17.0 requires tensorflow<2.18,>=2.17, but you have tensorflow 2.12.0 which is incompatible.\n",
      "flair 0.13.1 requires urllib3<2.0.0,>=1.0.0, but you have urllib3 2.2.2 which is incompatible.\n",
      "pyfume 0.3.4 requires numpy==1.24.4, but you have numpy 1.23.5 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed gast-0.4.0 google-auth-oauthlib-1.0.0 jax-0.4.30 jaxlib-0.4.30 keras-2.12.0 numpy-1.23.5 tensorboard-2.12.3 tensorflow-2.12.0 tensorflow-estimator-2.12.0\n"
     ]
    }
   ],
   "source": [
    "!pip install torch torchvision keras tensorflow[and-cuda]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-18 12:40:33.011695: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-10-18 12:40:33.033278: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-10-18 12:40:33.400877: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "# FC Modules\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms.v2 as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# CNN Modules\n",
    "import keras\n",
    "from keras.datasets import fashion_mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Flatten, Conv2D, MaxPooling2D, Input, RandomFlip, RandomRotation, Dropout\n",
    "from keras.optimizers import SGD, Adam\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "# Tools/Other Modules\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import subplots\n",
    "import tensorflow as tf\n",
    "import csv\n",
    "import pandas as pd\n",
    "from ast import literal_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available() # Gotta have that CUDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-16 11:12:47.355937: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:982] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-10-16 11:12:47.409306: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:982] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-10-16 11:12:47.409331: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:982] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.config.list_physical_devices('GPU') # Make sure keras like CUDA too"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set Random Seeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "keras.utils.set_random_seed(42)\n",
    "np.random.seed(42)\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jum_uBK3SA8Q"
   },
   "source": [
    "## Part 1\n",
    "\n",
    "### PyTorch FC ANN MNIST Implementation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "id": "YDg8ZzyzSKpu"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mist861/anaconda3/lib/python3.11/site-packages/torchvision/transforms/v2/_deprecated.py:42: UserWarning: The transform `ToTensor()` is deprecated and will be removed in a future release. Instead, please use `v2.Compose([v2.ToImage(), v2.ToDtype(torch.float32, scale=True)])`.Output is equivalent up to float precision.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Transformations --> this is a \"pre-processing step\" that's typical for image processing methods\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),  # Convert PIL image or numpy.ndarray to tensor\n",
    "    transforms.Normalize((0.5,), (0.5,))  # Normalize data to range [-1, 1]\n",
    "    #transforms.RandomHorizontalFlip(),\n",
    "    #transforms.RandomVerticalFlip(),\n",
    "    #transforms.RandomRotation((-45,45))\n",
    "])\n",
    "# This dataset is already \"sorted\" as part of the import method, but no \"validation\" set has been selected in this case\n",
    "# Loading the FashionMNIST dataset\n",
    "\n",
    "train_dataset = torchvision.datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "test_dataset = torchvision.datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "# Training and Testing loaders\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "id": "3DRu0tGPS0dy"
   },
   "outputs": [],
   "source": [
    "# Mapping the labels for the MNIST dataset -- later we'll see that this using the \"keras to_categorical\" method as discussed in class\n",
    "labels_map = {\n",
    "    0: \"0\", 1: \"1\", 2: \"2\", 3: \"3\", 4: \"4\",\n",
    "    5: \"5\", 6: \"6\", 7: \"7\", 8: \"8\", 9: \"9\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 675
    },
    "id": "DnFh3_pTS8fD",
    "outputId": "5dc06071-596f-47f2-90e9-846eca184a30"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x800 with 9 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#This cell is designed to display a few images from the dataset\n",
    "#It isn't necessary to run this, but it can help give a better idea of the challanges your model will face\n",
    "figure = plt.figure(figsize=(8, 8))\n",
    "cols, rows = 3, 3\n",
    "\n",
    "# Displaying figures from the dataset randomly\n",
    "for i in range(1, cols * rows + 1):\n",
    "    sample_idx = torch.randint(len(train_dataset), size=(1,)).item()\n",
    "    img, label = train_dataset[sample_idx]\n",
    "    figure.add_subplot(rows, cols, i)\n",
    "    plt.title(labels_map[label])\n",
    "    plt.axis(\"off\")\n",
    "    plt.imshow(img.squeeze(), cmap=\"gray\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "id": "_z_Aj5LWS_-f"
   },
   "outputs": [],
   "source": [
    "#Here we define the model parameters -- the general strucutre as provided here will produce a fully connected network [28x28] --> 32 --> 16 --> 10\n",
    "class MLP(nn.Module): #MLP stands for \"Multi-Layer Perceptron\"\n",
    "    def __init__(self, inputs, neurons, dropout, activation): #this initializes the structure of the network\n",
    "    #def __init__(self): #this initializes the structure of the network\n",
    "        super(MLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(inputs, neurons[0]) ## First fully connected linear layer, 28*28 input features and 32 outputs\n",
    "        for layer in range(2,len(neurons)): # Create the remaining fully connected layers, based on the given size of the outputs\n",
    "            setattr(self, f'fc{layer}', nn.Linear(neurons[layer-2], neurons[layer-1])) # Of course, the input is the size of the previous output\n",
    "            if (dropout == True) and (layer % 2 == 0): # If the dropout flag is set, add a dropout layer every 2 layers\n",
    "                self.dropout = torch.nn.Dropout(0.2)\n",
    "        setattr(self, f'fc{len(neurons)}', nn.Linear(neurons[len(neurons)-2], neurons[len(neurons)-1])) ## 10 output features because MNIST has 10 target classes\n",
    "        self.size = len(neurons)\n",
    "        self.activation = activation # Define the array of activation functions\n",
    "    \n",
    "    def forward(self, train_data): #this modifies the elements of the intial structure defined above\n",
    "        train_data = train_data.view(-1, 28 * 28) #the array is sent in as a vector\n",
    "        for layer in range(1,self.size):\n",
    "            train_data = getattr(torch, f'{self.activation[layer-1]}')(getattr(self, f'fc{layer}')(train_data)) # Set the activation functions for each layer, except the output layer\n",
    "        train_data = getattr(self, f'fc{self.size}')(train_data) # No modifications to the activation of the output layer, that's handled by the optimizer\n",
    "        return train_data\n",
    "\n",
    "    def fit(self, train_loader, optimizer, loss_function, learning_rate, weight_decay, batch_size, epochs): # Aside from the tensor changes, this was left alone\n",
    "        criterion = getattr(nn, f'{loss_function}')()\n",
    "        optimizer = getattr(optim, f'{optimizer}')(self.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "        for epoch in range(epochs):\n",
    "            self.train()\n",
    "            running_loss = 0.0\n",
    "            for i, data in enumerate(train_loader, 0):\n",
    "                inputs, labels = data\n",
    "                inputs = torch.Tensor(inputs).to(device) # These both have to be cast to GPU tensors or it implodes\n",
    "                labels = torch.Tensor(labels).to(device)\n",
    "                optimizer.zero_grad()\n",
    "                outputs = self(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                running_loss += loss.item()\n",
    "                if i % batch_size == (batch_size - 1):  # print every 100 mini-batches\n",
    "                    print(f'Epoch {epoch + 1}, Batch {i + 1}, Loss: {running_loss / 100}')\n",
    "                    running_loss = 0.0\n",
    "\n",
    "    def score(self, test_data): # Like the above method, no changes except to the GPU tensors\n",
    "        self.eval()\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        with torch.no_grad():\n",
    "            for data in test_data:\n",
    "                images, labels = data\n",
    "                images = torch.Tensor(images).to(device) # These.  Still have to be cast to GPU\n",
    "                labels = torch.Tensor(labels).to(device)\n",
    "                outputs = model(images)\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "        score = correct / total\n",
    "        print(f'Accuracy on test set: {score}%')\n",
    "        return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Batch 100, Loss: 2.320899908542633\n",
      "Epoch 1, Batch 200, Loss: 2.31400821685791\n",
      "Epoch 1, Batch 300, Loss: 2.303220727443695\n",
      "Epoch 1, Batch 400, Loss: 2.302109658718109\n",
      "Epoch 1, Batch 500, Loss: 2.298563554286957\n",
      "Epoch 1, Batch 600, Loss: 2.295964391231537\n",
      "Epoch 1, Batch 700, Loss: 2.287771234512329\n",
      "Epoch 1, Batch 800, Loss: 2.2840355372428895\n",
      "Epoch 1, Batch 900, Loss: 2.2825369954109194\n",
      "Epoch 2, Batch 100, Loss: 2.275078468322754\n",
      "Epoch 2, Batch 200, Loss: 2.2705150699615477\n",
      "Epoch 2, Batch 300, Loss: 2.2652318406105043\n",
      "Epoch 2, Batch 400, Loss: 2.2631150770187376\n",
      "Epoch 2, Batch 500, Loss: 2.259621148109436\n",
      "Epoch 2, Batch 600, Loss: 2.253930525779724\n",
      "Epoch 2, Batch 700, Loss: 2.248571267127991\n",
      "Epoch 2, Batch 800, Loss: 2.2497659659385683\n",
      "Epoch 2, Batch 900, Loss: 2.2432373929023743\n",
      "Epoch 3, Batch 100, Loss: 2.227466549873352\n",
      "Epoch 3, Batch 200, Loss: 2.2226767921447754\n",
      "Epoch 3, Batch 300, Loss: 2.2189650440216067\n",
      "Epoch 3, Batch 400, Loss: 2.215651500225067\n",
      "Epoch 3, Batch 500, Loss: 2.2120893144607545\n",
      "Epoch 3, Batch 600, Loss: 2.196643328666687\n",
      "Epoch 3, Batch 700, Loss: 2.1948779249191284\n",
      "Epoch 3, Batch 800, Loss: 2.1865216445922853\n",
      "Epoch 3, Batch 900, Loss: 2.1785918283462524\n"
     ]
    }
   ],
   "source": [
    "# Testing the above with a random model\n",
    "\n",
    "model = MLP((28*28), [30, 10, 10], True, ['sigmoid', 'relu'])\n",
    "model.to(device)\n",
    "model.fit(train_loader, 'SGD', 'CrossEntropyLoss', 0.002, 0, 100, 3)\n",
    "#print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0.2957%\n"
     ]
    }
   ],
   "source": [
    "score = model.score(test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the range of parameters for the grid search\n",
    "parameters = {\n",
    "    'inputs': 28*28,\n",
    "    'number_of_layers': [1, 2, 3, 4],\n",
    "    'outputs': 10,\n",
    "    'neurons_per_layer': [10, 20, 30, 40, 50],\n",
    "    'dropout_layers': [False, True],\n",
    "    'activation_functions': ['relu', 'tanh', 'sigmoid'],\n",
    "    'optimizers': ['Adam', 'SGD'],\n",
    "    'learning_rates': [0.001, 0.003, 0.01, 0.03, 0.1, 0.3],\n",
    "    'weight_decays': [0.01, 0.03, 0.1, 0.3, 1],\n",
    "    'loss_functions': ['CrossEntropyLoss'],\n",
    "    'batches': [100,200,300],\n",
    "    'epochs': [25] # This is going to take so long that I decided to keep it short\n",
    "}\n",
    "\n",
    "results = {}\n",
    "best_parameters = {}\n",
    "worst_parameters = {}\n",
    "for parameter in parameters:\n",
    "    results[parameter] = []\n",
    "    best_parameters[parameter] = None\n",
    "    worst_parameters[parameter] = None\n",
    "\n",
    "results['score'] = []\n",
    "best_parameters['score'] = -1\n",
    "worst_parameters['score'] = 1\n",
    "combination = 0\n",
    "local_layers = {'neurons':[],'activations':[]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting for combination 0\n",
      "784\n",
      "1\n",
      "10\n",
      "[10, 10]\n",
      "False\n",
      "['sigmoid']\n",
      "Adam\n",
      "0.3\n",
      "1\n",
      "CrossEntropyLoss\n",
      "300\n",
      "25\n",
      "0\n",
      "Epoch 1, Batch 300, Loss: 6.939032137393951\n",
      "Epoch 1, Batch 600, Loss: 6.939589216709137\n",
      "Epoch 1, Batch 900, Loss: 7.026755197048187\n",
      "Epoch 2, Batch 300, Loss: 6.990331830978394\n",
      "Epoch 2, Batch 600, Loss: 7.009798233509064\n",
      "Epoch 2, Batch 900, Loss: 7.06328198671341\n",
      "Epoch 3, Batch 300, Loss: 7.0206928944587705\n",
      "Epoch 3, Batch 600, Loss: 7.04870913028717\n",
      "Epoch 3, Batch 900, Loss: 7.049217858314514\n",
      "Epoch 4, Batch 300, Loss: 7.024928114414215\n",
      "Epoch 4, Batch 600, Loss: 7.073163459300995\n",
      "Epoch 4, Batch 900, Loss: 7.054160330295563\n",
      "Epoch 5, Batch 300, Loss: 7.02917270898819\n",
      "Epoch 5, Batch 600, Loss: 7.048529734611511\n",
      "Epoch 5, Batch 900, Loss: 7.0216766810417175\n",
      "Epoch 6, Batch 300, Loss: 7.005801627635956\n",
      "Epoch 6, Batch 600, Loss: 6.9944692850112915\n",
      "Epoch 6, Batch 900, Loss: 6.999129559993744\n",
      "Epoch 7, Batch 300, Loss: 6.985286393165588\n",
      "Epoch 7, Batch 600, Loss: 7.00192095041275\n",
      "Epoch 7, Batch 900, Loss: 7.01396294593811\n",
      "Epoch 8, Batch 300, Loss: 7.010152797698975\n",
      "Epoch 8, Batch 600, Loss: 7.010225634574891\n",
      "Epoch 8, Batch 900, Loss: 6.992111148834229\n",
      "Epoch 9, Batch 300, Loss: 7.005576996803284\n",
      "Epoch 9, Batch 600, Loss: 7.020050880908966\n",
      "Epoch 9, Batch 900, Loss: 7.052835583686829\n",
      "Epoch 10, Batch 300, Loss: 7.070365986824036\n",
      "Epoch 10, Batch 600, Loss: 7.057727792263031\n",
      "Epoch 10, Batch 900, Loss: 7.0518632507324215\n",
      "Epoch 11, Batch 300, Loss: 7.04984228849411\n",
      "Epoch 11, Batch 600, Loss: 7.091826434135437\n",
      "Epoch 11, Batch 900, Loss: 7.067282943725586\n",
      "Epoch 12, Batch 300, Loss: 7.078273088932037\n",
      "Epoch 12, Batch 600, Loss: 7.041239876747131\n",
      "Epoch 12, Batch 900, Loss: 7.000425539016724\n",
      "Epoch 13, Batch 300, Loss: 7.022801997661591\n",
      "Epoch 13, Batch 600, Loss: 7.0101728749275205\n",
      "Epoch 13, Batch 900, Loss: 6.998752012252807\n",
      "Epoch 14, Batch 300, Loss: 7.0226247215271\n",
      "Epoch 14, Batch 600, Loss: 7.029188404083252\n",
      "Epoch 14, Batch 900, Loss: 7.021179704666138\n",
      "Epoch 15, Batch 300, Loss: 7.022781801223755\n",
      "Epoch 15, Batch 600, Loss: 7.027174754142761\n",
      "Epoch 15, Batch 900, Loss: 7.067828755378724\n",
      "Epoch 16, Batch 300, Loss: 7.0077075052261355\n",
      "Epoch 16, Batch 600, Loss: 7.05253142118454\n",
      "Epoch 16, Batch 900, Loss: 7.033748486042023\n",
      "Epoch 17, Batch 300, Loss: 7.049495484828949\n",
      "Epoch 17, Batch 600, Loss: 7.022984192371369\n",
      "Epoch 17, Batch 900, Loss: 7.014465620517731\n",
      "Epoch 18, Batch 300, Loss: 7.0331028985977175\n",
      "Epoch 18, Batch 600, Loss: 6.986077983379364\n",
      "Epoch 18, Batch 900, Loss: 6.9786979413032535\n",
      "Epoch 19, Batch 300, Loss: 6.974238653182983\n",
      "Epoch 19, Batch 600, Loss: 6.970551238059998\n",
      "Epoch 19, Batch 900, Loss: 6.977891447544098\n",
      "Epoch 20, Batch 300, Loss: 6.98650840997696\n",
      "Epoch 20, Batch 600, Loss: 6.972922191619873\n",
      "Epoch 20, Batch 900, Loss: 6.980523915290832\n",
      "Epoch 21, Batch 300, Loss: 6.963604164123535\n",
      "Epoch 21, Batch 600, Loss: 7.0009137678146365\n",
      "Epoch 21, Batch 900, Loss: 6.982842099666596\n",
      "Epoch 22, Batch 300, Loss: 6.985008630752564\n",
      "Epoch 22, Batch 600, Loss: 6.9686702585220335\n",
      "Epoch 22, Batch 900, Loss: 6.974070358276367\n",
      "Epoch 23, Batch 300, Loss: 6.978617849349976\n",
      "Epoch 23, Batch 600, Loss: 6.9746934914588925\n",
      "Epoch 23, Batch 900, Loss: 6.972129430770874\n",
      "Epoch 24, Batch 300, Loss: 6.981321451663971\n",
      "Epoch 24, Batch 600, Loss: 6.980369563102722\n",
      "Epoch 24, Batch 900, Loss: 6.961660642623901\n",
      "Epoch 25, Batch 300, Loss: 6.963355226516724\n",
      "Epoch 25, Batch 600, Loss: 6.96611124753952\n",
      "Epoch 25, Batch 900, Loss: 6.9667640709877015\n",
      "Accuracy on test set: 0.098%\n",
      "Fitting for combination 1\n",
      "784\n",
      "1\n",
      "10\n",
      "[10, 10]\n",
      "False\n",
      "['relu']\n",
      "SGD\n",
      "0.001\n",
      "0.03\n",
      "CrossEntropyLoss\n",
      "300\n",
      "25\n",
      "0\n",
      "Epoch 1, Batch 300, Loss: 6.687278945446014\n",
      "Epoch 1, Batch 600, Loss: 6.158210581541061\n",
      "Epoch 1, Batch 900, Loss: 5.679231982231141\n",
      "Epoch 2, Batch 300, Loss: 5.209910340309143\n",
      "Epoch 2, Batch 600, Loss: 4.796550724506378\n",
      "Epoch 2, Batch 900, Loss: 4.38231320977211\n",
      "Epoch 3, Batch 300, Loss: 3.9244837403297423\n",
      "Epoch 3, Batch 600, Loss: 3.59667058467865\n",
      "Epoch 3, Batch 900, Loss: 3.2726138401031495\n",
      "Epoch 4, Batch 300, Loss: 3.021089683175087\n",
      "Epoch 4, Batch 600, Loss: 2.74962012052536\n",
      "Epoch 4, Batch 900, Loss: 2.6263485181331636\n",
      "Epoch 5, Batch 300, Loss: 2.433509474992752\n",
      "Epoch 5, Batch 600, Loss: 2.324500806927681\n",
      "Epoch 5, Batch 900, Loss: 2.218149036169052\n",
      "Epoch 6, Batch 300, Loss: 2.12065595805645\n",
      "Epoch 6, Batch 600, Loss: 2.0430558890104296\n",
      "Epoch 6, Batch 900, Loss: 1.9596808245778083\n",
      "Epoch 7, Batch 300, Loss: 1.9047941446304322\n",
      "Epoch 7, Batch 600, Loss: 1.867834651172161\n",
      "Epoch 7, Batch 900, Loss: 1.8137494218349457\n",
      "Epoch 8, Batch 300, Loss: 1.7763562840223313\n",
      "Epoch 8, Batch 600, Loss: 1.7132789254188538\n",
      "Epoch 8, Batch 900, Loss: 1.7101863434910773\n",
      "Epoch 9, Batch 300, Loss: 1.6797506919503211\n",
      "Epoch 9, Batch 600, Loss: 1.649891700744629\n",
      "Epoch 9, Batch 900, Loss: 1.6151438760757446\n",
      "Epoch 10, Batch 300, Loss: 1.586058071255684\n",
      "Epoch 10, Batch 600, Loss: 1.564443847835064\n",
      "Epoch 10, Batch 900, Loss: 1.591638140976429\n",
      "Epoch 11, Batch 300, Loss: 1.5660599905252457\n",
      "Epoch 11, Batch 600, Loss: 1.5320523193478583\n",
      "Epoch 11, Batch 900, Loss: 1.4787288150191307\n",
      "Epoch 12, Batch 300, Loss: 1.4893410444259643\n",
      "Epoch 12, Batch 600, Loss: 1.4915296864509582\n",
      "Epoch 12, Batch 900, Loss: 1.465345030874014\n",
      "Epoch 13, Batch 300, Loss: 1.4242344832420348\n",
      "Epoch 13, Batch 600, Loss: 1.4716673700511456\n",
      "Epoch 13, Batch 900, Loss: 1.4424173071980477\n",
      "Epoch 14, Batch 300, Loss: 1.426060070991516\n",
      "Epoch 14, Batch 600, Loss: 1.4124191027879716\n",
      "Epoch 14, Batch 900, Loss: 1.4094518667459488\n",
      "Epoch 15, Batch 300, Loss: 1.3935777148604394\n",
      "Epoch 15, Batch 600, Loss: 1.4029740011692047\n",
      "Epoch 15, Batch 900, Loss: 1.378799537718296\n",
      "Epoch 16, Batch 300, Loss: 1.3777645194530488\n",
      "Epoch 16, Batch 600, Loss: 1.364201471954584\n",
      "Epoch 16, Batch 900, Loss: 1.3728922829031944\n",
      "Epoch 17, Batch 300, Loss: 1.3630268582701683\n",
      "Epoch 17, Batch 600, Loss: 1.3403641283512115\n",
      "Epoch 17, Batch 900, Loss: 1.3448423343896865\n",
      "Epoch 18, Batch 300, Loss: 1.3564613929390907\n",
      "Epoch 18, Batch 600, Loss: 1.303153710514307\n",
      "Epoch 18, Batch 900, Loss: 1.3386505150794983\n",
      "Epoch 19, Batch 300, Loss: 1.3271706940233707\n",
      "Epoch 19, Batch 600, Loss: 1.3230816912651062\n",
      "Epoch 19, Batch 900, Loss: 1.317791404724121\n",
      "Epoch 20, Batch 300, Loss: 1.2766073028743268\n",
      "Epoch 20, Batch 600, Loss: 1.3035956047475339\n",
      "Epoch 20, Batch 900, Loss: 1.3285652604699134\n",
      "Epoch 21, Batch 300, Loss: 1.2915764421224594\n",
      "Epoch 21, Batch 600, Loss: 1.2911937069892883\n",
      "Epoch 21, Batch 900, Loss: 1.2946951980888843\n",
      "Epoch 22, Batch 300, Loss: 1.2685657234489918\n",
      "Epoch 22, Batch 600, Loss: 1.2815510277450084\n",
      "Epoch 22, Batch 900, Loss: 1.2977524234354496\n",
      "Epoch 23, Batch 300, Loss: 1.2798349490761758\n",
      "Epoch 23, Batch 600, Loss: 1.2512032674252986\n",
      "Epoch 23, Batch 900, Loss: 1.2812146538496016\n",
      "Epoch 24, Batch 300, Loss: 1.2588829009234905\n",
      "Epoch 24, Batch 600, Loss: 1.2548459747433662\n",
      "Epoch 24, Batch 900, Loss: 1.2707349807024002\n",
      "Epoch 25, Batch 300, Loss: 1.2385517370700836\n",
      "Epoch 25, Batch 600, Loss: 1.273011526912451\n",
      "Epoch 25, Batch 900, Loss: 1.2546389350295066\n",
      "Accuracy on test set: 0.8959%\n",
      "Fitting for combination 2\n",
      "784\n",
      "1\n",
      "10\n",
      "[20, 10]\n",
      "False\n",
      "['sigmoid']\n",
      "Adam\n",
      "0.3\n",
      "1\n",
      "CrossEntropyLoss\n",
      "100\n",
      "25\n",
      "0\n",
      "Epoch 1, Batch 100, Loss: 2.329932713508606\n",
      "Epoch 1, Batch 200, Loss: 2.332109408378601\n",
      "Epoch 1, Batch 300, Loss: 2.3280421352386473\n",
      "Epoch 1, Batch 400, Loss: 2.328209662437439\n",
      "Epoch 1, Batch 500, Loss: 2.456178560256958\n",
      "Epoch 1, Batch 600, Loss: 2.512015380859375\n",
      "Epoch 1, Batch 700, Loss: 2.513905737400055\n",
      "Epoch 1, Batch 800, Loss: 2.3265096664428713\n",
      "Epoch 1, Batch 900, Loss: 2.32688081741333\n",
      "Epoch 2, Batch 100, Loss: 2.4817767786979674\n",
      "Epoch 2, Batch 200, Loss: 2.3888032722473143\n",
      "Epoch 2, Batch 300, Loss: 2.417587423324585\n",
      "Epoch 2, Batch 400, Loss: 2.3372582530975343\n",
      "Epoch 2, Batch 500, Loss: 2.555905132293701\n",
      "Epoch 2, Batch 600, Loss: 2.3415778493881225\n",
      "Epoch 2, Batch 700, Loss: 2.380161499977112\n",
      "Epoch 2, Batch 800, Loss: 2.3374096393585204\n",
      "Epoch 2, Batch 900, Loss: 2.3403092765808107\n",
      "Epoch 3, Batch 100, Loss: 2.402763524055481\n",
      "Epoch 3, Batch 200, Loss: 2.4150286412239073\n",
      "Epoch 3, Batch 300, Loss: 2.387175736427307\n",
      "Epoch 3, Batch 400, Loss: 2.4515117692947386\n",
      "Epoch 3, Batch 500, Loss: 2.4293726682662964\n",
      "Epoch 3, Batch 600, Loss: 2.4191340970993043\n",
      "Epoch 3, Batch 700, Loss: 2.4452849864959716\n",
      "Epoch 3, Batch 800, Loss: 2.43439425945282\n",
      "Epoch 3, Batch 900, Loss: 2.369251892566681\n",
      "Epoch 4, Batch 100, Loss: 2.4267175912857057\n",
      "Epoch 4, Batch 200, Loss: 2.448164224624634\n",
      "Epoch 4, Batch 300, Loss: 2.4486123967170714\n",
      "Epoch 4, Batch 400, Loss: 2.4305528163909913\n",
      "Epoch 4, Batch 500, Loss: 2.499223222732544\n",
      "Epoch 4, Batch 600, Loss: 2.395946695804596\n",
      "Epoch 4, Batch 700, Loss: 2.4482044625282287\n",
      "Epoch 4, Batch 800, Loss: 2.4241950249671937\n",
      "Epoch 4, Batch 900, Loss: 2.427021949291229\n",
      "Epoch 5, Batch 100, Loss: 2.4303826665878296\n",
      "Epoch 5, Batch 200, Loss: 2.431295645236969\n",
      "Epoch 5, Batch 300, Loss: 2.4791175293922425\n",
      "Epoch 5, Batch 400, Loss: 2.4613654613494873\n",
      "Epoch 5, Batch 500, Loss: 2.384778227806091\n",
      "Epoch 5, Batch 600, Loss: 2.4810519766807557\n",
      "Epoch 5, Batch 700, Loss: 2.4531136965751648\n",
      "Epoch 5, Batch 800, Loss: 2.3961348056793215\n",
      "Epoch 5, Batch 900, Loss: 2.4334248876571656\n",
      "Epoch 6, Batch 100, Loss: 2.4379516720771788\n",
      "Epoch 6, Batch 200, Loss: 2.431595799922943\n",
      "Epoch 6, Batch 300, Loss: 2.3887543845176697\n",
      "Epoch 6, Batch 400, Loss: 2.4757231521606444\n",
      "Epoch 6, Batch 500, Loss: 2.415220401287079\n",
      "Epoch 6, Batch 600, Loss: 2.4692509317398073\n",
      "Epoch 6, Batch 700, Loss: 2.4175753831863402\n",
      "Epoch 6, Batch 800, Loss: 2.4341204500198366\n",
      "Epoch 6, Batch 900, Loss: 2.4821565675735475\n",
      "Epoch 7, Batch 100, Loss: 2.3764654183387757\n",
      "Epoch 7, Batch 200, Loss: 2.445713098049164\n",
      "Epoch 7, Batch 300, Loss: 2.4861245369911193\n",
      "Epoch 7, Batch 400, Loss: 2.4153987312316896\n",
      "Epoch 7, Batch 500, Loss: 2.470881929397583\n",
      "Epoch 7, Batch 600, Loss: 2.381141998767853\n",
      "Epoch 7, Batch 700, Loss: 2.4567668318748472\n",
      "Epoch 7, Batch 800, Loss: 2.443062562942505\n",
      "Epoch 7, Batch 900, Loss: 2.3911118292808533\n",
      "Epoch 8, Batch 100, Loss: 2.4539720797538758\n",
      "Epoch 8, Batch 200, Loss: 2.4527365517616273\n",
      "Epoch 8, Batch 300, Loss: 2.4547660112380982\n",
      "Epoch 8, Batch 400, Loss: 2.3800520277023316\n",
      "Epoch 8, Batch 500, Loss: 2.401825804710388\n",
      "Epoch 8, Batch 600, Loss: 2.432103497982025\n",
      "Epoch 8, Batch 700, Loss: 2.4839086747169494\n",
      "Epoch 8, Batch 800, Loss: 2.489684202671051\n",
      "Epoch 8, Batch 900, Loss: 2.4119213104248045\n",
      "Epoch 9, Batch 100, Loss: 2.427814955711365\n",
      "Epoch 9, Batch 200, Loss: 2.443906264305115\n",
      "Epoch 9, Batch 300, Loss: 2.468138635158539\n",
      "Epoch 9, Batch 400, Loss: 2.4352755522727967\n",
      "Epoch 9, Batch 500, Loss: 2.418979330062866\n",
      "Epoch 9, Batch 600, Loss: 2.4898541712760927\n",
      "Epoch 9, Batch 700, Loss: 2.418708026409149\n",
      "Epoch 9, Batch 800, Loss: 2.421841661930084\n",
      "Epoch 9, Batch 900, Loss: 2.4461486959457397\n",
      "Epoch 10, Batch 100, Loss: 2.43871821641922\n",
      "Epoch 10, Batch 200, Loss: 2.427962341308594\n",
      "Epoch 10, Batch 300, Loss: 2.4076829314231873\n",
      "Epoch 10, Batch 400, Loss: 2.407007324695587\n",
      "Epoch 10, Batch 500, Loss: 2.414770295619965\n",
      "Epoch 10, Batch 600, Loss: 2.438380763530731\n",
      "Epoch 10, Batch 700, Loss: 2.4657487511634826\n",
      "Epoch 10, Batch 800, Loss: 2.391150221824646\n",
      "Epoch 10, Batch 900, Loss: 2.4735829639434814\n",
      "Epoch 11, Batch 100, Loss: 2.401148056983948\n",
      "Epoch 11, Batch 200, Loss: 2.4601439356803896\n",
      "Epoch 11, Batch 300, Loss: 2.4377468061447143\n",
      "Epoch 11, Batch 400, Loss: 2.392231967449188\n",
      "Epoch 11, Batch 500, Loss: 2.4476345229148864\n",
      "Epoch 11, Batch 600, Loss: 2.401867854595184\n",
      "Epoch 11, Batch 700, Loss: 2.4298547315597534\n",
      "Epoch 11, Batch 800, Loss: 2.425677809715271\n",
      "Epoch 11, Batch 900, Loss: 2.444611554145813\n",
      "Epoch 12, Batch 100, Loss: 2.4049132680892944\n",
      "Epoch 12, Batch 200, Loss: 2.4195669865608216\n",
      "Epoch 12, Batch 300, Loss: 2.4455895113945005\n",
      "Epoch 12, Batch 400, Loss: 2.4025120687484742\n",
      "Epoch 12, Batch 500, Loss: 2.3990841460227967\n",
      "Epoch 12, Batch 600, Loss: 2.3931195878982545\n",
      "Epoch 12, Batch 700, Loss: 2.516897838115692\n",
      "Epoch 12, Batch 800, Loss: 2.442114460468292\n",
      "Epoch 12, Batch 900, Loss: 2.3851768279075625\n",
      "Epoch 13, Batch 100, Loss: 2.448720018863678\n",
      "Epoch 13, Batch 200, Loss: 2.4153830432891845\n",
      "Epoch 13, Batch 300, Loss: 2.437299256324768\n",
      "Epoch 13, Batch 400, Loss: 2.521311626434326\n",
      "Epoch 13, Batch 500, Loss: 2.376555528640747\n",
      "Epoch 13, Batch 600, Loss: 2.4256546211242678\n",
      "Epoch 13, Batch 700, Loss: 2.4255047488212584\n",
      "Epoch 13, Batch 800, Loss: 2.4220570945739746\n",
      "Epoch 13, Batch 900, Loss: 2.5078681468963624\n",
      "Epoch 14, Batch 100, Loss: 2.4183357620239256\n",
      "Epoch 14, Batch 200, Loss: 2.477969880104065\n",
      "Epoch 14, Batch 300, Loss: 2.4498446798324585\n",
      "Epoch 14, Batch 400, Loss: 2.436187584400177\n",
      "Epoch 14, Batch 500, Loss: 2.4148542499542236\n",
      "Epoch 14, Batch 600, Loss: 2.460361993312836\n",
      "Epoch 14, Batch 700, Loss: 2.412643885612488\n",
      "Epoch 14, Batch 800, Loss: 2.429411199092865\n",
      "Epoch 14, Batch 900, Loss: 2.4402613949775698\n",
      "Epoch 15, Batch 100, Loss: 2.4485580015182493\n",
      "Epoch 15, Batch 200, Loss: 2.4271850895881655\n",
      "Epoch 15, Batch 300, Loss: 2.434889509677887\n",
      "Epoch 15, Batch 400, Loss: 2.4126974511146546\n",
      "Epoch 15, Batch 500, Loss: 2.4268631672859193\n",
      "Epoch 15, Batch 600, Loss: 2.501344130039215\n",
      "Epoch 15, Batch 700, Loss: 2.370826449394226\n",
      "Epoch 15, Batch 800, Loss: 2.4251789259910583\n",
      "Epoch 15, Batch 900, Loss: 2.446198627948761\n",
      "Epoch 16, Batch 100, Loss: 2.4429658794403077\n",
      "Epoch 16, Batch 200, Loss: 2.4204254484176637\n",
      "Epoch 16, Batch 300, Loss: 2.4213666486740113\n",
      "Epoch 16, Batch 400, Loss: 2.3957327890396116\n",
      "Epoch 16, Batch 500, Loss: 2.449829297065735\n",
      "Epoch 16, Batch 600, Loss: 2.4169323301315306\n",
      "Epoch 16, Batch 700, Loss: 2.4437087774276733\n",
      "Epoch 16, Batch 800, Loss: 2.3931872987747194\n",
      "Epoch 16, Batch 900, Loss: 2.510489411354065\n",
      "Epoch 17, Batch 100, Loss: 2.4600708961486815\n",
      "Epoch 17, Batch 200, Loss: 2.4328484034538267\n",
      "Epoch 17, Batch 300, Loss: 2.4180502891540527\n",
      "Epoch 17, Batch 400, Loss: 2.397153789997101\n",
      "Epoch 17, Batch 500, Loss: 2.401360795497894\n",
      "Epoch 17, Batch 600, Loss: 2.4391103386878967\n",
      "Epoch 17, Batch 700, Loss: 2.461690118312836\n",
      "Epoch 17, Batch 800, Loss: 2.4127038383483885\n",
      "Epoch 17, Batch 900, Loss: 2.40025906085968\n",
      "Epoch 18, Batch 100, Loss: 2.44043573141098\n",
      "Epoch 18, Batch 200, Loss: 2.45362637758255\n",
      "Epoch 18, Batch 300, Loss: 2.4281962299346924\n",
      "Epoch 18, Batch 400, Loss: 2.416850788593292\n",
      "Epoch 18, Batch 500, Loss: 2.4013510155677795\n",
      "Epoch 18, Batch 600, Loss: 2.413939950466156\n",
      "Epoch 18, Batch 700, Loss: 2.436367540359497\n",
      "Epoch 18, Batch 800, Loss: 2.4388678812980653\n",
      "Epoch 18, Batch 900, Loss: 2.4390571236610414\n",
      "Epoch 19, Batch 100, Loss: 2.459472620487213\n",
      "Epoch 19, Batch 200, Loss: 2.4743116879463196\n",
      "Epoch 19, Batch 300, Loss: 2.388012273311615\n",
      "Epoch 19, Batch 400, Loss: 2.4099177598953245\n",
      "Epoch 19, Batch 500, Loss: 2.4543178987503054\n",
      "Epoch 19, Batch 600, Loss: 2.3899912595748902\n",
      "Epoch 19, Batch 700, Loss: 2.419728298187256\n",
      "Epoch 19, Batch 800, Loss: 2.4367161178588868\n",
      "Epoch 19, Batch 900, Loss: 2.4636058259010314\n",
      "Epoch 20, Batch 100, Loss: 2.418343267440796\n",
      "Epoch 20, Batch 200, Loss: 2.4738612008094787\n",
      "Epoch 20, Batch 300, Loss: 2.4022629022598267\n",
      "Epoch 20, Batch 400, Loss: 2.3943954849243165\n",
      "Epoch 20, Batch 500, Loss: 2.424013867378235\n",
      "Epoch 20, Batch 600, Loss: 2.395296370983124\n",
      "Epoch 20, Batch 700, Loss: 2.4628514742851255\n",
      "Epoch 20, Batch 800, Loss: 2.496212697029114\n",
      "Epoch 20, Batch 900, Loss: 2.4217262840270997\n",
      "Epoch 21, Batch 100, Loss: 2.4169699263572695\n",
      "Epoch 21, Batch 200, Loss: 2.443340468406677\n",
      "Epoch 21, Batch 300, Loss: 2.386058349609375\n",
      "Epoch 21, Batch 400, Loss: 2.4310383296012876\n",
      "Epoch 21, Batch 500, Loss: 2.4261304450035097\n",
      "Epoch 21, Batch 600, Loss: 2.4386832547187804\n",
      "Epoch 21, Batch 700, Loss: 2.468532159328461\n",
      "Epoch 21, Batch 800, Loss: 2.4519874715805052\n",
      "Epoch 21, Batch 900, Loss: 2.4309251213073733\n",
      "Epoch 22, Batch 100, Loss: 2.4147514295578003\n",
      "Epoch 22, Batch 200, Loss: 2.4116639041900636\n",
      "Epoch 22, Batch 300, Loss: 2.4136983585357665\n",
      "Epoch 22, Batch 400, Loss: 2.419306445121765\n",
      "Epoch 22, Batch 500, Loss: 2.3784436416625976\n",
      "Epoch 22, Batch 600, Loss: 2.461333518028259\n",
      "Epoch 22, Batch 700, Loss: 2.432703833580017\n",
      "Epoch 22, Batch 800, Loss: 2.3666734981536863\n",
      "Epoch 22, Batch 900, Loss: 2.4242497730255126\n",
      "Epoch 23, Batch 100, Loss: 2.427295229434967\n",
      "Epoch 23, Batch 200, Loss: 2.4615562295913698\n",
      "Epoch 23, Batch 300, Loss: 2.3983347606658936\n",
      "Epoch 23, Batch 400, Loss: 2.4752928113937376\n",
      "Epoch 23, Batch 500, Loss: 2.427038459777832\n",
      "Epoch 23, Batch 600, Loss: 2.431781270503998\n",
      "Epoch 23, Batch 700, Loss: 2.4710739064216614\n",
      "Epoch 23, Batch 800, Loss: 2.4227833461761477\n",
      "Epoch 23, Batch 900, Loss: 2.3829318833351136\n",
      "Epoch 24, Batch 100, Loss: 2.4296738719940185\n",
      "Epoch 24, Batch 200, Loss: 2.4533643555641174\n",
      "Epoch 24, Batch 300, Loss: 2.4536333203315737\n",
      "Epoch 24, Batch 400, Loss: 2.4217265892028808\n",
      "Epoch 24, Batch 500, Loss: 2.4476539993286135\n",
      "Epoch 24, Batch 600, Loss: 2.396212646961212\n",
      "Epoch 24, Batch 700, Loss: 2.4263780736923217\n",
      "Epoch 24, Batch 800, Loss: 2.4199027228355408\n",
      "Epoch 24, Batch 900, Loss: 2.432866582870483\n",
      "Epoch 25, Batch 100, Loss: 2.485622322559357\n",
      "Epoch 25, Batch 200, Loss: 2.4003424406051637\n",
      "Epoch 25, Batch 300, Loss: 2.420458426475525\n",
      "Epoch 25, Batch 400, Loss: 2.391696689128876\n",
      "Epoch 25, Batch 500, Loss: 2.4611955952644347\n",
      "Epoch 25, Batch 600, Loss: 2.4149333381652833\n",
      "Epoch 25, Batch 700, Loss: 2.471438677310944\n",
      "Epoch 25, Batch 800, Loss: 2.419618489742279\n",
      "Epoch 25, Batch 900, Loss: 2.5215795373916627\n",
      "Accuracy on test set: 0.0892%\n",
      "Fitting for combination 3\n",
      "784\n",
      "1\n",
      "10\n",
      "[20, 10]\n",
      "True\n",
      "['relu']\n",
      "SGD\n",
      "0.003\n",
      "0.3\n",
      "CrossEntropyLoss\n",
      "200\n",
      "25\n",
      "0\n",
      "Epoch 1, Batch 200, Loss: 4.3036682677268985\n",
      "Epoch 1, Batch 400, Loss: 3.818427323102951\n",
      "Epoch 1, Batch 600, Loss: 3.410661005973816\n",
      "Epoch 1, Batch 800, Loss: 3.0694762301445007\n",
      "Epoch 2, Batch 200, Loss: 2.6792605197429658\n",
      "Epoch 2, Batch 400, Loss: 2.5196960878372194\n",
      "Epoch 2, Batch 600, Loss: 2.4509904193878174\n",
      "Epoch 2, Batch 800, Loss: 2.3900767385959627\n",
      "Epoch 3, Batch 200, Loss: 2.275206835269928\n",
      "Epoch 3, Batch 400, Loss: 2.250466458797455\n",
      "Epoch 3, Batch 600, Loss: 2.233760813474655\n",
      "Epoch 3, Batch 800, Loss: 2.2158321380615233\n",
      "Epoch 4, Batch 200, Loss: 2.174118227958679\n",
      "Epoch 4, Batch 400, Loss: 2.1797624415159227\n",
      "Epoch 4, Batch 600, Loss: 2.1608225947618482\n",
      "Epoch 4, Batch 800, Loss: 2.146008874177933\n",
      "Epoch 5, Batch 200, Loss: 2.145501943230629\n",
      "Epoch 5, Batch 400, Loss: 2.142169070839882\n",
      "Epoch 5, Batch 600, Loss: 2.1334763181209566\n",
      "Epoch 5, Batch 800, Loss: 2.1235984659194944\n",
      "Epoch 6, Batch 200, Loss: 2.1299891114234923\n",
      "Epoch 6, Batch 400, Loss: 2.123319319486618\n",
      "Epoch 6, Batch 600, Loss: 2.1316501146554945\n",
      "Epoch 6, Batch 800, Loss: 2.1155686384439467\n",
      "Epoch 7, Batch 200, Loss: 2.121544930934906\n",
      "Epoch 7, Batch 400, Loss: 2.128032645583153\n",
      "Epoch 7, Batch 600, Loss: 2.123216267824173\n",
      "Epoch 7, Batch 800, Loss: 2.1170583564043044\n",
      "Epoch 8, Batch 200, Loss: 2.117931451201439\n",
      "Epoch 8, Batch 400, Loss: 2.110192909240723\n",
      "Epoch 8, Batch 600, Loss: 2.1203035748004915\n",
      "Epoch 8, Batch 800, Loss: 2.127999001741409\n",
      "Epoch 9, Batch 200, Loss: 2.1276381367444994\n",
      "Epoch 9, Batch 400, Loss: 2.1144398039579393\n",
      "Epoch 9, Batch 600, Loss: 2.110231957435608\n",
      "Epoch 9, Batch 800, Loss: 2.1117634838819503\n",
      "Epoch 10, Batch 200, Loss: 2.116880774497986\n",
      "Epoch 10, Batch 400, Loss: 2.1120717906951905\n",
      "Epoch 10, Batch 600, Loss: 2.1172476601600647\n",
      "Epoch 10, Batch 800, Loss: 2.1208855479955675\n",
      "Epoch 11, Batch 200, Loss: 2.118316295146942\n",
      "Epoch 11, Batch 400, Loss: 2.122410164475441\n",
      "Epoch 11, Batch 600, Loss: 2.115512174963951\n",
      "Epoch 11, Batch 800, Loss: 2.1108062839508057\n",
      "Epoch 12, Batch 200, Loss: 2.122690646648407\n",
      "Epoch 12, Batch 400, Loss: 2.1071144342422485\n",
      "Epoch 12, Batch 600, Loss: 2.1071921932697295\n",
      "Epoch 12, Batch 800, Loss: 2.119359781742096\n",
      "Epoch 13, Batch 200, Loss: 2.103990671038628\n",
      "Epoch 13, Batch 400, Loss: 2.117892087697983\n",
      "Epoch 13, Batch 600, Loss: 2.1140604782104493\n",
      "Epoch 13, Batch 800, Loss: 2.112779455780983\n",
      "Epoch 14, Batch 200, Loss: 2.113550965189934\n",
      "Epoch 14, Batch 400, Loss: 2.114759731888771\n",
      "Epoch 14, Batch 600, Loss: 2.113667365312576\n",
      "Epoch 14, Batch 800, Loss: 2.1074841463565828\n",
      "Epoch 15, Batch 200, Loss: 2.101162431836128\n",
      "Epoch 15, Batch 400, Loss: 2.1180776530504226\n",
      "Epoch 15, Batch 600, Loss: 2.1326777815818785\n",
      "Epoch 15, Batch 800, Loss: 2.1126280355453493\n",
      "Epoch 16, Batch 200, Loss: 2.1031632906198503\n",
      "Epoch 16, Batch 400, Loss: 2.1221342206001284\n",
      "Epoch 16, Batch 600, Loss: 2.1057704240083694\n",
      "Epoch 16, Batch 800, Loss: 2.1071844816207888\n",
      "Epoch 17, Batch 200, Loss: 2.105389348268509\n",
      "Epoch 17, Batch 400, Loss: 2.122497083544731\n",
      "Epoch 17, Batch 600, Loss: 2.1336084318161013\n",
      "Epoch 17, Batch 800, Loss: 2.0978462928533554\n",
      "Epoch 18, Batch 200, Loss: 2.1256510323286055\n",
      "Epoch 18, Batch 400, Loss: 2.098662281036377\n",
      "Epoch 18, Batch 600, Loss: 2.121872493624687\n",
      "Epoch 18, Batch 800, Loss: 2.1236489647626877\n",
      "Epoch 19, Batch 200, Loss: 2.100479831695557\n",
      "Epoch 19, Batch 400, Loss: 2.1097191089391707\n",
      "Epoch 19, Batch 600, Loss: 2.110464754104614\n",
      "Epoch 19, Batch 800, Loss: 2.1096789652109145\n",
      "Epoch 20, Batch 200, Loss: 2.1242258048057554\n",
      "Epoch 20, Batch 400, Loss: 2.1110963898897173\n",
      "Epoch 20, Batch 600, Loss: 2.117125317454338\n",
      "Epoch 20, Batch 800, Loss: 2.1017031252384184\n",
      "Epoch 21, Batch 200, Loss: 2.109026061296463\n",
      "Epoch 21, Batch 400, Loss: 2.096298414468765\n",
      "Epoch 21, Batch 600, Loss: 2.0997099590301516\n",
      "Epoch 21, Batch 800, Loss: 2.1276611429452896\n",
      "Epoch 22, Batch 200, Loss: 2.1107753670215605\n",
      "Epoch 22, Batch 400, Loss: 2.1153330862522126\n",
      "Epoch 22, Batch 600, Loss: 2.1159278780221937\n",
      "Epoch 22, Batch 800, Loss: 2.1112931609153747\n",
      "Epoch 23, Batch 200, Loss: 2.124780510663986\n",
      "Epoch 23, Batch 400, Loss: 2.11140343606472\n",
      "Epoch 23, Batch 600, Loss: 2.101786455512047\n",
      "Epoch 23, Batch 800, Loss: 2.108171820640564\n",
      "Epoch 24, Batch 200, Loss: 2.1125350970029833\n",
      "Epoch 24, Batch 400, Loss: 2.1086539828777315\n",
      "Epoch 24, Batch 600, Loss: 2.111341864466667\n",
      "Epoch 24, Batch 800, Loss: 2.1087938219308855\n",
      "Epoch 25, Batch 200, Loss: 2.1186705684661864\n",
      "Epoch 25, Batch 400, Loss: 2.10383646607399\n",
      "Epoch 25, Batch 600, Loss: 2.1238515108823774\n",
      "Epoch 25, Batch 800, Loss: 2.1094979596138\n",
      "Accuracy on test set: 0.8162%\n",
      "Fitting for combination 4\n",
      "784\n",
      "1\n",
      "10\n",
      "[30, 10]\n",
      "False\n",
      "['tanh']\n",
      "Adam\n",
      "0.001\n",
      "0.01\n",
      "CrossEntropyLoss\n",
      "100\n",
      "25\n",
      "0\n",
      "Epoch 1, Batch 100, Loss: 1.2289456444978715\n",
      "Epoch 1, Batch 200, Loss: 0.6360366985201835\n",
      "Epoch 1, Batch 300, Loss: 0.4943488672375679\n",
      "Epoch 1, Batch 400, Loss: 0.41598585456609727\n",
      "Epoch 1, Batch 500, Loss: 0.4078484085202217\n",
      "Epoch 1, Batch 600, Loss: 0.3854722660779953\n",
      "Epoch 1, Batch 700, Loss: 0.37091549426317216\n",
      "Epoch 1, Batch 800, Loss: 0.37044937297701835\n",
      "Epoch 1, Batch 900, Loss: 0.34700050979852676\n",
      "Epoch 2, Batch 100, Loss: 0.33319814056158065\n",
      "Epoch 2, Batch 200, Loss: 0.3461779883503914\n",
      "Epoch 2, Batch 300, Loss: 0.3340342001616955\n",
      "Epoch 2, Batch 400, Loss: 0.3369737745821476\n",
      "Epoch 2, Batch 500, Loss: 0.3210598900914192\n",
      "Epoch 2, Batch 600, Loss: 0.33334853053092955\n",
      "Epoch 2, Batch 700, Loss: 0.30976594537496566\n",
      "Epoch 2, Batch 800, Loss: 0.320931029766798\n",
      "Epoch 2, Batch 900, Loss: 0.3253383004665375\n",
      "Epoch 3, Batch 100, Loss: 0.31997011438012124\n",
      "Epoch 3, Batch 200, Loss: 0.30610265836119654\n",
      "Epoch 3, Batch 300, Loss: 0.3156576843559742\n",
      "Epoch 3, Batch 400, Loss: 0.3219861724972725\n",
      "Epoch 3, Batch 500, Loss: 0.3055259270220995\n",
      "Epoch 3, Batch 600, Loss: 0.29424751371145247\n",
      "Epoch 3, Batch 700, Loss: 0.30642599672079085\n",
      "Epoch 3, Batch 800, Loss: 0.31357542783021924\n",
      "Epoch 3, Batch 900, Loss: 0.31839738503098486\n",
      "Epoch 4, Batch 100, Loss: 0.3068646866083145\n",
      "Epoch 4, Batch 200, Loss: 0.3030232621729374\n",
      "Epoch 4, Batch 300, Loss: 0.3044662490487099\n",
      "Epoch 4, Batch 400, Loss: 0.3016882386803627\n",
      "Epoch 4, Batch 500, Loss: 0.31542747110128405\n",
      "Epoch 4, Batch 600, Loss: 0.30358231723308565\n",
      "Epoch 4, Batch 700, Loss: 0.2909160405397415\n",
      "Epoch 4, Batch 800, Loss: 0.2997304108738899\n",
      "Epoch 4, Batch 900, Loss: 0.3053629767894745\n",
      "Epoch 5, Batch 100, Loss: 0.29874710232019425\n",
      "Epoch 5, Batch 200, Loss: 0.3045708426833153\n",
      "Epoch 5, Batch 300, Loss: 0.31166955903172494\n",
      "Epoch 5, Batch 400, Loss: 0.28989161118865014\n",
      "Epoch 5, Batch 500, Loss: 0.2936976671218872\n",
      "Epoch 5, Batch 600, Loss: 0.28435183987021445\n",
      "Epoch 5, Batch 700, Loss: 0.3008055327832699\n",
      "Epoch 5, Batch 800, Loss: 0.29645657137036324\n",
      "Epoch 5, Batch 900, Loss: 0.30827871710062027\n",
      "Epoch 6, Batch 100, Loss: 0.2875653889775276\n",
      "Epoch 6, Batch 200, Loss: 0.2872230398654938\n",
      "Epoch 6, Batch 300, Loss: 0.2834594245254993\n",
      "Epoch 6, Batch 400, Loss: 0.30138968825340273\n",
      "Epoch 6, Batch 500, Loss: 0.3010082940757275\n",
      "Epoch 6, Batch 600, Loss: 0.30633478693664074\n",
      "Epoch 6, Batch 700, Loss: 0.2955333442986012\n",
      "Epoch 6, Batch 800, Loss: 0.2907102756202221\n",
      "Epoch 6, Batch 900, Loss: 0.30393232822418215\n",
      "Epoch 7, Batch 100, Loss: 0.30163067191839216\n",
      "Epoch 7, Batch 200, Loss: 0.28165278494358065\n",
      "Epoch 7, Batch 300, Loss: 0.29009678527712823\n",
      "Epoch 7, Batch 400, Loss: 0.29734730765223505\n",
      "Epoch 7, Batch 500, Loss: 0.2870867842435837\n",
      "Epoch 7, Batch 600, Loss: 0.28833819419145584\n",
      "Epoch 7, Batch 700, Loss: 0.30564303040504454\n",
      "Epoch 7, Batch 800, Loss: 0.27903746865689755\n",
      "Epoch 7, Batch 900, Loss: 0.2745251325517893\n",
      "Epoch 8, Batch 100, Loss: 0.2772835859656334\n",
      "Epoch 8, Batch 200, Loss: 0.292310199290514\n",
      "Epoch 8, Batch 300, Loss: 0.2886861053109169\n",
      "Epoch 8, Batch 400, Loss: 0.3031947423517704\n",
      "Epoch 8, Batch 500, Loss: 0.2811522381007671\n",
      "Epoch 8, Batch 600, Loss: 0.28873000107705593\n",
      "Epoch 8, Batch 700, Loss: 0.2889209498465061\n",
      "Epoch 8, Batch 800, Loss: 0.29264777585864066\n",
      "Epoch 8, Batch 900, Loss: 0.29324070900678634\n",
      "Epoch 9, Batch 100, Loss: 0.28818985521793367\n",
      "Epoch 9, Batch 200, Loss: 0.28840845994651315\n",
      "Epoch 9, Batch 300, Loss: 0.2711361525952816\n",
      "Epoch 9, Batch 400, Loss: 0.276209210306406\n",
      "Epoch 9, Batch 500, Loss: 0.31239762336015703\n",
      "Epoch 9, Batch 600, Loss: 0.2933055453002453\n",
      "Epoch 9, Batch 700, Loss: 0.29033979788422587\n",
      "Epoch 9, Batch 800, Loss: 0.29002059131860736\n",
      "Epoch 9, Batch 900, Loss: 0.29572272032499314\n",
      "Epoch 10, Batch 100, Loss: 0.28903338238596915\n",
      "Epoch 10, Batch 200, Loss: 0.2845686089992523\n",
      "Epoch 10, Batch 300, Loss: 0.2854641841351986\n",
      "Epoch 10, Batch 400, Loss: 0.28299722671508787\n",
      "Epoch 10, Batch 500, Loss: 0.29795098446309565\n",
      "Epoch 10, Batch 600, Loss: 0.2705094084143639\n",
      "Epoch 10, Batch 700, Loss: 0.28479669146239756\n",
      "Epoch 10, Batch 800, Loss: 0.29476407408714295\n",
      "Epoch 10, Batch 900, Loss: 0.30091070622205734\n",
      "Epoch 11, Batch 100, Loss: 0.2758399991691112\n",
      "Epoch 11, Batch 200, Loss: 0.2743093101680279\n",
      "Epoch 11, Batch 300, Loss: 0.29296003356575967\n",
      "Epoch 11, Batch 400, Loss: 0.2926678931713104\n",
      "Epoch 11, Batch 500, Loss: 0.27947552770376205\n",
      "Epoch 11, Batch 600, Loss: 0.2799442404508591\n",
      "Epoch 11, Batch 700, Loss: 0.29557340309023855\n",
      "Epoch 11, Batch 800, Loss: 0.31212271973490713\n",
      "Epoch 11, Batch 900, Loss: 0.2879768258333206\n",
      "Epoch 12, Batch 100, Loss: 0.2716765795648098\n",
      "Epoch 12, Batch 200, Loss: 0.28832615360617636\n",
      "Epoch 12, Batch 300, Loss: 0.2978370656073093\n",
      "Epoch 12, Batch 400, Loss: 0.30092184215784074\n",
      "Epoch 12, Batch 500, Loss: 0.29668765112757683\n",
      "Epoch 12, Batch 600, Loss: 0.2775281421840191\n",
      "Epoch 12, Batch 700, Loss: 0.27894103780388835\n",
      "Epoch 12, Batch 800, Loss: 0.29128222435712814\n",
      "Epoch 12, Batch 900, Loss: 0.28419248029589655\n",
      "Epoch 13, Batch 100, Loss: 0.2885163240134716\n",
      "Epoch 13, Batch 200, Loss: 0.28102737352252005\n",
      "Epoch 13, Batch 300, Loss: 0.27240278482437136\n",
      "Epoch 13, Batch 400, Loss: 0.2813594084978104\n",
      "Epoch 13, Batch 500, Loss: 0.2785428035259247\n",
      "Epoch 13, Batch 600, Loss: 0.2827830496430397\n",
      "Epoch 13, Batch 700, Loss: 0.27928194478154184\n",
      "Epoch 13, Batch 800, Loss: 0.30728417351841925\n",
      "Epoch 13, Batch 900, Loss: 0.29271913185715676\n",
      "Epoch 14, Batch 100, Loss: 0.2788675808906555\n",
      "Epoch 14, Batch 200, Loss: 0.27531512022018434\n",
      "Epoch 14, Batch 300, Loss: 0.28080657988786695\n",
      "Epoch 14, Batch 400, Loss: 0.2923240377008915\n",
      "Epoch 14, Batch 500, Loss: 0.2934699735045433\n",
      "Epoch 14, Batch 600, Loss: 0.28918807819485665\n",
      "Epoch 14, Batch 700, Loss: 0.283051033988595\n",
      "Epoch 14, Batch 800, Loss: 0.2866444780677557\n",
      "Epoch 14, Batch 900, Loss: 0.2904128073900938\n",
      "Epoch 15, Batch 100, Loss: 0.28326110407710076\n",
      "Epoch 15, Batch 200, Loss: 0.2897124308347702\n",
      "Epoch 15, Batch 300, Loss: 0.2711415033042431\n",
      "Epoch 15, Batch 400, Loss: 0.28379751682281495\n",
      "Epoch 15, Batch 500, Loss: 0.2911349581182003\n",
      "Epoch 15, Batch 600, Loss: 0.29019285008311274\n",
      "Epoch 15, Batch 700, Loss: 0.28855898350477216\n",
      "Epoch 15, Batch 800, Loss: 0.2903644222766161\n",
      "Epoch 15, Batch 900, Loss: 0.2823102532327175\n",
      "Epoch 16, Batch 100, Loss: 0.28464559964835645\n",
      "Epoch 16, Batch 200, Loss: 0.274308797121048\n",
      "Epoch 16, Batch 300, Loss: 0.2979069718718529\n",
      "Epoch 16, Batch 400, Loss: 0.28570315793156625\n",
      "Epoch 16, Batch 500, Loss: 0.29161341667175295\n",
      "Epoch 16, Batch 600, Loss: 0.28914163291454315\n",
      "Epoch 16, Batch 700, Loss: 0.29485458716750146\n",
      "Epoch 16, Batch 800, Loss: 0.26169171430170535\n",
      "Epoch 16, Batch 900, Loss: 0.27673093616962435\n",
      "Epoch 17, Batch 100, Loss: 0.28368175312876703\n",
      "Epoch 17, Batch 200, Loss: 0.28720524072647097\n",
      "Epoch 17, Batch 300, Loss: 0.2896052058041096\n",
      "Epoch 17, Batch 400, Loss: 0.2925854405760765\n",
      "Epoch 17, Batch 500, Loss: 0.2913894414901733\n",
      "Epoch 17, Batch 600, Loss: 0.2777030809223652\n",
      "Epoch 17, Batch 700, Loss: 0.2701811619102955\n",
      "Epoch 17, Batch 800, Loss: 0.2750576363503933\n",
      "Epoch 17, Batch 900, Loss: 0.28836055859923365\n",
      "Epoch 18, Batch 100, Loss: 0.2797889389097691\n",
      "Epoch 18, Batch 200, Loss: 0.27851678401231766\n",
      "Epoch 18, Batch 300, Loss: 0.2911104284226894\n",
      "Epoch 18, Batch 400, Loss: 0.27939379185438157\n",
      "Epoch 18, Batch 500, Loss: 0.2766190046072006\n",
      "Epoch 18, Batch 600, Loss: 0.28009938925504685\n",
      "Epoch 18, Batch 700, Loss: 0.2963379450142384\n",
      "Epoch 18, Batch 800, Loss: 0.2858500681817532\n",
      "Epoch 18, Batch 900, Loss: 0.28829902291297915\n",
      "Epoch 19, Batch 100, Loss: 0.26771163783967494\n",
      "Epoch 19, Batch 200, Loss: 0.2904005317389965\n",
      "Epoch 19, Batch 300, Loss: 0.2946978759765625\n",
      "Epoch 19, Batch 400, Loss: 0.273086875975132\n",
      "Epoch 19, Batch 500, Loss: 0.2958147862553597\n",
      "Epoch 19, Batch 600, Loss: 0.28703686460852623\n",
      "Epoch 19, Batch 700, Loss: 0.27730642437934877\n",
      "Epoch 19, Batch 800, Loss: 0.29893491804599764\n",
      "Epoch 19, Batch 900, Loss: 0.279370346814394\n",
      "Epoch 20, Batch 100, Loss: 0.27811074137687686\n",
      "Epoch 20, Batch 200, Loss: 0.2903708826005459\n",
      "Epoch 20, Batch 300, Loss: 0.2844404448568821\n",
      "Epoch 20, Batch 400, Loss: 0.27566047877073285\n",
      "Epoch 20, Batch 500, Loss: 0.26959249705076216\n",
      "Epoch 20, Batch 600, Loss: 0.29647123835980893\n",
      "Epoch 20, Batch 700, Loss: 0.2740135093778372\n",
      "Epoch 20, Batch 800, Loss: 0.2939593131840229\n",
      "Epoch 20, Batch 900, Loss: 0.28789935156702995\n",
      "Epoch 21, Batch 100, Loss: 0.27443129152059553\n",
      "Epoch 21, Batch 200, Loss: 0.2725473859906197\n",
      "Epoch 21, Batch 300, Loss: 0.2841190566122532\n",
      "Epoch 21, Batch 400, Loss: 0.287874449044466\n",
      "Epoch 21, Batch 500, Loss: 0.3004247711598873\n",
      "Epoch 21, Batch 600, Loss: 0.2867564955353737\n",
      "Epoch 21, Batch 700, Loss: 0.2928751465678215\n",
      "Epoch 21, Batch 800, Loss: 0.27020941659808156\n",
      "Epoch 21, Batch 900, Loss: 0.2785870762169361\n",
      "Epoch 22, Batch 100, Loss: 0.2934010127186775\n",
      "Epoch 22, Batch 200, Loss: 0.29467470541596413\n",
      "Epoch 22, Batch 300, Loss: 0.2871853716671467\n",
      "Epoch 22, Batch 400, Loss: 0.28973486602306364\n",
      "Epoch 22, Batch 500, Loss: 0.28090729005634785\n",
      "Epoch 22, Batch 600, Loss: 0.2801155136525631\n",
      "Epoch 22, Batch 700, Loss: 0.2803859725594521\n",
      "Epoch 22, Batch 800, Loss: 0.275329175144434\n",
      "Epoch 22, Batch 900, Loss: 0.28191394567489625\n",
      "Epoch 23, Batch 100, Loss: 0.2934016190469265\n",
      "Epoch 23, Batch 200, Loss: 0.2946328815817833\n",
      "Epoch 23, Batch 300, Loss: 0.27922036565840247\n",
      "Epoch 23, Batch 400, Loss: 0.2848031747341156\n",
      "Epoch 23, Batch 500, Loss: 0.27923425182700157\n",
      "Epoch 23, Batch 600, Loss: 0.2831145848333836\n",
      "Epoch 23, Batch 700, Loss: 0.2843815490603447\n",
      "Epoch 23, Batch 800, Loss: 0.28377812579274175\n",
      "Epoch 23, Batch 900, Loss: 0.2630653516948223\n",
      "Epoch 24, Batch 100, Loss: 0.2812930914759636\n",
      "Epoch 24, Batch 200, Loss: 0.27134987637400626\n",
      "Epoch 24, Batch 300, Loss: 0.2754194189608097\n",
      "Epoch 24, Batch 400, Loss: 0.27755785822868345\n",
      "Epoch 24, Batch 500, Loss: 0.28802620187401773\n",
      "Epoch 24, Batch 600, Loss: 0.28646747797727584\n",
      "Epoch 24, Batch 700, Loss: 0.28201906085014344\n",
      "Epoch 24, Batch 800, Loss: 0.2841851507127285\n",
      "Epoch 24, Batch 900, Loss: 0.29258458167314527\n",
      "Epoch 25, Batch 100, Loss: 0.2794346335530281\n",
      "Epoch 25, Batch 200, Loss: 0.2837022240459919\n",
      "Epoch 25, Batch 300, Loss: 0.27553822234272957\n",
      "Epoch 25, Batch 400, Loss: 0.27923318862915036\n",
      "Epoch 25, Batch 500, Loss: 0.2853767591714859\n",
      "Epoch 25, Batch 600, Loss: 0.2800541599094868\n",
      "Epoch 25, Batch 700, Loss: 0.28983696803450587\n",
      "Epoch 25, Batch 800, Loss: 0.3024048785865307\n",
      "Epoch 25, Batch 900, Loss: 0.2829710365831852\n",
      "Accuracy on test set: 0.9315%\n",
      "Fitting for combination 5\n",
      "784\n",
      "1\n",
      "10\n",
      "[30, 10]\n",
      "True\n",
      "['sigmoid']\n",
      "SGD\n",
      "0.01\n",
      "0.01\n",
      "CrossEntropyLoss\n",
      "300\n",
      "25\n",
      "0\n",
      "Epoch 1, Batch 300, Loss: 6.575785593986511\n",
      "Epoch 1, Batch 600, Loss: 5.79237077832222\n",
      "Epoch 1, Batch 900, Loss: 4.993632674217224\n",
      "Epoch 2, Batch 300, Loss: 4.247848165035248\n",
      "Epoch 2, Batch 600, Loss: 3.717213965654373\n",
      "Epoch 2, Batch 900, Loss: 3.3421768134832384\n",
      "Epoch 3, Batch 300, Loss: 2.9742104494571686\n",
      "Epoch 3, Batch 600, Loss: 2.7625972485542296\n",
      "Epoch 3, Batch 900, Loss: 2.5871762400865554\n",
      "Epoch 4, Batch 300, Loss: 2.4081653362512587\n",
      "Epoch 4, Batch 600, Loss: 2.2715441250801085\n",
      "Epoch 4, Batch 900, Loss: 2.1655030769109724\n",
      "Epoch 5, Batch 300, Loss: 2.049129037261009\n",
      "Epoch 5, Batch 600, Loss: 2.012364293038845\n",
      "Epoch 5, Batch 900, Loss: 1.9442722541093826\n",
      "Epoch 6, Batch 300, Loss: 1.8745356604456902\n",
      "Epoch 6, Batch 600, Loss: 1.8326365154981614\n",
      "Epoch 6, Batch 900, Loss: 1.7887365055084228\n",
      "Epoch 7, Batch 300, Loss: 1.7489707893133164\n",
      "Epoch 7, Batch 600, Loss: 1.7187723979353904\n",
      "Epoch 7, Batch 900, Loss: 1.7081039655208587\n",
      "Epoch 8, Batch 300, Loss: 1.6732776963710785\n",
      "Epoch 8, Batch 600, Loss: 1.6412805616855621\n",
      "Epoch 8, Batch 900, Loss: 1.63739172488451\n",
      "Epoch 9, Batch 300, Loss: 1.6011440593004227\n",
      "Epoch 9, Batch 600, Loss: 1.5905218070745468\n",
      "Epoch 9, Batch 900, Loss: 1.603001755774021\n",
      "Epoch 10, Batch 300, Loss: 1.557196359038353\n",
      "Epoch 10, Batch 600, Loss: 1.5747573274374007\n",
      "Epoch 10, Batch 900, Loss: 1.5500982144474984\n",
      "Epoch 11, Batch 300, Loss: 1.544470949470997\n",
      "Epoch 11, Batch 600, Loss: 1.5380045562982558\n",
      "Epoch 11, Batch 900, Loss: 1.5218487936258316\n",
      "Epoch 12, Batch 300, Loss: 1.5071533891558646\n",
      "Epoch 12, Batch 600, Loss: 1.4999541163444519\n",
      "Epoch 12, Batch 900, Loss: 1.5267116171121597\n",
      "Epoch 13, Batch 300, Loss: 1.4984533697366715\n",
      "Epoch 13, Batch 600, Loss: 1.498195654153824\n",
      "Epoch 13, Batch 900, Loss: 1.4781452259421348\n",
      "Epoch 14, Batch 300, Loss: 1.481219899058342\n",
      "Epoch 14, Batch 600, Loss: 1.4890967872738838\n",
      "Epoch 14, Batch 900, Loss: 1.4729154154658317\n",
      "Epoch 15, Batch 300, Loss: 1.4582894933223725\n",
      "Epoch 15, Batch 600, Loss: 1.488681511580944\n",
      "Epoch 15, Batch 900, Loss: 1.46016944617033\n",
      "Epoch 16, Batch 300, Loss: 1.4687017977237702\n",
      "Epoch 16, Batch 600, Loss: 1.4500137668848039\n",
      "Epoch 16, Batch 900, Loss: 1.4593571242690087\n",
      "Epoch 17, Batch 300, Loss: 1.4561636972427368\n",
      "Epoch 17, Batch 600, Loss: 1.442997736930847\n",
      "Epoch 17, Batch 900, Loss: 1.4557071521878242\n",
      "Epoch 18, Batch 300, Loss: 1.4454898551106452\n",
      "Epoch 18, Batch 600, Loss: 1.4460650515556335\n",
      "Epoch 18, Batch 900, Loss: 1.4524262329936029\n",
      "Epoch 19, Batch 300, Loss: 1.4293763372302055\n",
      "Epoch 19, Batch 600, Loss: 1.4594429057836533\n",
      "Epoch 19, Batch 900, Loss: 1.4422406217455863\n",
      "Epoch 20, Batch 300, Loss: 1.456041234433651\n",
      "Epoch 20, Batch 600, Loss: 1.4266070213913917\n",
      "Epoch 20, Batch 900, Loss: 1.4324885174632072\n",
      "Epoch 21, Batch 300, Loss: 1.4223750334978105\n",
      "Epoch 21, Batch 600, Loss: 1.4320669013261795\n",
      "Epoch 21, Batch 900, Loss: 1.4427126574516296\n",
      "Epoch 22, Batch 300, Loss: 1.4240880554914475\n",
      "Epoch 22, Batch 600, Loss: 1.4310264816880227\n",
      "Epoch 22, Batch 900, Loss: 1.4315871879458428\n",
      "Epoch 23, Batch 300, Loss: 1.440355577468872\n",
      "Epoch 23, Batch 600, Loss: 1.4264712491631508\n",
      "Epoch 23, Batch 900, Loss: 1.419167267680168\n",
      "Epoch 24, Batch 300, Loss: 1.4428577771782876\n",
      "Epoch 24, Batch 600, Loss: 1.412039341032505\n",
      "Epoch 24, Batch 900, Loss: 1.426962394118309\n",
      "Epoch 25, Batch 300, Loss: 1.4109791681170463\n",
      "Epoch 25, Batch 600, Loss: 1.4185304030776025\n",
      "Epoch 25, Batch 900, Loss: 1.4382279622554779\n",
      "Accuracy on test set: 0.9059%\n",
      "Fitting for combination 6\n",
      "784\n",
      "1\n",
      "10\n",
      "[40, 10]\n",
      "True\n",
      "['relu']\n",
      "Adam\n",
      "0.1\n",
      "0.1\n",
      "CrossEntropyLoss\n",
      "300\n",
      "25\n",
      "0\n",
      "Epoch 1, Batch 300, Loss: 13.412592260837554\n",
      "Epoch 1, Batch 600, Loss: 10.463089131116867\n",
      "Epoch 1, Batch 900, Loss: 11.691938338279725\n",
      "Epoch 2, Batch 300, Loss: 9.404695239067077\n",
      "Epoch 2, Batch 600, Loss: 9.537222694158555\n",
      "Epoch 2, Batch 900, Loss: 11.382262328863144\n",
      "Epoch 3, Batch 300, Loss: 9.362988935709\n",
      "Epoch 3, Batch 600, Loss: 9.327326972484588\n",
      "Epoch 3, Batch 900, Loss: 11.068853446245193\n",
      "Epoch 4, Batch 300, Loss: 9.795216364860535\n",
      "Epoch 4, Batch 600, Loss: 12.625402375459672\n",
      "Epoch 4, Batch 900, Loss: 9.795620653629303\n",
      "Epoch 5, Batch 300, Loss: 7.673096052408218\n",
      "Epoch 5, Batch 600, Loss: 9.82050266981125\n",
      "Epoch 5, Batch 900, Loss: 11.160790168046951\n",
      "Epoch 6, Batch 300, Loss: 9.599064708948136\n",
      "Epoch 6, Batch 600, Loss: 11.196357961893082\n",
      "Epoch 6, Batch 900, Loss: 11.69775853395462\n",
      "Epoch 7, Batch 300, Loss: 10.424038672447205\n",
      "Epoch 7, Batch 600, Loss: 10.995001207590104\n",
      "Epoch 7, Batch 900, Loss: 9.608818275928497\n",
      "Epoch 8, Batch 300, Loss: 11.078061499595641\n",
      "Epoch 8, Batch 600, Loss: 11.131023763418197\n",
      "Epoch 8, Batch 900, Loss: 12.935361108779908\n",
      "Epoch 9, Batch 300, Loss: 8.593689723014831\n",
      "Epoch 9, Batch 600, Loss: 15.089883083105088\n",
      "Epoch 9, Batch 900, Loss: 8.146691858768463\n",
      "Epoch 10, Batch 300, Loss: 9.766277058124542\n",
      "Epoch 10, Batch 600, Loss: 11.219402314424515\n",
      "Epoch 10, Batch 900, Loss: 9.177016265392304\n",
      "Epoch 11, Batch 300, Loss: 9.784366873502732\n",
      "Epoch 11, Batch 600, Loss: 12.782190828323364\n",
      "Epoch 11, Batch 900, Loss: 8.682479765415192\n",
      "Epoch 12, Batch 300, Loss: 10.812506700754165\n",
      "Epoch 12, Batch 600, Loss: 11.413457205295563\n",
      "Epoch 12, Batch 900, Loss: 17.72226106643677\n",
      "Epoch 13, Batch 300, Loss: 8.140248280763625\n",
      "Epoch 13, Batch 600, Loss: 11.052044384479522\n",
      "Epoch 13, Batch 900, Loss: 10.031253942251205\n",
      "Epoch 14, Batch 300, Loss: 8.209413871765136\n",
      "Epoch 14, Batch 600, Loss: 9.13478551506996\n",
      "Epoch 14, Batch 900, Loss: 9.510304126739502\n",
      "Epoch 15, Batch 300, Loss: 10.874026999473571\n",
      "Epoch 15, Batch 600, Loss: 9.329826834201812\n",
      "Epoch 15, Batch 900, Loss: 16.39528089404106\n",
      "Epoch 16, Batch 300, Loss: 9.669911992549896\n",
      "Epoch 16, Batch 600, Loss: 8.833892952203751\n",
      "Epoch 16, Batch 900, Loss: 10.259315530061722\n",
      "Epoch 17, Batch 300, Loss: 9.344407088756562\n",
      "Epoch 17, Batch 600, Loss: 11.276756230592728\n",
      "Epoch 17, Batch 900, Loss: 10.91766729593277\n",
      "Epoch 18, Batch 300, Loss: 12.400454082489013\n",
      "Epoch 18, Batch 600, Loss: 13.754477331638336\n",
      "Epoch 18, Batch 900, Loss: 13.302867197990418\n",
      "Epoch 19, Batch 300, Loss: 7.8841939187049865\n",
      "Epoch 19, Batch 600, Loss: 8.964725497961044\n",
      "Epoch 19, Batch 900, Loss: 12.00410198688507\n",
      "Epoch 20, Batch 300, Loss: 16.651437150239943\n",
      "Epoch 20, Batch 600, Loss: 9.630514630079269\n",
      "Epoch 20, Batch 900, Loss: 9.14817393541336\n",
      "Epoch 21, Batch 300, Loss: 10.402854868173598\n",
      "Epoch 21, Batch 600, Loss: 7.606688836812973\n",
      "Epoch 21, Batch 900, Loss: 11.285031596422195\n",
      "Epoch 22, Batch 300, Loss: 12.043685286045074\n",
      "Epoch 22, Batch 600, Loss: 8.250481417179108\n",
      "Epoch 22, Batch 900, Loss: 11.037883322238923\n",
      "Epoch 23, Batch 300, Loss: 11.506142084598542\n",
      "Epoch 23, Batch 600, Loss: 8.198137203454971\n",
      "Epoch 23, Batch 900, Loss: 10.718727427721024\n",
      "Epoch 24, Batch 300, Loss: 10.141281718015671\n",
      "Epoch 24, Batch 600, Loss: 10.235536420345307\n",
      "Epoch 24, Batch 900, Loss: 9.334696378707886\n",
      "Epoch 25, Batch 300, Loss: 8.799881108999251\n",
      "Epoch 25, Batch 600, Loss: 11.27397928237915\n",
      "Epoch 25, Batch 900, Loss: 12.106190220117568\n",
      "Accuracy on test set: 0.2036%\n",
      "Fitting for combination 7\n",
      "784\n",
      "1\n",
      "10\n",
      "[40, 10]\n",
      "False\n",
      "['relu']\n",
      "SGD\n",
      "0.3\n",
      "0.03\n",
      "CrossEntropyLoss\n",
      "100\n",
      "25\n",
      "0\n",
      "Epoch 1, Batch 100, Loss: 2.2750924110412596\n",
      "Epoch 1, Batch 200, Loss: 2.1847180676460267\n",
      "Epoch 1, Batch 300, Loss: 2.1334956526756286\n",
      "Epoch 1, Batch 400, Loss: 2.3031631755828856\n",
      "Epoch 1, Batch 500, Loss: 2.301608533859253\n",
      "Epoch 1, Batch 600, Loss: 2.30137149810791\n",
      "Epoch 1, Batch 700, Loss: 2.2995172548294067\n",
      "Epoch 1, Batch 800, Loss: 2.301585032939911\n",
      "Epoch 1, Batch 900, Loss: 2.302267544269562\n",
      "Epoch 2, Batch 100, Loss: 2.30242240190506\n",
      "Epoch 2, Batch 200, Loss: 2.2973124623298644\n",
      "Epoch 2, Batch 300, Loss: 2.2235874736309054\n",
      "Epoch 2, Batch 400, Loss: 2.268167951107025\n",
      "Epoch 2, Batch 500, Loss: 2.2819191467761994\n",
      "Epoch 2, Batch 600, Loss: 2.301466028690338\n",
      "Epoch 2, Batch 700, Loss: 2.301484451293945\n",
      "Epoch 2, Batch 800, Loss: 2.3024579000473024\n",
      "Epoch 2, Batch 900, Loss: 2.3018633198738097\n",
      "Epoch 3, Batch 100, Loss: 2.302874352931976\n",
      "Epoch 3, Batch 200, Loss: 2.3023346138000487\n",
      "Epoch 3, Batch 300, Loss: 2.3014061641693115\n",
      "Epoch 3, Batch 400, Loss: 2.3025536704063416\n",
      "Epoch 3, Batch 500, Loss: 2.3025350093841555\n",
      "Epoch 3, Batch 600, Loss: 2.302287497520447\n",
      "Epoch 3, Batch 700, Loss: 2.302425830364227\n",
      "Epoch 3, Batch 800, Loss: 2.301788692474365\n",
      "Epoch 3, Batch 900, Loss: 2.3028035879135134\n",
      "Epoch 4, Batch 100, Loss: 2.305712342262268\n",
      "Epoch 4, Batch 200, Loss: 2.301215045452118\n",
      "Epoch 4, Batch 300, Loss: 2.3020377278327944\n",
      "Epoch 4, Batch 400, Loss: 2.3023581981658934\n",
      "Epoch 4, Batch 500, Loss: 2.30303537607193\n",
      "Epoch 4, Batch 600, Loss: 2.302629814147949\n",
      "Epoch 4, Batch 700, Loss: 2.301652801036835\n",
      "Epoch 4, Batch 800, Loss: 2.3014406323432923\n",
      "Epoch 4, Batch 900, Loss: 2.3019529509544374\n",
      "Epoch 5, Batch 100, Loss: 2.302953975200653\n",
      "Epoch 5, Batch 200, Loss: 2.30175039768219\n",
      "Epoch 5, Batch 300, Loss: 2.302353403568268\n",
      "Epoch 5, Batch 400, Loss: 2.3020925092697144\n",
      "Epoch 5, Batch 500, Loss: 2.302776663303375\n",
      "Epoch 5, Batch 600, Loss: 2.302395672798157\n",
      "Epoch 5, Batch 700, Loss: 2.301284337043762\n",
      "Epoch 5, Batch 800, Loss: 2.3022480511665346\n",
      "Epoch 5, Batch 900, Loss: 2.3015497803688048\n",
      "Epoch 6, Batch 100, Loss: 2.300958731174469\n",
      "Epoch 6, Batch 200, Loss: 2.3021690678596496\n",
      "Epoch 6, Batch 300, Loss: 2.30170375585556\n",
      "Epoch 6, Batch 400, Loss: 2.3026756763458254\n",
      "Epoch 6, Batch 500, Loss: 2.301776113510132\n",
      "Epoch 6, Batch 600, Loss: 2.302718176841736\n",
      "Epoch 6, Batch 700, Loss: 2.302304129600525\n",
      "Epoch 6, Batch 800, Loss: 2.302464289665222\n",
      "Epoch 6, Batch 900, Loss: 2.3019306206703187\n",
      "Epoch 7, Batch 100, Loss: 2.2987952399253846\n",
      "Epoch 7, Batch 200, Loss: 2.3016536140441897\n",
      "Epoch 7, Batch 300, Loss: 2.3029406809806825\n",
      "Epoch 7, Batch 400, Loss: 2.3024531388282776\n",
      "Epoch 7, Batch 500, Loss: 2.301963975429535\n",
      "Epoch 7, Batch 600, Loss: 2.3011702060699464\n",
      "Epoch 7, Batch 700, Loss: 2.303254573345184\n",
      "Epoch 7, Batch 800, Loss: 2.301698269844055\n",
      "Epoch 7, Batch 900, Loss: 2.3022746801376344\n",
      "Epoch 8, Batch 100, Loss: 2.3008281660079954\n",
      "Epoch 8, Batch 200, Loss: 2.302253143787384\n",
      "Epoch 8, Batch 300, Loss: 2.3013300633430482\n",
      "Epoch 8, Batch 400, Loss: 2.30306533575058\n",
      "Epoch 8, Batch 500, Loss: 2.3022325587272645\n",
      "Epoch 8, Batch 600, Loss: 2.3022272276878355\n",
      "Epoch 8, Batch 700, Loss: 2.3008597540855407\n",
      "Epoch 8, Batch 800, Loss: 2.302692165374756\n",
      "Epoch 8, Batch 900, Loss: 2.302225797176361\n",
      "Epoch 9, Batch 100, Loss: 2.302662687301636\n",
      "Epoch 9, Batch 200, Loss: 2.3010316944122313\n",
      "Epoch 9, Batch 300, Loss: 2.3024207735061646\n",
      "Epoch 9, Batch 400, Loss: 2.3020028400421144\n",
      "Epoch 9, Batch 500, Loss: 2.3011693811416625\n",
      "Epoch 9, Batch 600, Loss: 2.3025681233406066\n",
      "Epoch 9, Batch 700, Loss: 2.302447416782379\n",
      "Epoch 9, Batch 800, Loss: 2.3018683576583863\n",
      "Epoch 9, Batch 900, Loss: 2.303238880634308\n",
      "Epoch 10, Batch 100, Loss: 2.3027141523361205\n",
      "Epoch 10, Batch 200, Loss: 2.302516584396362\n",
      "Epoch 10, Batch 300, Loss: 2.3019792652130127\n",
      "Epoch 10, Batch 400, Loss: 2.3032315587997436\n",
      "Epoch 10, Batch 500, Loss: 2.3017242836952208\n",
      "Epoch 10, Batch 600, Loss: 2.3028513860702513\n",
      "Epoch 10, Batch 700, Loss: 2.3022285962104796\n",
      "Epoch 10, Batch 800, Loss: 2.3011656761169434\n",
      "Epoch 10, Batch 900, Loss: 2.3017672729492187\n",
      "Epoch 11, Batch 100, Loss: 2.3023961901664736\n",
      "Epoch 11, Batch 200, Loss: 2.301678035259247\n",
      "Epoch 11, Batch 300, Loss: 2.302217001914978\n",
      "Epoch 11, Batch 400, Loss: 2.303127329349518\n",
      "Epoch 11, Batch 500, Loss: 2.300998179912567\n",
      "Epoch 11, Batch 600, Loss: 2.302170693874359\n",
      "Epoch 11, Batch 700, Loss: 2.3025713896751405\n",
      "Epoch 11, Batch 800, Loss: 2.302830443382263\n",
      "Epoch 11, Batch 900, Loss: 2.3011675572395323\n",
      "Epoch 12, Batch 100, Loss: 2.302576162815094\n",
      "Epoch 12, Batch 200, Loss: 2.303265166282654\n",
      "Epoch 12, Batch 300, Loss: 2.301974368095398\n",
      "Epoch 12, Batch 400, Loss: 2.302983326911926\n",
      "Epoch 12, Batch 500, Loss: 2.302574894428253\n",
      "Epoch 12, Batch 600, Loss: 2.30200306892395\n",
      "Epoch 12, Batch 700, Loss: 2.2917149233818055\n",
      "Epoch 12, Batch 800, Loss: 2.3008105731010438\n",
      "Epoch 12, Batch 900, Loss: 2.3026326799392702\n",
      "Epoch 13, Batch 100, Loss: 2.285174641609192\n",
      "Epoch 13, Batch 200, Loss: 2.1465398597717287\n",
      "Epoch 13, Batch 300, Loss: 2.217880755662918\n",
      "Epoch 13, Batch 400, Loss: 2.303137619495392\n",
      "Epoch 13, Batch 500, Loss: 2.2833458042144774\n",
      "Epoch 13, Batch 600, Loss: 2.303096601963043\n",
      "Epoch 13, Batch 700, Loss: 2.3021408772468566\n",
      "Epoch 13, Batch 800, Loss: 2.302195405960083\n",
      "Epoch 13, Batch 900, Loss: 2.3020290970802306\n",
      "Epoch 14, Batch 100, Loss: 2.3010118770599366\n",
      "Epoch 14, Batch 200, Loss: 2.302355496883392\n",
      "Epoch 14, Batch 300, Loss: 2.3018960332870484\n",
      "Epoch 14, Batch 400, Loss: 2.30254695892334\n",
      "Epoch 14, Batch 500, Loss: 2.2967796540260315\n",
      "Epoch 14, Batch 600, Loss: 2.299480345249176\n",
      "Epoch 14, Batch 700, Loss: 2.3025962233543398\n",
      "Epoch 14, Batch 800, Loss: 2.3020034766197206\n",
      "Epoch 14, Batch 900, Loss: 2.3022543597221374\n",
      "Epoch 15, Batch 100, Loss: 2.3022286343574523\n",
      "Epoch 15, Batch 200, Loss: 2.303053286075592\n",
      "Epoch 15, Batch 300, Loss: 2.3028513383865357\n",
      "Epoch 15, Batch 400, Loss: 2.300420734882355\n",
      "Epoch 15, Batch 500, Loss: 2.300566954612732\n",
      "Epoch 15, Batch 600, Loss: 2.302888934612274\n",
      "Epoch 15, Batch 700, Loss: 2.3012223863601684\n",
      "Epoch 15, Batch 800, Loss: 2.3016035747528076\n",
      "Epoch 15, Batch 900, Loss: 2.3018831491470335\n",
      "Epoch 16, Batch 100, Loss: 2.301956543922424\n",
      "Epoch 16, Batch 200, Loss: 2.301748442649841\n",
      "Epoch 16, Batch 300, Loss: 2.302626647949219\n",
      "Epoch 16, Batch 400, Loss: 2.302027759552002\n",
      "Epoch 16, Batch 500, Loss: 2.3024268794059752\n",
      "Epoch 16, Batch 600, Loss: 2.3027354884147644\n",
      "Epoch 16, Batch 700, Loss: 2.3014557456970213\n",
      "Epoch 16, Batch 800, Loss: 2.301196131706238\n",
      "Epoch 16, Batch 900, Loss: 2.302673637866974\n",
      "Epoch 17, Batch 100, Loss: 2.3028993916511538\n",
      "Epoch 17, Batch 200, Loss: 2.3024177408218383\n",
      "Epoch 17, Batch 300, Loss: 2.302281589508057\n",
      "Epoch 17, Batch 400, Loss: 2.3021472096443176\n",
      "Epoch 17, Batch 500, Loss: 2.302370715141296\n",
      "Epoch 17, Batch 600, Loss: 2.3022203969955446\n",
      "Epoch 17, Batch 700, Loss: 2.3029276657104494\n",
      "Epoch 17, Batch 800, Loss: 2.302373309135437\n",
      "Epoch 17, Batch 900, Loss: 2.301639769077301\n",
      "Epoch 18, Batch 100, Loss: 2.3019878268241882\n",
      "Epoch 18, Batch 200, Loss: 2.3020938777923585\n",
      "Epoch 18, Batch 300, Loss: 2.3021640276908872\n",
      "Epoch 18, Batch 400, Loss: 2.302538216114044\n",
      "Epoch 18, Batch 500, Loss: 2.302029223442078\n",
      "Epoch 18, Batch 600, Loss: 2.3011234331130983\n",
      "Epoch 18, Batch 700, Loss: 2.301985363960266\n",
      "Epoch 18, Batch 800, Loss: 2.302083497047424\n",
      "Epoch 18, Batch 900, Loss: 2.301550350189209\n",
      "Epoch 19, Batch 100, Loss: 2.3033639192581177\n",
      "Epoch 19, Batch 200, Loss: 2.301420395374298\n",
      "Epoch 19, Batch 300, Loss: 2.3013476610183714\n",
      "Epoch 19, Batch 400, Loss: 2.300766706466675\n",
      "Epoch 19, Batch 500, Loss: 2.302225182056427\n",
      "Epoch 19, Batch 600, Loss: 2.301823928356171\n",
      "Epoch 19, Batch 700, Loss: 2.3022874569892884\n",
      "Epoch 19, Batch 800, Loss: 2.302078995704651\n",
      "Epoch 19, Batch 900, Loss: 2.3034335923194886\n",
      "Epoch 20, Batch 100, Loss: 2.301092753410339\n",
      "Epoch 20, Batch 200, Loss: 2.3023105573654177\n",
      "Epoch 20, Batch 300, Loss: 2.3024104833602905\n",
      "Epoch 20, Batch 400, Loss: 2.302173159122467\n",
      "Epoch 20, Batch 500, Loss: 2.301810998916626\n",
      "Epoch 20, Batch 600, Loss: 2.302218267917633\n",
      "Epoch 20, Batch 700, Loss: 2.3034608340263367\n",
      "Epoch 20, Batch 800, Loss: 2.302532138824463\n",
      "Epoch 20, Batch 900, Loss: 2.3017317175865175\n",
      "Epoch 21, Batch 100, Loss: 2.302323942184448\n",
      "Epoch 21, Batch 200, Loss: 2.3010526752471923\n",
      "Epoch 21, Batch 300, Loss: 2.3021734142303467\n",
      "Epoch 21, Batch 400, Loss: 2.301595721244812\n",
      "Epoch 21, Batch 500, Loss: 2.302830181121826\n",
      "Epoch 21, Batch 600, Loss: 2.3024607515335083\n",
      "Epoch 21, Batch 700, Loss: 2.301069989204407\n",
      "Epoch 21, Batch 800, Loss: 2.3030362486839295\n",
      "Epoch 21, Batch 900, Loss: 2.302785408496857\n",
      "Epoch 22, Batch 100, Loss: 2.3019821858406067\n",
      "Epoch 22, Batch 200, Loss: 2.301829285621643\n",
      "Epoch 22, Batch 300, Loss: 2.3028321146965025\n",
      "Epoch 22, Batch 400, Loss: 2.3020203924179077\n",
      "Epoch 22, Batch 500, Loss: 2.302703573703766\n",
      "Epoch 22, Batch 600, Loss: 2.302589330673218\n",
      "Epoch 22, Batch 700, Loss: 2.3028491711616517\n",
      "Epoch 22, Batch 800, Loss: 2.3016425728797913\n",
      "Epoch 22, Batch 900, Loss: 2.301083035469055\n",
      "Epoch 23, Batch 100, Loss: 2.3024181008338926\n",
      "Epoch 23, Batch 200, Loss: 2.302932250499725\n",
      "Epoch 23, Batch 300, Loss: 2.3006334614753725\n",
      "Epoch 23, Batch 400, Loss: 2.301789872646332\n",
      "Epoch 23, Batch 500, Loss: 2.302882080078125\n",
      "Epoch 23, Batch 600, Loss: 2.3018834233283996\n",
      "Epoch 23, Batch 700, Loss: 2.301536173820496\n",
      "Epoch 23, Batch 800, Loss: 2.3018412446975707\n",
      "Epoch 23, Batch 900, Loss: 2.3035057258605955\n",
      "Epoch 24, Batch 100, Loss: 2.301807713508606\n",
      "Epoch 24, Batch 200, Loss: 2.300307216644287\n",
      "Epoch 24, Batch 300, Loss: 2.3016087579727174\n",
      "Epoch 24, Batch 400, Loss: 2.298460450172424\n",
      "Epoch 24, Batch 500, Loss: 2.3014851784706116\n",
      "Epoch 24, Batch 600, Loss: 2.301410777568817\n",
      "Epoch 24, Batch 700, Loss: 2.3032311987876892\n",
      "Epoch 24, Batch 800, Loss: 2.3024799942970278\n",
      "Epoch 24, Batch 900, Loss: 2.303399405479431\n",
      "Epoch 25, Batch 100, Loss: 2.302451195716858\n",
      "Epoch 25, Batch 200, Loss: 2.3008970403671265\n",
      "Epoch 25, Batch 300, Loss: 2.302055661678314\n",
      "Epoch 25, Batch 400, Loss: 2.301797456741333\n",
      "Epoch 25, Batch 500, Loss: 2.3027661275863647\n",
      "Epoch 25, Batch 600, Loss: 2.302136814594269\n",
      "Epoch 25, Batch 700, Loss: 2.302692165374756\n",
      "Epoch 25, Batch 800, Loss: 2.301722424030304\n",
      "Epoch 25, Batch 900, Loss: 2.3010738253593446\n",
      "Accuracy on test set: 0.1135%\n",
      "Fitting for combination 8\n",
      "784\n",
      "1\n",
      "10\n",
      "[50, 10]\n",
      "False\n",
      "['tanh']\n",
      "Adam\n",
      "0.01\n",
      "0.3\n",
      "CrossEntropyLoss\n",
      "100\n",
      "25\n",
      "0\n",
      "Epoch 1, Batch 100, Loss: 1.7426222145557404\n",
      "Epoch 1, Batch 200, Loss: 1.655221526622772\n",
      "Epoch 1, Batch 300, Loss: 1.601983473300934\n",
      "Epoch 1, Batch 400, Loss: 1.6167764961719513\n",
      "Epoch 1, Batch 500, Loss: 1.6414983665943146\n",
      "Epoch 1, Batch 600, Loss: 1.5959131145477294\n",
      "Epoch 1, Batch 700, Loss: 1.654275164604187\n",
      "Epoch 1, Batch 800, Loss: 1.655551780462265\n",
      "Epoch 1, Batch 900, Loss: 1.6056265866756438\n",
      "Epoch 2, Batch 100, Loss: 1.6474172163009644\n",
      "Epoch 2, Batch 200, Loss: 1.6186477410793305\n",
      "Epoch 2, Batch 300, Loss: 1.648177411556244\n",
      "Epoch 2, Batch 400, Loss: 1.6313390672206878\n",
      "Epoch 2, Batch 500, Loss: 1.6137111818790435\n",
      "Epoch 2, Batch 600, Loss: 1.5862270033359527\n",
      "Epoch 2, Batch 700, Loss: 1.6515606880187987\n",
      "Epoch 2, Batch 800, Loss: 1.5996995067596436\n",
      "Epoch 2, Batch 900, Loss: 1.6061362946033477\n",
      "Epoch 3, Batch 100, Loss: 1.6151418924331664\n",
      "Epoch 3, Batch 200, Loss: 1.6054616034030915\n",
      "Epoch 3, Batch 300, Loss: 1.6490075886249542\n",
      "Epoch 3, Batch 400, Loss: 1.6220190501213074\n",
      "Epoch 3, Batch 500, Loss: 1.605273084640503\n",
      "Epoch 3, Batch 600, Loss: 1.5722656178474426\n",
      "Epoch 3, Batch 700, Loss: 1.653433347940445\n",
      "Epoch 3, Batch 800, Loss: 1.6106751430034638\n",
      "Epoch 3, Batch 900, Loss: 1.6191059958934784\n",
      "Epoch 4, Batch 100, Loss: 1.611508368253708\n",
      "Epoch 4, Batch 200, Loss: 1.602930715084076\n",
      "Epoch 4, Batch 300, Loss: 1.6310385870933533\n",
      "Epoch 4, Batch 400, Loss: 1.6640782296657561\n",
      "Epoch 4, Batch 500, Loss: 1.6785113680362702\n",
      "Epoch 4, Batch 600, Loss: 1.6298598456382751\n",
      "Epoch 4, Batch 700, Loss: 1.6362262499332427\n",
      "Epoch 4, Batch 800, Loss: 1.6831533801555634\n",
      "Epoch 4, Batch 900, Loss: 1.5786575603485107\n",
      "Epoch 5, Batch 100, Loss: 1.6416799008846283\n",
      "Epoch 5, Batch 200, Loss: 1.600185227394104\n",
      "Epoch 5, Batch 300, Loss: 1.6327348279953002\n",
      "Epoch 5, Batch 400, Loss: 1.6104236125946045\n",
      "Epoch 5, Batch 500, Loss: 1.60340665102005\n",
      "Epoch 5, Batch 600, Loss: 1.6258607590198517\n",
      "Epoch 5, Batch 700, Loss: 1.570896178483963\n",
      "Epoch 5, Batch 800, Loss: 1.6024879908561707\n",
      "Epoch 5, Batch 900, Loss: 1.6381452536582948\n",
      "Epoch 6, Batch 100, Loss: 1.6140903067588805\n",
      "Epoch 6, Batch 200, Loss: 1.5900838458538056\n",
      "Epoch 6, Batch 300, Loss: 1.6563770174980164\n",
      "Epoch 6, Batch 400, Loss: 1.6610585749149323\n",
      "Epoch 6, Batch 500, Loss: 1.6140999281406403\n",
      "Epoch 6, Batch 600, Loss: 1.6326042199134827\n",
      "Epoch 6, Batch 700, Loss: 1.6453863894939422\n",
      "Epoch 6, Batch 800, Loss: 1.654551831483841\n",
      "Epoch 6, Batch 900, Loss: 1.6543966817855835\n",
      "Epoch 7, Batch 100, Loss: 1.6447574889659882\n",
      "Epoch 7, Batch 200, Loss: 1.6138320612907409\n",
      "Epoch 7, Batch 300, Loss: 1.6130812048912049\n",
      "Epoch 7, Batch 400, Loss: 1.6325455844402312\n",
      "Epoch 7, Batch 500, Loss: 1.593633815050125\n",
      "Epoch 7, Batch 600, Loss: 1.6666184592247009\n",
      "Epoch 7, Batch 700, Loss: 1.6449463176727295\n",
      "Epoch 7, Batch 800, Loss: 1.6748093926906586\n",
      "Epoch 7, Batch 900, Loss: 1.6048345363140106\n",
      "Epoch 8, Batch 100, Loss: 1.6603243720531464\n",
      "Epoch 8, Batch 200, Loss: 1.6510363471508027\n",
      "Epoch 8, Batch 300, Loss: 1.6301531541347503\n",
      "Epoch 8, Batch 400, Loss: 1.6475095307826997\n",
      "Epoch 8, Batch 500, Loss: 1.6607805001735687\n",
      "Epoch 8, Batch 600, Loss: 1.6286165237426757\n",
      "Epoch 8, Batch 700, Loss: 1.6511980772018433\n",
      "Epoch 8, Batch 800, Loss: 1.6221794724464416\n",
      "Epoch 8, Batch 900, Loss: 1.6849012613296508\n",
      "Epoch 9, Batch 100, Loss: 1.6371667170524598\n",
      "Epoch 9, Batch 200, Loss: 1.6112308061122895\n",
      "Epoch 9, Batch 300, Loss: 1.6388816940784454\n",
      "Epoch 9, Batch 400, Loss: 1.648595130443573\n",
      "Epoch 9, Batch 500, Loss: 1.6174340331554413\n",
      "Epoch 9, Batch 600, Loss: 1.622498482465744\n",
      "Epoch 9, Batch 700, Loss: 1.6254038202762604\n",
      "Epoch 9, Batch 800, Loss: 1.6131964683532716\n",
      "Epoch 9, Batch 900, Loss: 1.6073417747020722\n",
      "Epoch 10, Batch 100, Loss: 1.6481392633914949\n",
      "Epoch 10, Batch 200, Loss: 1.6434096789360046\n",
      "Epoch 10, Batch 300, Loss: 1.695740487575531\n",
      "Epoch 10, Batch 400, Loss: 1.5992664790153504\n",
      "Epoch 10, Batch 500, Loss: 1.6250150668621064\n",
      "Epoch 10, Batch 600, Loss: 1.6784897756576538\n",
      "Epoch 10, Batch 700, Loss: 1.6619923448562621\n",
      "Epoch 10, Batch 800, Loss: 1.6299851584434508\n",
      "Epoch 10, Batch 900, Loss: 1.596276808977127\n",
      "Epoch 11, Batch 100, Loss: 1.6480123603343964\n",
      "Epoch 11, Batch 200, Loss: 1.6296469724178315\n",
      "Epoch 11, Batch 300, Loss: 1.631060860157013\n",
      "Epoch 11, Batch 400, Loss: 1.6449215185642243\n",
      "Epoch 11, Batch 500, Loss: 1.600482268333435\n",
      "Epoch 11, Batch 600, Loss: 1.6597491574287415\n",
      "Epoch 11, Batch 700, Loss: 1.6483902323246002\n",
      "Epoch 11, Batch 800, Loss: 1.5907331597805023\n",
      "Epoch 11, Batch 900, Loss: 1.6322021543979646\n",
      "Epoch 12, Batch 100, Loss: 1.6713866651058198\n",
      "Epoch 12, Batch 200, Loss: 1.6259738171100617\n",
      "Epoch 12, Batch 300, Loss: 1.6061823201179504\n",
      "Epoch 12, Batch 400, Loss: 1.6358636367321013\n",
      "Epoch 12, Batch 500, Loss: 1.6236982238292694\n",
      "Epoch 12, Batch 600, Loss: 1.6757367849349976\n",
      "Epoch 12, Batch 700, Loss: 1.6210800862312318\n",
      "Epoch 12, Batch 800, Loss: 1.6361310875415802\n",
      "Epoch 12, Batch 900, Loss: 1.6223494267463685\n",
      "Epoch 13, Batch 100, Loss: 1.5732992506027221\n",
      "Epoch 13, Batch 200, Loss: 1.656204447746277\n",
      "Epoch 13, Batch 300, Loss: 1.6575693356990815\n",
      "Epoch 13, Batch 400, Loss: 1.6430580377578736\n",
      "Epoch 13, Batch 500, Loss: 1.6156994354724885\n",
      "Epoch 13, Batch 600, Loss: 1.6529667043685914\n",
      "Epoch 13, Batch 700, Loss: 1.604782942533493\n",
      "Epoch 13, Batch 800, Loss: 1.6525361227989197\n",
      "Epoch 13, Batch 900, Loss: 1.6131052017211913\n",
      "Epoch 14, Batch 100, Loss: 1.6197296106815338\n",
      "Epoch 14, Batch 200, Loss: 1.614711971282959\n",
      "Epoch 14, Batch 300, Loss: 1.645030288696289\n",
      "Epoch 14, Batch 400, Loss: 1.6193695485591888\n",
      "Epoch 14, Batch 500, Loss: 1.6122374081611632\n",
      "Epoch 14, Batch 600, Loss: 1.6156753063201905\n",
      "Epoch 14, Batch 700, Loss: 1.6393259251117707\n",
      "Epoch 14, Batch 800, Loss: 1.6334739053249359\n",
      "Epoch 14, Batch 900, Loss: 1.6141266167163848\n",
      "Epoch 15, Batch 100, Loss: 1.609381409883499\n",
      "Epoch 15, Batch 200, Loss: 1.6491833782196046\n",
      "Epoch 15, Batch 300, Loss: 1.5927323925495147\n",
      "Epoch 15, Batch 400, Loss: 1.639073712825775\n",
      "Epoch 15, Batch 500, Loss: 1.6589405453205108\n",
      "Epoch 15, Batch 600, Loss: 1.6583524310588837\n",
      "Epoch 15, Batch 700, Loss: 1.647837301492691\n",
      "Epoch 15, Batch 800, Loss: 1.588769861459732\n",
      "Epoch 15, Batch 900, Loss: 1.6501388597488402\n",
      "Epoch 16, Batch 100, Loss: 1.611130942106247\n",
      "Epoch 16, Batch 200, Loss: 1.5692059099674225\n",
      "Epoch 16, Batch 300, Loss: 1.6401789820194244\n",
      "Epoch 16, Batch 400, Loss: 1.5803054761886597\n",
      "Epoch 16, Batch 500, Loss: 1.617516987323761\n",
      "Epoch 16, Batch 600, Loss: 1.6490222704410553\n",
      "Epoch 16, Batch 700, Loss: 1.6009255850315094\n",
      "Epoch 16, Batch 800, Loss: 1.6463015484809875\n",
      "Epoch 16, Batch 900, Loss: 1.6328006744384767\n",
      "Epoch 17, Batch 100, Loss: 1.6779394042491913\n",
      "Epoch 17, Batch 200, Loss: 1.6308091282844543\n",
      "Epoch 17, Batch 300, Loss: 1.6786823165416718\n",
      "Epoch 17, Batch 400, Loss: 1.6076824069023132\n",
      "Epoch 17, Batch 500, Loss: 1.6367970740795135\n",
      "Epoch 17, Batch 600, Loss: 1.6009536457061768\n",
      "Epoch 17, Batch 700, Loss: 1.6192077958583833\n",
      "Epoch 17, Batch 800, Loss: 1.647795978784561\n",
      "Epoch 17, Batch 900, Loss: 1.677861851453781\n",
      "Epoch 18, Batch 100, Loss: 1.605174205303192\n",
      "Epoch 18, Batch 200, Loss: 1.6310763251781464\n",
      "Epoch 18, Batch 300, Loss: 1.6187543666362763\n",
      "Epoch 18, Batch 400, Loss: 1.6248989236354827\n",
      "Epoch 18, Batch 500, Loss: 1.6530851137638092\n",
      "Epoch 18, Batch 600, Loss: 1.6351287949085236\n",
      "Epoch 18, Batch 700, Loss: 1.6596519505977632\n",
      "Epoch 18, Batch 800, Loss: 1.654628698825836\n",
      "Epoch 18, Batch 900, Loss: 1.6315082001686096\n",
      "Epoch 19, Batch 100, Loss: 1.6131530344486236\n",
      "Epoch 19, Batch 200, Loss: 1.673597550392151\n",
      "Epoch 19, Batch 300, Loss: 1.6075086772441864\n",
      "Epoch 19, Batch 400, Loss: 1.630480489730835\n",
      "Epoch 19, Batch 500, Loss: 1.615620754957199\n",
      "Epoch 19, Batch 600, Loss: 1.639456776380539\n",
      "Epoch 19, Batch 700, Loss: 1.6222733068466186\n",
      "Epoch 19, Batch 800, Loss: 1.6542812514305114\n",
      "Epoch 19, Batch 900, Loss: 1.6518605899810792\n",
      "Epoch 20, Batch 100, Loss: 1.6722714626789092\n",
      "Epoch 20, Batch 200, Loss: 1.6734476125240325\n",
      "Epoch 20, Batch 300, Loss: 1.633523290157318\n",
      "Epoch 20, Batch 400, Loss: 1.6575388324260711\n",
      "Epoch 20, Batch 500, Loss: 1.6161602425575257\n",
      "Epoch 20, Batch 600, Loss: 1.6836041808128357\n",
      "Epoch 20, Batch 700, Loss: 1.6764975666999817\n",
      "Epoch 20, Batch 800, Loss: 1.6798184788227082\n",
      "Epoch 20, Batch 900, Loss: 1.6336861741542816\n",
      "Epoch 21, Batch 100, Loss: 1.6128929507732392\n",
      "Epoch 21, Batch 200, Loss: 1.6137171840667726\n",
      "Epoch 21, Batch 300, Loss: 1.6660640943050384\n",
      "Epoch 21, Batch 400, Loss: 1.7013280081748963\n",
      "Epoch 21, Batch 500, Loss: 1.6559631145000457\n",
      "Epoch 21, Batch 600, Loss: 1.6398358273506164\n",
      "Epoch 21, Batch 700, Loss: 1.629046561717987\n",
      "Epoch 21, Batch 800, Loss: 1.6544018125534057\n",
      "Epoch 21, Batch 900, Loss: 1.6282125532627105\n",
      "Epoch 22, Batch 100, Loss: 1.6424067687988282\n",
      "Epoch 22, Batch 200, Loss: 1.6668155467510224\n",
      "Epoch 22, Batch 300, Loss: 1.631671656370163\n",
      "Epoch 22, Batch 400, Loss: 1.6408973145484924\n",
      "Epoch 22, Batch 500, Loss: 1.6348853266239167\n",
      "Epoch 22, Batch 600, Loss: 1.686322158575058\n",
      "Epoch 22, Batch 700, Loss: 1.6640601563453674\n",
      "Epoch 22, Batch 800, Loss: 1.6312280070781708\n",
      "Epoch 22, Batch 900, Loss: 1.623364006280899\n",
      "Epoch 23, Batch 100, Loss: 1.6186071932315826\n",
      "Epoch 23, Batch 200, Loss: 1.6203139519691467\n",
      "Epoch 23, Batch 300, Loss: 1.6193047678470611\n",
      "Epoch 23, Batch 400, Loss: 1.6056590569019318\n",
      "Epoch 23, Batch 500, Loss: 1.6373972690105438\n",
      "Epoch 23, Batch 600, Loss: 1.669076417684555\n",
      "Epoch 23, Batch 700, Loss: 1.6532446694374086\n",
      "Epoch 23, Batch 800, Loss: 1.6550974869728088\n",
      "Epoch 23, Batch 900, Loss: 1.6079148399829863\n",
      "Epoch 24, Batch 100, Loss: 1.6746250534057616\n",
      "Epoch 24, Batch 200, Loss: 1.6465774393081665\n",
      "Epoch 24, Batch 300, Loss: 1.6575925147533417\n",
      "Epoch 24, Batch 400, Loss: 1.6477287828922271\n",
      "Epoch 24, Batch 500, Loss: 1.6577141296863556\n",
      "Epoch 24, Batch 600, Loss: 1.6373328626155854\n",
      "Epoch 24, Batch 700, Loss: 1.5998081040382386\n",
      "Epoch 24, Batch 800, Loss: 1.6240164518356324\n",
      "Epoch 24, Batch 900, Loss: 1.614605964422226\n",
      "Epoch 25, Batch 100, Loss: 1.6102761495113374\n",
      "Epoch 25, Batch 200, Loss: 1.6408165395259857\n",
      "Epoch 25, Batch 300, Loss: 1.6305221581459046\n",
      "Epoch 25, Batch 400, Loss: 1.6694043672084808\n",
      "Epoch 25, Batch 500, Loss: 1.624660737514496\n",
      "Epoch 25, Batch 600, Loss: 1.6757714772224426\n",
      "Epoch 25, Batch 700, Loss: 1.6702577340602875\n",
      "Epoch 25, Batch 800, Loss: 1.6169861769676208\n",
      "Epoch 25, Batch 900, Loss: 1.645866301059723\n",
      "Accuracy on test set: 0.522%\n",
      "Fitting for combination 9\n",
      "784\n",
      "1\n",
      "10\n",
      "[50, 10]\n",
      "True\n",
      "['relu']\n",
      "SGD\n",
      "0.3\n",
      "0.1\n",
      "CrossEntropyLoss\n",
      "300\n",
      "25\n",
      "0\n",
      "Epoch 1, Batch 300, Loss: 6.9181681871414185\n",
      "Epoch 1, Batch 600, Loss: 6.8834971642494205\n",
      "Epoch 1, Batch 900, Loss: 6.90103964805603\n",
      "Epoch 2, Batch 300, Loss: 6.905897524356842\n",
      "Epoch 2, Batch 600, Loss: 6.893104435205459\n",
      "Epoch 2, Batch 900, Loss: 6.847466741800308\n",
      "Epoch 3, Batch 300, Loss: 6.907020337581635\n",
      "Epoch 3, Batch 600, Loss: 6.905785217285156\n",
      "Epoch 3, Batch 900, Loss: 6.906597735881806\n",
      "Epoch 4, Batch 300, Loss: 6.907519066333771\n",
      "Epoch 4, Batch 600, Loss: 6.906378026008606\n",
      "Epoch 4, Batch 900, Loss: 6.907271630764008\n",
      "Epoch 5, Batch 300, Loss: 6.904927597045899\n",
      "Epoch 5, Batch 600, Loss: 6.9058396434783935\n",
      "Epoch 5, Batch 900, Loss: 6.907548174858094\n",
      "Epoch 6, Batch 300, Loss: 6.906808269023895\n",
      "Epoch 6, Batch 600, Loss: 6.906171162128448\n",
      "Epoch 6, Batch 900, Loss: 6.90605544090271\n",
      "Epoch 7, Batch 300, Loss: 6.905492193698883\n",
      "Epoch 7, Batch 600, Loss: 6.905425853729248\n",
      "Epoch 7, Batch 900, Loss: 6.906580820083618\n",
      "Epoch 8, Batch 300, Loss: 6.90674298286438\n",
      "Epoch 8, Batch 600, Loss: 6.906810953617096\n",
      "Epoch 8, Batch 900, Loss: 6.9057605886459354\n",
      "Epoch 9, Batch 300, Loss: 6.906592116355896\n",
      "Epoch 9, Batch 600, Loss: 6.907540695667267\n",
      "Epoch 9, Batch 900, Loss: 6.904503860473633\n",
      "Epoch 10, Batch 300, Loss: 6.906667397022248\n",
      "Epoch 10, Batch 600, Loss: 6.905347311496735\n",
      "Epoch 10, Batch 900, Loss: 6.907163710594177\n",
      "Epoch 11, Batch 300, Loss: 6.906306958198547\n",
      "Epoch 11, Batch 600, Loss: 6.905916934013367\n",
      "Epoch 11, Batch 900, Loss: 6.906482846736908\n",
      "Epoch 12, Batch 300, Loss: 6.906792948246002\n",
      "Epoch 12, Batch 600, Loss: 6.906564803123474\n",
      "Epoch 12, Batch 900, Loss: 6.906198074817658\n",
      "Epoch 13, Batch 300, Loss: 6.906146998405457\n",
      "Epoch 13, Batch 600, Loss: 6.906582658290863\n",
      "Epoch 13, Batch 900, Loss: 6.90682986497879\n",
      "Epoch 14, Batch 300, Loss: 6.906963381767273\n",
      "Epoch 14, Batch 600, Loss: 6.906001329421997\n",
      "Epoch 14, Batch 900, Loss: 6.906001262664795\n",
      "Epoch 15, Batch 300, Loss: 6.906494460105896\n",
      "Epoch 15, Batch 600, Loss: 6.907134006023407\n",
      "Epoch 15, Batch 900, Loss: 6.906367928981781\n",
      "Epoch 16, Batch 300, Loss: 6.907754020690918\n",
      "Epoch 16, Batch 600, Loss: 6.906102173328399\n",
      "Epoch 16, Batch 900, Loss: 6.9058387351036075\n",
      "Epoch 17, Batch 300, Loss: 6.907232692241669\n",
      "Epoch 17, Batch 600, Loss: 6.907109386920929\n",
      "Epoch 17, Batch 900, Loss: 6.905665371417999\n",
      "Epoch 18, Batch 300, Loss: 6.903022408485413\n",
      "Epoch 18, Batch 600, Loss: 6.906598980426788\n",
      "Epoch 18, Batch 900, Loss: 6.906779870986939\n",
      "Epoch 19, Batch 300, Loss: 6.906214733123779\n",
      "Epoch 19, Batch 600, Loss: 6.908340919017792\n",
      "Epoch 19, Batch 900, Loss: 6.905814139842987\n",
      "Epoch 20, Batch 300, Loss: 6.906222388744355\n",
      "Epoch 20, Batch 600, Loss: 6.904415774345398\n",
      "Epoch 20, Batch 900, Loss: 6.90684404373169\n",
      "Epoch 21, Batch 300, Loss: 6.906820838451385\n",
      "Epoch 21, Batch 600, Loss: 6.904927961826324\n",
      "Epoch 21, Batch 900, Loss: 6.907234933376312\n",
      "Epoch 22, Batch 300, Loss: 6.906617150306702\n",
      "Epoch 22, Batch 600, Loss: 6.90498950958252\n",
      "Epoch 22, Batch 900, Loss: 6.907764313220977\n",
      "Epoch 23, Batch 300, Loss: 6.905640392303467\n",
      "Epoch 23, Batch 600, Loss: 6.905798563957214\n",
      "Epoch 23, Batch 900, Loss: 6.9063348698616025\n",
      "Epoch 24, Batch 300, Loss: 6.906821682453155\n",
      "Epoch 24, Batch 600, Loss: 6.906842436790466\n",
      "Epoch 24, Batch 900, Loss: 6.906387479305267\n",
      "Epoch 25, Batch 300, Loss: 6.907431724071503\n",
      "Epoch 25, Batch 600, Loss: 6.90753653049469\n",
      "Epoch 25, Batch 900, Loss: 6.905540513992309\n",
      "Accuracy on test set: 0.101%\n",
      "Fitting for combination 10\n",
      "784\n",
      "2\n",
      "10\n",
      "[30, 10, 10]\n",
      "False\n",
      "['tanh', 'relu']\n",
      "Adam\n",
      "0.03\n",
      "0.3\n",
      "CrossEntropyLoss\n",
      "300\n",
      "25\n",
      "0\n",
      "Epoch 1, Batch 300, Loss: 6.907553613185883\n",
      "Epoch 1, Batch 600, Loss: 6.90651939868927\n",
      "Epoch 1, Batch 900, Loss: 6.907212169170379\n",
      "Epoch 2, Batch 300, Loss: 6.909453692436219\n",
      "Epoch 2, Batch 600, Loss: 6.907323474884033\n",
      "Epoch 2, Batch 900, Loss: 6.909020586013794\n",
      "Epoch 3, Batch 300, Loss: 6.908383195400238\n",
      "Epoch 3, Batch 600, Loss: 6.90766056060791\n",
      "Epoch 3, Batch 900, Loss: 6.909393773078919\n",
      "Epoch 4, Batch 300, Loss: 6.909294934272766\n",
      "Epoch 4, Batch 600, Loss: 6.909046456813813\n",
      "Epoch 4, Batch 900, Loss: 6.906279969215393\n",
      "Epoch 5, Batch 300, Loss: 6.908934178352356\n",
      "Epoch 5, Batch 600, Loss: 6.908377368450164\n",
      "Epoch 5, Batch 900, Loss: 6.907397055625916\n",
      "Epoch 6, Batch 300, Loss: 6.9080451035499575\n",
      "Epoch 6, Batch 600, Loss: 6.906982955932617\n",
      "Epoch 6, Batch 900, Loss: 6.909074811935425\n",
      "Epoch 7, Batch 300, Loss: 6.908232328891754\n",
      "Epoch 7, Batch 600, Loss: 6.907592461109162\n",
      "Epoch 7, Batch 900, Loss: 6.908019306659699\n",
      "Epoch 8, Batch 300, Loss: 6.907176632881164\n",
      "Epoch 8, Batch 600, Loss: 6.909076387882233\n",
      "Epoch 8, Batch 900, Loss: 6.908277993202209\n",
      "Epoch 9, Batch 300, Loss: 6.907749729156494\n",
      "Epoch 9, Batch 600, Loss: 6.908126292228698\n",
      "Epoch 9, Batch 900, Loss: 6.907605645656585\n",
      "Epoch 10, Batch 300, Loss: 6.9078253293037415\n",
      "Epoch 10, Batch 600, Loss: 6.908700225353241\n",
      "Epoch 10, Batch 900, Loss: 6.907340865135193\n",
      "Epoch 11, Batch 300, Loss: 6.909133238792419\n",
      "Epoch 11, Batch 600, Loss: 6.908262612819672\n",
      "Epoch 11, Batch 900, Loss: 6.906144580841064\n",
      "Epoch 12, Batch 300, Loss: 6.908488445281982\n",
      "Epoch 12, Batch 600, Loss: 6.907024877071381\n",
      "Epoch 12, Batch 900, Loss: 6.908565452098847\n",
      "Epoch 13, Batch 300, Loss: 6.907999904155731\n",
      "Epoch 13, Batch 600, Loss: 6.908277413845062\n",
      "Epoch 13, Batch 900, Loss: 6.9074740934371945\n",
      "Epoch 14, Batch 300, Loss: 6.90694904088974\n",
      "Epoch 14, Batch 600, Loss: 6.908432595729828\n",
      "Epoch 14, Batch 900, Loss: 6.908931090831756\n",
      "Epoch 15, Batch 300, Loss: 6.90849399805069\n",
      "Epoch 15, Batch 600, Loss: 6.907921571731567\n",
      "Epoch 15, Batch 900, Loss: 6.907712891101837\n",
      "Epoch 16, Batch 300, Loss: 6.906856186389923\n",
      "Epoch 16, Batch 600, Loss: 6.907397661209107\n",
      "Epoch 16, Batch 900, Loss: 6.908260262012481\n",
      "Epoch 17, Batch 300, Loss: 6.909394755363464\n",
      "Epoch 17, Batch 600, Loss: 6.906789035797119\n",
      "Epoch 17, Batch 900, Loss: 6.9085338187217715\n",
      "Epoch 18, Batch 300, Loss: 6.90816294670105\n",
      "Epoch 18, Batch 600, Loss: 6.908424260616303\n",
      "Epoch 18, Batch 900, Loss: 6.907553009986877\n",
      "Epoch 19, Batch 300, Loss: 6.906749079227447\n",
      "Epoch 19, Batch 600, Loss: 6.906503069400787\n",
      "Epoch 19, Batch 900, Loss: 6.907680871486664\n",
      "Epoch 20, Batch 300, Loss: 6.908868124485016\n",
      "Epoch 20, Batch 600, Loss: 6.907603976726532\n",
      "Epoch 20, Batch 900, Loss: 6.908080475330353\n",
      "Epoch 21, Batch 300, Loss: 6.9085093379020694\n",
      "Epoch 21, Batch 600, Loss: 6.907146518230438\n",
      "Epoch 21, Batch 900, Loss: 6.907089440822602\n",
      "Epoch 22, Batch 300, Loss: 6.906053445339203\n",
      "Epoch 22, Batch 600, Loss: 6.907845158576965\n",
      "Epoch 22, Batch 900, Loss: 6.908267381191254\n",
      "Epoch 23, Batch 300, Loss: 6.908584091663361\n",
      "Epoch 23, Batch 600, Loss: 6.908205735683441\n",
      "Epoch 23, Batch 900, Loss: 6.90874609708786\n",
      "Epoch 24, Batch 300, Loss: 6.909565231800079\n",
      "Epoch 24, Batch 600, Loss: 6.909394023418426\n",
      "Epoch 24, Batch 900, Loss: 6.90813606262207\n",
      "Epoch 25, Batch 300, Loss: 6.908102817535401\n",
      "Epoch 25, Batch 600, Loss: 6.907241704463959\n",
      "Epoch 25, Batch 900, Loss: 6.9093265628814695\n",
      "Accuracy on test set: 0.1028%\n",
      "Fitting for combination 11\n",
      "784\n",
      "2\n",
      "10\n",
      "[30, 10, 10]\n",
      "True\n",
      "['tanh', 'relu']\n",
      "SGD\n",
      "0.003\n",
      "0.01\n",
      "CrossEntropyLoss\n",
      "200\n",
      "25\n",
      "0\n",
      "Epoch 1, Batch 200, Loss: 4.550321404933929\n",
      "Epoch 1, Batch 400, Loss: 4.3685062217712405\n",
      "Epoch 1, Batch 600, Loss: 4.199277733564377\n",
      "Epoch 1, Batch 800, Loss: 4.0226799929142\n",
      "Epoch 2, Batch 200, Loss: 3.7017871832847593\n",
      "Epoch 2, Batch 400, Loss: 3.490445591211319\n",
      "Epoch 2, Batch 600, Loss: 3.278340849876404\n",
      "Epoch 2, Batch 800, Loss: 3.0677677071094513\n",
      "Epoch 3, Batch 200, Loss: 2.725966747999191\n",
      "Epoch 3, Batch 400, Loss: 2.56101850271225\n",
      "Epoch 3, Batch 600, Loss: 2.4060578852891923\n",
      "Epoch 3, Batch 800, Loss: 2.2579139989614485\n",
      "Epoch 4, Batch 200, Loss: 2.026534627079964\n",
      "Epoch 4, Batch 400, Loss: 1.9067575126886367\n",
      "Epoch 4, Batch 600, Loss: 1.8030416035652161\n",
      "Epoch 4, Batch 800, Loss: 1.713921844959259\n",
      "Epoch 5, Batch 200, Loss: 1.5367416262626648\n",
      "Epoch 5, Batch 400, Loss: 1.4599102115631104\n",
      "Epoch 5, Batch 600, Loss: 1.426929178237915\n",
      "Epoch 5, Batch 800, Loss: 1.3420784562826156\n",
      "Epoch 6, Batch 200, Loss: 1.2538668850064278\n",
      "Epoch 6, Batch 400, Loss: 1.2110979703068734\n",
      "Epoch 6, Batch 600, Loss: 1.1657820719480514\n",
      "Epoch 6, Batch 800, Loss: 1.1098642522096633\n",
      "Epoch 7, Batch 200, Loss: 1.0632136482000352\n",
      "Epoch 7, Batch 400, Loss: 1.0509387090802194\n",
      "Epoch 7, Batch 600, Loss: 1.011815097630024\n",
      "Epoch 7, Batch 800, Loss: 0.992608036994934\n",
      "Epoch 8, Batch 200, Loss: 0.9697767940163612\n",
      "Epoch 8, Batch 400, Loss: 0.9426424071192742\n",
      "Epoch 8, Batch 600, Loss: 0.8999049407243729\n",
      "Epoch 8, Batch 800, Loss: 0.905576016008854\n",
      "Epoch 9, Batch 200, Loss: 0.8716826501488686\n",
      "Epoch 9, Batch 400, Loss: 0.8734926244616509\n",
      "Epoch 9, Batch 600, Loss: 0.8537674199044705\n",
      "Epoch 9, Batch 800, Loss: 0.8342134138941765\n",
      "Epoch 10, Batch 200, Loss: 0.8182687585055828\n",
      "Epoch 10, Batch 400, Loss: 0.813302990347147\n",
      "Epoch 10, Batch 600, Loss: 0.7999897004663944\n",
      "Epoch 10, Batch 800, Loss: 0.7982708816230297\n",
      "Epoch 11, Batch 200, Loss: 0.7877878008782864\n",
      "Epoch 11, Batch 400, Loss: 0.7868660819530487\n",
      "Epoch 11, Batch 600, Loss: 0.7589502437412738\n",
      "Epoch 11, Batch 800, Loss: 0.7610813175141812\n",
      "Epoch 12, Batch 200, Loss: 0.7427324756979943\n",
      "Epoch 12, Batch 400, Loss: 0.7466328543424606\n",
      "Epoch 12, Batch 600, Loss: 0.7401647144556045\n",
      "Epoch 12, Batch 800, Loss: 0.7248190619051457\n",
      "Epoch 13, Batch 200, Loss: 0.7266587652266026\n",
      "Epoch 13, Batch 400, Loss: 0.7254070909321308\n",
      "Epoch 13, Batch 600, Loss: 0.7083976519107819\n",
      "Epoch 13, Batch 800, Loss: 0.7031665851175785\n",
      "Epoch 14, Batch 200, Loss: 0.7061535602807999\n",
      "Epoch 14, Batch 400, Loss: 0.6850724504888057\n",
      "Epoch 14, Batch 600, Loss: 0.6802876104414463\n",
      "Epoch 14, Batch 800, Loss: 0.6937520718574524\n",
      "Epoch 15, Batch 200, Loss: 0.6834757722914219\n",
      "Epoch 15, Batch 400, Loss: 0.6759768228232861\n",
      "Epoch 15, Batch 600, Loss: 0.663429941534996\n",
      "Epoch 15, Batch 800, Loss: 0.6653403380513191\n",
      "Epoch 16, Batch 200, Loss: 0.6614343468844891\n",
      "Epoch 16, Batch 400, Loss: 0.6570485961437226\n",
      "Epoch 16, Batch 600, Loss: 0.6436010357737542\n",
      "Epoch 16, Batch 800, Loss: 0.6667323072254657\n",
      "Epoch 17, Batch 200, Loss: 0.6396624556183815\n",
      "Epoch 17, Batch 400, Loss: 0.6278790663182735\n",
      "Epoch 17, Batch 600, Loss: 0.6581656254827977\n",
      "Epoch 17, Batch 800, Loss: 0.6413064987957477\n",
      "Epoch 18, Batch 200, Loss: 0.6381290563941002\n",
      "Epoch 18, Batch 400, Loss: 0.6410750445723533\n",
      "Epoch 18, Batch 600, Loss: 0.6296199242770671\n",
      "Epoch 18, Batch 800, Loss: 0.6155238568782806\n",
      "Epoch 19, Batch 200, Loss: 0.6157905161380768\n",
      "Epoch 19, Batch 400, Loss: 0.6118520784378052\n",
      "Epoch 19, Batch 600, Loss: 0.617083954885602\n",
      "Epoch 19, Batch 800, Loss: 0.6065231032669545\n",
      "Epoch 20, Batch 200, Loss: 0.601168851852417\n",
      "Epoch 20, Batch 400, Loss: 0.6331871329247951\n",
      "Epoch 20, Batch 600, Loss: 0.5792051712423563\n",
      "Epoch 20, Batch 800, Loss: 0.617980619519949\n",
      "Epoch 21, Batch 200, Loss: 0.6066691826283932\n",
      "Epoch 21, Batch 400, Loss: 0.5814108899235726\n",
      "Epoch 21, Batch 600, Loss: 0.611609580218792\n",
      "Epoch 21, Batch 800, Loss: 0.5994168002903462\n",
      "Epoch 22, Batch 200, Loss: 0.6026900523900985\n",
      "Epoch 22, Batch 400, Loss: 0.582315012216568\n",
      "Epoch 22, Batch 600, Loss: 0.5927076026797294\n",
      "Epoch 22, Batch 800, Loss: 0.5813852635025978\n",
      "Epoch 23, Batch 200, Loss: 0.5738058391958475\n",
      "Epoch 23, Batch 400, Loss: 0.5862321239709855\n",
      "Epoch 23, Batch 600, Loss: 0.5720414370298386\n",
      "Epoch 23, Batch 800, Loss: 0.5934518583118915\n",
      "Epoch 24, Batch 200, Loss: 0.579062092974782\n",
      "Epoch 24, Batch 400, Loss: 0.5568975572288036\n",
      "Epoch 24, Batch 600, Loss: 0.569945994913578\n",
      "Epoch 24, Batch 800, Loss: 0.5935107082873583\n",
      "Epoch 25, Batch 200, Loss: 0.5601243231445551\n",
      "Epoch 25, Batch 400, Loss: 0.5773404787480831\n",
      "Epoch 25, Batch 600, Loss: 0.5739610214531422\n",
      "Epoch 25, Batch 800, Loss: 0.5735250543057918\n",
      "Accuracy on test set: 0.9252%\n",
      "Fitting for combination 12\n",
      "784\n",
      "2\n",
      "10\n",
      "[30, 20, 10]\n",
      "False\n",
      "['tanh', 'relu']\n",
      "Adam\n",
      "0.1\n",
      "0.1\n",
      "CrossEntropyLoss\n",
      "300\n",
      "25\n",
      "0\n",
      "Epoch 1, Batch 300, Loss: 6.939299421310425\n",
      "Epoch 1, Batch 600, Loss: 6.913640081882477\n",
      "Epoch 1, Batch 900, Loss: 6.918105261325836\n",
      "Epoch 2, Batch 300, Loss: 6.9162058472633365\n",
      "Epoch 2, Batch 600, Loss: 6.9158126068115235\n",
      "Epoch 2, Batch 900, Loss: 6.917112326622009\n",
      "Epoch 3, Batch 300, Loss: 6.9195318388938905\n",
      "Epoch 3, Batch 600, Loss: 6.918141171932221\n",
      "Epoch 3, Batch 900, Loss: 6.915973467826843\n",
      "Epoch 4, Batch 300, Loss: 6.914779028892517\n",
      "Epoch 4, Batch 600, Loss: 6.921969826221466\n",
      "Epoch 4, Batch 900, Loss: 8.332620794773101\n",
      "Epoch 5, Batch 300, Loss: 6.9573057794570925\n",
      "Epoch 5, Batch 600, Loss: 6.913857405185699\n",
      "Epoch 5, Batch 900, Loss: 6.917703170776367\n",
      "Epoch 6, Batch 300, Loss: 6.915705947875977\n",
      "Epoch 6, Batch 600, Loss: 6.919093058109284\n",
      "Epoch 6, Batch 900, Loss: 6.9136919283866884\n",
      "Epoch 7, Batch 300, Loss: 6.917150845527649\n",
      "Epoch 7, Batch 600, Loss: 6.917180490493775\n",
      "Epoch 7, Batch 900, Loss: 6.918059992790222\n",
      "Epoch 8, Batch 300, Loss: 6.91581125497818\n",
      "Epoch 8, Batch 600, Loss: 6.9195615005493165\n",
      "Epoch 8, Batch 900, Loss: 6.916478621959686\n",
      "Epoch 9, Batch 300, Loss: 6.91767881155014\n",
      "Epoch 9, Batch 600, Loss: 6.915924017429352\n",
      "Epoch 9, Batch 900, Loss: 6.9170460653305055\n",
      "Epoch 10, Batch 300, Loss: 6.91761337518692\n",
      "Epoch 10, Batch 600, Loss: 6.9219598698616025\n",
      "Epoch 10, Batch 900, Loss: 6.9192390370368955\n",
      "Epoch 11, Batch 300, Loss: 6.9178949427604675\n",
      "Epoch 11, Batch 600, Loss: 6.913465292453766\n",
      "Epoch 11, Batch 900, Loss: 6.919689674377441\n",
      "Epoch 12, Batch 300, Loss: 6.91679769039154\n",
      "Epoch 12, Batch 600, Loss: 6.91522200345993\n",
      "Epoch 12, Batch 900, Loss: 7.118464057445526\n",
      "Epoch 13, Batch 300, Loss: 7.449556133747101\n",
      "Epoch 13, Batch 600, Loss: 6.926806855201721\n",
      "Epoch 13, Batch 900, Loss: 6.9166956758499145\n",
      "Epoch 14, Batch 300, Loss: 6.916889491081238\n",
      "Epoch 14, Batch 600, Loss: 6.914222712516785\n",
      "Epoch 14, Batch 900, Loss: 6.921753425598144\n",
      "Epoch 15, Batch 300, Loss: 6.916676640510559\n",
      "Epoch 15, Batch 600, Loss: 6.915621383190155\n",
      "Epoch 15, Batch 900, Loss: 6.920956871509552\n",
      "Epoch 16, Batch 300, Loss: 6.920545868873596\n",
      "Epoch 16, Batch 600, Loss: 6.918093702793121\n",
      "Epoch 16, Batch 900, Loss: 6.917151758670807\n",
      "Epoch 17, Batch 300, Loss: 6.918040335178375\n",
      "Epoch 17, Batch 600, Loss: 6.916974120140075\n",
      "Epoch 17, Batch 900, Loss: 6.915453126430512\n",
      "Epoch 18, Batch 300, Loss: 6.917549514770508\n",
      "Epoch 18, Batch 600, Loss: 6.920604314804077\n",
      "Epoch 18, Batch 900, Loss: 6.916049630641937\n",
      "Epoch 19, Batch 300, Loss: 7.443008043766022\n",
      "Epoch 19, Batch 600, Loss: 7.342301127910614\n",
      "Epoch 19, Batch 900, Loss: 6.912205717563629\n",
      "Epoch 20, Batch 300, Loss: 6.916998198032379\n",
      "Epoch 20, Batch 600, Loss: 6.917091534137726\n",
      "Epoch 20, Batch 900, Loss: 6.9142641258239745\n",
      "Epoch 21, Batch 300, Loss: 6.914264180660248\n",
      "Epoch 21, Batch 600, Loss: 6.913211476802826\n",
      "Epoch 21, Batch 900, Loss: 6.919734828472137\n",
      "Epoch 22, Batch 300, Loss: 6.918283200263977\n",
      "Epoch 22, Batch 600, Loss: 6.920865588188171\n",
      "Epoch 22, Batch 900, Loss: 6.915754823684693\n",
      "Epoch 23, Batch 300, Loss: 6.918629186153412\n",
      "Epoch 23, Batch 600, Loss: 6.916410014629364\n",
      "Epoch 23, Batch 900, Loss: 6.916182913780212\n",
      "Epoch 24, Batch 300, Loss: 7.838167471885681\n",
      "Epoch 24, Batch 600, Loss: 6.890446689128876\n",
      "Epoch 24, Batch 900, Loss: 6.915032520294189\n",
      "Epoch 25, Batch 300, Loss: 6.9152865624427795\n",
      "Epoch 25, Batch 600, Loss: 6.915331544876099\n",
      "Epoch 25, Batch 900, Loss: 6.914177539348603\n",
      "Accuracy on test set: 0.0974%\n",
      "Fitting for combination 13\n",
      "784\n",
      "2\n",
      "10\n",
      "[30, 20, 10]\n",
      "True\n",
      "['tanh', 'sigmoid']\n",
      "SGD\n",
      "0.03\n",
      "0.03\n",
      "CrossEntropyLoss\n",
      "100\n",
      "25\n",
      "0\n",
      "Epoch 1, Batch 100, Loss: 2.298234074115753\n",
      "Epoch 1, Batch 200, Loss: 2.2129303121566775\n",
      "Epoch 1, Batch 300, Loss: 2.1506089067459104\n",
      "Epoch 1, Batch 400, Loss: 2.070577881336212\n",
      "Epoch 1, Batch 500, Loss: 1.9860387349128723\n",
      "Epoch 1, Batch 600, Loss: 1.9000499618053437\n",
      "Epoch 1, Batch 700, Loss: 1.8170928764343262\n",
      "Epoch 1, Batch 800, Loss: 1.7483901476860046\n",
      "Epoch 1, Batch 900, Loss: 1.6817449414730072\n",
      "Epoch 2, Batch 100, Loss: 1.5958021295070648\n",
      "Epoch 2, Batch 200, Loss: 1.550010360479355\n",
      "Epoch 2, Batch 300, Loss: 1.5140989625453949\n",
      "Epoch 2, Batch 400, Loss: 1.4746271216869353\n",
      "Epoch 2, Batch 500, Loss: 1.433937510251999\n",
      "Epoch 2, Batch 600, Loss: 1.4115175926685333\n",
      "Epoch 2, Batch 700, Loss: 1.3787305808067323\n",
      "Epoch 2, Batch 800, Loss: 1.35422745347023\n",
      "Epoch 2, Batch 900, Loss: 1.3282213187217713\n",
      "Epoch 3, Batch 100, Loss: 1.3094143640995026\n",
      "Epoch 3, Batch 200, Loss: 1.2927621734142303\n",
      "Epoch 3, Batch 300, Loss: 1.2924085462093353\n",
      "Epoch 3, Batch 400, Loss: 1.2668390572071075\n",
      "Epoch 3, Batch 500, Loss: 1.281466438770294\n",
      "Epoch 3, Batch 600, Loss: 1.2580880677700044\n",
      "Epoch 3, Batch 700, Loss: 1.2492133057117463\n",
      "Epoch 3, Batch 800, Loss: 1.2500165057182313\n",
      "Epoch 3, Batch 900, Loss: 1.2384274542331695\n",
      "Epoch 4, Batch 100, Loss: 1.2343987572193145\n",
      "Epoch 4, Batch 200, Loss: 1.2213851583003998\n",
      "Epoch 4, Batch 300, Loss: 1.2349613201618195\n",
      "Epoch 4, Batch 400, Loss: 1.2276104187965393\n",
      "Epoch 4, Batch 500, Loss: 1.2170385062694549\n",
      "Epoch 4, Batch 600, Loss: 1.2030298674106599\n",
      "Epoch 4, Batch 700, Loss: 1.1945051681995391\n",
      "Epoch 4, Batch 800, Loss: 1.193890038728714\n",
      "Epoch 4, Batch 900, Loss: 1.1779574143886566\n",
      "Epoch 5, Batch 100, Loss: 1.1781610810756684\n",
      "Epoch 5, Batch 200, Loss: 1.1819000053405762\n",
      "Epoch 5, Batch 300, Loss: 1.1823682618141174\n",
      "Epoch 5, Batch 400, Loss: 1.1714029002189636\n",
      "Epoch 5, Batch 500, Loss: 1.179109444618225\n",
      "Epoch 5, Batch 600, Loss: 1.1744335997104645\n",
      "Epoch 5, Batch 700, Loss: 1.1688357001543046\n",
      "Epoch 5, Batch 800, Loss: 1.153446244597435\n",
      "Epoch 5, Batch 900, Loss: 1.1607460588216783\n",
      "Epoch 6, Batch 100, Loss: 1.1582943964004517\n",
      "Epoch 6, Batch 200, Loss: 1.1620164287090302\n",
      "Epoch 6, Batch 300, Loss: 1.1463483881950378\n",
      "Epoch 6, Batch 400, Loss: 1.1537819600105286\n",
      "Epoch 6, Batch 500, Loss: 1.1427127939462662\n",
      "Epoch 6, Batch 600, Loss: 1.1514555704593659\n",
      "Epoch 6, Batch 700, Loss: 1.1560774976015091\n",
      "Epoch 6, Batch 800, Loss: 1.1431979072093963\n",
      "Epoch 6, Batch 900, Loss: 1.1392810982465744\n",
      "Epoch 7, Batch 100, Loss: 1.1379259365797043\n",
      "Epoch 7, Batch 200, Loss: 1.1423991590738296\n",
      "Epoch 7, Batch 300, Loss: 1.1405507612228394\n",
      "Epoch 7, Batch 400, Loss: 1.1494052678346633\n",
      "Epoch 7, Batch 500, Loss: 1.1398500663042068\n",
      "Epoch 7, Batch 600, Loss: 1.1385732758045197\n",
      "Epoch 7, Batch 700, Loss: 1.1314066910743714\n",
      "Epoch 7, Batch 800, Loss: 1.1444873690605164\n",
      "Epoch 7, Batch 900, Loss: 1.1216554540395736\n",
      "Epoch 8, Batch 100, Loss: 1.1235363525152207\n",
      "Epoch 8, Batch 200, Loss: 1.130971936583519\n",
      "Epoch 8, Batch 300, Loss: 1.134462497830391\n",
      "Epoch 8, Batch 400, Loss: 1.1210539507865906\n",
      "Epoch 8, Batch 500, Loss: 1.138377788066864\n",
      "Epoch 8, Batch 600, Loss: 1.126619035601616\n",
      "Epoch 8, Batch 700, Loss: 1.1331885075569152\n",
      "Epoch 8, Batch 800, Loss: 1.1246814888715744\n",
      "Epoch 8, Batch 900, Loss: 1.126041950583458\n",
      "Epoch 9, Batch 100, Loss: 1.1207000398635865\n",
      "Epoch 9, Batch 200, Loss: 1.1274348020553588\n",
      "Epoch 9, Batch 300, Loss: 1.1241950905323028\n",
      "Epoch 9, Batch 400, Loss: 1.1189047014713287\n",
      "Epoch 9, Batch 500, Loss: 1.127023873925209\n",
      "Epoch 9, Batch 600, Loss: 1.1203143054246902\n",
      "Epoch 9, Batch 700, Loss: 1.1132425433397293\n",
      "Epoch 9, Batch 800, Loss: 1.113667269349098\n",
      "Epoch 9, Batch 900, Loss: 1.1169755059480666\n",
      "Epoch 10, Batch 100, Loss: 1.1257614010572434\n",
      "Epoch 10, Batch 200, Loss: 1.111269065141678\n",
      "Epoch 10, Batch 300, Loss: 1.117773072719574\n",
      "Epoch 10, Batch 400, Loss: 1.0997060495615005\n",
      "Epoch 10, Batch 500, Loss: 1.0999798303842545\n",
      "Epoch 10, Batch 600, Loss: 1.1157228338718415\n",
      "Epoch 10, Batch 700, Loss: 1.1101250165700913\n",
      "Epoch 10, Batch 800, Loss: 1.1132019972801208\n",
      "Epoch 10, Batch 900, Loss: 1.1041728001832962\n",
      "Epoch 11, Batch 100, Loss: 1.0930494087934495\n",
      "Epoch 11, Batch 200, Loss: 1.10289914727211\n",
      "Epoch 11, Batch 300, Loss: 1.094163148999214\n",
      "Epoch 11, Batch 400, Loss: 1.1051007908582688\n",
      "Epoch 11, Batch 500, Loss: 1.1022257095575332\n",
      "Epoch 11, Batch 600, Loss: 1.0888231498003007\n",
      "Epoch 11, Batch 700, Loss: 1.0899419748783112\n",
      "Epoch 11, Batch 800, Loss: 1.1012481671571732\n",
      "Epoch 11, Batch 900, Loss: 1.1048986494541169\n",
      "Epoch 12, Batch 100, Loss: 1.1006716895103454\n",
      "Epoch 12, Batch 200, Loss: 1.0757932829856873\n",
      "Epoch 12, Batch 300, Loss: 1.0840599471330643\n",
      "Epoch 12, Batch 400, Loss: 1.0937447303533554\n",
      "Epoch 12, Batch 500, Loss: 1.0852709037065507\n",
      "Epoch 12, Batch 600, Loss: 1.0840856581926346\n",
      "Epoch 12, Batch 700, Loss: 1.0836893957853317\n",
      "Epoch 12, Batch 800, Loss: 1.0755496352910996\n",
      "Epoch 12, Batch 900, Loss: 1.078828879594803\n",
      "Epoch 13, Batch 100, Loss: 1.0666089940071106\n",
      "Epoch 13, Batch 200, Loss: 1.0700321382284164\n",
      "Epoch 13, Batch 300, Loss: 1.0767095875740051\n",
      "Epoch 13, Batch 400, Loss: 1.069643884897232\n",
      "Epoch 13, Batch 500, Loss: 1.067914569377899\n",
      "Epoch 13, Batch 600, Loss: 1.0608015942573548\n",
      "Epoch 13, Batch 700, Loss: 1.063216537833214\n",
      "Epoch 13, Batch 800, Loss: 1.0738341170549393\n",
      "Epoch 13, Batch 900, Loss: 1.0676938837766647\n",
      "Epoch 14, Batch 100, Loss: 1.0686336904764175\n",
      "Epoch 14, Batch 200, Loss: 1.0592820626497268\n",
      "Epoch 14, Batch 300, Loss: 1.0587812411785125\n",
      "Epoch 14, Batch 400, Loss: 1.0565799522399901\n",
      "Epoch 14, Batch 500, Loss: 1.0629943543672562\n",
      "Epoch 14, Batch 600, Loss: 1.0386242604255675\n",
      "Epoch 14, Batch 700, Loss: 1.0567516046762466\n",
      "Epoch 14, Batch 800, Loss: 1.0429606658220292\n",
      "Epoch 14, Batch 900, Loss: 1.042072205543518\n",
      "Epoch 15, Batch 100, Loss: 1.0471045100688934\n",
      "Epoch 15, Batch 200, Loss: 1.0381120979785918\n",
      "Epoch 15, Batch 300, Loss: 1.0410173058509826\n",
      "Epoch 15, Batch 400, Loss: 1.0212228220701218\n",
      "Epoch 15, Batch 500, Loss: 1.0303644359111785\n",
      "Epoch 15, Batch 600, Loss: 1.0191296350955963\n",
      "Epoch 15, Batch 700, Loss: 1.0212148869037627\n",
      "Epoch 15, Batch 800, Loss: 1.010990139245987\n",
      "Epoch 15, Batch 900, Loss: 1.0098721015453338\n",
      "Epoch 16, Batch 100, Loss: 1.0032515966892241\n",
      "Epoch 16, Batch 200, Loss: 1.0119434797763824\n",
      "Epoch 16, Batch 300, Loss: 1.0025465101003648\n",
      "Epoch 16, Batch 400, Loss: 1.0058147251605987\n",
      "Epoch 16, Batch 500, Loss: 0.9963927549123764\n",
      "Epoch 16, Batch 600, Loss: 1.0040499931573867\n",
      "Epoch 16, Batch 700, Loss: 0.9931073886156082\n",
      "Epoch 16, Batch 800, Loss: 0.9884261292219162\n",
      "Epoch 16, Batch 900, Loss: 0.9882928532361984\n",
      "Epoch 17, Batch 100, Loss: 0.9828397792577743\n",
      "Epoch 17, Batch 200, Loss: 0.9912838459014892\n",
      "Epoch 17, Batch 300, Loss: 0.9875491803884506\n",
      "Epoch 17, Batch 400, Loss: 0.9900197702646255\n",
      "Epoch 17, Batch 500, Loss: 0.9764565205574036\n",
      "Epoch 17, Batch 600, Loss: 0.9846199989318848\n",
      "Epoch 17, Batch 700, Loss: 0.9764497500658035\n",
      "Epoch 17, Batch 800, Loss: 0.9954895049333572\n",
      "Epoch 17, Batch 900, Loss: 1.0002688390016556\n",
      "Epoch 18, Batch 100, Loss: 0.99076709151268\n",
      "Epoch 18, Batch 200, Loss: 0.9825996422767639\n",
      "Epoch 18, Batch 300, Loss: 0.9886334091424942\n",
      "Epoch 18, Batch 400, Loss: 0.9928983157873154\n",
      "Epoch 18, Batch 500, Loss: 0.9746007078886032\n",
      "Epoch 18, Batch 600, Loss: 0.9856987512111663\n",
      "Epoch 18, Batch 700, Loss: 0.9864282917976379\n",
      "Epoch 18, Batch 800, Loss: 0.9755587351322174\n",
      "Epoch 18, Batch 900, Loss: 0.9782181692123413\n",
      "Epoch 19, Batch 100, Loss: 0.9781630867719651\n",
      "Epoch 19, Batch 200, Loss: 0.9741652864217758\n",
      "Epoch 19, Batch 300, Loss: 0.983947583436966\n",
      "Epoch 19, Batch 400, Loss: 0.978770586848259\n",
      "Epoch 19, Batch 500, Loss: 0.9799956810474396\n",
      "Epoch 19, Batch 600, Loss: 0.9820125490427017\n",
      "Epoch 19, Batch 700, Loss: 0.9713570839166641\n",
      "Epoch 19, Batch 800, Loss: 0.9819798988103866\n",
      "Epoch 19, Batch 900, Loss: 0.9807225108146668\n",
      "Epoch 20, Batch 100, Loss: 0.9868790048360825\n",
      "Epoch 20, Batch 200, Loss: 0.9857749372720719\n",
      "Epoch 20, Batch 300, Loss: 0.9748652684688568\n",
      "Epoch 20, Batch 400, Loss: 0.991830273270607\n",
      "Epoch 20, Batch 500, Loss: 0.9803883022069931\n",
      "Epoch 20, Batch 600, Loss: 0.9615312802791596\n",
      "Epoch 20, Batch 700, Loss: 0.9746283096075058\n",
      "Epoch 20, Batch 800, Loss: 0.9832380729913711\n",
      "Epoch 20, Batch 900, Loss: 0.9776538109779358\n",
      "Epoch 21, Batch 100, Loss: 0.9750130093097686\n",
      "Epoch 21, Batch 200, Loss: 0.9734254777431488\n",
      "Epoch 21, Batch 300, Loss: 0.9703861212730408\n",
      "Epoch 21, Batch 400, Loss: 0.9891047841310501\n",
      "Epoch 21, Batch 500, Loss: 0.9771329492330552\n",
      "Epoch 21, Batch 600, Loss: 0.9915502619743347\n",
      "Epoch 21, Batch 700, Loss: 0.9866116458177566\n",
      "Epoch 21, Batch 800, Loss: 0.9703411334753036\n",
      "Epoch 21, Batch 900, Loss: 0.9725024783611298\n",
      "Epoch 22, Batch 100, Loss: 0.9719671630859374\n",
      "Epoch 22, Batch 200, Loss: 0.9849260497093201\n",
      "Epoch 22, Batch 300, Loss: 0.9630766469240188\n",
      "Epoch 22, Batch 400, Loss: 0.9774234026670456\n",
      "Epoch 22, Batch 500, Loss: 0.9760521465539932\n",
      "Epoch 22, Batch 600, Loss: 0.9872850370407105\n",
      "Epoch 22, Batch 700, Loss: 0.9883896660804748\n",
      "Epoch 22, Batch 800, Loss: 0.9849820518493653\n",
      "Epoch 22, Batch 900, Loss: 0.9697329634428025\n",
      "Epoch 23, Batch 100, Loss: 0.9655326122045517\n",
      "Epoch 23, Batch 200, Loss: 0.9782169377803802\n",
      "Epoch 23, Batch 300, Loss: 0.9808065611124038\n",
      "Epoch 23, Batch 400, Loss: 0.9670077860355377\n",
      "Epoch 23, Batch 500, Loss: 0.9659905451536178\n",
      "Epoch 23, Batch 600, Loss: 0.9863542956113815\n",
      "Epoch 23, Batch 700, Loss: 0.9783767211437225\n",
      "Epoch 23, Batch 800, Loss: 0.9950908470153809\n",
      "Epoch 23, Batch 900, Loss: 0.9809529250860214\n",
      "Epoch 24, Batch 100, Loss: 0.9883529305458069\n",
      "Epoch 24, Batch 200, Loss: 0.9820091396570205\n",
      "Epoch 24, Batch 300, Loss: 0.9711258721351623\n",
      "Epoch 24, Batch 400, Loss: 0.974700219631195\n",
      "Epoch 24, Batch 500, Loss: 0.9771173125505448\n",
      "Epoch 24, Batch 600, Loss: 0.9815577721595764\n",
      "Epoch 24, Batch 700, Loss: 0.9634583067893981\n",
      "Epoch 24, Batch 800, Loss: 0.9729715019464493\n",
      "Epoch 24, Batch 900, Loss: 0.9862300151586533\n",
      "Epoch 25, Batch 100, Loss: 0.9720339995622634\n",
      "Epoch 25, Batch 200, Loss: 0.9772377002239228\n",
      "Epoch 25, Batch 300, Loss: 0.9829762041568756\n",
      "Epoch 25, Batch 400, Loss: 0.9805463510751724\n",
      "Epoch 25, Batch 500, Loss: 0.9713885295391083\n",
      "Epoch 25, Batch 600, Loss: 0.9672794383764267\n",
      "Epoch 25, Batch 700, Loss: 0.9832996207475663\n",
      "Epoch 25, Batch 800, Loss: 0.9847881346940994\n",
      "Epoch 25, Batch 900, Loss: 0.981649340391159\n",
      "Accuracy on test set: 0.7916%\n",
      "Fitting for combination 14\n",
      "784\n",
      "2\n",
      "10\n",
      "[30, 30, 10]\n",
      "True\n",
      "['tanh', 'sigmoid']\n",
      "Adam\n",
      "0.1\n",
      "0.3\n",
      "CrossEntropyLoss\n",
      "200\n",
      "25\n",
      "0\n",
      "Epoch 1, Batch 200, Loss: 4.756429996490478\n",
      "Epoch 1, Batch 400, Loss: 4.738057405948639\n",
      "Epoch 1, Batch 600, Loss: 4.74320778131485\n",
      "Epoch 1, Batch 800, Loss: 4.739105818271637\n",
      "Epoch 2, Batch 200, Loss: 4.719770600795746\n",
      "Epoch 2, Batch 400, Loss: 4.741884531974793\n",
      "Epoch 2, Batch 600, Loss: 4.73981427192688\n",
      "Epoch 2, Batch 800, Loss: 4.772796633243561\n",
      "Epoch 3, Batch 200, Loss: 4.754563479423523\n",
      "Epoch 3, Batch 400, Loss: 4.743660252094269\n",
      "Epoch 3, Batch 600, Loss: 4.742365725040436\n",
      "Epoch 3, Batch 800, Loss: 4.7484599256515505\n",
      "Epoch 4, Batch 200, Loss: 4.788505685329437\n",
      "Epoch 4, Batch 400, Loss: 4.73514598608017\n",
      "Epoch 4, Batch 600, Loss: 4.753868181705474\n",
      "Epoch 4, Batch 800, Loss: 4.71376247882843\n",
      "Epoch 5, Batch 200, Loss: 4.714785168170929\n",
      "Epoch 5, Batch 400, Loss: 4.747489919662476\n",
      "Epoch 5, Batch 600, Loss: 4.740084824562072\n",
      "Epoch 5, Batch 800, Loss: 4.756831111907959\n",
      "Epoch 6, Batch 200, Loss: 4.763826186656952\n",
      "Epoch 6, Batch 400, Loss: 4.778686437606812\n",
      "Epoch 6, Batch 600, Loss: 4.76721079826355\n",
      "Epoch 6, Batch 800, Loss: 4.746648585796356\n",
      "Epoch 7, Batch 200, Loss: 4.73974592924118\n",
      "Epoch 7, Batch 400, Loss: 4.746433615684509\n",
      "Epoch 7, Batch 600, Loss: 4.779849166870117\n",
      "Epoch 7, Batch 800, Loss: 4.737533648014068\n",
      "Epoch 8, Batch 200, Loss: 4.77108113527298\n",
      "Epoch 8, Batch 400, Loss: 4.763761134147644\n",
      "Epoch 8, Batch 600, Loss: 4.740909979343415\n",
      "Epoch 8, Batch 800, Loss: 4.727261335849762\n",
      "Epoch 9, Batch 200, Loss: 4.71666357755661\n",
      "Epoch 9, Batch 400, Loss: 4.716948447227478\n",
      "Epoch 9, Batch 600, Loss: 4.753629012107849\n",
      "Epoch 9, Batch 800, Loss: 4.749027180671692\n",
      "Epoch 10, Batch 200, Loss: 4.762763023376465\n",
      "Epoch 10, Batch 400, Loss: 4.72121639251709\n",
      "Epoch 10, Batch 600, Loss: 4.7545409321784975\n",
      "Epoch 10, Batch 800, Loss: 4.753141541481018\n",
      "Epoch 11, Batch 200, Loss: 4.716674618721008\n",
      "Epoch 11, Batch 400, Loss: 4.7551172041893\n",
      "Epoch 11, Batch 600, Loss: 4.765383470058441\n",
      "Epoch 11, Batch 800, Loss: 4.736374373435974\n",
      "Epoch 12, Batch 200, Loss: 4.73557065486908\n",
      "Epoch 12, Batch 400, Loss: 4.764563574790954\n",
      "Epoch 12, Batch 600, Loss: 4.765463442802429\n",
      "Epoch 12, Batch 800, Loss: 4.741620855331421\n",
      "Epoch 13, Batch 200, Loss: 4.768412284851074\n",
      "Epoch 13, Batch 400, Loss: 4.7606281518936155\n",
      "Epoch 13, Batch 600, Loss: 4.731655278205872\n",
      "Epoch 13, Batch 800, Loss: 4.762746119499207\n",
      "Epoch 14, Batch 200, Loss: 4.76025621175766\n",
      "Epoch 14, Batch 400, Loss: 4.741930451393127\n",
      "Epoch 14, Batch 600, Loss: 4.728363938331604\n",
      "Epoch 14, Batch 800, Loss: 4.7314120030403135\n",
      "Epoch 15, Batch 200, Loss: 4.7538142991065975\n",
      "Epoch 15, Batch 400, Loss: 4.733102948665619\n",
      "Epoch 15, Batch 600, Loss: 4.740265119075775\n",
      "Epoch 15, Batch 800, Loss: 4.749694185256958\n",
      "Epoch 16, Batch 200, Loss: 4.752378664016724\n",
      "Epoch 16, Batch 400, Loss: 4.7238384222984315\n",
      "Epoch 16, Batch 600, Loss: 4.783834137916565\n",
      "Epoch 16, Batch 800, Loss: 4.759045464992523\n",
      "Epoch 17, Batch 200, Loss: 4.754886951446533\n",
      "Epoch 17, Batch 400, Loss: 4.75630597114563\n",
      "Epoch 17, Batch 600, Loss: 4.745083820819855\n",
      "Epoch 17, Batch 800, Loss: 4.762251853942871\n",
      "Epoch 18, Batch 200, Loss: 4.758249752521515\n",
      "Epoch 18, Batch 400, Loss: 4.719114089012146\n",
      "Epoch 18, Batch 600, Loss: 4.736891944408416\n",
      "Epoch 18, Batch 800, Loss: 4.7553830909729005\n",
      "Epoch 19, Batch 200, Loss: 4.732372200489044\n",
      "Epoch 19, Batch 400, Loss: 4.790172183513642\n",
      "Epoch 19, Batch 600, Loss: 4.717168371677399\n",
      "Epoch 19, Batch 800, Loss: 4.738463382720948\n",
      "Epoch 20, Batch 200, Loss: 4.762059395313263\n",
      "Epoch 20, Batch 400, Loss: 4.748976399898529\n",
      "Epoch 20, Batch 600, Loss: 4.752871687412262\n",
      "Epoch 20, Batch 800, Loss: 4.762367749214173\n",
      "Epoch 21, Batch 200, Loss: 4.753560721874237\n",
      "Epoch 21, Batch 400, Loss: 4.783980050086975\n",
      "Epoch 21, Batch 600, Loss: 4.741951141357422\n",
      "Epoch 21, Batch 800, Loss: 4.753964102268219\n",
      "Epoch 22, Batch 200, Loss: 4.7396901106834415\n",
      "Epoch 22, Batch 400, Loss: 4.745744876861572\n",
      "Epoch 22, Batch 600, Loss: 4.778849205970764\n",
      "Epoch 22, Batch 800, Loss: 4.751621019840241\n",
      "Epoch 23, Batch 200, Loss: 4.7380065965652465\n",
      "Epoch 23, Batch 400, Loss: 4.731864030361176\n",
      "Epoch 23, Batch 600, Loss: 4.764520134925842\n",
      "Epoch 23, Batch 800, Loss: 4.7424091362953185\n",
      "Epoch 24, Batch 200, Loss: 4.757229745388031\n",
      "Epoch 24, Batch 400, Loss: 4.738075952529908\n",
      "Epoch 24, Batch 600, Loss: 4.766081173419952\n",
      "Epoch 24, Batch 800, Loss: 4.752758414745331\n",
      "Epoch 25, Batch 200, Loss: 4.737209572792053\n",
      "Epoch 25, Batch 400, Loss: 4.72748916387558\n",
      "Epoch 25, Batch 600, Loss: 4.77387540102005\n",
      "Epoch 25, Batch 800, Loss: 4.7677660655975345\n",
      "Accuracy on test set: 0.0892%\n",
      "Fitting for combination 15\n",
      "784\n",
      "2\n",
      "10\n",
      "[30, 30, 10]\n",
      "False\n",
      "['tanh', 'sigmoid']\n",
      "SGD\n",
      "0.03\n",
      "0.01\n",
      "CrossEntropyLoss\n",
      "100\n",
      "25\n",
      "0\n",
      "Epoch 1, Batch 100, Loss: 2.287403905391693\n",
      "Epoch 1, Batch 200, Loss: 2.2016804003715515\n",
      "Epoch 1, Batch 300, Loss: 2.1018004631996154\n",
      "Epoch 1, Batch 400, Loss: 1.9793232297897339\n",
      "Epoch 1, Batch 500, Loss: 1.8339256811141968\n",
      "Epoch 1, Batch 600, Loss: 1.684393780231476\n",
      "Epoch 1, Batch 700, Loss: 1.5251667523384094\n",
      "Epoch 1, Batch 800, Loss: 1.402746069431305\n",
      "Epoch 1, Batch 900, Loss: 1.3003991734981537\n",
      "Epoch 2, Batch 100, Loss: 1.1574854069948197\n",
      "Epoch 2, Batch 200, Loss: 1.0783349466323853\n",
      "Epoch 2, Batch 300, Loss: 1.0322690117359161\n",
      "Epoch 2, Batch 400, Loss: 0.9615111047029495\n",
      "Epoch 2, Batch 500, Loss: 0.9328547257184983\n",
      "Epoch 2, Batch 600, Loss: 0.8882515150308609\n",
      "Epoch 2, Batch 700, Loss: 0.8407289868593216\n",
      "Epoch 2, Batch 800, Loss: 0.8094834494590759\n",
      "Epoch 2, Batch 900, Loss: 0.7990233331918717\n",
      "Epoch 3, Batch 100, Loss: 0.7679063701629638\n",
      "Epoch 3, Batch 200, Loss: 0.7395707267522812\n",
      "Epoch 3, Batch 300, Loss: 0.7026677358150483\n",
      "Epoch 3, Batch 400, Loss: 0.7002730363607407\n",
      "Epoch 3, Batch 500, Loss: 0.7090361732244491\n",
      "Epoch 3, Batch 600, Loss: 0.6697481203079224\n",
      "Epoch 3, Batch 700, Loss: 0.6523687589168549\n",
      "Epoch 3, Batch 800, Loss: 0.6349028691649437\n",
      "Epoch 3, Batch 900, Loss: 0.6428158909082413\n",
      "Epoch 4, Batch 100, Loss: 0.6291371104121208\n",
      "Epoch 4, Batch 200, Loss: 0.611938380599022\n",
      "Epoch 4, Batch 300, Loss: 0.5920370233058929\n",
      "Epoch 4, Batch 400, Loss: 0.5912359154224396\n",
      "Epoch 4, Batch 500, Loss: 0.5890111055970192\n",
      "Epoch 4, Batch 600, Loss: 0.588506433069706\n",
      "Epoch 4, Batch 700, Loss: 0.5757993924617767\n",
      "Epoch 4, Batch 800, Loss: 0.5609919431805611\n",
      "Epoch 4, Batch 900, Loss: 0.5607969120144844\n",
      "Epoch 5, Batch 100, Loss: 0.5479010409116745\n",
      "Epoch 5, Batch 200, Loss: 0.5428570711612701\n",
      "Epoch 5, Batch 300, Loss: 0.5456734317541122\n",
      "Epoch 5, Batch 400, Loss: 0.5398619273304939\n",
      "Epoch 5, Batch 500, Loss: 0.5349624744057655\n",
      "Epoch 5, Batch 600, Loss: 0.52000802308321\n",
      "Epoch 5, Batch 700, Loss: 0.5315442624688148\n",
      "Epoch 5, Batch 800, Loss: 0.5170272105932235\n",
      "Epoch 5, Batch 900, Loss: 0.529904353916645\n",
      "Epoch 6, Batch 100, Loss: 0.4973760911822319\n",
      "Epoch 6, Batch 200, Loss: 0.5000501054525376\n",
      "Epoch 6, Batch 300, Loss: 0.509777951836586\n",
      "Epoch 6, Batch 400, Loss: 0.5189832293987274\n",
      "Epoch 6, Batch 500, Loss: 0.5107436606287956\n",
      "Epoch 6, Batch 600, Loss: 0.5072551545500755\n",
      "Epoch 6, Batch 700, Loss: 0.5018154007196426\n",
      "Epoch 6, Batch 800, Loss: 0.5076158279180527\n",
      "Epoch 6, Batch 900, Loss: 0.5000949975848198\n",
      "Epoch 7, Batch 100, Loss: 0.4970628324151039\n",
      "Epoch 7, Batch 200, Loss: 0.4941014057397842\n",
      "Epoch 7, Batch 300, Loss: 0.49690460711717604\n",
      "Epoch 7, Batch 400, Loss: 0.4832398596405983\n",
      "Epoch 7, Batch 500, Loss: 0.48830273389816287\n",
      "Epoch 7, Batch 600, Loss: 0.49459458887577057\n",
      "Epoch 7, Batch 700, Loss: 0.4896873599290848\n",
      "Epoch 7, Batch 800, Loss: 0.48153862088918686\n",
      "Epoch 7, Batch 900, Loss: 0.4752340766787529\n",
      "Epoch 8, Batch 100, Loss: 0.47703928530216216\n",
      "Epoch 8, Batch 200, Loss: 0.4782795938849449\n",
      "Epoch 8, Batch 300, Loss: 0.47682910561561587\n",
      "Epoch 8, Batch 400, Loss: 0.4813548931479454\n",
      "Epoch 8, Batch 500, Loss: 0.4754137459397316\n",
      "Epoch 8, Batch 600, Loss: 0.4676122584939003\n",
      "Epoch 8, Batch 700, Loss: 0.47255885690450666\n",
      "Epoch 8, Batch 800, Loss: 0.4712505266070366\n",
      "Epoch 8, Batch 900, Loss: 0.4781504222750664\n",
      "Epoch 9, Batch 100, Loss: 0.46514509350061417\n",
      "Epoch 9, Batch 200, Loss: 0.4712080094218254\n",
      "Epoch 9, Batch 300, Loss: 0.46846419483423235\n",
      "Epoch 9, Batch 400, Loss: 0.45773267894983294\n",
      "Epoch 9, Batch 500, Loss: 0.46698386400938036\n",
      "Epoch 9, Batch 600, Loss: 0.47208975613117216\n",
      "Epoch 9, Batch 700, Loss: 0.4727363556623459\n",
      "Epoch 9, Batch 800, Loss: 0.4727550765872002\n",
      "Epoch 9, Batch 900, Loss: 0.4717094624042511\n",
      "Epoch 10, Batch 100, Loss: 0.46184934228658675\n",
      "Epoch 10, Batch 200, Loss: 0.4627454116940498\n",
      "Epoch 10, Batch 300, Loss: 0.4647541573643684\n",
      "Epoch 10, Batch 400, Loss: 0.46301058173179627\n",
      "Epoch 10, Batch 500, Loss: 0.4547333088517189\n",
      "Epoch 10, Batch 600, Loss: 0.46602368384599685\n",
      "Epoch 10, Batch 700, Loss: 0.46782225847244263\n",
      "Epoch 10, Batch 800, Loss: 0.46272502303123475\n",
      "Epoch 10, Batch 900, Loss: 0.46172421514987944\n",
      "Epoch 11, Batch 100, Loss: 0.45611786872148513\n",
      "Epoch 11, Batch 200, Loss: 0.46854438990354536\n",
      "Epoch 11, Batch 300, Loss: 0.4519743254780769\n",
      "Epoch 11, Batch 400, Loss: 0.4547717043757439\n",
      "Epoch 11, Batch 500, Loss: 0.462650790810585\n",
      "Epoch 11, Batch 600, Loss: 0.46101405799388884\n",
      "Epoch 11, Batch 700, Loss: 0.4520439094305038\n",
      "Epoch 11, Batch 800, Loss: 0.4533482357859612\n",
      "Epoch 11, Batch 900, Loss: 0.45374465227127075\n",
      "Epoch 12, Batch 100, Loss: 0.45283953577280045\n",
      "Epoch 12, Batch 200, Loss: 0.46202524453401567\n",
      "Epoch 12, Batch 300, Loss: 0.4565443137288094\n",
      "Epoch 12, Batch 400, Loss: 0.44634804844856263\n",
      "Epoch 12, Batch 500, Loss: 0.471671949326992\n",
      "Epoch 12, Batch 600, Loss: 0.45015252321958543\n",
      "Epoch 12, Batch 700, Loss: 0.4463334819674492\n",
      "Epoch 12, Batch 800, Loss: 0.4718838280439377\n",
      "Epoch 12, Batch 900, Loss: 0.4419332554936409\n",
      "Epoch 13, Batch 100, Loss: 0.4509068527817726\n",
      "Epoch 13, Batch 200, Loss: 0.4441774100065231\n",
      "Epoch 13, Batch 300, Loss: 0.44840052127838137\n",
      "Epoch 13, Batch 400, Loss: 0.45469040364027025\n",
      "Epoch 13, Batch 500, Loss: 0.4486057612299919\n",
      "Epoch 13, Batch 600, Loss: 0.4656580790877342\n",
      "Epoch 13, Batch 700, Loss: 0.44589065104722975\n",
      "Epoch 13, Batch 800, Loss: 0.44866461157798765\n",
      "Epoch 13, Batch 900, Loss: 0.4518691062927246\n",
      "Epoch 14, Batch 100, Loss: 0.4379649952054024\n",
      "Epoch 14, Batch 200, Loss: 0.43688612163066864\n",
      "Epoch 14, Batch 300, Loss: 0.4608521944284439\n",
      "Epoch 14, Batch 400, Loss: 0.4406672403216362\n",
      "Epoch 14, Batch 500, Loss: 0.4492581760883331\n",
      "Epoch 14, Batch 600, Loss: 0.4529982677102089\n",
      "Epoch 14, Batch 700, Loss: 0.4493907514214516\n",
      "Epoch 14, Batch 800, Loss: 0.4653408005833626\n",
      "Epoch 14, Batch 900, Loss: 0.44900540679693224\n",
      "Epoch 15, Batch 100, Loss: 0.45049044221639634\n",
      "Epoch 15, Batch 200, Loss: 0.44347367256879805\n",
      "Epoch 15, Batch 300, Loss: 0.4423308742046356\n",
      "Epoch 15, Batch 400, Loss: 0.4357112729549408\n",
      "Epoch 15, Batch 500, Loss: 0.4512207245826721\n",
      "Epoch 15, Batch 600, Loss: 0.4540331348776817\n",
      "Epoch 15, Batch 700, Loss: 0.4532084301114082\n",
      "Epoch 15, Batch 800, Loss: 0.44689019709825517\n",
      "Epoch 15, Batch 900, Loss: 0.45295886904001237\n",
      "Epoch 16, Batch 100, Loss: 0.44878473043441774\n",
      "Epoch 16, Batch 200, Loss: 0.44756599187850954\n",
      "Epoch 16, Batch 300, Loss: 0.4423612332344055\n",
      "Epoch 16, Batch 400, Loss: 0.43089451670646667\n",
      "Epoch 16, Batch 500, Loss: 0.44953701585531236\n",
      "Epoch 16, Batch 600, Loss: 0.4404514607787132\n",
      "Epoch 16, Batch 700, Loss: 0.4489255237579346\n",
      "Epoch 16, Batch 800, Loss: 0.45084923565387724\n",
      "Epoch 16, Batch 900, Loss: 0.46075132191181184\n",
      "Epoch 17, Batch 100, Loss: 0.44139424681663514\n",
      "Epoch 17, Batch 200, Loss: 0.45159150958061217\n",
      "Epoch 17, Batch 300, Loss: 0.44927306711673737\n",
      "Epoch 17, Batch 400, Loss: 0.45337541252374647\n",
      "Epoch 17, Batch 500, Loss: 0.4470349797606468\n",
      "Epoch 17, Batch 600, Loss: 0.43453300684690477\n",
      "Epoch 17, Batch 700, Loss: 0.4409400901198387\n",
      "Epoch 17, Batch 800, Loss: 0.4377417767047882\n",
      "Epoch 17, Batch 900, Loss: 0.44421891927719115\n",
      "Epoch 18, Batch 100, Loss: 0.44282778531312944\n",
      "Epoch 18, Batch 200, Loss: 0.4541628560423851\n",
      "Epoch 18, Batch 300, Loss: 0.4491636621952057\n",
      "Epoch 18, Batch 400, Loss: 0.44972307592630384\n",
      "Epoch 18, Batch 500, Loss: 0.4400027799606323\n",
      "Epoch 18, Batch 600, Loss: 0.44117983400821686\n",
      "Epoch 18, Batch 700, Loss: 0.4408303701877594\n",
      "Epoch 18, Batch 800, Loss: 0.44681661158800123\n",
      "Epoch 18, Batch 900, Loss: 0.430727875828743\n",
      "Epoch 19, Batch 100, Loss: 0.4338195031881332\n",
      "Epoch 19, Batch 200, Loss: 0.44470693320035937\n",
      "Epoch 19, Batch 300, Loss: 0.43506698191165927\n",
      "Epoch 19, Batch 400, Loss: 0.44819380760192873\n",
      "Epoch 19, Batch 500, Loss: 0.43994680285453797\n",
      "Epoch 19, Batch 600, Loss: 0.45113188326358794\n",
      "Epoch 19, Batch 700, Loss: 0.4440262979269028\n",
      "Epoch 19, Batch 800, Loss: 0.4408722147345543\n",
      "Epoch 19, Batch 900, Loss: 0.44221763253211976\n",
      "Epoch 20, Batch 100, Loss: 0.4344878315925598\n",
      "Epoch 20, Batch 200, Loss: 0.4438517090678215\n",
      "Epoch 20, Batch 300, Loss: 0.43182129710912703\n",
      "Epoch 20, Batch 400, Loss: 0.45349720746278765\n",
      "Epoch 20, Batch 500, Loss: 0.4360914632678032\n",
      "Epoch 20, Batch 600, Loss: 0.4409188869595528\n",
      "Epoch 20, Batch 700, Loss: 0.44333601027727126\n",
      "Epoch 20, Batch 800, Loss: 0.4471797896921635\n",
      "Epoch 20, Batch 900, Loss: 0.43729703783988955\n",
      "Epoch 21, Batch 100, Loss: 0.4347771906852722\n",
      "Epoch 21, Batch 200, Loss: 0.4486154767870903\n",
      "Epoch 21, Batch 300, Loss: 0.4457489088177681\n",
      "Epoch 21, Batch 400, Loss: 0.439262789785862\n",
      "Epoch 21, Batch 500, Loss: 0.4517798781394958\n",
      "Epoch 21, Batch 600, Loss: 0.43196860074996946\n",
      "Epoch 21, Batch 700, Loss: 0.45066278874874116\n",
      "Epoch 21, Batch 800, Loss: 0.43578995317220687\n",
      "Epoch 21, Batch 900, Loss: 0.4333248031139374\n",
      "Epoch 22, Batch 100, Loss: 0.4363230884075165\n",
      "Epoch 22, Batch 200, Loss: 0.45262359708547595\n",
      "Epoch 22, Batch 300, Loss: 0.4486427628993988\n",
      "Epoch 22, Batch 400, Loss: 0.44073255628347396\n",
      "Epoch 22, Batch 500, Loss: 0.4371362432837486\n",
      "Epoch 22, Batch 600, Loss: 0.42838746279478074\n",
      "Epoch 22, Batch 700, Loss: 0.4365175434947014\n",
      "Epoch 22, Batch 800, Loss: 0.44613959819078447\n",
      "Epoch 22, Batch 900, Loss: 0.43706914722919465\n",
      "Epoch 23, Batch 100, Loss: 0.43665591299533846\n",
      "Epoch 23, Batch 200, Loss: 0.43741310477256773\n",
      "Epoch 23, Batch 300, Loss: 0.4346780154109001\n",
      "Epoch 23, Batch 400, Loss: 0.45601550936698915\n",
      "Epoch 23, Batch 500, Loss: 0.4298670047521591\n",
      "Epoch 23, Batch 600, Loss: 0.4425287428498268\n",
      "Epoch 23, Batch 700, Loss: 0.44481929391622543\n",
      "Epoch 23, Batch 800, Loss: 0.4385672891139984\n",
      "Epoch 23, Batch 900, Loss: 0.43404838293790815\n",
      "Epoch 24, Batch 100, Loss: 0.4411791476607323\n",
      "Epoch 24, Batch 200, Loss: 0.4409587940573692\n",
      "Epoch 24, Batch 300, Loss: 0.4322943291068077\n",
      "Epoch 24, Batch 400, Loss: 0.44426777750253676\n",
      "Epoch 24, Batch 500, Loss: 0.4389703258872032\n",
      "Epoch 24, Batch 600, Loss: 0.43786211907863615\n",
      "Epoch 24, Batch 700, Loss: 0.43942067414522173\n",
      "Epoch 24, Batch 800, Loss: 0.4421441662311554\n",
      "Epoch 24, Batch 900, Loss: 0.4368758779764175\n",
      "Epoch 25, Batch 100, Loss: 0.4267715755105019\n",
      "Epoch 25, Batch 200, Loss: 0.42938646882772447\n",
      "Epoch 25, Batch 300, Loss: 0.44377888351678846\n",
      "Epoch 25, Batch 400, Loss: 0.43795967280864717\n",
      "Epoch 25, Batch 500, Loss: 0.4453180551528931\n",
      "Epoch 25, Batch 600, Loss: 0.43844610154628755\n",
      "Epoch 25, Batch 700, Loss: 0.43441786140203476\n",
      "Epoch 25, Batch 800, Loss: 0.4444494867324829\n",
      "Epoch 25, Batch 900, Loss: 0.4362461793422699\n",
      "Accuracy on test set: 0.9102%\n",
      "Fitting for combination 16\n",
      "784\n",
      "2\n",
      "10\n",
      "[30, 40, 10]\n",
      "False\n",
      "['tanh', 'sigmoid']\n",
      "Adam\n",
      "0.03\n",
      "1\n",
      "CrossEntropyLoss\n",
      "200\n",
      "25\n",
      "0\n",
      "Epoch 1, Batch 200, Loss: 4.644046952724457\n",
      "Epoch 1, Batch 400, Loss: 4.639602246284485\n",
      "Epoch 1, Batch 600, Loss: 4.645419270992279\n",
      "Epoch 1, Batch 800, Loss: 4.645420958995819\n",
      "Epoch 2, Batch 200, Loss: 4.646112916469574\n",
      "Epoch 2, Batch 400, Loss: 4.653928446769714\n",
      "Epoch 2, Batch 600, Loss: 4.649039051532745\n",
      "Epoch 2, Batch 800, Loss: 4.6398375034332275\n",
      "Epoch 3, Batch 200, Loss: 4.639846308231354\n",
      "Epoch 3, Batch 400, Loss: 4.6441161870956424\n",
      "Epoch 3, Batch 600, Loss: 4.649348442554474\n",
      "Epoch 3, Batch 800, Loss: 4.649917166233063\n",
      "Epoch 4, Batch 200, Loss: 4.6482311892509465\n",
      "Epoch 4, Batch 400, Loss: 4.6526984691619875\n",
      "Epoch 4, Batch 600, Loss: 4.6500010538101195\n",
      "Epoch 4, Batch 800, Loss: 4.642504417896271\n",
      "Epoch 5, Batch 200, Loss: 4.658885231018067\n",
      "Epoch 5, Batch 400, Loss: 4.649301953315735\n",
      "Epoch 5, Batch 600, Loss: 4.644699611663818\n",
      "Epoch 5, Batch 800, Loss: 4.649089996814728\n",
      "Epoch 6, Batch 200, Loss: 4.649612939357757\n",
      "Epoch 6, Batch 400, Loss: 4.641589350700379\n",
      "Epoch 6, Batch 600, Loss: 4.651272230148315\n",
      "Epoch 6, Batch 800, Loss: 4.653949818611145\n",
      "Epoch 7, Batch 200, Loss: 4.639970080852509\n",
      "Epoch 7, Batch 400, Loss: 4.647438621520996\n",
      "Epoch 7, Batch 600, Loss: 4.647938539981842\n",
      "Epoch 7, Batch 800, Loss: 4.643939874172211\n",
      "Epoch 8, Batch 200, Loss: 4.643550689220429\n",
      "Epoch 8, Batch 400, Loss: 4.650490787029266\n",
      "Epoch 8, Batch 600, Loss: 4.639020934104919\n",
      "Epoch 8, Batch 800, Loss: 4.649049627780914\n",
      "Epoch 9, Batch 200, Loss: 4.649619927406311\n",
      "Epoch 9, Batch 400, Loss: 4.644291334152221\n",
      "Epoch 9, Batch 600, Loss: 4.648942723274231\n",
      "Epoch 9, Batch 800, Loss: 4.639235405921936\n",
      "Epoch 10, Batch 200, Loss: 4.639835386276245\n",
      "Epoch 10, Batch 400, Loss: 4.642977433204651\n",
      "Epoch 10, Batch 600, Loss: 4.638166556358337\n",
      "Epoch 10, Batch 800, Loss: 4.644467604160309\n",
      "Epoch 11, Batch 200, Loss: 4.6489402866363525\n",
      "Epoch 11, Batch 400, Loss: 4.64695853471756\n",
      "Epoch 11, Batch 600, Loss: 4.6433907318115235\n",
      "Epoch 11, Batch 800, Loss: 4.64209056854248\n",
      "Epoch 12, Batch 200, Loss: 4.647398021221161\n",
      "Epoch 12, Batch 400, Loss: 4.640527770519257\n",
      "Epoch 12, Batch 600, Loss: 4.652513742446899\n",
      "Epoch 12, Batch 800, Loss: 4.657883903980255\n",
      "Epoch 13, Batch 200, Loss: 4.649413352012634\n",
      "Epoch 13, Batch 400, Loss: 4.64588169336319\n",
      "Epoch 13, Batch 600, Loss: 4.647645530700683\n",
      "Epoch 13, Batch 800, Loss: 4.652714831829071\n",
      "Epoch 14, Batch 200, Loss: 4.644361340999604\n",
      "Epoch 14, Batch 400, Loss: 4.647099332809448\n",
      "Epoch 14, Batch 600, Loss: 4.635674183368683\n",
      "Epoch 14, Batch 800, Loss: 4.644728906154633\n",
      "Epoch 15, Batch 200, Loss: 4.647913987636566\n",
      "Epoch 15, Batch 400, Loss: 4.647268314361572\n",
      "Epoch 15, Batch 600, Loss: 4.653148794174195\n",
      "Epoch 15, Batch 800, Loss: 4.65108402967453\n",
      "Epoch 16, Batch 200, Loss: 4.651456551551819\n",
      "Epoch 16, Batch 400, Loss: 4.645976278781891\n",
      "Epoch 16, Batch 600, Loss: 4.64337188243866\n",
      "Epoch 16, Batch 800, Loss: 4.6492624092102055\n",
      "Epoch 17, Batch 200, Loss: 4.646491446495056\n",
      "Epoch 17, Batch 400, Loss: 4.652588717937469\n",
      "Epoch 17, Batch 600, Loss: 4.643040163516998\n",
      "Epoch 17, Batch 800, Loss: 4.654078266620636\n",
      "Epoch 18, Batch 200, Loss: 4.636163516044617\n",
      "Epoch 18, Batch 400, Loss: 4.645665838718414\n",
      "Epoch 18, Batch 600, Loss: 4.645493412017823\n",
      "Epoch 18, Batch 800, Loss: 4.634821910858154\n",
      "Epoch 19, Batch 200, Loss: 4.6508358240127565\n",
      "Epoch 19, Batch 400, Loss: 4.6513182425498965\n",
      "Epoch 19, Batch 600, Loss: 4.655090045928955\n",
      "Epoch 19, Batch 800, Loss: 4.644773287773132\n",
      "Epoch 20, Batch 200, Loss: 4.651330628395081\n",
      "Epoch 20, Batch 400, Loss: 4.6493134331703185\n",
      "Epoch 20, Batch 600, Loss: 4.642701694965362\n",
      "Epoch 20, Batch 800, Loss: 4.646605651378632\n",
      "Epoch 21, Batch 200, Loss: 4.648493790626526\n",
      "Epoch 21, Batch 400, Loss: 4.6465140414237975\n",
      "Epoch 21, Batch 600, Loss: 4.641581654548645\n",
      "Epoch 21, Batch 800, Loss: 4.6453556632995605\n",
      "Epoch 22, Batch 200, Loss: 4.640063636302948\n",
      "Epoch 22, Batch 400, Loss: 4.640924468040466\n",
      "Epoch 22, Batch 600, Loss: 4.6422875881195065\n",
      "Epoch 22, Batch 800, Loss: 4.6429203772544865\n",
      "Epoch 23, Batch 200, Loss: 4.645682063102722\n",
      "Epoch 23, Batch 400, Loss: 4.643850224018097\n",
      "Epoch 23, Batch 600, Loss: 4.6470240879058835\n",
      "Epoch 23, Batch 800, Loss: 4.6378919672966\n",
      "Epoch 24, Batch 200, Loss: 4.663545672893524\n",
      "Epoch 24, Batch 400, Loss: 4.642382354736328\n",
      "Epoch 24, Batch 600, Loss: 4.650186839103699\n",
      "Epoch 24, Batch 800, Loss: 4.646527905464172\n",
      "Epoch 25, Batch 200, Loss: 4.634645256996155\n",
      "Epoch 25, Batch 400, Loss: 4.654275231361389\n",
      "Epoch 25, Batch 600, Loss: 4.646253540515899\n",
      "Epoch 25, Batch 800, Loss: 4.651118144989014\n",
      "Accuracy on test set: 0.1032%\n",
      "Fitting for combination 17\n",
      "784\n",
      "2\n",
      "10\n",
      "[30, 40, 10]\n",
      "True\n",
      "['tanh', 'sigmoid']\n",
      "SGD\n",
      "0.01\n",
      "1\n",
      "CrossEntropyLoss\n",
      "300\n",
      "25\n",
      "0\n",
      "Epoch 1, Batch 300, Loss: 6.914804558753968\n",
      "Epoch 1, Batch 600, Loss: 6.904716765880584\n",
      "Epoch 1, Batch 900, Loss: 6.905943949222564\n",
      "Epoch 2, Batch 300, Loss: 6.904315326213837\n",
      "Epoch 2, Batch 600, Loss: 6.90561532497406\n",
      "Epoch 2, Batch 900, Loss: 6.905489330291748\n",
      "Epoch 3, Batch 300, Loss: 6.9058421206474305\n",
      "Epoch 3, Batch 600, Loss: 6.904415299892426\n",
      "Epoch 3, Batch 900, Loss: 6.905033898353577\n",
      "Epoch 4, Batch 300, Loss: 6.905574696063995\n",
      "Epoch 4, Batch 600, Loss: 6.904782512187958\n",
      "Epoch 4, Batch 900, Loss: 6.905798192024231\n",
      "Epoch 5, Batch 300, Loss: 6.904836685657501\n",
      "Epoch 5, Batch 600, Loss: 6.904912040233612\n",
      "Epoch 5, Batch 900, Loss: 6.905790014266968\n",
      "Epoch 6, Batch 300, Loss: 6.90526243686676\n",
      "Epoch 6, Batch 600, Loss: 6.90579665184021\n",
      "Epoch 6, Batch 900, Loss: 6.905404868125916\n",
      "Epoch 7, Batch 300, Loss: 6.905440738201142\n",
      "Epoch 7, Batch 600, Loss: 6.9054537510871885\n",
      "Epoch 7, Batch 900, Loss: 6.90446142911911\n",
      "Epoch 8, Batch 300, Loss: 6.90422307729721\n",
      "Epoch 8, Batch 600, Loss: 6.906293499469757\n",
      "Epoch 8, Batch 900, Loss: 6.905387868881226\n",
      "Epoch 9, Batch 300, Loss: 6.9050567555427556\n",
      "Epoch 9, Batch 600, Loss: 6.905575203895569\n",
      "Epoch 9, Batch 900, Loss: 6.905771582126618\n",
      "Epoch 10, Batch 300, Loss: 6.905082936286926\n",
      "Epoch 10, Batch 600, Loss: 6.905325658321381\n",
      "Epoch 10, Batch 900, Loss: 6.905267143249512\n",
      "Epoch 11, Batch 300, Loss: 6.906343772411346\n",
      "Epoch 11, Batch 600, Loss: 6.904212822914124\n",
      "Epoch 11, Batch 900, Loss: 6.905512697696686\n",
      "Epoch 12, Batch 300, Loss: 6.904924323558808\n",
      "Epoch 12, Batch 600, Loss: 6.905035238265992\n",
      "Epoch 12, Batch 900, Loss: 6.904800686836243\n",
      "Epoch 13, Batch 300, Loss: 6.906048254966736\n",
      "Epoch 13, Batch 600, Loss: 6.903444344997406\n",
      "Epoch 13, Batch 900, Loss: 6.905907168388366\n",
      "Epoch 14, Batch 300, Loss: 6.904196939468384\n",
      "Epoch 14, Batch 600, Loss: 6.905159502029419\n",
      "Epoch 14, Batch 900, Loss: 6.904682645797729\n",
      "Epoch 15, Batch 300, Loss: 6.905651535987854\n",
      "Epoch 15, Batch 600, Loss: 6.905744745731353\n",
      "Epoch 15, Batch 900, Loss: 6.905423491001129\n",
      "Epoch 16, Batch 300, Loss: 6.905566771030426\n",
      "Epoch 16, Batch 600, Loss: 6.904601547718048\n",
      "Epoch 16, Batch 900, Loss: 6.905515820980072\n",
      "Epoch 17, Batch 300, Loss: 6.90428375005722\n",
      "Epoch 17, Batch 600, Loss: 6.905517296791077\n",
      "Epoch 17, Batch 900, Loss: 6.9045876693725585\n",
      "Epoch 18, Batch 300, Loss: 6.905956583023071\n",
      "Epoch 18, Batch 600, Loss: 6.906084761619568\n",
      "Epoch 18, Batch 900, Loss: 6.904156067371368\n",
      "Epoch 19, Batch 300, Loss: 6.904920551776886\n",
      "Epoch 19, Batch 600, Loss: 6.905665047168732\n",
      "Epoch 19, Batch 900, Loss: 6.904537546634674\n",
      "Epoch 20, Batch 300, Loss: 6.905930395126343\n",
      "Epoch 20, Batch 600, Loss: 6.905293264389038\n",
      "Epoch 20, Batch 900, Loss: 6.905105090141296\n",
      "Epoch 21, Batch 300, Loss: 6.90564397573471\n",
      "Epoch 21, Batch 600, Loss: 6.904738676548004\n",
      "Epoch 21, Batch 900, Loss: 6.9055155491828915\n",
      "Epoch 22, Batch 300, Loss: 6.905471518039703\n",
      "Epoch 22, Batch 600, Loss: 6.9067057251930235\n",
      "Epoch 22, Batch 900, Loss: 6.903971955776215\n",
      "Epoch 23, Batch 300, Loss: 6.905265879631043\n",
      "Epoch 23, Batch 600, Loss: 6.905371618270874\n",
      "Epoch 23, Batch 900, Loss: 6.90513997554779\n",
      "Epoch 24, Batch 300, Loss: 6.903200578689575\n",
      "Epoch 24, Batch 600, Loss: 6.906050374507904\n",
      "Epoch 24, Batch 900, Loss: 6.9062377548217775\n",
      "Epoch 25, Batch 300, Loss: 6.905601255893707\n",
      "Epoch 25, Batch 600, Loss: 6.904747190475464\n",
      "Epoch 25, Batch 900, Loss: 6.905471458435058\n",
      "Accuracy on test set: 0.1135%\n",
      "Fitting for combination 18\n",
      "784\n",
      "2\n",
      "10\n",
      "[30, 50, 10]\n",
      "True\n",
      "['tanh', 'sigmoid']\n",
      "Adam\n",
      "0.01\n",
      "0.01\n",
      "CrossEntropyLoss\n",
      "200\n",
      "25\n",
      "0\n",
      "Epoch 1, Batch 200, Loss: 2.506759244799614\n",
      "Epoch 1, Batch 400, Loss: 1.593608663082123\n",
      "Epoch 1, Batch 600, Loss: 1.4550070843100549\n",
      "Epoch 1, Batch 800, Loss: 1.5280919194221496\n",
      "Epoch 2, Batch 200, Loss: 1.4452264785766602\n",
      "Epoch 2, Batch 400, Loss: 1.4028585371375084\n",
      "Epoch 2, Batch 600, Loss: 1.434096956551075\n",
      "Epoch 2, Batch 800, Loss: 1.4127710205316544\n",
      "Epoch 3, Batch 200, Loss: 1.428954954147339\n",
      "Epoch 3, Batch 400, Loss: 1.4539234545826911\n",
      "Epoch 3, Batch 600, Loss: 1.4601174256205558\n",
      "Epoch 3, Batch 800, Loss: 1.4418227216601371\n",
      "Epoch 4, Batch 200, Loss: 1.4605773264169692\n",
      "Epoch 4, Batch 400, Loss: 1.4357802340388297\n",
      "Epoch 4, Batch 600, Loss: 1.4097399488091469\n",
      "Epoch 4, Batch 800, Loss: 1.3854804801940919\n",
      "Epoch 5, Batch 200, Loss: 1.389709551036358\n",
      "Epoch 5, Batch 400, Loss: 1.4564628806710243\n",
      "Epoch 5, Batch 600, Loss: 1.3851239982247352\n",
      "Epoch 5, Batch 800, Loss: 1.4138827875256539\n",
      "Epoch 6, Batch 200, Loss: 1.406617729961872\n",
      "Epoch 6, Batch 400, Loss: 1.4293125998973846\n",
      "Epoch 6, Batch 600, Loss: 1.4134746488928795\n",
      "Epoch 6, Batch 800, Loss: 1.4045145446062088\n",
      "Epoch 7, Batch 200, Loss: 1.469581114947796\n",
      "Epoch 7, Batch 400, Loss: 1.4340674430131912\n",
      "Epoch 7, Batch 600, Loss: 1.4533154380321502\n",
      "Epoch 7, Batch 800, Loss: 1.4314249354600905\n",
      "Epoch 8, Batch 200, Loss: 1.3890107768774032\n",
      "Epoch 8, Batch 400, Loss: 1.4460242593288422\n",
      "Epoch 8, Batch 600, Loss: 1.3732310289144516\n",
      "Epoch 8, Batch 800, Loss: 1.4318307173252105\n",
      "Epoch 9, Batch 200, Loss: 1.4790528267621994\n",
      "Epoch 9, Batch 400, Loss: 1.4077741464972495\n",
      "Epoch 9, Batch 600, Loss: 1.4660909369587898\n",
      "Epoch 9, Batch 800, Loss: 1.4037672382593156\n",
      "Epoch 10, Batch 200, Loss: 1.473320302963257\n",
      "Epoch 10, Batch 400, Loss: 1.4150025644898414\n",
      "Epoch 10, Batch 600, Loss: 1.4339890253543854\n",
      "Epoch 10, Batch 800, Loss: 1.451336866915226\n",
      "Epoch 11, Batch 200, Loss: 1.391577883064747\n",
      "Epoch 11, Batch 400, Loss: 1.4223303768038749\n",
      "Epoch 11, Batch 600, Loss: 1.440656973719597\n",
      "Epoch 11, Batch 800, Loss: 1.4075985512137412\n",
      "Epoch 12, Batch 200, Loss: 1.4139378118515014\n",
      "Epoch 12, Batch 400, Loss: 1.4186217734217643\n",
      "Epoch 12, Batch 600, Loss: 1.4536233386397361\n",
      "Epoch 12, Batch 800, Loss: 1.4509572863578797\n",
      "Epoch 13, Batch 200, Loss: 1.4192055353522302\n",
      "Epoch 13, Batch 400, Loss: 1.4062877956032753\n",
      "Epoch 13, Batch 600, Loss: 1.4250992369651794\n",
      "Epoch 13, Batch 800, Loss: 1.4003080147504807\n",
      "Epoch 14, Batch 200, Loss: 1.4176854282617568\n",
      "Epoch 14, Batch 400, Loss: 1.4671328914165498\n",
      "Epoch 14, Batch 600, Loss: 1.4265810737013818\n",
      "Epoch 14, Batch 800, Loss: 1.387999795973301\n",
      "Epoch 15, Batch 200, Loss: 1.438249552845955\n",
      "Epoch 15, Batch 400, Loss: 1.4396288347244264\n",
      "Epoch 15, Batch 600, Loss: 1.409347452521324\n",
      "Epoch 15, Batch 800, Loss: 1.4443203422427178\n",
      "Epoch 16, Batch 200, Loss: 1.3927801272273064\n",
      "Epoch 16, Batch 400, Loss: 1.4705369302630424\n",
      "Epoch 16, Batch 600, Loss: 1.41839091360569\n",
      "Epoch 16, Batch 800, Loss: 1.4368122977018356\n",
      "Epoch 17, Batch 200, Loss: 1.4135372459888458\n",
      "Epoch 17, Batch 400, Loss: 1.4473876547813416\n",
      "Epoch 17, Batch 600, Loss: 1.383493170440197\n",
      "Epoch 17, Batch 800, Loss: 1.4325332799553872\n",
      "Epoch 18, Batch 200, Loss: 1.3823021560907365\n",
      "Epoch 18, Batch 400, Loss: 1.4425478726625443\n",
      "Epoch 18, Batch 600, Loss: 1.3979565444588662\n",
      "Epoch 18, Batch 800, Loss: 1.4034137436747551\n",
      "Epoch 19, Batch 200, Loss: 1.436812398135662\n",
      "Epoch 19, Batch 400, Loss: 1.4215894117951393\n",
      "Epoch 19, Batch 600, Loss: 1.410243819653988\n",
      "Epoch 19, Batch 800, Loss: 1.444722740650177\n",
      "Epoch 20, Batch 200, Loss: 1.4078887078166007\n",
      "Epoch 20, Batch 400, Loss: 1.3884868183732033\n",
      "Epoch 20, Batch 600, Loss: 1.3972815802693368\n",
      "Epoch 20, Batch 800, Loss: 1.417502546608448\n",
      "Epoch 21, Batch 200, Loss: 1.409992254972458\n",
      "Epoch 21, Batch 400, Loss: 1.478215639591217\n",
      "Epoch 21, Batch 600, Loss: 1.4235535663366319\n",
      "Epoch 21, Batch 800, Loss: 1.3953522229194641\n",
      "Epoch 22, Batch 200, Loss: 1.4113330110907554\n",
      "Epoch 22, Batch 400, Loss: 1.4934458762407303\n",
      "Epoch 22, Batch 600, Loss: 1.490024006664753\n",
      "Epoch 22, Batch 800, Loss: 1.4477476465702057\n",
      "Epoch 23, Batch 200, Loss: 1.4804473131895066\n",
      "Epoch 23, Batch 400, Loss: 1.4851077461242677\n",
      "Epoch 23, Batch 600, Loss: 1.4184406524896622\n",
      "Epoch 23, Batch 800, Loss: 1.391500744819641\n",
      "Epoch 24, Batch 200, Loss: 1.3872593623399734\n",
      "Epoch 24, Batch 400, Loss: 1.466371445953846\n",
      "Epoch 24, Batch 600, Loss: 1.4510687965154647\n",
      "Epoch 24, Batch 800, Loss: 1.4645111829042434\n",
      "Epoch 25, Batch 200, Loss: 1.4160054603219032\n",
      "Epoch 25, Batch 400, Loss: 1.3909843987226487\n",
      "Epoch 25, Batch 600, Loss: 1.4560269540548325\n",
      "Epoch 25, Batch 800, Loss: 1.4063050177693368\n",
      "Accuracy on test set: 0.8334%\n",
      "Fitting for combination 19\n",
      "784\n",
      "2\n",
      "10\n",
      "[30, 50, 10]\n",
      "True\n",
      "['tanh', 'relu']\n",
      "SGD\n",
      "0.3\n",
      "0.1\n",
      "CrossEntropyLoss\n",
      "300\n",
      "25\n",
      "0\n",
      "Epoch 1, Batch 300, Loss: 5.768118484020233\n",
      "Epoch 1, Batch 600, Loss: 5.6597468793392185\n",
      "Epoch 1, Batch 900, Loss: 5.705560661554337\n",
      "Epoch 2, Batch 300, Loss: 5.677668108940124\n",
      "Epoch 2, Batch 600, Loss: 5.694152536392212\n",
      "Epoch 2, Batch 900, Loss: 5.730341466665268\n",
      "Epoch 3, Batch 300, Loss: 5.768402833938598\n",
      "Epoch 3, Batch 600, Loss: 5.71148821592331\n",
      "Epoch 3, Batch 900, Loss: 5.743984986543655\n",
      "Epoch 4, Batch 300, Loss: 5.748915261030197\n",
      "Epoch 4, Batch 600, Loss: 5.77547562122345\n",
      "Epoch 4, Batch 900, Loss: 5.712294397354126\n",
      "Epoch 5, Batch 300, Loss: 5.630987215042114\n",
      "Epoch 5, Batch 600, Loss: 5.696876071691513\n",
      "Epoch 5, Batch 900, Loss: 5.685284739732742\n",
      "Epoch 6, Batch 300, Loss: 5.8441913080215455\n",
      "Epoch 6, Batch 600, Loss: 5.784859036207199\n",
      "Epoch 6, Batch 900, Loss: 5.702109715938568\n",
      "Epoch 7, Batch 300, Loss: 5.760375053882599\n",
      "Epoch 7, Batch 600, Loss: 5.731194096803665\n",
      "Epoch 7, Batch 900, Loss: 5.752986302375794\n",
      "Epoch 8, Batch 300, Loss: 5.757024575471878\n",
      "Epoch 8, Batch 600, Loss: 5.9329015731811525\n",
      "Epoch 8, Batch 900, Loss: 6.041411699056625\n",
      "Epoch 9, Batch 300, Loss: 6.1220982706546785\n",
      "Epoch 9, Batch 600, Loss: 6.1645662724971775\n",
      "Epoch 9, Batch 900, Loss: 6.148715238571167\n",
      "Epoch 10, Batch 300, Loss: 6.1173258149623875\n",
      "Epoch 10, Batch 600, Loss: 6.101960742473603\n",
      "Epoch 10, Batch 900, Loss: 6.129777106046677\n",
      "Epoch 11, Batch 300, Loss: 6.129036312103271\n",
      "Epoch 11, Batch 600, Loss: 6.169936919212342\n",
      "Epoch 11, Batch 900, Loss: 6.208132466077805\n",
      "Epoch 12, Batch 300, Loss: 6.189333869218826\n",
      "Epoch 12, Batch 600, Loss: 6.036418093442917\n",
      "Epoch 12, Batch 900, Loss: 6.157092914581299\n",
      "Epoch 13, Batch 300, Loss: 6.0605987966060635\n",
      "Epoch 13, Batch 600, Loss: 6.061792467832565\n",
      "Epoch 13, Batch 900, Loss: 6.170194348096848\n",
      "Epoch 14, Batch 300, Loss: 6.113969980478287\n",
      "Epoch 14, Batch 600, Loss: 6.103613097667694\n",
      "Epoch 14, Batch 900, Loss: 6.40603021144867\n",
      "Epoch 15, Batch 300, Loss: 6.155244306325913\n",
      "Epoch 15, Batch 600, Loss: 6.501829259395599\n",
      "Epoch 15, Batch 900, Loss: 6.499405442476273\n",
      "Epoch 16, Batch 300, Loss: 6.3940350484848025\n",
      "Epoch 16, Batch 600, Loss: 6.4727463471889495\n",
      "Epoch 16, Batch 900, Loss: 6.541987806558609\n",
      "Epoch 17, Batch 300, Loss: 6.429975082874298\n",
      "Epoch 17, Batch 600, Loss: 6.440244963169098\n",
      "Epoch 17, Batch 900, Loss: 6.4269517958164215\n",
      "Epoch 18, Batch 300, Loss: 6.372480525970459\n",
      "Epoch 18, Batch 600, Loss: 6.259488730430603\n",
      "Epoch 18, Batch 900, Loss: 6.275516427755356\n",
      "Epoch 19, Batch 300, Loss: 6.249551494121551\n",
      "Epoch 19, Batch 600, Loss: 6.247444615364075\n",
      "Epoch 19, Batch 900, Loss: 6.25963994383812\n",
      "Epoch 20, Batch 300, Loss: 6.285535689592361\n",
      "Epoch 20, Batch 600, Loss: 6.314718490839004\n",
      "Epoch 20, Batch 900, Loss: 6.3092697858810425\n",
      "Epoch 21, Batch 300, Loss: 6.264245263338089\n",
      "Epoch 21, Batch 600, Loss: 6.235686899423599\n",
      "Epoch 21, Batch 900, Loss: 6.324323160648346\n",
      "Epoch 22, Batch 300, Loss: 6.33605703830719\n",
      "Epoch 22, Batch 600, Loss: 6.384555503129959\n",
      "Epoch 22, Batch 900, Loss: 6.2935987317562105\n",
      "Epoch 23, Batch 300, Loss: 6.376808100938797\n",
      "Epoch 23, Batch 600, Loss: 6.320750386714935\n",
      "Epoch 23, Batch 900, Loss: 6.3242241370677945\n",
      "Epoch 24, Batch 300, Loss: 6.382334892749786\n",
      "Epoch 24, Batch 600, Loss: 6.244379510879517\n",
      "Epoch 24, Batch 900, Loss: 6.259411418437958\n",
      "Epoch 25, Batch 300, Loss: 6.308785086870193\n",
      "Epoch 25, Batch 600, Loss: 6.245093584060669\n",
      "Epoch 25, Batch 900, Loss: 6.3770582497119905\n",
      "Accuracy on test set: 0.193%\n",
      "Fitting for combination 20\n",
      "784\n",
      "3\n",
      "10\n",
      "[30, 10, 10, 10]\n",
      "True\n",
      "['tanh', 'sigmoid', 'sigmoid']\n",
      "Adam\n",
      "0.1\n",
      "1\n",
      "CrossEntropyLoss\n",
      "100\n",
      "25\n",
      "0\n",
      "Epoch 1, Batch 100, Loss: 2.3086431646347045\n",
      "Epoch 1, Batch 200, Loss: 2.3093318152427673\n",
      "Epoch 1, Batch 300, Loss: 2.3070865845680237\n",
      "Epoch 1, Batch 400, Loss: 2.308987846374512\n",
      "Epoch 1, Batch 500, Loss: 2.3100587701797486\n",
      "Epoch 1, Batch 600, Loss: 2.3088917088508607\n",
      "Epoch 1, Batch 700, Loss: 2.310110993385315\n",
      "Epoch 1, Batch 800, Loss: 2.3074409222602843\n",
      "Epoch 1, Batch 900, Loss: 2.308693392276764\n",
      "Epoch 2, Batch 100, Loss: 2.307911777496338\n",
      "Epoch 2, Batch 200, Loss: 2.309866530895233\n",
      "Epoch 2, Batch 300, Loss: 2.3084658360481263\n",
      "Epoch 2, Batch 400, Loss: 2.3106047344207763\n",
      "Epoch 2, Batch 500, Loss: 2.313373281955719\n",
      "Epoch 2, Batch 600, Loss: 2.30917927980423\n",
      "Epoch 2, Batch 700, Loss: 2.307970108985901\n",
      "Epoch 2, Batch 800, Loss: 2.310712718963623\n",
      "Epoch 2, Batch 900, Loss: 2.3087236523628234\n",
      "Epoch 3, Batch 100, Loss: 2.310185763835907\n",
      "Epoch 3, Batch 200, Loss: 2.310763382911682\n",
      "Epoch 3, Batch 300, Loss: 2.309548168182373\n",
      "Epoch 3, Batch 400, Loss: 2.3085889196395875\n",
      "Epoch 3, Batch 500, Loss: 2.309330577850342\n",
      "Epoch 3, Batch 600, Loss: 2.3090999126434326\n",
      "Epoch 3, Batch 700, Loss: 2.3096570277214052\n",
      "Epoch 3, Batch 800, Loss: 2.3093075060844424\n",
      "Epoch 3, Batch 900, Loss: 2.313772585391998\n",
      "Epoch 4, Batch 100, Loss: 2.308065254688263\n",
      "Epoch 4, Batch 200, Loss: 2.312405936717987\n",
      "Epoch 4, Batch 300, Loss: 2.309345443248749\n",
      "Epoch 4, Batch 400, Loss: 2.309416434764862\n",
      "Epoch 4, Batch 500, Loss: 2.309669327735901\n",
      "Epoch 4, Batch 600, Loss: 2.308773910999298\n",
      "Epoch 4, Batch 700, Loss: 2.310779812335968\n",
      "Epoch 4, Batch 800, Loss: 2.3131492829322813\n",
      "Epoch 4, Batch 900, Loss: 2.311510145664215\n",
      "Epoch 5, Batch 100, Loss: 2.308401334285736\n",
      "Epoch 5, Batch 200, Loss: 2.308665888309479\n",
      "Epoch 5, Batch 300, Loss: 2.308135416507721\n",
      "Epoch 5, Batch 400, Loss: 2.311528642177582\n",
      "Epoch 5, Batch 500, Loss: 2.311077935695648\n",
      "Epoch 5, Batch 600, Loss: 2.3124826550483704\n",
      "Epoch 5, Batch 700, Loss: 2.3113105058670045\n",
      "Epoch 5, Batch 800, Loss: 2.306597714424133\n",
      "Epoch 5, Batch 900, Loss: 2.309480028152466\n",
      "Epoch 6, Batch 100, Loss: 2.3094503140449523\n",
      "Epoch 6, Batch 200, Loss: 2.3115683460235594\n",
      "Epoch 6, Batch 300, Loss: 2.3099836134910583\n",
      "Epoch 6, Batch 400, Loss: 2.3127993965148925\n",
      "Epoch 6, Batch 500, Loss: 2.309614374637604\n",
      "Epoch 6, Batch 600, Loss: 2.312190091609955\n",
      "Epoch 6, Batch 700, Loss: 2.3090172743797304\n",
      "Epoch 6, Batch 800, Loss: 2.3100789070129393\n",
      "Epoch 6, Batch 900, Loss: 2.3091797709465025\n",
      "Epoch 7, Batch 100, Loss: 2.311797158718109\n",
      "Epoch 7, Batch 200, Loss: 2.3088205552101133\n",
      "Epoch 7, Batch 300, Loss: 2.308549678325653\n",
      "Epoch 7, Batch 400, Loss: 2.3091969537734984\n",
      "Epoch 7, Batch 500, Loss: 2.3124931120872496\n",
      "Epoch 7, Batch 600, Loss: 2.310291154384613\n",
      "Epoch 7, Batch 700, Loss: 2.306748046875\n",
      "Epoch 7, Batch 800, Loss: 2.3113661217689514\n",
      "Epoch 7, Batch 900, Loss: 2.310142529010773\n",
      "Epoch 8, Batch 100, Loss: 2.3110174131393433\n",
      "Epoch 8, Batch 200, Loss: 2.3098441791534423\n",
      "Epoch 8, Batch 300, Loss: 2.308363263607025\n",
      "Epoch 8, Batch 400, Loss: 2.307523539066315\n",
      "Epoch 8, Batch 500, Loss: 2.3094610500335695\n",
      "Epoch 8, Batch 600, Loss: 2.3108055710792543\n",
      "Epoch 8, Batch 700, Loss: 2.3082819962501526\n",
      "Epoch 8, Batch 800, Loss: 2.308060472011566\n",
      "Epoch 8, Batch 900, Loss: 2.312515857219696\n",
      "Epoch 9, Batch 100, Loss: 2.3092733812332153\n",
      "Epoch 9, Batch 200, Loss: 2.308273401260376\n",
      "Epoch 9, Batch 300, Loss: 2.3061451387405394\n",
      "Epoch 9, Batch 400, Loss: 2.31150128364563\n",
      "Epoch 9, Batch 500, Loss: 2.309984540939331\n",
      "Epoch 9, Batch 600, Loss: 2.307714433670044\n",
      "Epoch 9, Batch 700, Loss: 2.3114208245277403\n",
      "Epoch 9, Batch 800, Loss: 2.308207437992096\n",
      "Epoch 9, Batch 900, Loss: 2.312046229839325\n",
      "Epoch 10, Batch 100, Loss: 2.3077503657341003\n",
      "Epoch 10, Batch 200, Loss: 2.30751651763916\n",
      "Epoch 10, Batch 300, Loss: 2.3084621453285217\n",
      "Epoch 10, Batch 400, Loss: 2.31044438123703\n",
      "Epoch 10, Batch 500, Loss: 2.307589886188507\n",
      "Epoch 10, Batch 600, Loss: 2.307704792022705\n",
      "Epoch 10, Batch 700, Loss: 2.3115067505836486\n",
      "Epoch 10, Batch 800, Loss: 2.3075075793266295\n",
      "Epoch 10, Batch 900, Loss: 2.3108689904212953\n",
      "Epoch 11, Batch 100, Loss: 2.310695447921753\n",
      "Epoch 11, Batch 200, Loss: 2.308173866271973\n",
      "Epoch 11, Batch 300, Loss: 2.3100215077400206\n",
      "Epoch 11, Batch 400, Loss: 2.3132467579841616\n",
      "Epoch 11, Batch 500, Loss: 2.310403289794922\n",
      "Epoch 11, Batch 600, Loss: 2.3142576169967652\n",
      "Epoch 11, Batch 700, Loss: 2.3102227354049685\n",
      "Epoch 11, Batch 800, Loss: 2.3105684328079223\n",
      "Epoch 11, Batch 900, Loss: 2.3094227719306946\n",
      "Epoch 12, Batch 100, Loss: 2.311146173477173\n",
      "Epoch 12, Batch 200, Loss: 2.3098900151252746\n",
      "Epoch 12, Batch 300, Loss: 2.3084505009651184\n",
      "Epoch 12, Batch 400, Loss: 2.310125412940979\n",
      "Epoch 12, Batch 500, Loss: 2.31047479391098\n",
      "Epoch 12, Batch 600, Loss: 2.311237423419952\n",
      "Epoch 12, Batch 700, Loss: 2.3082332587242127\n",
      "Epoch 12, Batch 800, Loss: 2.309713509082794\n",
      "Epoch 12, Batch 900, Loss: 2.3101525020599367\n",
      "Epoch 13, Batch 100, Loss: 2.308945257663727\n",
      "Epoch 13, Batch 200, Loss: 2.3138054537773134\n",
      "Epoch 13, Batch 300, Loss: 2.309806568622589\n",
      "Epoch 13, Batch 400, Loss: 2.3077652215957642\n",
      "Epoch 13, Batch 500, Loss: 2.3079163002967835\n",
      "Epoch 13, Batch 600, Loss: 2.3116368794441224\n",
      "Epoch 13, Batch 700, Loss: 2.313721218109131\n",
      "Epoch 13, Batch 800, Loss: 2.3071130824089052\n",
      "Epoch 13, Batch 900, Loss: 2.3124872922897337\n",
      "Epoch 14, Batch 100, Loss: 2.3127862358093263\n",
      "Epoch 14, Batch 200, Loss: 2.3100528359413146\n",
      "Epoch 14, Batch 300, Loss: 2.3124564027786256\n",
      "Epoch 14, Batch 400, Loss: 2.31195280790329\n",
      "Epoch 14, Batch 500, Loss: 2.3137440299987793\n",
      "Epoch 14, Batch 600, Loss: 2.313023371696472\n",
      "Epoch 14, Batch 700, Loss: 2.3107788658142088\n",
      "Epoch 14, Batch 800, Loss: 2.306931300163269\n",
      "Epoch 14, Batch 900, Loss: 2.3087042951583863\n",
      "Epoch 15, Batch 100, Loss: 2.311590147018433\n",
      "Epoch 15, Batch 200, Loss: 2.3103057622909544\n",
      "Epoch 15, Batch 300, Loss: 2.312187728881836\n",
      "Epoch 15, Batch 400, Loss: 2.313945677280426\n",
      "Epoch 15, Batch 500, Loss: 2.3097086691856386\n",
      "Epoch 15, Batch 600, Loss: 2.309263577461243\n",
      "Epoch 15, Batch 700, Loss: 2.309016807079315\n",
      "Epoch 15, Batch 800, Loss: 2.310079345703125\n",
      "Epoch 15, Batch 900, Loss: 2.3119420218467712\n",
      "Epoch 16, Batch 100, Loss: 2.30817186832428\n",
      "Epoch 16, Batch 200, Loss: 2.3105968832969666\n",
      "Epoch 16, Batch 300, Loss: 2.308383278846741\n",
      "Epoch 16, Batch 400, Loss: 2.310701971054077\n",
      "Epoch 16, Batch 500, Loss: 2.3107209992408753\n",
      "Epoch 16, Batch 600, Loss: 2.3118387055397034\n",
      "Epoch 16, Batch 700, Loss: 2.3095681715011596\n",
      "Epoch 16, Batch 800, Loss: 2.3080464220046997\n",
      "Epoch 16, Batch 900, Loss: 2.3131061816215515\n",
      "Epoch 17, Batch 100, Loss: 2.3089124631881712\n",
      "Epoch 17, Batch 200, Loss: 2.31070170879364\n",
      "Epoch 17, Batch 300, Loss: 2.309296038150787\n",
      "Epoch 17, Batch 400, Loss: 2.308324942588806\n",
      "Epoch 17, Batch 500, Loss: 2.3097015976905824\n",
      "Epoch 17, Batch 600, Loss: 2.311089265346527\n",
      "Epoch 17, Batch 700, Loss: 2.3077318549156187\n",
      "Epoch 17, Batch 800, Loss: 2.310964274406433\n",
      "Epoch 17, Batch 900, Loss: 2.31265656709671\n",
      "Epoch 18, Batch 100, Loss: 2.3097551798820497\n",
      "Epoch 18, Batch 200, Loss: 2.3119066643714903\n",
      "Epoch 18, Batch 300, Loss: 2.3050945949554444\n",
      "Epoch 18, Batch 400, Loss: 2.307305374145508\n",
      "Epoch 18, Batch 500, Loss: 2.3099623703956604\n",
      "Epoch 18, Batch 600, Loss: 2.3091438722610476\n",
      "Epoch 18, Batch 700, Loss: 2.311561632156372\n",
      "Epoch 18, Batch 800, Loss: 2.3098071885108946\n",
      "Epoch 18, Batch 900, Loss: 2.31135023355484\n",
      "Epoch 19, Batch 100, Loss: 2.3096491956710814\n",
      "Epoch 19, Batch 200, Loss: 2.3102841925621034\n",
      "Epoch 19, Batch 300, Loss: 2.30962721824646\n",
      "Epoch 19, Batch 400, Loss: 2.3089729833602903\n",
      "Epoch 19, Batch 500, Loss: 2.3056363606452943\n",
      "Epoch 19, Batch 600, Loss: 2.3099714970588683\n",
      "Epoch 19, Batch 700, Loss: 2.31129953622818\n",
      "Epoch 19, Batch 800, Loss: 2.3083831167221067\n",
      "Epoch 19, Batch 900, Loss: 2.3115235781669616\n",
      "Epoch 20, Batch 100, Loss: 2.310536844730377\n",
      "Epoch 20, Batch 200, Loss: 2.311505205631256\n",
      "Epoch 20, Batch 300, Loss: 2.3084970903396607\n",
      "Epoch 20, Batch 400, Loss: 2.308957052230835\n",
      "Epoch 20, Batch 500, Loss: 2.310179579257965\n",
      "Epoch 20, Batch 600, Loss: 2.3121655702590944\n",
      "Epoch 20, Batch 700, Loss: 2.309152595996857\n",
      "Epoch 20, Batch 800, Loss: 2.308243658542633\n",
      "Epoch 20, Batch 900, Loss: 2.310492305755615\n",
      "Epoch 21, Batch 100, Loss: 2.309903554916382\n",
      "Epoch 21, Batch 200, Loss: 2.309464144706726\n",
      "Epoch 21, Batch 300, Loss: 2.309312176704407\n",
      "Epoch 21, Batch 400, Loss: 2.309196856021881\n",
      "Epoch 21, Batch 500, Loss: 2.3086264300346375\n",
      "Epoch 21, Batch 600, Loss: 2.3103146076202394\n",
      "Epoch 21, Batch 700, Loss: 2.3048478960990906\n",
      "Epoch 21, Batch 800, Loss: 2.310371685028076\n",
      "Epoch 21, Batch 900, Loss: 2.3116158175468446\n",
      "Epoch 22, Batch 100, Loss: 2.312419354915619\n",
      "Epoch 22, Batch 200, Loss: 2.3093636202812196\n",
      "Epoch 22, Batch 300, Loss: 2.3111096835136413\n",
      "Epoch 22, Batch 400, Loss: 2.311714241504669\n",
      "Epoch 22, Batch 500, Loss: 2.311349084377289\n",
      "Epoch 22, Batch 600, Loss: 2.31087566614151\n",
      "Epoch 22, Batch 700, Loss: 2.3106377220153806\n",
      "Epoch 22, Batch 800, Loss: 2.310062689781189\n",
      "Epoch 22, Batch 900, Loss: 2.3090055847167967\n",
      "Epoch 23, Batch 100, Loss: 2.309070553779602\n",
      "Epoch 23, Batch 200, Loss: 2.3088555216789244\n",
      "Epoch 23, Batch 300, Loss: 2.3094252276420595\n",
      "Epoch 23, Batch 400, Loss: 2.307996883392334\n",
      "Epoch 23, Batch 500, Loss: 2.3069934582710268\n",
      "Epoch 23, Batch 600, Loss: 2.3132649421691895\n",
      "Epoch 23, Batch 700, Loss: 2.312145941257477\n",
      "Epoch 23, Batch 800, Loss: 2.310684549808502\n",
      "Epoch 23, Batch 900, Loss: 2.3138030314445497\n",
      "Epoch 24, Batch 100, Loss: 2.3113219952583313\n",
      "Epoch 24, Batch 200, Loss: 2.306908915042877\n",
      "Epoch 24, Batch 300, Loss: 2.307537360191345\n",
      "Epoch 24, Batch 400, Loss: 2.3088856983184813\n",
      "Epoch 24, Batch 500, Loss: 2.310666170120239\n",
      "Epoch 24, Batch 600, Loss: 2.3107629203796387\n",
      "Epoch 24, Batch 700, Loss: 2.307338471412659\n",
      "Epoch 24, Batch 800, Loss: 2.3111498737335205\n",
      "Epoch 24, Batch 900, Loss: 2.3098843336105346\n",
      "Epoch 25, Batch 100, Loss: 2.311167483329773\n",
      "Epoch 25, Batch 200, Loss: 2.31110867023468\n",
      "Epoch 25, Batch 300, Loss: 2.310423617362976\n",
      "Epoch 25, Batch 400, Loss: 2.3083107089996338\n",
      "Epoch 25, Batch 500, Loss: 2.3088002371788026\n",
      "Epoch 25, Batch 600, Loss: 2.3088041043281553\n",
      "Epoch 25, Batch 700, Loss: 2.3097153067588807\n",
      "Epoch 25, Batch 800, Loss: 2.3112141823768617\n",
      "Epoch 25, Batch 900, Loss: 2.3137120461463927\n",
      "Accuracy on test set: 0.1135%\n",
      "Fitting for combination 21\n",
      "784\n",
      "3\n",
      "10\n",
      "[30, 10, 10, 10]\n",
      "False\n",
      "['tanh', 'sigmoid', 'sigmoid']\n",
      "SGD\n",
      "0.1\n",
      "0.01\n",
      "CrossEntropyLoss\n",
      "200\n",
      "25\n",
      "0\n",
      "Epoch 1, Batch 200, Loss: 4.605193393230438\n",
      "Epoch 1, Batch 400, Loss: 4.591338706016541\n",
      "Epoch 1, Batch 600, Loss: 4.584093170166016\n",
      "Epoch 1, Batch 800, Loss: 4.571242978572846\n",
      "Epoch 2, Batch 200, Loss: 4.427188258171082\n",
      "Epoch 2, Batch 400, Loss: 4.195230506658554\n",
      "Epoch 2, Batch 600, Loss: 4.0007454669475555\n",
      "Epoch 2, Batch 800, Loss: 3.9049443113803863\n",
      "Epoch 3, Batch 200, Loss: 3.784810793399811\n",
      "Epoch 3, Batch 400, Loss: 3.7266991293430327\n",
      "Epoch 3, Batch 600, Loss: 3.660913109779358\n",
      "Epoch 3, Batch 800, Loss: 3.617478291988373\n",
      "Epoch 4, Batch 200, Loss: 3.558539160490036\n",
      "Epoch 4, Batch 400, Loss: 3.510118052959442\n",
      "Epoch 4, Batch 600, Loss: 3.49630720615387\n",
      "Epoch 4, Batch 800, Loss: 3.479565554857254\n",
      "Epoch 5, Batch 200, Loss: 3.4694187664985656\n",
      "Epoch 5, Batch 400, Loss: 3.4580471301078797\n",
      "Epoch 5, Batch 600, Loss: 3.4519712126255033\n",
      "Epoch 5, Batch 800, Loss: 3.4376353657245637\n",
      "Epoch 6, Batch 200, Loss: 3.425180729627609\n",
      "Epoch 6, Batch 400, Loss: 3.427515341043472\n",
      "Epoch 6, Batch 600, Loss: 3.4052125990390776\n",
      "Epoch 6, Batch 800, Loss: 3.413840044736862\n",
      "Epoch 7, Batch 200, Loss: 3.3950184094905853\n",
      "Epoch 7, Batch 400, Loss: 3.405172951221466\n",
      "Epoch 7, Batch 600, Loss: 3.3872969019412995\n",
      "Epoch 7, Batch 800, Loss: 3.3776176607608797\n",
      "Epoch 8, Batch 200, Loss: 3.3802527916431426\n",
      "Epoch 8, Batch 400, Loss: 3.365935256481171\n",
      "Epoch 8, Batch 600, Loss: 3.357338538169861\n",
      "Epoch 8, Batch 800, Loss: 3.3586266374588014\n",
      "Epoch 9, Batch 200, Loss: 3.3170331287384034\n",
      "Epoch 9, Batch 400, Loss: 3.315553444623947\n",
      "Epoch 9, Batch 600, Loss: 3.291502655744553\n",
      "Epoch 9, Batch 800, Loss: 3.270170738697052\n",
      "Epoch 10, Batch 200, Loss: 3.242223033905029\n",
      "Epoch 10, Batch 400, Loss: 3.221051055192947\n",
      "Epoch 10, Batch 600, Loss: 3.2374939835071563\n",
      "Epoch 10, Batch 800, Loss: 3.205615026950836\n",
      "Epoch 11, Batch 200, Loss: 3.17211496591568\n",
      "Epoch 11, Batch 400, Loss: 3.1675359404087065\n",
      "Epoch 11, Batch 600, Loss: 3.17019061088562\n",
      "Epoch 11, Batch 800, Loss: 3.1759057331085203\n",
      "Epoch 12, Batch 200, Loss: 3.147730915546417\n",
      "Epoch 12, Batch 400, Loss: 3.149158868789673\n",
      "Epoch 12, Batch 600, Loss: 3.1496694469451905\n",
      "Epoch 12, Batch 800, Loss: 3.1315959334373473\n",
      "Epoch 13, Batch 200, Loss: 3.116756879091263\n",
      "Epoch 13, Batch 400, Loss: 3.1240953469276427\n",
      "Epoch 13, Batch 600, Loss: 3.1384271430969237\n",
      "Epoch 13, Batch 800, Loss: 3.138424639701843\n",
      "Epoch 14, Batch 200, Loss: 3.111038610935211\n",
      "Epoch 14, Batch 400, Loss: 3.1162585365772246\n",
      "Epoch 14, Batch 600, Loss: 3.1185269737243653\n",
      "Epoch 14, Batch 800, Loss: 3.1070923936367034\n",
      "Epoch 15, Batch 200, Loss: 3.1156304907798766\n",
      "Epoch 15, Batch 400, Loss: 3.1102811121940612\n",
      "Epoch 15, Batch 600, Loss: 3.11400083899498\n",
      "Epoch 15, Batch 800, Loss: 3.1011945843696593\n",
      "Epoch 16, Batch 200, Loss: 3.1200362277030944\n",
      "Epoch 16, Batch 400, Loss: 3.1022326958179476\n",
      "Epoch 16, Batch 600, Loss: 3.096869010925293\n",
      "Epoch 16, Batch 800, Loss: 3.112926857471466\n",
      "Epoch 17, Batch 200, Loss: 3.117553902864456\n",
      "Epoch 17, Batch 400, Loss: 3.0836820447444917\n",
      "Epoch 17, Batch 600, Loss: 3.0887022650241853\n",
      "Epoch 17, Batch 800, Loss: 3.092108974456787\n",
      "Epoch 18, Batch 200, Loss: 3.074254448413849\n",
      "Epoch 18, Batch 400, Loss: 3.09664075255394\n",
      "Epoch 18, Batch 600, Loss: 3.097033783197403\n",
      "Epoch 18, Batch 800, Loss: 3.099990167617798\n",
      "Epoch 19, Batch 200, Loss: 3.100963237285614\n",
      "Epoch 19, Batch 400, Loss: 3.0971107947826386\n",
      "Epoch 19, Batch 600, Loss: 3.1001075959205626\n",
      "Epoch 19, Batch 800, Loss: 3.0863607358932494\n",
      "Epoch 20, Batch 200, Loss: 3.102721209526062\n",
      "Epoch 20, Batch 400, Loss: 3.0963383972644807\n",
      "Epoch 20, Batch 600, Loss: 3.089730906486511\n",
      "Epoch 20, Batch 800, Loss: 3.088745937347412\n",
      "Epoch 21, Batch 200, Loss: 3.092146428823471\n",
      "Epoch 21, Batch 400, Loss: 3.107223850488663\n",
      "Epoch 21, Batch 600, Loss: 3.0864650321006777\n",
      "Epoch 21, Batch 800, Loss: 3.0822611117362975\n",
      "Epoch 22, Batch 200, Loss: 3.0986982369422913\n",
      "Epoch 22, Batch 400, Loss: 3.0920501470565798\n",
      "Epoch 22, Batch 600, Loss: 3.1018434166908264\n",
      "Epoch 22, Batch 800, Loss: 3.084275634288788\n",
      "Epoch 23, Batch 200, Loss: 3.090286284685135\n",
      "Epoch 23, Batch 400, Loss: 3.0846937024593353\n",
      "Epoch 23, Batch 600, Loss: 3.0766996586322786\n",
      "Epoch 23, Batch 800, Loss: 3.0900568997859956\n",
      "Epoch 24, Batch 200, Loss: 3.092713369131088\n",
      "Epoch 24, Batch 400, Loss: 3.087322577238083\n",
      "Epoch 24, Batch 600, Loss: 3.090170180797577\n",
      "Epoch 24, Batch 800, Loss: 3.0924816954135896\n",
      "Epoch 25, Batch 200, Loss: 3.0936365365982055\n",
      "Epoch 25, Batch 400, Loss: 3.084691973924637\n",
      "Epoch 25, Batch 600, Loss: 3.0833119893074037\n",
      "Epoch 25, Batch 800, Loss: 3.075328571796417\n",
      "Accuracy on test set: 0.3557%\n",
      "Fitting for combination 22\n",
      "784\n",
      "3\n",
      "10\n",
      "[30, 10, 20, 10]\n",
      "False\n",
      "['tanh', 'sigmoid', 'tanh']\n",
      "Adam\n",
      "0.01\n",
      "0.03\n",
      "CrossEntropyLoss\n",
      "100\n",
      "25\n",
      "0\n",
      "Epoch 1, Batch 100, Loss: 2.302840144634247\n",
      "Epoch 1, Batch 200, Loss: 2.3022051739692686\n",
      "Epoch 1, Batch 300, Loss: 2.301658308506012\n",
      "Epoch 1, Batch 400, Loss: 2.302257523536682\n",
      "Epoch 1, Batch 500, Loss: 2.3019770097732546\n",
      "Epoch 1, Batch 600, Loss: 2.301720213890076\n",
      "Epoch 1, Batch 700, Loss: 2.3026464700698854\n",
      "Epoch 1, Batch 800, Loss: 2.301474950313568\n",
      "Epoch 1, Batch 900, Loss: 2.3023757767677306\n",
      "Epoch 2, Batch 100, Loss: 2.301962571144104\n",
      "Epoch 2, Batch 200, Loss: 2.301835696697235\n",
      "Epoch 2, Batch 300, Loss: 2.3027302956581117\n",
      "Epoch 2, Batch 400, Loss: 2.302700629234314\n",
      "Epoch 2, Batch 500, Loss: 2.3019063806533815\n",
      "Epoch 2, Batch 600, Loss: 2.301053683757782\n",
      "Epoch 2, Batch 700, Loss: 2.3015291833877565\n",
      "Epoch 2, Batch 800, Loss: 2.301942744255066\n",
      "Epoch 2, Batch 900, Loss: 2.3039108324050903\n",
      "Epoch 3, Batch 100, Loss: 2.3020758986473084\n",
      "Epoch 3, Batch 200, Loss: 2.3032408452033994\n",
      "Epoch 3, Batch 300, Loss: 2.30201845407486\n",
      "Epoch 3, Batch 400, Loss: 2.302226469516754\n",
      "Epoch 3, Batch 500, Loss: 2.30215354681015\n",
      "Epoch 3, Batch 600, Loss: 2.3024848008155825\n",
      "Epoch 3, Batch 700, Loss: 2.3010563397407533\n",
      "Epoch 3, Batch 800, Loss: 2.300976994037628\n",
      "Epoch 3, Batch 900, Loss: 2.3019219851493835\n",
      "Epoch 4, Batch 100, Loss: 2.3030197715759275\n",
      "Epoch 4, Batch 200, Loss: 2.3021557784080504\n",
      "Epoch 4, Batch 300, Loss: 2.302239935398102\n",
      "Epoch 4, Batch 400, Loss: 2.30242125749588\n",
      "Epoch 4, Batch 500, Loss: 2.300603506565094\n",
      "Epoch 4, Batch 600, Loss: 2.302459192276001\n",
      "Epoch 4, Batch 700, Loss: 2.3023494577407835\n",
      "Epoch 4, Batch 800, Loss: 2.30141318321228\n",
      "Epoch 4, Batch 900, Loss: 2.3023264098167417\n",
      "Epoch 5, Batch 100, Loss: 2.3026707983016967\n",
      "Epoch 5, Batch 200, Loss: 2.301933317184448\n",
      "Epoch 5, Batch 300, Loss: 2.303671805858612\n",
      "Epoch 5, Batch 400, Loss: 2.301863989830017\n",
      "Epoch 5, Batch 500, Loss: 2.3018693017959593\n",
      "Epoch 5, Batch 600, Loss: 2.300611753463745\n",
      "Epoch 5, Batch 700, Loss: 2.3008622097969056\n",
      "Epoch 5, Batch 800, Loss: 2.302753839492798\n",
      "Epoch 5, Batch 900, Loss: 2.302274911403656\n",
      "Epoch 6, Batch 100, Loss: 2.3031781029701235\n",
      "Epoch 6, Batch 200, Loss: 2.3024400424957276\n",
      "Epoch 6, Batch 300, Loss: 2.3020833134651184\n",
      "Epoch 6, Batch 400, Loss: 2.303300907611847\n",
      "Epoch 6, Batch 500, Loss: 2.3024266171455383\n",
      "Epoch 6, Batch 600, Loss: 2.3018183302879334\n",
      "Epoch 6, Batch 700, Loss: 2.3018032932281494\n",
      "Epoch 6, Batch 800, Loss: 2.3014235353469847\n",
      "Epoch 6, Batch 900, Loss: 2.3011923480033873\n",
      "Epoch 7, Batch 100, Loss: 2.3019456696510314\n",
      "Epoch 7, Batch 200, Loss: 2.302225716114044\n",
      "Epoch 7, Batch 300, Loss: 2.301157224178314\n",
      "Epoch 7, Batch 400, Loss: 2.302585678100586\n",
      "Epoch 7, Batch 500, Loss: 2.3026695680618285\n",
      "Epoch 7, Batch 600, Loss: 2.3026613068580626\n",
      "Epoch 7, Batch 700, Loss: 2.301984419822693\n",
      "Epoch 7, Batch 800, Loss: 2.302282247543335\n",
      "Epoch 7, Batch 900, Loss: 2.3020829272270205\n",
      "Epoch 8, Batch 100, Loss: 2.302634053230286\n",
      "Epoch 8, Batch 200, Loss: 2.30256263256073\n",
      "Epoch 8, Batch 300, Loss: 2.3017499351501467\n",
      "Epoch 8, Batch 400, Loss: 2.30248245716095\n",
      "Epoch 8, Batch 500, Loss: 2.302257289886475\n",
      "Epoch 8, Batch 600, Loss: 2.302803621292114\n",
      "Epoch 8, Batch 700, Loss: 2.302557294368744\n",
      "Epoch 8, Batch 800, Loss: 2.3027613043785093\n",
      "Epoch 8, Batch 900, Loss: 2.3020630288124084\n",
      "Epoch 9, Batch 100, Loss: 2.3018159580230715\n",
      "Epoch 9, Batch 200, Loss: 2.3028936052322386\n",
      "Epoch 9, Batch 300, Loss: 2.3027430820465087\n",
      "Epoch 9, Batch 400, Loss: 2.3002257227897642\n",
      "Epoch 9, Batch 500, Loss: 2.300933849811554\n",
      "Epoch 9, Batch 600, Loss: 2.301594178676605\n",
      "Epoch 9, Batch 700, Loss: 2.301948652267456\n",
      "Epoch 9, Batch 800, Loss: 2.302418580055237\n",
      "Epoch 9, Batch 900, Loss: 2.303276629447937\n",
      "Epoch 10, Batch 100, Loss: 2.3024090194702147\n",
      "Epoch 10, Batch 200, Loss: 2.302235713005066\n",
      "Epoch 10, Batch 300, Loss: 2.3017852759361266\n",
      "Epoch 10, Batch 400, Loss: 2.3011239337921143\n",
      "Epoch 10, Batch 500, Loss: 2.3027100729942322\n",
      "Epoch 10, Batch 600, Loss: 2.3019285798072815\n",
      "Epoch 10, Batch 700, Loss: 2.3018079948425294\n",
      "Epoch 10, Batch 800, Loss: 2.3026672124862673\n",
      "Epoch 10, Batch 900, Loss: 2.3018872427940367\n",
      "Epoch 11, Batch 100, Loss: 2.3024048137664797\n",
      "Epoch 11, Batch 200, Loss: 2.3012436628341675\n",
      "Epoch 11, Batch 300, Loss: 2.301484272480011\n",
      "Epoch 11, Batch 400, Loss: 2.3015634536743166\n",
      "Epoch 11, Batch 500, Loss: 2.302977912425995\n",
      "Epoch 11, Batch 600, Loss: 2.301841289997101\n",
      "Epoch 11, Batch 700, Loss: 2.302269036769867\n",
      "Epoch 11, Batch 800, Loss: 2.3016735672950746\n",
      "Epoch 11, Batch 900, Loss: 2.302983283996582\n",
      "Epoch 12, Batch 100, Loss: 2.3015240478515624\n",
      "Epoch 12, Batch 200, Loss: 2.3015951251983644\n",
      "Epoch 12, Batch 300, Loss: 2.3032410645484926\n",
      "Epoch 12, Batch 400, Loss: 2.3024318981170655\n",
      "Epoch 12, Batch 500, Loss: 2.302278609275818\n",
      "Epoch 12, Batch 600, Loss: 2.301864583492279\n",
      "Epoch 12, Batch 700, Loss: 2.3024312472343444\n",
      "Epoch 12, Batch 800, Loss: 2.302398965358734\n",
      "Epoch 12, Batch 900, Loss: 2.302396490573883\n",
      "Epoch 13, Batch 100, Loss: 2.302360169887543\n",
      "Epoch 13, Batch 200, Loss: 2.3027012014389037\n",
      "Epoch 13, Batch 300, Loss: 2.3021658444404602\n",
      "Epoch 13, Batch 400, Loss: 2.302691717147827\n",
      "Epoch 13, Batch 500, Loss: 2.302554683685303\n",
      "Epoch 13, Batch 600, Loss: 2.3024053502082826\n",
      "Epoch 13, Batch 700, Loss: 2.301645431518555\n",
      "Epoch 13, Batch 800, Loss: 2.3008285188674926\n",
      "Epoch 13, Batch 900, Loss: 2.3030096316337585\n",
      "Epoch 14, Batch 100, Loss: 2.3026123666763305\n",
      "Epoch 14, Batch 200, Loss: 2.3021309399604797\n",
      "Epoch 14, Batch 300, Loss: 2.3019381523132325\n",
      "Epoch 14, Batch 400, Loss: 2.3022380447387696\n",
      "Epoch 14, Batch 500, Loss: 2.3023582768440245\n",
      "Epoch 14, Batch 600, Loss: 2.301533291339874\n",
      "Epoch 14, Batch 700, Loss: 2.3014456701278685\n",
      "Epoch 14, Batch 800, Loss: 2.3015479230880738\n",
      "Epoch 14, Batch 900, Loss: 2.302758367061615\n",
      "Epoch 15, Batch 100, Loss: 2.3016501593589784\n",
      "Epoch 15, Batch 200, Loss: 2.301761085987091\n",
      "Epoch 15, Batch 300, Loss: 2.3022123432159423\n",
      "Epoch 15, Batch 400, Loss: 2.302464232444763\n",
      "Epoch 15, Batch 500, Loss: 2.301603889465332\n",
      "Epoch 15, Batch 600, Loss: 2.3021724343299867\n",
      "Epoch 15, Batch 700, Loss: 2.301662769317627\n",
      "Epoch 15, Batch 800, Loss: 2.3030445909500123\n",
      "Epoch 15, Batch 900, Loss: 2.3027754640579223\n",
      "Epoch 16, Batch 100, Loss: 2.302360737323761\n",
      "Epoch 16, Batch 200, Loss: 2.3015558695793152\n",
      "Epoch 16, Batch 300, Loss: 2.303182911872864\n",
      "Epoch 16, Batch 400, Loss: 2.3025049710273744\n",
      "Epoch 16, Batch 500, Loss: 2.3008923864364625\n",
      "Epoch 16, Batch 600, Loss: 2.3011057996749877\n",
      "Epoch 16, Batch 700, Loss: 2.301613972187042\n",
      "Epoch 16, Batch 800, Loss: 2.3024296307563783\n",
      "Epoch 16, Batch 900, Loss: 2.3033329248428345\n",
      "Epoch 17, Batch 100, Loss: 2.3027081799507143\n",
      "Epoch 17, Batch 200, Loss: 2.3022533226013184\n",
      "Epoch 17, Batch 300, Loss: 2.302290153503418\n",
      "Epoch 17, Batch 400, Loss: 2.304207670688629\n",
      "Epoch 17, Batch 500, Loss: 2.302809643745422\n",
      "Epoch 17, Batch 600, Loss: 2.302931807041168\n",
      "Epoch 17, Batch 700, Loss: 2.302652404308319\n",
      "Epoch 17, Batch 800, Loss: 2.3015112352371214\n",
      "Epoch 17, Batch 900, Loss: 2.3023909282684327\n",
      "Epoch 18, Batch 100, Loss: 2.30181529045105\n",
      "Epoch 18, Batch 200, Loss: 2.3014340019226074\n",
      "Epoch 18, Batch 300, Loss: 2.3014971446990966\n",
      "Epoch 18, Batch 400, Loss: 2.3016109919548033\n",
      "Epoch 18, Batch 500, Loss: 2.3025102972984315\n",
      "Epoch 18, Batch 600, Loss: 2.302304439544678\n",
      "Epoch 18, Batch 700, Loss: 2.3027596712112426\n",
      "Epoch 18, Batch 800, Loss: 2.3025344824790954\n",
      "Epoch 18, Batch 900, Loss: 2.3022184276580813\n",
      "Epoch 19, Batch 100, Loss: 2.3020672154426576\n",
      "Epoch 19, Batch 200, Loss: 2.30119482755661\n",
      "Epoch 19, Batch 300, Loss: 2.3023566436767577\n",
      "Epoch 19, Batch 400, Loss: 2.302597529888153\n",
      "Epoch 19, Batch 500, Loss: 2.30271861076355\n",
      "Epoch 19, Batch 600, Loss: 2.301548616886139\n",
      "Epoch 19, Batch 700, Loss: 2.3023999905586243\n",
      "Epoch 19, Batch 800, Loss: 2.3025783610343935\n",
      "Epoch 19, Batch 900, Loss: 2.3011091446876524\n",
      "Epoch 20, Batch 100, Loss: 2.3031100702285765\n",
      "Epoch 20, Batch 200, Loss: 2.301851735115051\n",
      "Epoch 20, Batch 300, Loss: 2.3025904417037966\n",
      "Epoch 20, Batch 400, Loss: 2.3017264771461488\n",
      "Epoch 20, Batch 500, Loss: 2.3022468519210815\n",
      "Epoch 20, Batch 600, Loss: 2.3015618252754213\n",
      "Epoch 20, Batch 700, Loss: 2.302746798992157\n",
      "Epoch 20, Batch 800, Loss: 2.3019756412506105\n",
      "Epoch 20, Batch 900, Loss: 2.30283252954483\n",
      "Epoch 21, Batch 100, Loss: 2.3018694424629214\n",
      "Epoch 21, Batch 200, Loss: 2.3018781518936158\n",
      "Epoch 21, Batch 300, Loss: 2.302328290939331\n",
      "Epoch 21, Batch 400, Loss: 2.3017868566513062\n",
      "Epoch 21, Batch 500, Loss: 2.3021336030960082\n",
      "Epoch 21, Batch 600, Loss: 2.30197185754776\n",
      "Epoch 21, Batch 700, Loss: 2.3026561450958254\n",
      "Epoch 21, Batch 800, Loss: 2.302696306705475\n",
      "Epoch 21, Batch 900, Loss: 2.3018665170669554\n",
      "Epoch 22, Batch 100, Loss: 2.302436351776123\n",
      "Epoch 22, Batch 200, Loss: 2.302008242607117\n",
      "Epoch 22, Batch 300, Loss: 2.3027540111541747\n",
      "Epoch 22, Batch 400, Loss: 2.303440365791321\n",
      "Epoch 22, Batch 500, Loss: 2.3019241762161253\n",
      "Epoch 22, Batch 600, Loss: 2.3024225068092345\n",
      "Epoch 22, Batch 700, Loss: 2.302262558937073\n",
      "Epoch 22, Batch 800, Loss: 2.3021008014678954\n",
      "Epoch 22, Batch 900, Loss: 2.3014388346672057\n",
      "Epoch 23, Batch 100, Loss: 2.303362400531769\n",
      "Epoch 23, Batch 200, Loss: 2.301269221305847\n",
      "Epoch 23, Batch 300, Loss: 2.302005445957184\n",
      "Epoch 23, Batch 400, Loss: 2.3010137462615967\n",
      "Epoch 23, Batch 500, Loss: 2.302396547794342\n",
      "Epoch 23, Batch 600, Loss: 2.302075595855713\n",
      "Epoch 23, Batch 700, Loss: 2.3022414517402647\n",
      "Epoch 23, Batch 800, Loss: 2.301641411781311\n",
      "Epoch 23, Batch 900, Loss: 2.301822831630707\n",
      "Epoch 24, Batch 100, Loss: 2.302221052646637\n",
      "Epoch 24, Batch 200, Loss: 2.3035539269447325\n",
      "Epoch 24, Batch 300, Loss: 2.3027671480178835\n",
      "Epoch 24, Batch 400, Loss: 2.301464173793793\n",
      "Epoch 24, Batch 500, Loss: 2.300842435359955\n",
      "Epoch 24, Batch 600, Loss: 2.3027070999145507\n",
      "Epoch 24, Batch 700, Loss: 2.302123603820801\n",
      "Epoch 24, Batch 800, Loss: 2.3028735542297363\n",
      "Epoch 24, Batch 900, Loss: 2.303000636100769\n",
      "Epoch 25, Batch 100, Loss: 2.3013318181037903\n",
      "Epoch 25, Batch 200, Loss: 2.3025362920761108\n",
      "Epoch 25, Batch 300, Loss: 2.3017229318618773\n",
      "Epoch 25, Batch 400, Loss: 2.300887260437012\n",
      "Epoch 25, Batch 500, Loss: 2.302295396327972\n",
      "Epoch 25, Batch 600, Loss: 2.3027713298797607\n",
      "Epoch 25, Batch 700, Loss: 2.3012189245224\n",
      "Epoch 25, Batch 800, Loss: 2.3021677136421204\n",
      "Epoch 25, Batch 900, Loss: 2.302178616523743\n",
      "Accuracy on test set: 0.1028%\n",
      "Fitting for combination 23\n",
      "784\n",
      "3\n",
      "10\n",
      "[30, 10, 20, 10]\n",
      "False\n",
      "['tanh', 'sigmoid', 'sigmoid']\n",
      "SGD\n",
      "0.03\n",
      "0.01\n",
      "CrossEntropyLoss\n",
      "100\n",
      "25\n",
      "0\n",
      "Epoch 1, Batch 100, Loss: 2.3183868026733396\n",
      "Epoch 1, Batch 200, Loss: 2.3007091212272646\n",
      "Epoch 1, Batch 300, Loss: 2.2984277987480164\n",
      "Epoch 1, Batch 400, Loss: 2.2977064275741577\n",
      "Epoch 1, Batch 500, Loss: 2.2968812155723572\n",
      "Epoch 1, Batch 600, Loss: 2.29580491065979\n",
      "Epoch 1, Batch 700, Loss: 2.2964913058280945\n",
      "Epoch 1, Batch 800, Loss: 2.295166218280792\n",
      "Epoch 1, Batch 900, Loss: 2.2956497144699095\n",
      "Epoch 2, Batch 100, Loss: 2.2936977458000185\n",
      "Epoch 2, Batch 200, Loss: 2.2932592678070067\n",
      "Epoch 2, Batch 300, Loss: 2.2924534344673155\n",
      "Epoch 2, Batch 400, Loss: 2.291722445487976\n",
      "Epoch 2, Batch 500, Loss: 2.2908953189849854\n",
      "Epoch 2, Batch 600, Loss: 2.289009039402008\n",
      "Epoch 2, Batch 700, Loss: 2.286694722175598\n",
      "Epoch 2, Batch 800, Loss: 2.284259948730469\n",
      "Epoch 2, Batch 900, Loss: 2.2801926851272585\n",
      "Epoch 3, Batch 100, Loss: 2.2778785371780397\n",
      "Epoch 3, Batch 200, Loss: 2.27223895072937\n",
      "Epoch 3, Batch 300, Loss: 2.267495517730713\n",
      "Epoch 3, Batch 400, Loss: 2.2597000074386595\n",
      "Epoch 3, Batch 500, Loss: 2.2485253691673277\n",
      "Epoch 3, Batch 600, Loss: 2.233610315322876\n",
      "Epoch 3, Batch 700, Loss: 2.2184441614151003\n",
      "Epoch 3, Batch 800, Loss: 2.191648588180542\n",
      "Epoch 3, Batch 900, Loss: 2.166966190338135\n",
      "Epoch 4, Batch 100, Loss: 2.1225802588462828\n",
      "Epoch 4, Batch 200, Loss: 2.085650870800018\n",
      "Epoch 4, Batch 300, Loss: 2.049329929351807\n",
      "Epoch 4, Batch 400, Loss: 2.0073175287246703\n",
      "Epoch 4, Batch 500, Loss: 1.9823073065280914\n",
      "Epoch 4, Batch 600, Loss: 1.9514769613742828\n",
      "Epoch 4, Batch 700, Loss: 1.933523851633072\n",
      "Epoch 4, Batch 800, Loss: 1.9049179100990294\n",
      "Epoch 4, Batch 900, Loss: 1.8853892862796784\n",
      "Epoch 5, Batch 100, Loss: 1.8735515534877778\n",
      "Epoch 5, Batch 200, Loss: 1.8583131778240203\n",
      "Epoch 5, Batch 300, Loss: 1.849182698726654\n",
      "Epoch 5, Batch 400, Loss: 1.8379936254024505\n",
      "Epoch 5, Batch 500, Loss: 1.8269075977802276\n",
      "Epoch 5, Batch 600, Loss: 1.8208922839164734\n",
      "Epoch 5, Batch 700, Loss: 1.823729668855667\n",
      "Epoch 5, Batch 800, Loss: 1.8169566905498504\n",
      "Epoch 5, Batch 900, Loss: 1.8092106306552886\n",
      "Epoch 6, Batch 100, Loss: 1.8044557869434357\n",
      "Epoch 6, Batch 200, Loss: 1.8085836243629456\n",
      "Epoch 6, Batch 300, Loss: 1.8019064807891845\n",
      "Epoch 6, Batch 400, Loss: 1.8013923847675324\n",
      "Epoch 6, Batch 500, Loss: 1.8007944023609161\n",
      "Epoch 6, Batch 600, Loss: 1.8016922247409821\n",
      "Epoch 6, Batch 700, Loss: 1.7845274007320404\n",
      "Epoch 6, Batch 800, Loss: 1.78952481508255\n",
      "Epoch 6, Batch 900, Loss: 1.7904217207431794\n",
      "Epoch 7, Batch 100, Loss: 1.785205798149109\n",
      "Epoch 7, Batch 200, Loss: 1.78647447347641\n",
      "Epoch 7, Batch 300, Loss: 1.7952863073349\n",
      "Epoch 7, Batch 400, Loss: 1.7887241220474244\n",
      "Epoch 7, Batch 500, Loss: 1.7763328385353088\n",
      "Epoch 7, Batch 600, Loss: 1.7903616273403167\n",
      "Epoch 7, Batch 700, Loss: 1.7784915113449096\n",
      "Epoch 7, Batch 800, Loss: 1.7731400740146637\n",
      "Epoch 7, Batch 900, Loss: 1.7789198231697083\n",
      "Epoch 8, Batch 100, Loss: 1.775215082168579\n",
      "Epoch 8, Batch 200, Loss: 1.7795032095909118\n",
      "Epoch 8, Batch 300, Loss: 1.7648756980895997\n",
      "Epoch 8, Batch 400, Loss: 1.7674828958511353\n",
      "Epoch 8, Batch 500, Loss: 1.778663364648819\n",
      "Epoch 8, Batch 600, Loss: 1.7682176554203033\n",
      "Epoch 8, Batch 700, Loss: 1.7735653913021088\n",
      "Epoch 8, Batch 800, Loss: 1.7696446120738982\n",
      "Epoch 8, Batch 900, Loss: 1.7683515524864197\n",
      "Epoch 9, Batch 100, Loss: 1.761977756023407\n",
      "Epoch 9, Batch 200, Loss: 1.7606635308265686\n",
      "Epoch 9, Batch 300, Loss: 1.770193363428116\n",
      "Epoch 9, Batch 400, Loss: 1.7579842138290405\n",
      "Epoch 9, Batch 500, Loss: 1.7583316671848297\n",
      "Epoch 9, Batch 600, Loss: 1.7557143485546112\n",
      "Epoch 9, Batch 700, Loss: 1.7645756196975708\n",
      "Epoch 9, Batch 800, Loss: 1.7611144030094146\n",
      "Epoch 9, Batch 900, Loss: 1.750183732509613\n",
      "Epoch 10, Batch 100, Loss: 1.748353921175003\n",
      "Epoch 10, Batch 200, Loss: 1.74130153298378\n",
      "Epoch 10, Batch 300, Loss: 1.751695761680603\n",
      "Epoch 10, Batch 400, Loss: 1.7542358696460725\n",
      "Epoch 10, Batch 500, Loss: 1.7472159337997437\n",
      "Epoch 10, Batch 600, Loss: 1.745253312587738\n",
      "Epoch 10, Batch 700, Loss: 1.7402975690364837\n",
      "Epoch 10, Batch 800, Loss: 1.737087072134018\n",
      "Epoch 10, Batch 900, Loss: 1.7456248331069946\n",
      "Epoch 11, Batch 100, Loss: 1.7397641050815582\n",
      "Epoch 11, Batch 200, Loss: 1.7262963807582856\n",
      "Epoch 11, Batch 300, Loss: 1.7299909400939941\n",
      "Epoch 11, Batch 400, Loss: 1.7304251205921173\n",
      "Epoch 11, Batch 500, Loss: 1.7262313497066497\n",
      "Epoch 11, Batch 600, Loss: 1.7296382141113282\n",
      "Epoch 11, Batch 700, Loss: 1.7193739938735961\n",
      "Epoch 11, Batch 800, Loss: 1.7286116790771484\n",
      "Epoch 11, Batch 900, Loss: 1.7187084352970123\n",
      "Epoch 12, Batch 100, Loss: 1.7167820131778717\n",
      "Epoch 12, Batch 200, Loss: 1.7165408837795257\n",
      "Epoch 12, Batch 300, Loss: 1.7127887415885925\n",
      "Epoch 12, Batch 400, Loss: 1.7176299250125886\n",
      "Epoch 12, Batch 500, Loss: 1.7104706966876984\n",
      "Epoch 12, Batch 600, Loss: 1.7039581179618835\n",
      "Epoch 12, Batch 700, Loss: 1.6951788926124574\n",
      "Epoch 12, Batch 800, Loss: 1.7045802640914918\n",
      "Epoch 12, Batch 900, Loss: 1.7015118932723998\n",
      "Epoch 13, Batch 100, Loss: 1.6896562683582306\n",
      "Epoch 13, Batch 200, Loss: 1.689383203983307\n",
      "Epoch 13, Batch 300, Loss: 1.6932006072998047\n",
      "Epoch 13, Batch 400, Loss: 1.697442628145218\n",
      "Epoch 13, Batch 500, Loss: 1.6937807703018188\n",
      "Epoch 13, Batch 600, Loss: 1.690113695859909\n",
      "Epoch 13, Batch 700, Loss: 1.6890639078617096\n",
      "Epoch 13, Batch 800, Loss: 1.6794559729099274\n",
      "Epoch 13, Batch 900, Loss: 1.6771283066272735\n",
      "Epoch 14, Batch 100, Loss: 1.6809441673755645\n",
      "Epoch 14, Batch 200, Loss: 1.6826575458049775\n",
      "Epoch 14, Batch 300, Loss: 1.6648339903354645\n",
      "Epoch 14, Batch 400, Loss: 1.6713146948814392\n",
      "Epoch 14, Batch 500, Loss: 1.6766664326190948\n",
      "Epoch 14, Batch 600, Loss: 1.676179084777832\n",
      "Epoch 14, Batch 700, Loss: 1.6662241780757905\n",
      "Epoch 14, Batch 800, Loss: 1.661888244152069\n",
      "Epoch 14, Batch 900, Loss: 1.6616914367675781\n",
      "Epoch 15, Batch 100, Loss: 1.6644379234313964\n",
      "Epoch 15, Batch 200, Loss: 1.6624250161647796\n",
      "Epoch 15, Batch 300, Loss: 1.657833250761032\n",
      "Epoch 15, Batch 400, Loss: 1.6569602906703949\n",
      "Epoch 15, Batch 500, Loss: 1.6544708216190338\n",
      "Epoch 15, Batch 600, Loss: 1.65592871427536\n",
      "Epoch 15, Batch 700, Loss: 1.654013328552246\n",
      "Epoch 15, Batch 800, Loss: 1.6439652824401856\n",
      "Epoch 15, Batch 900, Loss: 1.6482999086380006\n",
      "Epoch 16, Batch 100, Loss: 1.6493996620178222\n",
      "Epoch 16, Batch 200, Loss: 1.6523437917232513\n",
      "Epoch 16, Batch 300, Loss: 1.6428587865829467\n",
      "Epoch 16, Batch 400, Loss: 1.6533301615715026\n",
      "Epoch 16, Batch 500, Loss: 1.6469400095939637\n",
      "Epoch 16, Batch 600, Loss: 1.6410283398628236\n",
      "Epoch 16, Batch 700, Loss: 1.642547447681427\n",
      "Epoch 16, Batch 800, Loss: 1.6344071459770202\n",
      "Epoch 16, Batch 900, Loss: 1.640808243751526\n",
      "Epoch 17, Batch 100, Loss: 1.6481673312187195\n",
      "Epoch 17, Batch 200, Loss: 1.6420844757556916\n",
      "Epoch 17, Batch 300, Loss: 1.6322674143314362\n",
      "Epoch 17, Batch 400, Loss: 1.638519104719162\n",
      "Epoch 17, Batch 500, Loss: 1.6315980339050293\n",
      "Epoch 17, Batch 600, Loss: 1.6339913940429687\n",
      "Epoch 17, Batch 700, Loss: 1.6427502310276032\n",
      "Epoch 17, Batch 800, Loss: 1.63355859041214\n",
      "Epoch 17, Batch 900, Loss: 1.6280210888385773\n",
      "Epoch 18, Batch 100, Loss: 1.6341128492355346\n",
      "Epoch 18, Batch 200, Loss: 1.6319355416297912\n",
      "Epoch 18, Batch 300, Loss: 1.6313924086093903\n",
      "Epoch 18, Batch 400, Loss: 1.6337575614452362\n",
      "Epoch 18, Batch 500, Loss: 1.6362864768505097\n",
      "Epoch 18, Batch 600, Loss: 1.6330303525924683\n",
      "Epoch 18, Batch 700, Loss: 1.629926736354828\n",
      "Epoch 18, Batch 800, Loss: 1.6276091432571411\n",
      "Epoch 18, Batch 900, Loss: 1.621965891122818\n",
      "Epoch 19, Batch 100, Loss: 1.6264466679096221\n",
      "Epoch 19, Batch 200, Loss: 1.6236298608779907\n",
      "Epoch 19, Batch 300, Loss: 1.626653869152069\n",
      "Epoch 19, Batch 400, Loss: 1.6244610261917114\n",
      "Epoch 19, Batch 500, Loss: 1.6235811877250672\n",
      "Epoch 19, Batch 600, Loss: 1.6206993448734284\n",
      "Epoch 19, Batch 700, Loss: 1.632618646621704\n",
      "Epoch 19, Batch 800, Loss: 1.6151780450344086\n",
      "Epoch 19, Batch 900, Loss: 1.6201912403106689\n",
      "Epoch 20, Batch 100, Loss: 1.6182868158817292\n",
      "Epoch 20, Batch 200, Loss: 1.6237203288078308\n",
      "Epoch 20, Batch 300, Loss: 1.6204441463947297\n",
      "Epoch 20, Batch 400, Loss: 1.614495713710785\n",
      "Epoch 20, Batch 500, Loss: 1.617969069480896\n",
      "Epoch 20, Batch 600, Loss: 1.614957444667816\n",
      "Epoch 20, Batch 700, Loss: 1.6252031910419464\n",
      "Epoch 20, Batch 800, Loss: 1.610830466747284\n",
      "Epoch 20, Batch 900, Loss: 1.6196950685977936\n",
      "Epoch 21, Batch 100, Loss: 1.615372461080551\n",
      "Epoch 21, Batch 200, Loss: 1.611829663515091\n",
      "Epoch 21, Batch 300, Loss: 1.6147950625419616\n",
      "Epoch 21, Batch 400, Loss: 1.6125330352783203\n",
      "Epoch 21, Batch 500, Loss: 1.6235610401630403\n",
      "Epoch 21, Batch 600, Loss: 1.611869704723358\n",
      "Epoch 21, Batch 700, Loss: 1.6209945356845856\n",
      "Epoch 21, Batch 800, Loss: 1.607315607070923\n",
      "Epoch 21, Batch 900, Loss: 1.6077916204929352\n",
      "Epoch 22, Batch 100, Loss: 1.6121285891532897\n",
      "Epoch 22, Batch 200, Loss: 1.6148624479770661\n",
      "Epoch 22, Batch 300, Loss: 1.6200567519664764\n",
      "Epoch 22, Batch 400, Loss: 1.609168518781662\n",
      "Epoch 22, Batch 500, Loss: 1.6012812399864196\n",
      "Epoch 22, Batch 600, Loss: 1.6081697130203247\n",
      "Epoch 22, Batch 700, Loss: 1.6096498417854308\n",
      "Epoch 22, Batch 800, Loss: 1.615877022743225\n",
      "Epoch 22, Batch 900, Loss: 1.6040023720264436\n",
      "Epoch 23, Batch 100, Loss: 1.6032906723022462\n",
      "Epoch 23, Batch 200, Loss: 1.6073848247528075\n",
      "Epoch 23, Batch 300, Loss: 1.61110378742218\n",
      "Epoch 23, Batch 400, Loss: 1.604828917980194\n",
      "Epoch 23, Batch 500, Loss: 1.6080109655857087\n",
      "Epoch 23, Batch 600, Loss: 1.6099554419517517\n",
      "Epoch 23, Batch 700, Loss: 1.6106661248207093\n",
      "Epoch 23, Batch 800, Loss: 1.6051822984218598\n",
      "Epoch 23, Batch 900, Loss: 1.60738560795784\n",
      "Epoch 24, Batch 100, Loss: 1.595943080186844\n",
      "Epoch 24, Batch 200, Loss: 1.601049200296402\n",
      "Epoch 24, Batch 300, Loss: 1.6073548185825348\n",
      "Epoch 24, Batch 400, Loss: 1.6068648755550385\n",
      "Epoch 24, Batch 500, Loss: 1.603904927968979\n",
      "Epoch 24, Batch 600, Loss: 1.6071052968502044\n",
      "Epoch 24, Batch 700, Loss: 1.5960579824447632\n",
      "Epoch 24, Batch 800, Loss: 1.6122879123687743\n",
      "Epoch 24, Batch 900, Loss: 1.5956749439239502\n",
      "Epoch 25, Batch 100, Loss: 1.5970017731189727\n",
      "Epoch 25, Batch 200, Loss: 1.604573930501938\n",
      "Epoch 25, Batch 300, Loss: 1.6012842094898223\n",
      "Epoch 25, Batch 400, Loss: 1.5939201927185058\n",
      "Epoch 25, Batch 500, Loss: 1.5990268433094024\n",
      "Epoch 25, Batch 600, Loss: 1.6103610599040985\n",
      "Epoch 25, Batch 700, Loss: 1.6009455585479737\n",
      "Epoch 25, Batch 800, Loss: 1.593425793647766\n",
      "Epoch 25, Batch 900, Loss: 1.5891340446472169\n",
      "Accuracy on test set: 0.3742%\n",
      "Fitting for combination 24\n",
      "784\n",
      "3\n",
      "10\n",
      "[30, 10, 30, 10]\n",
      "False\n",
      "['tanh', 'sigmoid', 'tanh']\n",
      "Adam\n",
      "0.1\n",
      "1\n",
      "CrossEntropyLoss\n",
      "100\n",
      "25\n",
      "0\n",
      "Epoch 1, Batch 100, Loss: 2.3025087213516233\n",
      "Epoch 1, Batch 200, Loss: 2.303448600769043\n",
      "Epoch 1, Batch 300, Loss: 2.30269216299057\n",
      "Epoch 1, Batch 400, Loss: 2.3035107541084288\n",
      "Epoch 1, Batch 500, Loss: 2.3025732111930846\n",
      "Epoch 1, Batch 600, Loss: 2.3036481523513794\n",
      "Epoch 1, Batch 700, Loss: 2.3029949116706847\n",
      "Epoch 1, Batch 800, Loss: 2.3027398085594175\n",
      "Epoch 1, Batch 900, Loss: 2.3021255850791933\n",
      "Epoch 2, Batch 100, Loss: 2.3027161169052124\n",
      "Epoch 2, Batch 200, Loss: 2.3035838651657103\n",
      "Epoch 2, Batch 300, Loss: 2.302613353729248\n",
      "Epoch 2, Batch 400, Loss: 2.303464229106903\n",
      "Epoch 2, Batch 500, Loss: 2.303061096668243\n",
      "Epoch 2, Batch 600, Loss: 2.3035375809669496\n",
      "Epoch 2, Batch 700, Loss: 2.3030103063583374\n",
      "Epoch 2, Batch 800, Loss: 2.30293984413147\n",
      "Epoch 2, Batch 900, Loss: 2.30337149143219\n",
      "Epoch 3, Batch 100, Loss: 2.3026563119888306\n",
      "Epoch 3, Batch 200, Loss: 2.302730405330658\n",
      "Epoch 3, Batch 300, Loss: 2.3022278642654417\n",
      "Epoch 3, Batch 400, Loss: 2.30370126247406\n",
      "Epoch 3, Batch 500, Loss: 2.3029449701309206\n",
      "Epoch 3, Batch 600, Loss: 2.302992489337921\n",
      "Epoch 3, Batch 700, Loss: 2.302644228935242\n",
      "Epoch 3, Batch 800, Loss: 2.3031679797172546\n",
      "Epoch 3, Batch 900, Loss: 2.30316942691803\n",
      "Epoch 4, Batch 100, Loss: 2.3024619960784913\n",
      "Epoch 4, Batch 200, Loss: 2.3025320959091187\n",
      "Epoch 4, Batch 300, Loss: 2.3037185287475586\n",
      "Epoch 4, Batch 400, Loss: 2.303099856376648\n",
      "Epoch 4, Batch 500, Loss: 2.3028305840492247\n",
      "Epoch 4, Batch 600, Loss: 2.3027274227142334\n",
      "Epoch 4, Batch 700, Loss: 2.30330246925354\n",
      "Epoch 4, Batch 800, Loss: 2.302866382598877\n",
      "Epoch 4, Batch 900, Loss: 2.3028813409805298\n",
      "Epoch 5, Batch 100, Loss: 2.3034111952781675\n",
      "Epoch 5, Batch 200, Loss: 2.3038960194587705\n",
      "Epoch 5, Batch 300, Loss: 2.303446831703186\n",
      "Epoch 5, Batch 400, Loss: 2.3033185315132143\n",
      "Epoch 5, Batch 500, Loss: 2.3032388067245484\n",
      "Epoch 5, Batch 600, Loss: 2.302822244167328\n",
      "Epoch 5, Batch 700, Loss: 2.302799279689789\n",
      "Epoch 5, Batch 800, Loss: 2.3034728240966795\n",
      "Epoch 5, Batch 900, Loss: 2.304194278717041\n",
      "Epoch 6, Batch 100, Loss: 2.302648873329163\n",
      "Epoch 6, Batch 200, Loss: 2.3031620955467225\n",
      "Epoch 6, Batch 300, Loss: 2.303639748096466\n",
      "Epoch 6, Batch 400, Loss: 2.303184413909912\n",
      "Epoch 6, Batch 500, Loss: 2.3034554719924927\n",
      "Epoch 6, Batch 600, Loss: 2.3030480623245237\n",
      "Epoch 6, Batch 700, Loss: 2.3029798460006714\n",
      "Epoch 6, Batch 800, Loss: 2.3029267525672914\n",
      "Epoch 6, Batch 900, Loss: 2.3033898091316223\n",
      "Epoch 7, Batch 100, Loss: 2.3035810732841493\n",
      "Epoch 7, Batch 200, Loss: 2.303800563812256\n",
      "Epoch 7, Batch 300, Loss: 2.3028849530220032\n",
      "Epoch 7, Batch 400, Loss: 2.302870967388153\n",
      "Epoch 7, Batch 500, Loss: 2.3024431753158567\n",
      "Epoch 7, Batch 600, Loss: 2.3024387192726135\n",
      "Epoch 7, Batch 700, Loss: 2.303360595703125\n",
      "Epoch 7, Batch 800, Loss: 2.3032197380065917\n",
      "Epoch 7, Batch 900, Loss: 2.3025738120079042\n",
      "Epoch 8, Batch 100, Loss: 2.3022366523742677\n",
      "Epoch 8, Batch 200, Loss: 2.301879494190216\n",
      "Epoch 8, Batch 300, Loss: 2.303114092350006\n",
      "Epoch 8, Batch 400, Loss: 2.302541687488556\n",
      "Epoch 8, Batch 500, Loss: 2.302993257045746\n",
      "Epoch 8, Batch 600, Loss: 2.302810478210449\n",
      "Epoch 8, Batch 700, Loss: 2.3030577945709227\n",
      "Epoch 8, Batch 800, Loss: 2.30268262386322\n",
      "Epoch 8, Batch 900, Loss: 2.3032370162010194\n",
      "Epoch 9, Batch 100, Loss: 2.302892608642578\n",
      "Epoch 9, Batch 200, Loss: 2.30250235080719\n",
      "Epoch 9, Batch 300, Loss: 2.302769672870636\n",
      "Epoch 9, Batch 400, Loss: 2.3025673413276673\n",
      "Epoch 9, Batch 500, Loss: 2.3033630990982057\n",
      "Epoch 9, Batch 600, Loss: 2.3031565308570863\n",
      "Epoch 9, Batch 700, Loss: 2.303355112075806\n",
      "Epoch 9, Batch 800, Loss: 2.303276176452637\n",
      "Epoch 9, Batch 900, Loss: 2.302174105644226\n",
      "Epoch 10, Batch 100, Loss: 2.3028056693077086\n",
      "Epoch 10, Batch 200, Loss: 2.302926104068756\n",
      "Epoch 10, Batch 300, Loss: 2.3027500081062318\n",
      "Epoch 10, Batch 400, Loss: 2.3030481576919555\n",
      "Epoch 10, Batch 500, Loss: 2.3027813601493836\n",
      "Epoch 10, Batch 600, Loss: 2.302908594608307\n",
      "Epoch 10, Batch 700, Loss: 2.3030567455291746\n",
      "Epoch 10, Batch 800, Loss: 2.3023182582855224\n",
      "Epoch 10, Batch 900, Loss: 2.3022341918945313\n",
      "Epoch 11, Batch 100, Loss: 2.303439610004425\n",
      "Epoch 11, Batch 200, Loss: 2.3031416606903075\n",
      "Epoch 11, Batch 300, Loss: 2.3031525087356566\n",
      "Epoch 11, Batch 400, Loss: 2.3026758337020876\n",
      "Epoch 11, Batch 500, Loss: 2.3028392386436463\n",
      "Epoch 11, Batch 600, Loss: 2.303037085533142\n",
      "Epoch 11, Batch 700, Loss: 2.302600362300873\n",
      "Epoch 11, Batch 800, Loss: 2.3032619738578797\n",
      "Epoch 11, Batch 900, Loss: 2.3028316473960877\n",
      "Epoch 12, Batch 100, Loss: 2.3022542881965635\n",
      "Epoch 12, Batch 200, Loss: 2.3036584877967834\n",
      "Epoch 12, Batch 300, Loss: 2.301684398651123\n",
      "Epoch 12, Batch 400, Loss: 2.3036783051490786\n",
      "Epoch 12, Batch 500, Loss: 2.302350473403931\n",
      "Epoch 12, Batch 600, Loss: 2.3021861600875853\n",
      "Epoch 12, Batch 700, Loss: 2.303129849433899\n",
      "Epoch 12, Batch 800, Loss: 2.3036215925216674\n",
      "Epoch 12, Batch 900, Loss: 2.3032702374458314\n",
      "Epoch 13, Batch 100, Loss: 2.303319797515869\n",
      "Epoch 13, Batch 200, Loss: 2.302706594467163\n",
      "Epoch 13, Batch 300, Loss: 2.3023425817489622\n",
      "Epoch 13, Batch 400, Loss: 2.3031199979782104\n",
      "Epoch 13, Batch 500, Loss: 2.3027924156188964\n",
      "Epoch 13, Batch 600, Loss: 2.303681218624115\n",
      "Epoch 13, Batch 700, Loss: 2.3031395864486695\n",
      "Epoch 13, Batch 800, Loss: 2.3026317620277403\n",
      "Epoch 13, Batch 900, Loss: 2.303757326602936\n",
      "Epoch 14, Batch 100, Loss: 2.303241014480591\n",
      "Epoch 14, Batch 200, Loss: 2.3029004549980163\n",
      "Epoch 14, Batch 300, Loss: 2.3033348393440245\n",
      "Epoch 14, Batch 400, Loss: 2.3035246086120607\n",
      "Epoch 14, Batch 500, Loss: 2.303343243598938\n",
      "Epoch 14, Batch 600, Loss: 2.3029103541374205\n",
      "Epoch 14, Batch 700, Loss: 2.302884130477905\n",
      "Epoch 14, Batch 800, Loss: 2.303049964904785\n",
      "Epoch 14, Batch 900, Loss: 2.302925052642822\n",
      "Epoch 15, Batch 100, Loss: 2.302839524745941\n",
      "Epoch 15, Batch 200, Loss: 2.3029096817970274\n",
      "Epoch 15, Batch 300, Loss: 2.3028125381469726\n",
      "Epoch 15, Batch 400, Loss: 2.3025076484680174\n",
      "Epoch 15, Batch 500, Loss: 2.303101739883423\n",
      "Epoch 15, Batch 600, Loss: 2.303357946872711\n",
      "Epoch 15, Batch 700, Loss: 2.30317640542984\n",
      "Epoch 15, Batch 800, Loss: 2.303603410720825\n",
      "Epoch 15, Batch 900, Loss: 2.3032231092453004\n",
      "Epoch 16, Batch 100, Loss: 2.30313800573349\n",
      "Epoch 16, Batch 200, Loss: 2.303205008506775\n",
      "Epoch 16, Batch 300, Loss: 2.302862219810486\n",
      "Epoch 16, Batch 400, Loss: 2.3031093406677248\n",
      "Epoch 16, Batch 500, Loss: 2.3030838656425474\n",
      "Epoch 16, Batch 600, Loss: 2.302643675804138\n",
      "Epoch 16, Batch 700, Loss: 2.30286518573761\n",
      "Epoch 16, Batch 800, Loss: 2.3026713633537295\n",
      "Epoch 16, Batch 900, Loss: 2.3027500891685486\n",
      "Epoch 17, Batch 100, Loss: 2.303360438346863\n",
      "Epoch 17, Batch 200, Loss: 2.303231735229492\n",
      "Epoch 17, Batch 300, Loss: 2.3027704668045046\n",
      "Epoch 17, Batch 400, Loss: 2.3035027480125425\n",
      "Epoch 17, Batch 500, Loss: 2.3021677231788633\n",
      "Epoch 17, Batch 600, Loss: 2.303979742527008\n",
      "Epoch 17, Batch 700, Loss: 2.303071279525757\n",
      "Epoch 17, Batch 800, Loss: 2.3023754739761353\n",
      "Epoch 17, Batch 900, Loss: 2.303684456348419\n",
      "Epoch 18, Batch 100, Loss: 2.302693543434143\n",
      "Epoch 18, Batch 200, Loss: 2.3021466898918153\n",
      "Epoch 18, Batch 300, Loss: 2.303050763607025\n",
      "Epoch 18, Batch 400, Loss: 2.303233187198639\n",
      "Epoch 18, Batch 500, Loss: 2.3031037998199464\n",
      "Epoch 18, Batch 600, Loss: 2.302526934146881\n",
      "Epoch 18, Batch 700, Loss: 2.304047420024872\n",
      "Epoch 18, Batch 800, Loss: 2.302671320438385\n",
      "Epoch 18, Batch 900, Loss: 2.302760100364685\n",
      "Epoch 19, Batch 100, Loss: 2.3029174327850344\n",
      "Epoch 19, Batch 200, Loss: 2.303146097660065\n",
      "Epoch 19, Batch 300, Loss: 2.302863085269928\n",
      "Epoch 19, Batch 400, Loss: 2.302597403526306\n",
      "Epoch 19, Batch 500, Loss: 2.3023241138458252\n",
      "Epoch 19, Batch 600, Loss: 2.3030645275115966\n",
      "Epoch 19, Batch 700, Loss: 2.302816026210785\n",
      "Epoch 19, Batch 800, Loss: 2.3026592922210694\n",
      "Epoch 19, Batch 900, Loss: 2.3042804408073425\n",
      "Epoch 20, Batch 100, Loss: 2.302575628757477\n",
      "Epoch 20, Batch 200, Loss: 2.3029053163528443\n",
      "Epoch 20, Batch 300, Loss: 2.303038182258606\n",
      "Epoch 20, Batch 400, Loss: 2.3040112018585206\n",
      "Epoch 20, Batch 500, Loss: 2.3033730220794677\n",
      "Epoch 20, Batch 600, Loss: 2.302983753681183\n",
      "Epoch 20, Batch 700, Loss: 2.303203842639923\n",
      "Epoch 20, Batch 800, Loss: 2.3028779292106627\n",
      "Epoch 20, Batch 900, Loss: 2.303504056930542\n",
      "Epoch 21, Batch 100, Loss: 2.3028082823753357\n",
      "Epoch 21, Batch 200, Loss: 2.3021928358078\n",
      "Epoch 21, Batch 300, Loss: 2.3037723684310913\n",
      "Epoch 21, Batch 400, Loss: 2.303659563064575\n",
      "Epoch 21, Batch 500, Loss: 2.303024477958679\n",
      "Epoch 21, Batch 600, Loss: 2.302483615875244\n",
      "Epoch 21, Batch 700, Loss: 2.3025017261505125\n",
      "Epoch 21, Batch 800, Loss: 2.3026612067222594\n",
      "Epoch 21, Batch 900, Loss: 2.302589497566223\n",
      "Epoch 22, Batch 100, Loss: 2.302480387687683\n",
      "Epoch 22, Batch 200, Loss: 2.302767415046692\n",
      "Epoch 22, Batch 300, Loss: 2.3021187376976013\n",
      "Epoch 22, Batch 400, Loss: 2.3023154282569886\n",
      "Epoch 22, Batch 500, Loss: 2.3035507369041444\n",
      "Epoch 22, Batch 600, Loss: 2.3028792428970335\n",
      "Epoch 22, Batch 700, Loss: 2.3021835398674013\n",
      "Epoch 22, Batch 800, Loss: 2.302841784954071\n",
      "Epoch 22, Batch 900, Loss: 2.3036344170570375\n",
      "Epoch 23, Batch 100, Loss: 2.303693690299988\n",
      "Epoch 23, Batch 200, Loss: 2.3020647978782653\n",
      "Epoch 23, Batch 300, Loss: 2.3027708530426025\n",
      "Epoch 23, Batch 400, Loss: 2.303356177806854\n",
      "Epoch 23, Batch 500, Loss: 2.302912588119507\n",
      "Epoch 23, Batch 600, Loss: 2.303409268856049\n",
      "Epoch 23, Batch 700, Loss: 2.3026224899291994\n",
      "Epoch 23, Batch 800, Loss: 2.303101351261139\n",
      "Epoch 23, Batch 900, Loss: 2.3027831149101257\n",
      "Epoch 24, Batch 100, Loss: 2.303760080337524\n",
      "Epoch 24, Batch 200, Loss: 2.3035575675964357\n",
      "Epoch 24, Batch 300, Loss: 2.3041006326675415\n",
      "Epoch 24, Batch 400, Loss: 2.3029594087600707\n",
      "Epoch 24, Batch 500, Loss: 2.3021476554870604\n",
      "Epoch 24, Batch 600, Loss: 2.3025226974487305\n",
      "Epoch 24, Batch 700, Loss: 2.3033658242225648\n",
      "Epoch 24, Batch 800, Loss: 2.302757534980774\n",
      "Epoch 24, Batch 900, Loss: 2.3023998236656187\n",
      "Epoch 25, Batch 100, Loss: 2.304050998687744\n",
      "Epoch 25, Batch 200, Loss: 2.30284791469574\n",
      "Epoch 25, Batch 300, Loss: 2.303216335773468\n",
      "Epoch 25, Batch 400, Loss: 2.302682647705078\n",
      "Epoch 25, Batch 500, Loss: 2.302830789089203\n",
      "Epoch 25, Batch 600, Loss: 2.3028533458709717\n",
      "Epoch 25, Batch 700, Loss: 2.30284547328949\n",
      "Epoch 25, Batch 800, Loss: 2.3030996990203856\n",
      "Epoch 25, Batch 900, Loss: 2.303057646751404\n",
      "Accuracy on test set: 0.098%\n",
      "Fitting for combination 25\n",
      "784\n",
      "3\n",
      "10\n",
      "[30, 10, 30, 10]\n",
      "True\n",
      "['tanh', 'sigmoid', 'tanh']\n",
      "SGD\n",
      "0.3\n",
      "0.1\n",
      "CrossEntropyLoss\n",
      "300\n",
      "25\n",
      "0\n",
      "Epoch 1, Batch 300, Loss: 6.905339677333831\n",
      "Epoch 1, Batch 600, Loss: 6.907346689701081\n",
      "Epoch 1, Batch 900, Loss: 6.907581822872162\n",
      "Epoch 2, Batch 300, Loss: 6.905393934249878\n",
      "Epoch 2, Batch 600, Loss: 6.905391702651977\n",
      "Epoch 2, Batch 900, Loss: 6.906856327056885\n",
      "Epoch 3, Batch 300, Loss: 6.907197790145874\n",
      "Epoch 3, Batch 600, Loss: 6.906768255233764\n",
      "Epoch 3, Batch 900, Loss: 6.905323934555054\n",
      "Epoch 4, Batch 300, Loss: 6.9061030626296995\n",
      "Epoch 4, Batch 600, Loss: 6.90728750705719\n",
      "Epoch 4, Batch 900, Loss: 6.9058576631546025\n",
      "Epoch 5, Batch 300, Loss: 6.905806212425232\n",
      "Epoch 5, Batch 600, Loss: 6.90712073802948\n",
      "Epoch 5, Batch 900, Loss: 6.904829285144806\n",
      "Epoch 6, Batch 300, Loss: 6.906059064865112\n",
      "Epoch 6, Batch 600, Loss: 6.90688681602478\n",
      "Epoch 6, Batch 900, Loss: 6.906528193950653\n",
      "Epoch 7, Batch 300, Loss: 6.906871314048767\n",
      "Epoch 7, Batch 600, Loss: 6.905777759552002\n",
      "Epoch 7, Batch 900, Loss: 6.906722447872162\n",
      "Epoch 8, Batch 300, Loss: 6.905397338867187\n",
      "Epoch 8, Batch 600, Loss: 6.906740384101868\n",
      "Epoch 8, Batch 900, Loss: 6.906757173538208\n",
      "Epoch 9, Batch 300, Loss: 6.906762540340424\n",
      "Epoch 9, Batch 600, Loss: 6.907148697376251\n",
      "Epoch 9, Batch 900, Loss: 6.9060661768913265\n",
      "Epoch 10, Batch 300, Loss: 6.905666649341583\n",
      "Epoch 10, Batch 600, Loss: 6.907558994293213\n",
      "Epoch 10, Batch 900, Loss: 6.905527837276459\n",
      "Epoch 11, Batch 300, Loss: 6.9055499792098995\n",
      "Epoch 11, Batch 600, Loss: 6.90627890586853\n",
      "Epoch 11, Batch 900, Loss: 6.905831158161163\n",
      "Epoch 12, Batch 300, Loss: 6.905805282592773\n",
      "Epoch 12, Batch 600, Loss: 6.905107910633087\n",
      "Epoch 12, Batch 900, Loss: 6.907259113788605\n",
      "Epoch 13, Batch 300, Loss: 6.909175703525543\n",
      "Epoch 13, Batch 600, Loss: 6.905147712230683\n",
      "Epoch 13, Batch 900, Loss: 6.906294751167297\n",
      "Epoch 14, Batch 300, Loss: 6.905686106681824\n",
      "Epoch 14, Batch 600, Loss: 6.9076125001907345\n",
      "Epoch 14, Batch 900, Loss: 6.905162572860718\n",
      "Epoch 15, Batch 300, Loss: 6.906241755485535\n",
      "Epoch 15, Batch 600, Loss: 6.905483219623566\n",
      "Epoch 15, Batch 900, Loss: 6.907160732746124\n",
      "Epoch 16, Batch 300, Loss: 6.907396850585937\n",
      "Epoch 16, Batch 600, Loss: 6.906296789646149\n",
      "Epoch 16, Batch 900, Loss: 6.907280123233795\n",
      "Epoch 17, Batch 300, Loss: 6.904413497447967\n",
      "Epoch 17, Batch 600, Loss: 6.907052879333496\n",
      "Epoch 17, Batch 900, Loss: 6.906258141994476\n",
      "Epoch 18, Batch 300, Loss: 6.907202444076538\n",
      "Epoch 18, Batch 600, Loss: 6.90626893043518\n",
      "Epoch 18, Batch 900, Loss: 6.90561146736145\n",
      "Epoch 19, Batch 300, Loss: 6.906567454338074\n",
      "Epoch 19, Batch 600, Loss: 6.905616383552552\n",
      "Epoch 19, Batch 900, Loss: 6.907161195278167\n",
      "Epoch 20, Batch 300, Loss: 6.907220635414124\n",
      "Epoch 20, Batch 600, Loss: 6.907169752120971\n",
      "Epoch 20, Batch 900, Loss: 6.905541615486145\n",
      "Epoch 21, Batch 300, Loss: 6.906200003623963\n",
      "Epoch 21, Batch 600, Loss: 6.906233320236206\n",
      "Epoch 21, Batch 900, Loss: 6.907157409191131\n",
      "Epoch 22, Batch 300, Loss: 6.905609366893768\n",
      "Epoch 22, Batch 600, Loss: 6.906055626869201\n",
      "Epoch 22, Batch 900, Loss: 6.906478371620178\n",
      "Epoch 23, Batch 300, Loss: 6.905955612659454\n",
      "Epoch 23, Batch 600, Loss: 6.907208256721496\n",
      "Epoch 23, Batch 900, Loss: 6.9053630352020265\n",
      "Epoch 24, Batch 300, Loss: 6.905613493919373\n",
      "Epoch 24, Batch 600, Loss: 6.907061238288879\n",
      "Epoch 24, Batch 900, Loss: 6.906046068668365\n",
      "Epoch 25, Batch 300, Loss: 6.9060233163833615\n",
      "Epoch 25, Batch 600, Loss: 6.906324265003204\n",
      "Epoch 25, Batch 900, Loss: 6.9050533485412595\n",
      "Accuracy on test set: 0.1135%\n",
      "Fitting for combination 26\n",
      "784\n",
      "3\n",
      "10\n",
      "[30, 10, 40, 10]\n",
      "False\n",
      "['tanh', 'sigmoid', 'relu']\n",
      "Adam\n",
      "0.003\n",
      "0.01\n",
      "CrossEntropyLoss\n",
      "100\n",
      "25\n",
      "0\n",
      "Epoch 1, Batch 100, Loss: 2.0800736212730406\n",
      "Epoch 1, Batch 200, Loss: 1.418357830643654\n",
      "Epoch 1, Batch 300, Loss: 1.1328289848566055\n",
      "Epoch 1, Batch 400, Loss: 0.9944091379642487\n",
      "Epoch 1, Batch 500, Loss: 0.8799910974502564\n",
      "Epoch 1, Batch 600, Loss: 0.8602873665094376\n",
      "Epoch 1, Batch 700, Loss: 0.8302098661661148\n",
      "Epoch 1, Batch 800, Loss: 0.7938859295845032\n",
      "Epoch 1, Batch 900, Loss: 0.7702629148960114\n",
      "Epoch 2, Batch 100, Loss: 0.7520196384191513\n",
      "Epoch 2, Batch 200, Loss: 0.7665738016366959\n",
      "Epoch 2, Batch 300, Loss: 0.7455388313531875\n",
      "Epoch 2, Batch 400, Loss: 0.753152260184288\n",
      "Epoch 2, Batch 500, Loss: 0.7320787864923477\n",
      "Epoch 2, Batch 600, Loss: 0.7163212066888809\n",
      "Epoch 2, Batch 700, Loss: 0.6833699575066566\n",
      "Epoch 2, Batch 800, Loss: 0.6028402364253997\n",
      "Epoch 2, Batch 900, Loss: 0.65023236900568\n",
      "Epoch 3, Batch 100, Loss: 0.5873355960845947\n",
      "Epoch 3, Batch 200, Loss: 0.6030958822369575\n",
      "Epoch 3, Batch 300, Loss: 0.604043984413147\n",
      "Epoch 3, Batch 400, Loss: 0.5915890011191368\n",
      "Epoch 3, Batch 500, Loss: 0.5864350637793541\n",
      "Epoch 3, Batch 600, Loss: 0.5481367430090904\n",
      "Epoch 3, Batch 700, Loss: 0.5249794301390648\n",
      "Epoch 3, Batch 800, Loss: 0.5144777849316597\n",
      "Epoch 3, Batch 900, Loss: 0.5238361069560051\n",
      "Epoch 4, Batch 100, Loss: 0.5053903025388717\n",
      "Epoch 4, Batch 200, Loss: 0.47580658495426176\n",
      "Epoch 4, Batch 300, Loss: 0.4909920445084572\n",
      "Epoch 4, Batch 400, Loss: 0.5027144894003868\n",
      "Epoch 4, Batch 500, Loss: 0.48349576115608217\n",
      "Epoch 4, Batch 600, Loss: 0.49476062297821044\n",
      "Epoch 4, Batch 700, Loss: 0.5046735489368439\n",
      "Epoch 4, Batch 800, Loss: 0.4967568525671959\n",
      "Epoch 4, Batch 900, Loss: 0.4730685862898827\n",
      "Epoch 5, Batch 100, Loss: 0.4738885787129402\n",
      "Epoch 5, Batch 200, Loss: 0.5016631600260735\n",
      "Epoch 5, Batch 300, Loss: 0.4435892933607101\n",
      "Epoch 5, Batch 400, Loss: 0.4948515245318413\n",
      "Epoch 5, Batch 500, Loss: 0.4448789514601231\n",
      "Epoch 5, Batch 600, Loss: 0.49673244267702105\n",
      "Epoch 5, Batch 700, Loss: 0.49111480355262754\n",
      "Epoch 5, Batch 800, Loss: 0.46704463511705396\n",
      "Epoch 5, Batch 900, Loss: 0.4655923976004124\n",
      "Epoch 6, Batch 100, Loss: 0.44044917181134224\n",
      "Epoch 6, Batch 200, Loss: 0.4655151778459549\n",
      "Epoch 6, Batch 300, Loss: 0.4659922455251217\n",
      "Epoch 6, Batch 400, Loss: 0.48609753131866457\n",
      "Epoch 6, Batch 500, Loss: 0.47195434585213664\n",
      "Epoch 6, Batch 600, Loss: 0.45565455079078676\n",
      "Epoch 6, Batch 700, Loss: 0.47763898700475693\n",
      "Epoch 6, Batch 800, Loss: 0.46129049599170685\n",
      "Epoch 6, Batch 900, Loss: 0.4703099340200424\n",
      "Epoch 7, Batch 100, Loss: 0.4677213180065155\n",
      "Epoch 7, Batch 200, Loss: 0.4472500222921372\n",
      "Epoch 7, Batch 300, Loss: 0.4299627913534641\n",
      "Epoch 7, Batch 400, Loss: 0.47931369781494143\n",
      "Epoch 7, Batch 500, Loss: 0.47602210238575937\n",
      "Epoch 7, Batch 600, Loss: 0.4476349526643753\n",
      "Epoch 7, Batch 700, Loss: 0.48330687880516054\n",
      "Epoch 7, Batch 800, Loss: 0.4603503356873989\n",
      "Epoch 7, Batch 900, Loss: 0.44591341495513914\n",
      "Epoch 8, Batch 100, Loss: 0.44888569086790087\n",
      "Epoch 8, Batch 200, Loss: 0.4647586491703987\n",
      "Epoch 8, Batch 300, Loss: 0.4528052705526352\n",
      "Epoch 8, Batch 400, Loss: 0.42827862858772275\n",
      "Epoch 8, Batch 500, Loss: 0.42357646733522414\n",
      "Epoch 8, Batch 600, Loss: 0.4328249141573906\n",
      "Epoch 8, Batch 700, Loss: 0.47188073068857195\n",
      "Epoch 8, Batch 800, Loss: 0.4404414464533329\n",
      "Epoch 8, Batch 900, Loss: 0.4591524051129818\n",
      "Epoch 9, Batch 100, Loss: 0.44355339840054514\n",
      "Epoch 9, Batch 200, Loss: 0.43141898691654207\n",
      "Epoch 9, Batch 300, Loss: 0.45443015605211257\n",
      "Epoch 9, Batch 400, Loss: 0.4453729909658432\n",
      "Epoch 9, Batch 500, Loss: 0.4194068662822247\n",
      "Epoch 9, Batch 600, Loss: 0.42727514207363126\n",
      "Epoch 9, Batch 700, Loss: 0.45355485409498214\n",
      "Epoch 9, Batch 800, Loss: 0.4344475799798965\n",
      "Epoch 9, Batch 900, Loss: 0.4459302116930485\n",
      "Epoch 10, Batch 100, Loss: 0.4558053506910801\n",
      "Epoch 10, Batch 200, Loss: 0.4307589575648308\n",
      "Epoch 10, Batch 300, Loss: 0.45817663699388506\n",
      "Epoch 10, Batch 400, Loss: 0.43617782950401307\n",
      "Epoch 10, Batch 500, Loss: 0.4255127876996994\n",
      "Epoch 10, Batch 600, Loss: 0.4338955780863762\n",
      "Epoch 10, Batch 700, Loss: 0.4353025770187378\n",
      "Epoch 10, Batch 800, Loss: 0.4427749057114124\n",
      "Epoch 10, Batch 900, Loss: 0.450509734749794\n",
      "Epoch 11, Batch 100, Loss: 0.4749038678407669\n",
      "Epoch 11, Batch 200, Loss: 0.42608335345983506\n",
      "Epoch 11, Batch 300, Loss: 0.4236781953275204\n",
      "Epoch 11, Batch 400, Loss: 0.4225394667685032\n",
      "Epoch 11, Batch 500, Loss: 0.41175348103046416\n",
      "Epoch 11, Batch 600, Loss: 0.4141796408593655\n",
      "Epoch 11, Batch 700, Loss: 0.46136453688144685\n",
      "Epoch 11, Batch 800, Loss: 0.43449136793613435\n",
      "Epoch 11, Batch 900, Loss: 0.43947658777236936\n",
      "Epoch 12, Batch 100, Loss: 0.4270705558359623\n",
      "Epoch 12, Batch 200, Loss: 0.4294814544916153\n",
      "Epoch 12, Batch 300, Loss: 0.4205863690376282\n",
      "Epoch 12, Batch 400, Loss: 0.44098403587937357\n",
      "Epoch 12, Batch 500, Loss: 0.43296331346035005\n",
      "Epoch 12, Batch 600, Loss: 0.43171548515558245\n",
      "Epoch 12, Batch 700, Loss: 0.4165554229915142\n",
      "Epoch 12, Batch 800, Loss: 0.43651955157518385\n",
      "Epoch 12, Batch 900, Loss: 0.4353403332829475\n",
      "Epoch 13, Batch 100, Loss: 0.4262422999739647\n",
      "Epoch 13, Batch 200, Loss: 0.42256812408566474\n",
      "Epoch 13, Batch 300, Loss: 0.4251406016945839\n",
      "Epoch 13, Batch 400, Loss: 0.4416921654343605\n",
      "Epoch 13, Batch 500, Loss: 0.39367771372199056\n",
      "Epoch 13, Batch 600, Loss: 0.47522646516561506\n",
      "Epoch 13, Batch 700, Loss: 0.43277247846126554\n",
      "Epoch 13, Batch 800, Loss: 0.4476345431804657\n",
      "Epoch 13, Batch 900, Loss: 0.42958380073308944\n",
      "Epoch 14, Batch 100, Loss: 0.40031468600034714\n",
      "Epoch 14, Batch 200, Loss: 0.40259947270154955\n",
      "Epoch 14, Batch 300, Loss: 0.44336783051490786\n",
      "Epoch 14, Batch 400, Loss: 0.43979813873767853\n",
      "Epoch 14, Batch 500, Loss: 0.43214745342731475\n",
      "Epoch 14, Batch 600, Loss: 0.4158848284184933\n",
      "Epoch 14, Batch 700, Loss: 0.42314477294683456\n",
      "Epoch 14, Batch 800, Loss: 0.4318720930814743\n",
      "Epoch 14, Batch 900, Loss: 0.43055204406380654\n",
      "Epoch 15, Batch 100, Loss: 0.4407771909236908\n",
      "Epoch 15, Batch 200, Loss: 0.4336930078268051\n",
      "Epoch 15, Batch 300, Loss: 0.41029731020331384\n",
      "Epoch 15, Batch 400, Loss: 0.43248149514198303\n",
      "Epoch 15, Batch 500, Loss: 0.44407583922147753\n",
      "Epoch 15, Batch 600, Loss: 0.42366900116205214\n",
      "Epoch 15, Batch 700, Loss: 0.44548405081033704\n",
      "Epoch 15, Batch 800, Loss: 0.43379751950502393\n",
      "Epoch 15, Batch 900, Loss: 0.43243317157030103\n",
      "Epoch 16, Batch 100, Loss: 0.4140484336018562\n",
      "Epoch 16, Batch 200, Loss: 0.4339072202146053\n",
      "Epoch 16, Batch 300, Loss: 0.4274121683835983\n",
      "Epoch 16, Batch 400, Loss: 0.4206272453069687\n",
      "Epoch 16, Batch 500, Loss: 0.4410727900266647\n",
      "Epoch 16, Batch 600, Loss: 0.43439793929457665\n",
      "Epoch 16, Batch 700, Loss: 0.43274960547685626\n",
      "Epoch 16, Batch 800, Loss: 0.4260643061995506\n",
      "Epoch 16, Batch 900, Loss: 0.41906641706824305\n",
      "Epoch 17, Batch 100, Loss: 0.4170741382241249\n",
      "Epoch 17, Batch 200, Loss: 0.43399771973490714\n",
      "Epoch 17, Batch 300, Loss: 0.4037733080983162\n",
      "Epoch 17, Batch 400, Loss: 0.4367963448166847\n",
      "Epoch 17, Batch 500, Loss: 0.427422663718462\n",
      "Epoch 17, Batch 600, Loss: 0.4502742876112461\n",
      "Epoch 17, Batch 700, Loss: 0.44685678780078886\n",
      "Epoch 17, Batch 800, Loss: 0.43483751118183134\n",
      "Epoch 17, Batch 900, Loss: 0.43957503125071523\n",
      "Epoch 18, Batch 100, Loss: 0.40089706286787985\n",
      "Epoch 18, Batch 200, Loss: 0.44093623086810113\n",
      "Epoch 18, Batch 300, Loss: 0.416983747035265\n",
      "Epoch 18, Batch 400, Loss: 0.41605790436267853\n",
      "Epoch 18, Batch 500, Loss: 0.45326130747795107\n",
      "Epoch 18, Batch 600, Loss: 0.42364870935678484\n",
      "Epoch 18, Batch 700, Loss: 0.41651910573244094\n",
      "Epoch 18, Batch 800, Loss: 0.435418725758791\n",
      "Epoch 18, Batch 900, Loss: 0.42868259906768796\n",
      "Epoch 19, Batch 100, Loss: 0.41540060818195346\n",
      "Epoch 19, Batch 200, Loss: 0.4174262909591198\n",
      "Epoch 19, Batch 300, Loss: 0.43202303946018217\n",
      "Epoch 19, Batch 400, Loss: 0.41300078779459\n",
      "Epoch 19, Batch 500, Loss: 0.4073187208175659\n",
      "Epoch 19, Batch 600, Loss: 0.42121031194925307\n",
      "Epoch 19, Batch 700, Loss: 0.4517429032921791\n",
      "Epoch 19, Batch 800, Loss: 0.4404008835554123\n",
      "Epoch 19, Batch 900, Loss: 0.4482130706310272\n",
      "Epoch 20, Batch 100, Loss: 0.44463811576366424\n",
      "Epoch 20, Batch 200, Loss: 0.41745618134737017\n",
      "Epoch 20, Batch 300, Loss: 0.3986013950407505\n",
      "Epoch 20, Batch 400, Loss: 0.44648724526166916\n",
      "Epoch 20, Batch 500, Loss: 0.4436766958236694\n",
      "Epoch 20, Batch 600, Loss: 0.42418474063277245\n",
      "Epoch 20, Batch 700, Loss: 0.43803046599030493\n",
      "Epoch 20, Batch 800, Loss: 0.432819409519434\n",
      "Epoch 20, Batch 900, Loss: 0.4470248317718506\n",
      "Epoch 21, Batch 100, Loss: 0.4220998126268387\n",
      "Epoch 21, Batch 200, Loss: 0.4263380944728851\n",
      "Epoch 21, Batch 300, Loss: 0.4213052675127983\n",
      "Epoch 21, Batch 400, Loss: 0.4321319405734539\n",
      "Epoch 21, Batch 500, Loss: 0.43887712523341177\n",
      "Epoch 21, Batch 600, Loss: 0.43542465955019\n",
      "Epoch 21, Batch 700, Loss: 0.40181381314992903\n",
      "Epoch 21, Batch 800, Loss: 0.43663144841790197\n",
      "Epoch 21, Batch 900, Loss: 0.44071078225970267\n",
      "Epoch 22, Batch 100, Loss: 0.4013268330693245\n",
      "Epoch 22, Batch 200, Loss: 0.4062495090067387\n",
      "Epoch 22, Batch 300, Loss: 0.44945536270737646\n",
      "Epoch 22, Batch 400, Loss: 0.4787202274799347\n",
      "Epoch 22, Batch 500, Loss: 0.4249227225780487\n",
      "Epoch 22, Batch 600, Loss: 0.4158667805790901\n",
      "Epoch 22, Batch 700, Loss: 0.44212791189551354\n",
      "Epoch 22, Batch 800, Loss: 0.4458852542936802\n",
      "Epoch 22, Batch 900, Loss: 0.41899286821484566\n",
      "Epoch 23, Batch 100, Loss: 0.4383447702229023\n",
      "Epoch 23, Batch 200, Loss: 0.42809026047587395\n",
      "Epoch 23, Batch 300, Loss: 0.4046688385307789\n",
      "Epoch 23, Batch 400, Loss: 0.43771238952875136\n",
      "Epoch 23, Batch 500, Loss: 0.4021062964200974\n",
      "Epoch 23, Batch 600, Loss: 0.43363492012023924\n",
      "Epoch 23, Batch 700, Loss: 0.43745740100741387\n",
      "Epoch 23, Batch 800, Loss: 0.4594815841317177\n",
      "Epoch 23, Batch 900, Loss: 0.4303389184176922\n",
      "Epoch 24, Batch 100, Loss: 0.43651540964841845\n",
      "Epoch 24, Batch 200, Loss: 0.43987948551774025\n",
      "Epoch 24, Batch 300, Loss: 0.4367850410938263\n",
      "Epoch 24, Batch 400, Loss: 0.438016597032547\n",
      "Epoch 24, Batch 500, Loss: 0.4269642896950245\n",
      "Epoch 24, Batch 600, Loss: 0.43579249650239943\n",
      "Epoch 24, Batch 700, Loss: 0.4398699480295181\n",
      "Epoch 24, Batch 800, Loss: 0.419985129237175\n",
      "Epoch 24, Batch 900, Loss: 0.4307678586244583\n",
      "Epoch 25, Batch 100, Loss: 0.43475284665822983\n",
      "Epoch 25, Batch 200, Loss: 0.4224147567152977\n",
      "Epoch 25, Batch 300, Loss: 0.4118021403253078\n",
      "Epoch 25, Batch 400, Loss: 0.4795534472167492\n",
      "Epoch 25, Batch 500, Loss: 0.42017464980483055\n",
      "Epoch 25, Batch 600, Loss: 0.4625055857002735\n",
      "Epoch 25, Batch 700, Loss: 0.47100799798965454\n",
      "Epoch 25, Batch 800, Loss: 0.42562493190169337\n",
      "Epoch 25, Batch 900, Loss: 0.4539536602795124\n",
      "Accuracy on test set: 0.9038%\n",
      "Fitting for combination 27\n",
      "784\n",
      "3\n",
      "10\n",
      "[30, 10, 40, 10]\n",
      "False\n",
      "['tanh', 'sigmoid', 'relu']\n",
      "SGD\n",
      "0.001\n",
      "0.01\n",
      "CrossEntropyLoss\n",
      "100\n",
      "25\n",
      "0\n",
      "Epoch 1, Batch 100, Loss: 2.3275117254257203\n",
      "Epoch 1, Batch 200, Loss: 2.3200470399856568\n",
      "Epoch 1, Batch 300, Loss: 2.3188312244415283\n",
      "Epoch 1, Batch 400, Loss: 2.3180799007415773\n",
      "Epoch 1, Batch 500, Loss: 2.3137529563903807\n",
      "Epoch 1, Batch 600, Loss: 2.3102414202690125\n",
      "Epoch 1, Batch 700, Loss: 2.3079668951034544\n",
      "Epoch 1, Batch 800, Loss: 2.308117802143097\n",
      "Epoch 1, Batch 900, Loss: 2.3043815732002257\n",
      "Epoch 2, Batch 100, Loss: 2.3016623783111574\n",
      "Epoch 2, Batch 200, Loss: 2.303869752883911\n",
      "Epoch 2, Batch 300, Loss: 2.3013162875175475\n",
      "Epoch 2, Batch 400, Loss: 2.2984513306617735\n",
      "Epoch 2, Batch 500, Loss: 2.2982818841934205\n",
      "Epoch 2, Batch 600, Loss: 2.2980635237693785\n",
      "Epoch 2, Batch 700, Loss: 2.297685649394989\n",
      "Epoch 2, Batch 800, Loss: 2.2976310324668883\n",
      "Epoch 2, Batch 900, Loss: 2.293980734348297\n",
      "Epoch 3, Batch 100, Loss: 2.2942394399642945\n",
      "Epoch 3, Batch 200, Loss: 2.294049482345581\n",
      "Epoch 3, Batch 300, Loss: 2.2930694341659548\n",
      "Epoch 3, Batch 400, Loss: 2.292033109664917\n",
      "Epoch 3, Batch 500, Loss: 2.291562507152557\n",
      "Epoch 3, Batch 600, Loss: 2.2909204316139222\n",
      "Epoch 3, Batch 700, Loss: 2.289271514415741\n",
      "Epoch 3, Batch 800, Loss: 2.2900954151153563\n",
      "Epoch 3, Batch 900, Loss: 2.288973345756531\n",
      "Epoch 4, Batch 100, Loss: 2.2883383417129517\n",
      "Epoch 4, Batch 200, Loss: 2.2875520157814027\n",
      "Epoch 4, Batch 300, Loss: 2.286773593425751\n",
      "Epoch 4, Batch 400, Loss: 2.286801452636719\n",
      "Epoch 4, Batch 500, Loss: 2.286354994773865\n",
      "Epoch 4, Batch 600, Loss: 2.286078221797943\n",
      "Epoch 4, Batch 700, Loss: 2.2843398571014406\n",
      "Epoch 4, Batch 800, Loss: 2.2845579981803894\n",
      "Epoch 4, Batch 900, Loss: 2.2837624979019164\n",
      "Epoch 5, Batch 100, Loss: 2.2833921575546263\n",
      "Epoch 5, Batch 200, Loss: 2.2816575956344605\n",
      "Epoch 5, Batch 300, Loss: 2.2813334560394285\n",
      "Epoch 5, Batch 400, Loss: 2.282118306159973\n",
      "Epoch 5, Batch 500, Loss: 2.2804336833953855\n",
      "Epoch 5, Batch 600, Loss: 2.279866621494293\n",
      "Epoch 5, Batch 700, Loss: 2.2801456809043885\n",
      "Epoch 5, Batch 800, Loss: 2.2788730573654177\n",
      "Epoch 5, Batch 900, Loss: 2.2794164657592773\n",
      "Epoch 6, Batch 100, Loss: 2.2764473128318787\n",
      "Epoch 6, Batch 200, Loss: 2.2774287366867068\n",
      "Epoch 6, Batch 300, Loss: 2.276496801376343\n",
      "Epoch 6, Batch 400, Loss: 2.27614177942276\n",
      "Epoch 6, Batch 500, Loss: 2.2753350520133973\n",
      "Epoch 6, Batch 600, Loss: 2.2746919202804565\n",
      "Epoch 6, Batch 700, Loss: 2.2733246231079103\n",
      "Epoch 6, Batch 800, Loss: 2.273494293689728\n",
      "Epoch 6, Batch 900, Loss: 2.2731982111930846\n",
      "Epoch 7, Batch 100, Loss: 2.272309031486511\n",
      "Epoch 7, Batch 200, Loss: 2.271083710193634\n",
      "Epoch 7, Batch 300, Loss: 2.2689831137657164\n",
      "Epoch 7, Batch 400, Loss: 2.2708526945114134\n",
      "Epoch 7, Batch 500, Loss: 2.2689704871177674\n",
      "Epoch 7, Batch 600, Loss: 2.267049744129181\n",
      "Epoch 7, Batch 700, Loss: 2.267473592758179\n",
      "Epoch 7, Batch 800, Loss: 2.2671709275245666\n",
      "Epoch 7, Batch 900, Loss: 2.2667058229446413\n",
      "Epoch 8, Batch 100, Loss: 2.2668683886528016\n",
      "Epoch 8, Batch 200, Loss: 2.263800332546234\n",
      "Epoch 8, Batch 300, Loss: 2.2622975373268126\n",
      "Epoch 8, Batch 400, Loss: 2.262833602428436\n",
      "Epoch 8, Batch 500, Loss: 2.263458151817322\n",
      "Epoch 8, Batch 600, Loss: 2.2608108210563658\n",
      "Epoch 8, Batch 700, Loss: 2.2605520248413087\n",
      "Epoch 8, Batch 800, Loss: 2.2598269844055174\n",
      "Epoch 8, Batch 900, Loss: 2.2580054807662964\n",
      "Epoch 9, Batch 100, Loss: 2.2581701040267945\n",
      "Epoch 9, Batch 200, Loss: 2.2576837134361267\n",
      "Epoch 9, Batch 300, Loss: 2.2566725969314576\n",
      "Epoch 9, Batch 400, Loss: 2.2546209192276\n",
      "Epoch 9, Batch 500, Loss: 2.2537838554382326\n",
      "Epoch 9, Batch 600, Loss: 2.2545161747932436\n",
      "Epoch 9, Batch 700, Loss: 2.249987781047821\n",
      "Epoch 9, Batch 800, Loss: 2.251018912792206\n",
      "Epoch 9, Batch 900, Loss: 2.250763397216797\n",
      "Epoch 10, Batch 100, Loss: 2.249579756259918\n",
      "Epoch 10, Batch 200, Loss: 2.24712605714798\n",
      "Epoch 10, Batch 300, Loss: 2.2494495272636414\n",
      "Epoch 10, Batch 400, Loss: 2.2443651580810546\n",
      "Epoch 10, Batch 500, Loss: 2.245089221000671\n",
      "Epoch 10, Batch 600, Loss: 2.2431809186935423\n",
      "Epoch 10, Batch 700, Loss: 2.2440101289749146\n",
      "Epoch 10, Batch 800, Loss: 2.241586649417877\n",
      "Epoch 10, Batch 900, Loss: 2.2414005327224733\n",
      "Epoch 11, Batch 100, Loss: 2.24194623708725\n",
      "Epoch 11, Batch 200, Loss: 2.2373483061790465\n",
      "Epoch 11, Batch 300, Loss: 2.234846954345703\n",
      "Epoch 11, Batch 400, Loss: 2.2361683464050293\n",
      "Epoch 11, Batch 500, Loss: 2.2353452682495116\n",
      "Epoch 11, Batch 600, Loss: 2.2332430243492127\n",
      "Epoch 11, Batch 700, Loss: 2.233347976207733\n",
      "Epoch 11, Batch 800, Loss: 2.2286507868766785\n",
      "Epoch 11, Batch 900, Loss: 2.2305308270454405\n",
      "Epoch 12, Batch 100, Loss: 2.2293767786026\n",
      "Epoch 12, Batch 200, Loss: 2.2280907106399535\n",
      "Epoch 12, Batch 300, Loss: 2.2246782779693604\n",
      "Epoch 12, Batch 400, Loss: 2.2250069046020506\n",
      "Epoch 12, Batch 500, Loss: 2.221966948509216\n",
      "Epoch 12, Batch 600, Loss: 2.219031512737274\n",
      "Epoch 12, Batch 700, Loss: 2.218397943973541\n",
      "Epoch 12, Batch 800, Loss: 2.217907943725586\n",
      "Epoch 12, Batch 900, Loss: 2.2175067710876464\n",
      "Epoch 13, Batch 100, Loss: 2.2161537122726442\n",
      "Epoch 13, Batch 200, Loss: 2.2148380160331724\n",
      "Epoch 13, Batch 300, Loss: 2.211057939529419\n",
      "Epoch 13, Batch 400, Loss: 2.2094110369682314\n",
      "Epoch 13, Batch 500, Loss: 2.2082609605789183\n",
      "Epoch 13, Batch 600, Loss: 2.205868103504181\n",
      "Epoch 13, Batch 700, Loss: 2.206575093269348\n",
      "Epoch 13, Batch 800, Loss: 2.2007452988624574\n",
      "Epoch 13, Batch 900, Loss: 2.1993381357192994\n",
      "Epoch 14, Batch 100, Loss: 2.2002348065376283\n",
      "Epoch 14, Batch 200, Loss: 2.1965732026100158\n",
      "Epoch 14, Batch 300, Loss: 2.197230865955353\n",
      "Epoch 14, Batch 400, Loss: 2.1901093482971192\n",
      "Epoch 14, Batch 500, Loss: 2.1927132415771484\n",
      "Epoch 14, Batch 600, Loss: 2.18769350528717\n",
      "Epoch 14, Batch 700, Loss: 2.1862325167655943\n",
      "Epoch 14, Batch 800, Loss: 2.1829097771644594\n",
      "Epoch 14, Batch 900, Loss: 2.181506700515747\n",
      "Epoch 15, Batch 100, Loss: 2.1769373488426207\n",
      "Epoch 15, Batch 200, Loss: 2.174450993537903\n",
      "Epoch 15, Batch 300, Loss: 2.177496168613434\n",
      "Epoch 15, Batch 400, Loss: 2.1745985889434816\n",
      "Epoch 15, Batch 500, Loss: 2.171509521007538\n",
      "Epoch 15, Batch 600, Loss: 2.168237204551697\n",
      "Epoch 15, Batch 700, Loss: 2.167192792892456\n",
      "Epoch 15, Batch 800, Loss: 2.1627081441879272\n",
      "Epoch 15, Batch 900, Loss: 2.1591654133796694\n",
      "Epoch 16, Batch 100, Loss: 2.1542565608024598\n",
      "Epoch 16, Batch 200, Loss: 2.152012462615967\n",
      "Epoch 16, Batch 300, Loss: 2.1562642884254455\n",
      "Epoch 16, Batch 400, Loss: 2.151510419845581\n",
      "Epoch 16, Batch 500, Loss: 2.145592017173767\n",
      "Epoch 16, Batch 600, Loss: 2.1395745396614076\n",
      "Epoch 16, Batch 700, Loss: 2.138655707836151\n",
      "Epoch 16, Batch 800, Loss: 2.1368673706054686\n",
      "Epoch 16, Batch 900, Loss: 2.1385182666778566\n",
      "Epoch 17, Batch 100, Loss: 2.1287423753738404\n",
      "Epoch 17, Batch 200, Loss: 2.1279580998420715\n",
      "Epoch 17, Batch 300, Loss: 2.126674246788025\n",
      "Epoch 17, Batch 400, Loss: 2.118826138973236\n",
      "Epoch 17, Batch 500, Loss: 2.1188464426994322\n",
      "Epoch 17, Batch 600, Loss: 2.1151997685432433\n",
      "Epoch 17, Batch 700, Loss: 2.108807632923126\n",
      "Epoch 17, Batch 800, Loss: 2.1080998826026915\n",
      "Epoch 17, Batch 900, Loss: 2.1040460109710692\n",
      "Epoch 18, Batch 100, Loss: 2.09957754611969\n",
      "Epoch 18, Batch 200, Loss: 2.090774931907654\n",
      "Epoch 18, Batch 300, Loss: 2.095209572315216\n",
      "Epoch 18, Batch 400, Loss: 2.092459673881531\n",
      "Epoch 18, Batch 500, Loss: 2.0849305772781372\n",
      "Epoch 18, Batch 600, Loss: 2.0750736665725706\n",
      "Epoch 18, Batch 700, Loss: 2.0779259777069092\n",
      "Epoch 18, Batch 800, Loss: 2.072470552921295\n",
      "Epoch 18, Batch 900, Loss: 2.066511092185974\n",
      "Epoch 19, Batch 100, Loss: 2.0592750573158263\n",
      "Epoch 19, Batch 200, Loss: 2.058674952983856\n",
      "Epoch 19, Batch 300, Loss: 2.055461375713348\n",
      "Epoch 19, Batch 400, Loss: 2.0514946138858794\n",
      "Epoch 19, Batch 500, Loss: 2.0433022034168244\n",
      "Epoch 19, Batch 600, Loss: 2.042240480184555\n",
      "Epoch 19, Batch 700, Loss: 2.036273274421692\n",
      "Epoch 19, Batch 800, Loss: 2.0353463888168335\n",
      "Epoch 19, Batch 900, Loss: 2.0223718547821044\n",
      "Epoch 20, Batch 100, Loss: 2.018945047855377\n",
      "Epoch 20, Batch 200, Loss: 2.013148047924042\n",
      "Epoch 20, Batch 300, Loss: 2.012632842063904\n",
      "Epoch 20, Batch 400, Loss: 2.0025810480117796\n",
      "Epoch 20, Batch 500, Loss: 2.0043515181541443\n",
      "Epoch 20, Batch 600, Loss: 1.9982365548610688\n",
      "Epoch 20, Batch 700, Loss: 1.9917040479183197\n",
      "Epoch 20, Batch 800, Loss: 1.988983200788498\n",
      "Epoch 20, Batch 900, Loss: 1.98490483045578\n",
      "Epoch 21, Batch 100, Loss: 1.9737462985515595\n",
      "Epoch 21, Batch 200, Loss: 1.9747712802886963\n",
      "Epoch 21, Batch 300, Loss: 1.9662754702568055\n",
      "Epoch 21, Batch 400, Loss: 1.9578301346302032\n",
      "Epoch 21, Batch 500, Loss: 1.947917023897171\n",
      "Epoch 21, Batch 600, Loss: 1.9505281352996826\n",
      "Epoch 21, Batch 700, Loss: 1.938514313697815\n",
      "Epoch 21, Batch 800, Loss: 1.9334395551681518\n",
      "Epoch 21, Batch 900, Loss: 1.9375474727153779\n",
      "Epoch 22, Batch 100, Loss: 1.9236931467056275\n",
      "Epoch 22, Batch 200, Loss: 1.9080911004543304\n",
      "Epoch 22, Batch 300, Loss: 1.9170891857147216\n",
      "Epoch 22, Batch 400, Loss: 1.90868554353714\n",
      "Epoch 22, Batch 500, Loss: 1.9030171489715577\n",
      "Epoch 22, Batch 600, Loss: 1.890907427072525\n",
      "Epoch 22, Batch 700, Loss: 1.8832594215869904\n",
      "Epoch 22, Batch 800, Loss: 1.888328971862793\n",
      "Epoch 22, Batch 900, Loss: 1.8771396160125733\n",
      "Epoch 23, Batch 100, Loss: 1.8679154622554779\n",
      "Epoch 23, Batch 200, Loss: 1.8661718368530273\n",
      "Epoch 23, Batch 300, Loss: 1.8555958008766174\n",
      "Epoch 23, Batch 400, Loss: 1.8438744580745696\n",
      "Epoch 23, Batch 500, Loss: 1.8415102100372314\n",
      "Epoch 23, Batch 600, Loss: 1.8447846925258637\n",
      "Epoch 23, Batch 700, Loss: 1.8364554369449615\n",
      "Epoch 23, Batch 800, Loss: 1.8270239543914795\n",
      "Epoch 23, Batch 900, Loss: 1.806810085773468\n",
      "Epoch 24, Batch 100, Loss: 1.8094238686561583\n",
      "Epoch 24, Batch 200, Loss: 1.8022825253009795\n",
      "Epoch 24, Batch 300, Loss: 1.802805495262146\n",
      "Epoch 24, Batch 400, Loss: 1.797669403553009\n",
      "Epoch 24, Batch 500, Loss: 1.7822060179710388\n",
      "Epoch 24, Batch 600, Loss: 1.7705939519405365\n",
      "Epoch 24, Batch 700, Loss: 1.7747055077552796\n",
      "Epoch 24, Batch 800, Loss: 1.7554570412635804\n",
      "Epoch 24, Batch 900, Loss: 1.7628906166553497\n",
      "Epoch 25, Batch 100, Loss: 1.745213429927826\n",
      "Epoch 25, Batch 200, Loss: 1.7362915205955505\n",
      "Epoch 25, Batch 300, Loss: 1.750170180797577\n",
      "Epoch 25, Batch 400, Loss: 1.7224820971488952\n",
      "Epoch 25, Batch 500, Loss: 1.7319685685634614\n",
      "Epoch 25, Batch 600, Loss: 1.7136647522449493\n",
      "Epoch 25, Batch 700, Loss: 1.7137858629226685\n",
      "Epoch 25, Batch 800, Loss: 1.7130807173252105\n",
      "Epoch 25, Batch 900, Loss: 1.6989229321479797\n",
      "Accuracy on test set: 0.4582%\n",
      "Fitting for combination 28\n",
      "784\n",
      "3\n",
      "10\n",
      "[30, 10, 50, 10]\n",
      "True\n",
      "['tanh', 'sigmoid', 'relu']\n",
      "Adam\n",
      "0.1\n",
      "0.03\n",
      "CrossEntropyLoss\n",
      "300\n",
      "25\n",
      "0\n",
      "Epoch 1, Batch 300, Loss: 6.936106112003326\n",
      "Epoch 1, Batch 600, Loss: 6.93848744392395\n",
      "Epoch 1, Batch 900, Loss: 6.924644515514374\n",
      "Epoch 2, Batch 300, Loss: 6.92754855632782\n",
      "Epoch 2, Batch 600, Loss: 6.937547478675842\n",
      "Epoch 2, Batch 900, Loss: 6.932967050075531\n",
      "Epoch 3, Batch 300, Loss: 6.940379166603089\n",
      "Epoch 3, Batch 600, Loss: 6.933468518257141\n",
      "Epoch 3, Batch 900, Loss: 6.926532897949219\n",
      "Epoch 4, Batch 300, Loss: 6.9344707560539245\n",
      "Epoch 4, Batch 600, Loss: 6.943761501312256\n",
      "Epoch 4, Batch 900, Loss: 6.9350408887863155\n",
      "Epoch 5, Batch 300, Loss: 6.936513597965241\n",
      "Epoch 5, Batch 600, Loss: 6.932724058628082\n",
      "Epoch 5, Batch 900, Loss: 6.938412218093872\n",
      "Epoch 6, Batch 300, Loss: 6.933588511943817\n",
      "Epoch 6, Batch 600, Loss: 6.934308166503906\n",
      "Epoch 6, Batch 900, Loss: 6.938716530799866\n",
      "Epoch 7, Batch 300, Loss: 6.936712455749512\n",
      "Epoch 7, Batch 600, Loss: 6.924981200695038\n",
      "Epoch 7, Batch 900, Loss: 6.933410425186157\n",
      "Epoch 8, Batch 300, Loss: 6.939091095924377\n",
      "Epoch 8, Batch 600, Loss: 6.929285395145416\n",
      "Epoch 8, Batch 900, Loss: 6.9320575976371765\n",
      "Epoch 9, Batch 300, Loss: 6.928238344192505\n",
      "Epoch 9, Batch 600, Loss: 6.955882709026337\n",
      "Epoch 9, Batch 900, Loss: 6.934624772071839\n",
      "Epoch 10, Batch 300, Loss: 6.93723240852356\n",
      "Epoch 10, Batch 600, Loss: 6.932598693370819\n",
      "Epoch 10, Batch 900, Loss: 6.931121997833252\n",
      "Epoch 11, Batch 300, Loss: 6.927604558467865\n",
      "Epoch 11, Batch 600, Loss: 6.940594401359558\n",
      "Epoch 11, Batch 900, Loss: 6.952421543598175\n",
      "Epoch 12, Batch 300, Loss: 6.92866063117981\n",
      "Epoch 12, Batch 600, Loss: 6.9256270575523375\n",
      "Epoch 12, Batch 900, Loss: 6.929288957118988\n",
      "Epoch 13, Batch 300, Loss: 6.93460697889328\n",
      "Epoch 13, Batch 600, Loss: 6.939898834228516\n",
      "Epoch 13, Batch 900, Loss: 6.936611790657043\n",
      "Epoch 14, Batch 300, Loss: 6.93845846414566\n",
      "Epoch 14, Batch 600, Loss: 6.9314633560180665\n",
      "Epoch 14, Batch 900, Loss: 6.936387083530426\n",
      "Epoch 15, Batch 300, Loss: 6.928165917396545\n",
      "Epoch 15, Batch 600, Loss: 6.930367739200592\n",
      "Epoch 15, Batch 900, Loss: 6.939530880451202\n",
      "Epoch 16, Batch 300, Loss: 6.932227077484131\n",
      "Epoch 16, Batch 600, Loss: 6.936273398399353\n",
      "Epoch 16, Batch 900, Loss: 6.938109719753266\n",
      "Epoch 17, Batch 300, Loss: 6.932833044528961\n",
      "Epoch 17, Batch 600, Loss: 6.936032619476318\n",
      "Epoch 17, Batch 900, Loss: 6.933358399868012\n",
      "Epoch 18, Batch 300, Loss: 6.929938793182373\n",
      "Epoch 18, Batch 600, Loss: 6.933339910507202\n",
      "Epoch 18, Batch 900, Loss: 6.937026906013489\n",
      "Epoch 19, Batch 300, Loss: 6.930262069702149\n",
      "Epoch 19, Batch 600, Loss: 6.954133460521698\n",
      "Epoch 19, Batch 900, Loss: 6.930552215576172\n",
      "Epoch 20, Batch 300, Loss: 6.931039338111877\n",
      "Epoch 20, Batch 600, Loss: 6.931751959323883\n",
      "Epoch 20, Batch 900, Loss: 6.933958971500397\n",
      "Epoch 21, Batch 300, Loss: 6.933403041362762\n",
      "Epoch 21, Batch 600, Loss: 6.934901058673859\n",
      "Epoch 21, Batch 900, Loss: 6.927407534122467\n",
      "Epoch 22, Batch 300, Loss: 6.943723690509796\n",
      "Epoch 22, Batch 600, Loss: 6.927918019294739\n",
      "Epoch 22, Batch 900, Loss: 6.9344802713394165\n",
      "Epoch 23, Batch 300, Loss: 6.933613452911377\n",
      "Epoch 23, Batch 600, Loss: 6.933435370922089\n",
      "Epoch 23, Batch 900, Loss: 6.9254700517654415\n",
      "Epoch 24, Batch 300, Loss: 6.9380451941490175\n",
      "Epoch 24, Batch 600, Loss: 6.935412013530732\n",
      "Epoch 24, Batch 900, Loss: 6.935399155616761\n",
      "Epoch 25, Batch 300, Loss: 6.933485178947449\n",
      "Epoch 25, Batch 600, Loss: 6.933600323200226\n",
      "Epoch 25, Batch 900, Loss: 6.9329617309570315\n",
      "Accuracy on test set: 0.1009%\n",
      "Fitting for combination 29\n",
      "784\n",
      "3\n",
      "10\n",
      "[30, 10, 50, 10]\n",
      "False\n",
      "['tanh', 'sigmoid', 'sigmoid']\n",
      "SGD\n",
      "0.003\n",
      "1\n",
      "CrossEntropyLoss\n",
      "100\n",
      "25\n",
      "0\n",
      "Epoch 1, Batch 100, Loss: 2.3392330193519593\n",
      "Epoch 1, Batch 200, Loss: 2.30967987537384\n",
      "Epoch 1, Batch 300, Loss: 2.3033331418037415\n",
      "Epoch 1, Batch 400, Loss: 2.301820342540741\n",
      "Epoch 1, Batch 500, Loss: 2.301772429943085\n",
      "Epoch 1, Batch 600, Loss: 2.3015062165260316\n",
      "Epoch 1, Batch 700, Loss: 2.300727779865265\n",
      "Epoch 1, Batch 800, Loss: 2.3021716022491456\n",
      "Epoch 1, Batch 900, Loss: 2.3013796281814574\n",
      "Epoch 2, Batch 100, Loss: 2.300719163417816\n",
      "Epoch 2, Batch 200, Loss: 2.3019867944717407\n",
      "Epoch 2, Batch 300, Loss: 2.301824107170105\n",
      "Epoch 2, Batch 400, Loss: 2.3008490896224973\n",
      "Epoch 2, Batch 500, Loss: 2.3014269709587096\n",
      "Epoch 2, Batch 600, Loss: 2.3008917593955993\n",
      "Epoch 2, Batch 700, Loss: 2.302128496170044\n",
      "Epoch 2, Batch 800, Loss: 2.301913118362427\n",
      "Epoch 2, Batch 900, Loss: 2.3023026394844055\n",
      "Epoch 3, Batch 100, Loss: 2.3021148824691773\n",
      "Epoch 3, Batch 200, Loss: 2.3021067118644716\n",
      "Epoch 3, Batch 300, Loss: 2.3016284036636354\n",
      "Epoch 3, Batch 400, Loss: 2.3014512872695922\n",
      "Epoch 3, Batch 500, Loss: 2.300874664783478\n",
      "Epoch 3, Batch 600, Loss: 2.3013506174087524\n",
      "Epoch 3, Batch 700, Loss: 2.3016950035095216\n",
      "Epoch 3, Batch 800, Loss: 2.301207013130188\n",
      "Epoch 3, Batch 900, Loss: 2.3013818860054016\n",
      "Epoch 4, Batch 100, Loss: 2.3018820023536684\n",
      "Epoch 4, Batch 200, Loss: 2.3017239284515383\n",
      "Epoch 4, Batch 300, Loss: 2.300723056793213\n",
      "Epoch 4, Batch 400, Loss: 2.3015777349472044\n",
      "Epoch 4, Batch 500, Loss: 2.3010321831703187\n",
      "Epoch 4, Batch 600, Loss: 2.3021024537086485\n",
      "Epoch 4, Batch 700, Loss: 2.301661491394043\n",
      "Epoch 4, Batch 800, Loss: 2.3013984966278076\n",
      "Epoch 4, Batch 900, Loss: 2.301641252040863\n",
      "Epoch 5, Batch 100, Loss: 2.3007784199714663\n",
      "Epoch 5, Batch 200, Loss: 2.3021783876419066\n",
      "Epoch 5, Batch 300, Loss: 2.301827437877655\n",
      "Epoch 5, Batch 400, Loss: 2.3013867378234862\n",
      "Epoch 5, Batch 500, Loss: 2.3015564823150636\n",
      "Epoch 5, Batch 600, Loss: 2.301601164340973\n",
      "Epoch 5, Batch 700, Loss: 2.301534140110016\n",
      "Epoch 5, Batch 800, Loss: 2.30057466506958\n",
      "Epoch 5, Batch 900, Loss: 2.301968688964844\n",
      "Epoch 6, Batch 100, Loss: 2.3020969891548155\n",
      "Epoch 6, Batch 200, Loss: 2.3016260862350464\n",
      "Epoch 6, Batch 300, Loss: 2.30130686044693\n",
      "Epoch 6, Batch 400, Loss: 2.3007712984085082\n",
      "Epoch 6, Batch 500, Loss: 2.301768112182617\n",
      "Epoch 6, Batch 600, Loss: 2.302053532600403\n",
      "Epoch 6, Batch 700, Loss: 2.3024242234230043\n",
      "Epoch 6, Batch 800, Loss: 2.301694030761719\n",
      "Epoch 6, Batch 900, Loss: 2.301034507751465\n",
      "Epoch 7, Batch 100, Loss: 2.300898406505585\n",
      "Epoch 7, Batch 200, Loss: 2.3011612606048586\n",
      "Epoch 7, Batch 300, Loss: 2.3015947008132933\n",
      "Epoch 7, Batch 400, Loss: 2.301858911514282\n",
      "Epoch 7, Batch 500, Loss: 2.3021802830696108\n",
      "Epoch 7, Batch 600, Loss: 2.300829327106476\n",
      "Epoch 7, Batch 700, Loss: 2.3021795201301574\n",
      "Epoch 7, Batch 800, Loss: 2.301810417175293\n",
      "Epoch 7, Batch 900, Loss: 2.3017394089698793\n",
      "Epoch 8, Batch 100, Loss: 2.301347575187683\n",
      "Epoch 8, Batch 200, Loss: 2.3012226819992065\n",
      "Epoch 8, Batch 300, Loss: 2.3018604564666747\n",
      "Epoch 8, Batch 400, Loss: 2.301482846736908\n",
      "Epoch 8, Batch 500, Loss: 2.301635112762451\n",
      "Epoch 8, Batch 600, Loss: 2.3013955903053285\n",
      "Epoch 8, Batch 700, Loss: 2.301348762512207\n",
      "Epoch 8, Batch 800, Loss: 2.3020584750175477\n",
      "Epoch 8, Batch 900, Loss: 2.3014270758628843\n",
      "Epoch 9, Batch 100, Loss: 2.301325535774231\n",
      "Epoch 9, Batch 200, Loss: 2.300949680805206\n",
      "Epoch 9, Batch 300, Loss: 2.301952052116394\n",
      "Epoch 9, Batch 400, Loss: 2.301389708518982\n",
      "Epoch 9, Batch 500, Loss: 2.301096158027649\n",
      "Epoch 9, Batch 600, Loss: 2.3018206429481505\n",
      "Epoch 9, Batch 700, Loss: 2.3018156599998476\n",
      "Epoch 9, Batch 800, Loss: 2.3013811326026916\n",
      "Epoch 9, Batch 900, Loss: 2.3016160488128663\n",
      "Epoch 10, Batch 100, Loss: 2.301403110027313\n",
      "Epoch 10, Batch 200, Loss: 2.3019929480552674\n",
      "Epoch 10, Batch 300, Loss: 2.300530877113342\n",
      "Epoch 10, Batch 400, Loss: 2.301890881061554\n",
      "Epoch 10, Batch 500, Loss: 2.302449018955231\n",
      "Epoch 10, Batch 600, Loss: 2.3016546845436094\n",
      "Epoch 10, Batch 700, Loss: 2.301950361728668\n",
      "Epoch 10, Batch 800, Loss: 2.300766077041626\n",
      "Epoch 10, Batch 900, Loss: 2.3016048288345337\n",
      "Epoch 11, Batch 100, Loss: 2.3019482040405275\n",
      "Epoch 11, Batch 200, Loss: 2.301332051753998\n",
      "Epoch 11, Batch 300, Loss: 2.3011302375793456\n",
      "Epoch 11, Batch 400, Loss: 2.3011969232559206\n",
      "Epoch 11, Batch 500, Loss: 2.301423206329346\n",
      "Epoch 11, Batch 600, Loss: 2.302259247303009\n",
      "Epoch 11, Batch 700, Loss: 2.301020927429199\n",
      "Epoch 11, Batch 800, Loss: 2.3018304252624513\n",
      "Epoch 11, Batch 900, Loss: 2.301472327709198\n",
      "Epoch 12, Batch 100, Loss: 2.3020895195007323\n",
      "Epoch 12, Batch 200, Loss: 2.301917088031769\n",
      "Epoch 12, Batch 300, Loss: 2.3006015276908873\n",
      "Epoch 12, Batch 400, Loss: 2.300750379562378\n",
      "Epoch 12, Batch 500, Loss: 2.3014482045173645\n",
      "Epoch 12, Batch 600, Loss: 2.301238360404968\n",
      "Epoch 12, Batch 700, Loss: 2.30162770986557\n",
      "Epoch 12, Batch 800, Loss: 2.301644997596741\n",
      "Epoch 12, Batch 900, Loss: 2.3021185636520385\n",
      "Epoch 13, Batch 100, Loss: 2.302070755958557\n",
      "Epoch 13, Batch 200, Loss: 2.3019296026229856\n",
      "Epoch 13, Batch 300, Loss: 2.301570131778717\n",
      "Epoch 13, Batch 400, Loss: 2.3019794464111327\n",
      "Epoch 13, Batch 500, Loss: 2.3015157628059386\n",
      "Epoch 13, Batch 600, Loss: 2.3018346428871155\n",
      "Epoch 13, Batch 700, Loss: 2.301364049911499\n",
      "Epoch 13, Batch 800, Loss: 2.300834443569183\n",
      "Epoch 13, Batch 900, Loss: 2.301491627693176\n",
      "Epoch 14, Batch 100, Loss: 2.3012782287597657\n",
      "Epoch 14, Batch 200, Loss: 2.301471798419952\n",
      "Epoch 14, Batch 300, Loss: 2.3018592596054077\n",
      "Epoch 14, Batch 400, Loss: 2.3014362144470213\n",
      "Epoch 14, Batch 500, Loss: 2.301856324672699\n",
      "Epoch 14, Batch 600, Loss: 2.3024163961410524\n",
      "Epoch 14, Batch 700, Loss: 2.3016289901733398\n",
      "Epoch 14, Batch 800, Loss: 2.300682899951935\n",
      "Epoch 14, Batch 900, Loss: 2.301747932434082\n",
      "Epoch 15, Batch 100, Loss: 2.3005871987342834\n",
      "Epoch 15, Batch 200, Loss: 2.3009538674354553\n",
      "Epoch 15, Batch 300, Loss: 2.3015153336524965\n",
      "Epoch 15, Batch 400, Loss: 2.3018318796157837\n",
      "Epoch 15, Batch 500, Loss: 2.302359540462494\n",
      "Epoch 15, Batch 600, Loss: 2.3019578099250793\n",
      "Epoch 15, Batch 700, Loss: 2.3020280504226687\n",
      "Epoch 15, Batch 800, Loss: 2.3012106585502625\n",
      "Epoch 15, Batch 900, Loss: 2.3012315940856936\n",
      "Epoch 16, Batch 100, Loss: 2.3018627548217774\n",
      "Epoch 16, Batch 200, Loss: 2.3020855021476745\n",
      "Epoch 16, Batch 300, Loss: 2.301364498138428\n",
      "Epoch 16, Batch 400, Loss: 2.301928074359894\n",
      "Epoch 16, Batch 500, Loss: 2.3014552187919617\n",
      "Epoch 16, Batch 600, Loss: 2.301723380088806\n",
      "Epoch 16, Batch 700, Loss: 2.301589469909668\n",
      "Epoch 16, Batch 800, Loss: 2.3011194968223574\n",
      "Epoch 16, Batch 900, Loss: 2.301276478767395\n",
      "Epoch 17, Batch 100, Loss: 2.3005679082870483\n",
      "Epoch 17, Batch 200, Loss: 2.3023136901855468\n",
      "Epoch 17, Batch 300, Loss: 2.301188714504242\n",
      "Epoch 17, Batch 400, Loss: 2.302396066188812\n",
      "Epoch 17, Batch 500, Loss: 2.3020003771781923\n",
      "Epoch 17, Batch 600, Loss: 2.3019558787345886\n",
      "Epoch 17, Batch 700, Loss: 2.3018567943573\n",
      "Epoch 17, Batch 800, Loss: 2.301012361049652\n",
      "Epoch 17, Batch 900, Loss: 2.300830409526825\n",
      "Epoch 18, Batch 100, Loss: 2.300854320526123\n",
      "Epoch 18, Batch 200, Loss: 2.3013750863075257\n",
      "Epoch 18, Batch 300, Loss: 2.3022420144081117\n",
      "Epoch 18, Batch 400, Loss: 2.301972646713257\n",
      "Epoch 18, Batch 500, Loss: 2.3019340682029723\n",
      "Epoch 18, Batch 600, Loss: 2.3006556153297426\n",
      "Epoch 18, Batch 700, Loss: 2.301635653972626\n",
      "Epoch 18, Batch 800, Loss: 2.301745436191559\n",
      "Epoch 18, Batch 900, Loss: 2.3015736985206603\n",
      "Epoch 19, Batch 100, Loss: 2.3009097146987916\n",
      "Epoch 19, Batch 200, Loss: 2.3013217139244078\n",
      "Epoch 19, Batch 300, Loss: 2.3008170413970945\n",
      "Epoch 19, Batch 400, Loss: 2.3015092277526854\n",
      "Epoch 19, Batch 500, Loss: 2.301027512550354\n",
      "Epoch 19, Batch 600, Loss: 2.301119785308838\n",
      "Epoch 19, Batch 700, Loss: 2.3024893450737\n",
      "Epoch 19, Batch 800, Loss: 2.3012972402572633\n",
      "Epoch 19, Batch 900, Loss: 2.3024412536621095\n",
      "Epoch 20, Batch 100, Loss: 2.3016105914115905\n",
      "Epoch 20, Batch 200, Loss: 2.3019780039787294\n",
      "Epoch 20, Batch 300, Loss: 2.302249207496643\n",
      "Epoch 20, Batch 400, Loss: 2.3019200921058656\n",
      "Epoch 20, Batch 500, Loss: 2.301166086196899\n",
      "Epoch 20, Batch 600, Loss: 2.3009795808792113\n",
      "Epoch 20, Batch 700, Loss: 2.3024588656425475\n",
      "Epoch 20, Batch 800, Loss: 2.3012795424461365\n",
      "Epoch 20, Batch 900, Loss: 2.3010695695877077\n",
      "Epoch 21, Batch 100, Loss: 2.3018787813186647\n",
      "Epoch 21, Batch 200, Loss: 2.30123694896698\n",
      "Epoch 21, Batch 300, Loss: 2.301671266555786\n",
      "Epoch 21, Batch 400, Loss: 2.3013086795806883\n",
      "Epoch 21, Batch 500, Loss: 2.301545774936676\n",
      "Epoch 21, Batch 600, Loss: 2.3020559430122374\n",
      "Epoch 21, Batch 700, Loss: 2.3015563464164734\n",
      "Epoch 21, Batch 800, Loss: 2.30136049747467\n",
      "Epoch 21, Batch 900, Loss: 2.301442093849182\n",
      "Epoch 22, Batch 100, Loss: 2.301678030490875\n",
      "Epoch 22, Batch 200, Loss: 2.302054340839386\n",
      "Epoch 22, Batch 300, Loss: 2.3016690683364867\n",
      "Epoch 22, Batch 400, Loss: 2.302147274017334\n",
      "Epoch 22, Batch 500, Loss: 2.301465210914612\n",
      "Epoch 22, Batch 600, Loss: 2.301451723575592\n",
      "Epoch 22, Batch 700, Loss: 2.3011493778228758\n",
      "Epoch 22, Batch 800, Loss: 2.301950762271881\n",
      "Epoch 22, Batch 900, Loss: 2.3010397815704344\n",
      "Epoch 23, Batch 100, Loss: 2.301434440612793\n",
      "Epoch 23, Batch 200, Loss: 2.301494059562683\n",
      "Epoch 23, Batch 300, Loss: 2.3009618854522706\n",
      "Epoch 23, Batch 400, Loss: 2.3015998721122743\n",
      "Epoch 23, Batch 500, Loss: 2.301804540157318\n",
      "Epoch 23, Batch 600, Loss: 2.3016209316253664\n",
      "Epoch 23, Batch 700, Loss: 2.30156081199646\n",
      "Epoch 23, Batch 800, Loss: 2.3021168541908263\n",
      "Epoch 23, Batch 900, Loss: 2.3008269667625427\n",
      "Epoch 24, Batch 100, Loss: 2.3015155506134035\n",
      "Epoch 24, Batch 200, Loss: 2.3014398860931395\n",
      "Epoch 24, Batch 300, Loss: 2.300891435146332\n",
      "Epoch 24, Batch 400, Loss: 2.3005329751968384\n",
      "Epoch 24, Batch 500, Loss: 2.3017789053916933\n",
      "Epoch 24, Batch 600, Loss: 2.3011119842529295\n",
      "Epoch 24, Batch 700, Loss: 2.3015380358695983\n",
      "Epoch 24, Batch 800, Loss: 2.303002800941467\n",
      "Epoch 24, Batch 900, Loss: 2.3019812297821045\n",
      "Epoch 25, Batch 100, Loss: 2.3012837505340578\n",
      "Epoch 25, Batch 200, Loss: 2.3014233708381653\n",
      "Epoch 25, Batch 300, Loss: 2.301620740890503\n",
      "Epoch 25, Batch 400, Loss: 2.3006730246543885\n",
      "Epoch 25, Batch 500, Loss: 2.3015295672416687\n",
      "Epoch 25, Batch 600, Loss: 2.301565330028534\n",
      "Epoch 25, Batch 700, Loss: 2.3012514019012453\n",
      "Epoch 25, Batch 800, Loss: 2.302651915550232\n",
      "Epoch 25, Batch 900, Loss: 2.3017731213569643\n",
      "Accuracy on test set: 0.1135%\n",
      "Fitting for combination 30\n",
      "784\n",
      "4\n",
      "10\n",
      "[30, 10, 10, 10, 10]\n",
      "True\n",
      "['tanh', 'sigmoid', 'relu', 'relu']\n",
      "Adam\n",
      "0.001\n",
      "0.01\n",
      "CrossEntropyLoss\n",
      "200\n",
      "25\n",
      "0\n",
      "Epoch 1, Batch 200, Loss: 4.366308467388153\n",
      "Epoch 1, Batch 400, Loss: 3.185661016702652\n",
      "Epoch 1, Batch 600, Loss: 2.522734053134918\n",
      "Epoch 1, Batch 800, Loss: 1.983599852323532\n",
      "Epoch 2, Batch 200, Loss: 1.46362173050642\n",
      "Epoch 2, Batch 400, Loss: 1.2932159820199012\n",
      "Epoch 2, Batch 600, Loss: 1.2145704525709151\n",
      "Epoch 2, Batch 800, Loss: 1.158442635834217\n",
      "Epoch 3, Batch 200, Loss: 1.0546020346879958\n",
      "Epoch 3, Batch 400, Loss: 1.0519735416769982\n",
      "Epoch 3, Batch 600, Loss: 1.0511952966451645\n",
      "Epoch 3, Batch 800, Loss: 0.9941181707382202\n",
      "Epoch 4, Batch 200, Loss: 1.009689072072506\n",
      "Epoch 4, Batch 400, Loss: 0.9501861099898815\n",
      "Epoch 4, Batch 600, Loss: 0.9081818616390228\n",
      "Epoch 4, Batch 800, Loss: 1.0123707929253578\n",
      "Epoch 5, Batch 200, Loss: 0.8851734921336174\n",
      "Epoch 5, Batch 400, Loss: 0.9100407269597054\n",
      "Epoch 5, Batch 600, Loss: 0.9311589618027211\n",
      "Epoch 5, Batch 800, Loss: 0.9516276922821999\n",
      "Epoch 6, Batch 200, Loss: 0.8876411859691143\n",
      "Epoch 6, Batch 400, Loss: 0.8774582576751709\n",
      "Epoch 6, Batch 600, Loss: 0.8802631171047688\n",
      "Epoch 6, Batch 800, Loss: 0.9376181595027446\n",
      "Epoch 7, Batch 200, Loss: 0.8942546764016152\n",
      "Epoch 7, Batch 400, Loss: 0.8891319699585438\n",
      "Epoch 7, Batch 600, Loss: 0.8743797957897186\n",
      "Epoch 7, Batch 800, Loss: 0.8881829865276814\n",
      "Epoch 8, Batch 200, Loss: 0.8380721618235111\n",
      "Epoch 8, Batch 400, Loss: 0.8628865751624107\n",
      "Epoch 8, Batch 600, Loss: 0.8767749753594398\n",
      "Epoch 8, Batch 800, Loss: 0.8798182393610477\n",
      "Epoch 9, Batch 200, Loss: 0.8604526121914386\n",
      "Epoch 9, Batch 400, Loss: 0.8831058304011822\n",
      "Epoch 9, Batch 600, Loss: 0.832407034933567\n",
      "Epoch 9, Batch 800, Loss: 0.8974262902140617\n",
      "Epoch 10, Batch 200, Loss: 0.8351628126204014\n",
      "Epoch 10, Batch 400, Loss: 0.8625846368074417\n",
      "Epoch 10, Batch 600, Loss: 0.8609987933933735\n",
      "Epoch 10, Batch 800, Loss: 0.8475757336616516\n",
      "Epoch 11, Batch 200, Loss: 0.8510070638358593\n",
      "Epoch 11, Batch 400, Loss: 0.8679235166311264\n",
      "Epoch 11, Batch 600, Loss: 0.8444900660216809\n",
      "Epoch 11, Batch 800, Loss: 0.827392708659172\n",
      "Epoch 12, Batch 200, Loss: 0.8466724719107152\n",
      "Epoch 12, Batch 400, Loss: 0.8555984935164451\n",
      "Epoch 12, Batch 600, Loss: 0.8221801705658436\n",
      "Epoch 12, Batch 800, Loss: 0.8100124460458755\n",
      "Epoch 13, Batch 200, Loss: 0.8268299548327923\n",
      "Epoch 13, Batch 400, Loss: 0.8377697704732419\n",
      "Epoch 13, Batch 600, Loss: 0.8430678050220013\n",
      "Epoch 13, Batch 800, Loss: 0.834242123812437\n",
      "Epoch 14, Batch 200, Loss: 0.8242648413777351\n",
      "Epoch 14, Batch 400, Loss: 0.825055828243494\n",
      "Epoch 14, Batch 600, Loss: 0.858989922106266\n",
      "Epoch 14, Batch 800, Loss: 0.8647314813733101\n",
      "Epoch 15, Batch 200, Loss: 0.8223428785800934\n",
      "Epoch 15, Batch 400, Loss: 0.8451254729926586\n",
      "Epoch 15, Batch 600, Loss: 0.8365396751463413\n",
      "Epoch 15, Batch 800, Loss: 0.8645668070018292\n",
      "Epoch 16, Batch 200, Loss: 0.8180838996171951\n",
      "Epoch 16, Batch 400, Loss: 0.8306179609894753\n",
      "Epoch 16, Batch 600, Loss: 0.8598904621601104\n",
      "Epoch 16, Batch 800, Loss: 0.8191452166438102\n",
      "Epoch 17, Batch 200, Loss: 0.836821004152298\n",
      "Epoch 17, Batch 400, Loss: 0.8281701494753361\n",
      "Epoch 17, Batch 600, Loss: 0.8385458397865295\n",
      "Epoch 17, Batch 800, Loss: 0.8453090590238571\n",
      "Epoch 18, Batch 200, Loss: 0.8307880073785782\n",
      "Epoch 18, Batch 400, Loss: 0.8376811608672142\n",
      "Epoch 18, Batch 600, Loss: 0.825980643928051\n",
      "Epoch 18, Batch 800, Loss: 0.8401997980475425\n",
      "Epoch 19, Batch 200, Loss: 0.8205535447597504\n",
      "Epoch 19, Batch 400, Loss: 0.837870225161314\n",
      "Epoch 19, Batch 600, Loss: 0.8207707433402538\n",
      "Epoch 19, Batch 800, Loss: 0.813804891705513\n",
      "Epoch 20, Batch 200, Loss: 0.813139581233263\n",
      "Epoch 20, Batch 400, Loss: 0.833286907672882\n",
      "Epoch 20, Batch 600, Loss: 0.8062183065712452\n",
      "Epoch 20, Batch 800, Loss: 0.8508358187973499\n",
      "Epoch 21, Batch 200, Loss: 0.8112973363697529\n",
      "Epoch 21, Batch 400, Loss: 0.7989002010226249\n",
      "Epoch 21, Batch 600, Loss: 0.8085041742026806\n",
      "Epoch 21, Batch 800, Loss: 0.839961880594492\n",
      "Epoch 22, Batch 200, Loss: 0.8177063274383545\n",
      "Epoch 22, Batch 400, Loss: 0.8078850603103638\n",
      "Epoch 22, Batch 600, Loss: 0.8056909635663032\n",
      "Epoch 22, Batch 800, Loss: 0.8012726840376854\n",
      "Epoch 23, Batch 200, Loss: 0.8088280527293682\n",
      "Epoch 23, Batch 400, Loss: 0.8353991484642029\n",
      "Epoch 23, Batch 600, Loss: 0.7901479494571686\n",
      "Epoch 23, Batch 800, Loss: 0.8285379068553448\n",
      "Epoch 24, Batch 200, Loss: 0.8220388677716255\n",
      "Epoch 24, Batch 400, Loss: 0.8064060837030411\n",
      "Epoch 24, Batch 600, Loss: 0.8007576926052571\n",
      "Epoch 24, Batch 800, Loss: 0.8394132752716541\n",
      "Epoch 25, Batch 200, Loss: 0.7895873828232288\n",
      "Epoch 25, Batch 400, Loss: 0.8275178226828576\n",
      "Epoch 25, Batch 600, Loss: 0.7981907203793526\n",
      "Epoch 25, Batch 800, Loss: 0.8076948402822017\n",
      "Accuracy on test set: 0.8953%\n",
      "Fitting for combination 31\n",
      "784\n",
      "4\n",
      "10\n",
      "[30, 10, 10, 10, 10]\n",
      "True\n",
      "['tanh', 'sigmoid', 'relu', 'tanh']\n",
      "SGD\n",
      "0.3\n",
      "0.01\n",
      "CrossEntropyLoss\n",
      "100\n",
      "25\n",
      "0\n",
      "Epoch 1, Batch 100, Loss: 2.280077352523804\n",
      "Epoch 1, Batch 200, Loss: 1.9393078458309174\n",
      "Epoch 1, Batch 300, Loss: 1.6191018664836883\n",
      "Epoch 1, Batch 400, Loss: 1.5020221614837646\n",
      "Epoch 1, Batch 500, Loss: 1.4298346650600433\n",
      "Epoch 1, Batch 600, Loss: 1.3163825297355651\n",
      "Epoch 1, Batch 700, Loss: 1.3524250262975692\n",
      "Epoch 1, Batch 800, Loss: 1.2644016009569168\n",
      "Epoch 1, Batch 900, Loss: 1.2769741076231003\n",
      "Epoch 2, Batch 100, Loss: 1.259671949148178\n",
      "Epoch 2, Batch 200, Loss: 1.2417452496290207\n",
      "Epoch 2, Batch 300, Loss: 1.1546269899606705\n",
      "Epoch 2, Batch 400, Loss: 1.1921173536777496\n",
      "Epoch 2, Batch 500, Loss: 1.2314781904220582\n",
      "Epoch 2, Batch 600, Loss: 1.1256657326221466\n",
      "Epoch 2, Batch 700, Loss: 1.1002064138650893\n",
      "Epoch 2, Batch 800, Loss: 1.152094920873642\n",
      "Epoch 2, Batch 900, Loss: 1.055571398139\n",
      "Epoch 3, Batch 100, Loss: 1.0082232385873795\n",
      "Epoch 3, Batch 200, Loss: 1.0269591587781906\n",
      "Epoch 3, Batch 300, Loss: 1.1306542360782623\n",
      "Epoch 3, Batch 400, Loss: 1.0539527958631516\n",
      "Epoch 3, Batch 500, Loss: 1.0770545691251754\n",
      "Epoch 3, Batch 600, Loss: 1.003728711605072\n",
      "Epoch 3, Batch 700, Loss: 0.9845675605535508\n",
      "Epoch 3, Batch 800, Loss: 0.9928545558452606\n",
      "Epoch 3, Batch 900, Loss: 1.0637751185894013\n",
      "Epoch 4, Batch 100, Loss: 0.9864862352609635\n",
      "Epoch 4, Batch 200, Loss: 1.073316615819931\n",
      "Epoch 4, Batch 300, Loss: 0.9356988489627838\n",
      "Epoch 4, Batch 400, Loss: 0.9487323355674744\n",
      "Epoch 4, Batch 500, Loss: 0.9708303523063659\n",
      "Epoch 4, Batch 600, Loss: 0.8979683446884156\n",
      "Epoch 4, Batch 700, Loss: 0.9561843633651733\n",
      "Epoch 4, Batch 800, Loss: 0.8707363677024841\n",
      "Epoch 4, Batch 900, Loss: 0.9275533699989319\n",
      "Epoch 5, Batch 100, Loss: 0.9788799703121185\n",
      "Epoch 5, Batch 200, Loss: 0.9447808972001076\n",
      "Epoch 5, Batch 300, Loss: 0.8612808167934418\n",
      "Epoch 5, Batch 400, Loss: 0.9522247016429901\n",
      "Epoch 5, Batch 500, Loss: 0.8421382230520248\n",
      "Epoch 5, Batch 600, Loss: 0.8826819199323654\n",
      "Epoch 5, Batch 700, Loss: 0.9284998083114624\n",
      "Epoch 5, Batch 800, Loss: 0.9602650636434555\n",
      "Epoch 5, Batch 900, Loss: 0.9294984418153763\n",
      "Epoch 6, Batch 100, Loss: 0.919657433629036\n",
      "Epoch 6, Batch 200, Loss: 0.8428504258394242\n",
      "Epoch 6, Batch 300, Loss: 0.8808395305275917\n",
      "Epoch 6, Batch 400, Loss: 0.8448720145225525\n",
      "Epoch 6, Batch 500, Loss: 0.8280441343784333\n",
      "Epoch 6, Batch 600, Loss: 0.9054525536298752\n",
      "Epoch 6, Batch 700, Loss: 0.9074786841869354\n",
      "Epoch 6, Batch 800, Loss: 0.9978962150216103\n",
      "Epoch 6, Batch 900, Loss: 0.8428216233849526\n",
      "Epoch 7, Batch 100, Loss: 0.896504470705986\n",
      "Epoch 7, Batch 200, Loss: 0.8874232855439186\n",
      "Epoch 7, Batch 300, Loss: 0.9716546094417572\n",
      "Epoch 7, Batch 400, Loss: 0.9342004972696304\n",
      "Epoch 7, Batch 500, Loss: 0.8306014037132263\n",
      "Epoch 7, Batch 600, Loss: 0.954077063202858\n",
      "Epoch 7, Batch 700, Loss: 0.9280463778972625\n",
      "Epoch 7, Batch 800, Loss: 0.8163478487730026\n",
      "Epoch 7, Batch 900, Loss: 0.919141736626625\n",
      "Epoch 8, Batch 100, Loss: 0.8465973755717278\n",
      "Epoch 8, Batch 200, Loss: 0.8750278848409653\n",
      "Epoch 8, Batch 300, Loss: 0.9289500617980957\n",
      "Epoch 8, Batch 400, Loss: 0.8262851530313492\n",
      "Epoch 8, Batch 500, Loss: 0.9683634734153748\n",
      "Epoch 8, Batch 600, Loss: 0.8826758843660355\n",
      "Epoch 8, Batch 700, Loss: 0.8186851137876511\n",
      "Epoch 8, Batch 800, Loss: 0.8677259540557861\n",
      "Epoch 8, Batch 900, Loss: 0.8399669483304024\n",
      "Epoch 9, Batch 100, Loss: 0.9686075627803803\n",
      "Epoch 9, Batch 200, Loss: 0.845759397149086\n",
      "Epoch 9, Batch 300, Loss: 0.8024951446056366\n",
      "Epoch 9, Batch 400, Loss: 0.8984689098596573\n",
      "Epoch 9, Batch 500, Loss: 0.9190318924188614\n",
      "Epoch 9, Batch 600, Loss: 0.8980677935481072\n",
      "Epoch 9, Batch 700, Loss: 0.901084017753601\n",
      "Epoch 9, Batch 800, Loss: 0.9125102788209916\n",
      "Epoch 9, Batch 900, Loss: 0.8724259752035141\n",
      "Epoch 10, Batch 100, Loss: 0.8355970227718353\n",
      "Epoch 10, Batch 200, Loss: 0.890022802054882\n",
      "Epoch 10, Batch 300, Loss: 0.9373491913080215\n",
      "Epoch 10, Batch 400, Loss: 0.8925728586316108\n",
      "Epoch 10, Batch 500, Loss: 0.925923969745636\n",
      "Epoch 10, Batch 600, Loss: 0.9604405558109284\n",
      "Epoch 10, Batch 700, Loss: 0.8982694953680038\n",
      "Epoch 10, Batch 800, Loss: 0.8762484675645829\n",
      "Epoch 10, Batch 900, Loss: 0.8013818275928497\n",
      "Epoch 11, Batch 100, Loss: 0.8665771615505219\n",
      "Epoch 11, Batch 200, Loss: 0.8434319433569908\n",
      "Epoch 11, Batch 300, Loss: 0.9573212039470672\n",
      "Epoch 11, Batch 400, Loss: 0.8675834703445434\n",
      "Epoch 11, Batch 500, Loss: 0.9432857111096382\n",
      "Epoch 11, Batch 600, Loss: 0.857877396941185\n",
      "Epoch 11, Batch 700, Loss: 0.9170664504170418\n",
      "Epoch 11, Batch 800, Loss: 0.8589090272784233\n",
      "Epoch 11, Batch 900, Loss: 0.8960739028453827\n",
      "Epoch 12, Batch 100, Loss: 0.872275413274765\n",
      "Epoch 12, Batch 200, Loss: 0.8734745466709137\n",
      "Epoch 12, Batch 300, Loss: 0.9057189497351646\n",
      "Epoch 12, Batch 400, Loss: 0.8605679965019226\n",
      "Epoch 12, Batch 500, Loss: 0.9282705944776535\n",
      "Epoch 12, Batch 600, Loss: 0.8408405184745789\n",
      "Epoch 12, Batch 700, Loss: 1.0023110929131507\n",
      "Epoch 12, Batch 800, Loss: 0.884183719754219\n",
      "Epoch 12, Batch 900, Loss: 0.9645196831226349\n",
      "Epoch 13, Batch 100, Loss: 0.8506878513097763\n",
      "Epoch 13, Batch 200, Loss: 0.8225504684448243\n",
      "Epoch 13, Batch 300, Loss: 0.892852337360382\n",
      "Epoch 13, Batch 400, Loss: 0.8747721284627914\n",
      "Epoch 13, Batch 500, Loss: 0.9548173135519028\n",
      "Epoch 13, Batch 600, Loss: 0.8220667693018914\n",
      "Epoch 13, Batch 700, Loss: 0.8173211795091629\n",
      "Epoch 13, Batch 800, Loss: 0.895705555677414\n",
      "Epoch 13, Batch 900, Loss: 0.8950550216436386\n",
      "Epoch 14, Batch 100, Loss: 0.9034780368208886\n",
      "Epoch 14, Batch 200, Loss: 0.8616682606935501\n",
      "Epoch 14, Batch 300, Loss: 0.8121092587709426\n",
      "Epoch 14, Batch 400, Loss: 0.8680958488583564\n",
      "Epoch 14, Batch 500, Loss: 0.8380408218502998\n",
      "Epoch 14, Batch 600, Loss: 0.8726450282335282\n",
      "Epoch 14, Batch 700, Loss: 0.8496920335292816\n",
      "Epoch 14, Batch 800, Loss: 0.7587412598729134\n",
      "Epoch 14, Batch 900, Loss: 0.8552612125873565\n",
      "Epoch 15, Batch 100, Loss: 0.8238661476969719\n",
      "Epoch 15, Batch 200, Loss: 0.8439236563444138\n",
      "Epoch 15, Batch 300, Loss: 0.9218555042147636\n",
      "Epoch 15, Batch 400, Loss: 0.9036473980545998\n",
      "Epoch 15, Batch 500, Loss: 0.8761836379766464\n",
      "Epoch 15, Batch 600, Loss: 0.8705825859308243\n",
      "Epoch 15, Batch 700, Loss: 0.9035158199071884\n",
      "Epoch 15, Batch 800, Loss: 0.9661057281494141\n",
      "Epoch 15, Batch 900, Loss: 0.8340143519639969\n",
      "Epoch 16, Batch 100, Loss: 0.8333034458756446\n",
      "Epoch 16, Batch 200, Loss: 0.8623421335220337\n",
      "Epoch 16, Batch 300, Loss: 0.8534253066778184\n",
      "Epoch 16, Batch 400, Loss: 0.8917498940229416\n",
      "Epoch 16, Batch 500, Loss: 0.844586250782013\n",
      "Epoch 16, Batch 600, Loss: 0.8776438134908676\n",
      "Epoch 16, Batch 700, Loss: 0.8678261014819145\n",
      "Epoch 16, Batch 800, Loss: 0.8228814879059791\n",
      "Epoch 16, Batch 900, Loss: 0.819995270371437\n",
      "Epoch 17, Batch 100, Loss: 0.8729982101917266\n",
      "Epoch 17, Batch 200, Loss: 0.8429695838689804\n",
      "Epoch 17, Batch 300, Loss: 0.8966838526725769\n",
      "Epoch 17, Batch 400, Loss: 0.8228619712591171\n",
      "Epoch 17, Batch 500, Loss: 0.9194412732124329\n",
      "Epoch 17, Batch 600, Loss: 0.8735635074973106\n",
      "Epoch 17, Batch 700, Loss: 0.9067638939619065\n",
      "Epoch 17, Batch 800, Loss: 0.7521946480870247\n",
      "Epoch 17, Batch 900, Loss: 0.9438782238960266\n",
      "Epoch 18, Batch 100, Loss: 0.9248551374673843\n",
      "Epoch 18, Batch 200, Loss: 0.8736919248104096\n",
      "Epoch 18, Batch 300, Loss: 0.8956439355015755\n",
      "Epoch 18, Batch 400, Loss: 0.9036218303442002\n",
      "Epoch 18, Batch 500, Loss: 0.9277383303642273\n",
      "Epoch 18, Batch 600, Loss: 0.8819447538256645\n",
      "Epoch 18, Batch 700, Loss: 0.8440526735782623\n",
      "Epoch 18, Batch 800, Loss: 0.8675991985201835\n",
      "Epoch 18, Batch 900, Loss: 0.9462873387336731\n",
      "Epoch 19, Batch 100, Loss: 0.9449278348684311\n",
      "Epoch 19, Batch 200, Loss: 0.8901861533522606\n",
      "Epoch 19, Batch 300, Loss: 0.8193941780924797\n",
      "Epoch 19, Batch 400, Loss: 0.8579797521233559\n",
      "Epoch 19, Batch 500, Loss: 0.8787625515460968\n",
      "Epoch 19, Batch 600, Loss: 0.8444744411110878\n",
      "Epoch 19, Batch 700, Loss: 0.908951359987259\n",
      "Epoch 19, Batch 800, Loss: 0.9220494419336319\n",
      "Epoch 19, Batch 900, Loss: 0.8871658092737198\n",
      "Epoch 20, Batch 100, Loss: 0.8761274185776711\n",
      "Epoch 20, Batch 200, Loss: 0.8550727435946465\n",
      "Epoch 20, Batch 300, Loss: 0.798966960310936\n",
      "Epoch 20, Batch 400, Loss: 0.798100762963295\n",
      "Epoch 20, Batch 500, Loss: 0.8890822646021843\n",
      "Epoch 20, Batch 600, Loss: 0.7662935891747474\n",
      "Epoch 20, Batch 700, Loss: 0.8831328463554382\n",
      "Epoch 20, Batch 800, Loss: 0.8290967011451721\n",
      "Epoch 20, Batch 900, Loss: 0.8740823870897293\n",
      "Epoch 21, Batch 100, Loss: 0.8296985772252082\n",
      "Epoch 21, Batch 200, Loss: 0.9395552390813827\n",
      "Epoch 21, Batch 300, Loss: 0.8793167835474014\n",
      "Epoch 21, Batch 400, Loss: 0.8261738038063049\n",
      "Epoch 21, Batch 500, Loss: 0.8343626874685287\n",
      "Epoch 21, Batch 600, Loss: 0.8365743598341941\n",
      "Epoch 21, Batch 700, Loss: 0.8099010720849037\n",
      "Epoch 21, Batch 800, Loss: 0.8098486241698265\n",
      "Epoch 21, Batch 900, Loss: 0.7386586222052575\n",
      "Epoch 22, Batch 100, Loss: 0.8453188395500183\n",
      "Epoch 22, Batch 200, Loss: 0.8614510035514832\n",
      "Epoch 22, Batch 300, Loss: 0.9175779223442078\n",
      "Epoch 22, Batch 400, Loss: 0.8308914163708687\n",
      "Epoch 22, Batch 500, Loss: 0.8355387532711029\n",
      "Epoch 22, Batch 600, Loss: 0.9142652624845504\n",
      "Epoch 22, Batch 700, Loss: 0.9523324680328369\n",
      "Epoch 22, Batch 800, Loss: 0.823222439289093\n",
      "Epoch 22, Batch 900, Loss: 0.8137202218174935\n",
      "Epoch 23, Batch 100, Loss: 0.9007148179411888\n",
      "Epoch 23, Batch 200, Loss: 0.8400530087947845\n",
      "Epoch 23, Batch 300, Loss: 0.8472122871875762\n",
      "Epoch 23, Batch 400, Loss: 0.9003534370660782\n",
      "Epoch 23, Batch 500, Loss: 0.7936429041624069\n",
      "Epoch 23, Batch 600, Loss: 0.8695891457796097\n",
      "Epoch 23, Batch 700, Loss: 0.8708055329322815\n",
      "Epoch 23, Batch 800, Loss: 0.8982594737410545\n",
      "Epoch 23, Batch 900, Loss: 0.9887343543767929\n",
      "Epoch 24, Batch 100, Loss: 0.8204992935061455\n",
      "Epoch 24, Batch 200, Loss: 0.9466791290044785\n",
      "Epoch 24, Batch 300, Loss: 0.8580406087636948\n",
      "Epoch 24, Batch 400, Loss: 0.8686828726530075\n",
      "Epoch 24, Batch 500, Loss: 0.8519486051797867\n",
      "Epoch 24, Batch 600, Loss: 0.9473445266485214\n",
      "Epoch 24, Batch 700, Loss: 0.9471580374240876\n",
      "Epoch 24, Batch 800, Loss: 0.842949121594429\n",
      "Epoch 24, Batch 900, Loss: 0.8393740332126618\n",
      "Epoch 25, Batch 100, Loss: 0.8691123378276825\n",
      "Epoch 25, Batch 200, Loss: 0.8729791390895844\n",
      "Epoch 25, Batch 300, Loss: 0.8430991351604462\n",
      "Epoch 25, Batch 400, Loss: 0.8456373891234398\n",
      "Epoch 25, Batch 500, Loss: 0.8902422460913658\n",
      "Epoch 25, Batch 600, Loss: 0.8295076930522919\n",
      "Epoch 25, Batch 700, Loss: 0.9014125737547874\n",
      "Epoch 25, Batch 800, Loss: 0.8641971865296364\n",
      "Epoch 25, Batch 900, Loss: 0.7828817123174667\n",
      "Accuracy on test set: 0.4651%\n",
      "Fitting for combination 32\n",
      "784\n",
      "4\n",
      "10\n",
      "[30, 10, 10, 20, 10]\n",
      "True\n",
      "['tanh', 'sigmoid', 'relu', 'relu']\n",
      "Adam\n",
      "0.003\n",
      "0.03\n",
      "CrossEntropyLoss\n",
      "300\n",
      "25\n",
      "0\n",
      "Epoch 1, Batch 300, Loss: 6.908538315296173\n",
      "Epoch 1, Batch 600, Loss: 6.904184963703155\n",
      "Epoch 1, Batch 900, Loss: 6.904178276062011\n",
      "Epoch 2, Batch 300, Loss: 6.903519144058228\n",
      "Epoch 2, Batch 600, Loss: 6.904270317554474\n",
      "Epoch 2, Batch 900, Loss: 6.904743828773499\n",
      "Epoch 3, Batch 300, Loss: 6.904222917556763\n",
      "Epoch 3, Batch 600, Loss: 6.90485146522522\n",
      "Epoch 3, Batch 900, Loss: 6.9047821998596195\n",
      "Epoch 4, Batch 300, Loss: 6.906706054210662\n",
      "Epoch 4, Batch 600, Loss: 6.904505906105041\n",
      "Epoch 4, Batch 900, Loss: 6.902964322566986\n",
      "Epoch 5, Batch 300, Loss: 6.905582089424133\n",
      "Epoch 5, Batch 600, Loss: 6.902362098693848\n",
      "Epoch 5, Batch 900, Loss: 6.904919185638428\n",
      "Epoch 6, Batch 300, Loss: 6.90359926700592\n",
      "Epoch 6, Batch 600, Loss: 6.904424412250519\n",
      "Epoch 6, Batch 900, Loss: 6.905795545578003\n",
      "Epoch 7, Batch 300, Loss: 6.906208233833313\n",
      "Epoch 7, Batch 600, Loss: 6.905016393661499\n",
      "Epoch 7, Batch 900, Loss: 6.903604118824005\n",
      "Epoch 8, Batch 300, Loss: 6.903879873752594\n",
      "Epoch 8, Batch 600, Loss: 6.906138687133789\n",
      "Epoch 8, Batch 900, Loss: 6.903841979503632\n",
      "Epoch 9, Batch 300, Loss: 6.903808150291443\n",
      "Epoch 9, Batch 600, Loss: 6.905045936107635\n",
      "Epoch 9, Batch 900, Loss: 6.9049218583107\n",
      "Epoch 10, Batch 300, Loss: 6.903682832717895\n",
      "Epoch 10, Batch 600, Loss: 6.904712302684784\n",
      "Epoch 10, Batch 900, Loss: 6.904817049503326\n",
      "Epoch 11, Batch 300, Loss: 6.9058398699760435\n",
      "Epoch 11, Batch 600, Loss: 6.904221918582916\n",
      "Epoch 11, Batch 900, Loss: 6.904153666496277\n",
      "Epoch 12, Batch 300, Loss: 6.903478229045868\n",
      "Epoch 12, Batch 600, Loss: 6.905627422332763\n",
      "Epoch 12, Batch 900, Loss: 6.9046937417984005\n",
      "Epoch 13, Batch 300, Loss: 6.904524581432343\n",
      "Epoch 13, Batch 600, Loss: 6.905597939491272\n",
      "Epoch 13, Batch 900, Loss: 6.90399418592453\n",
      "Epoch 14, Batch 300, Loss: 6.904200973510743\n",
      "Epoch 14, Batch 600, Loss: 6.905147836208344\n",
      "Epoch 14, Batch 900, Loss: 6.904937443733215\n",
      "Epoch 15, Batch 300, Loss: 6.905060799121856\n",
      "Epoch 15, Batch 600, Loss: 6.904161763191223\n",
      "Epoch 15, Batch 900, Loss: 6.904330778121948\n",
      "Epoch 16, Batch 300, Loss: 6.905441422462463\n",
      "Epoch 16, Batch 600, Loss: 6.904027373790741\n",
      "Epoch 16, Batch 900, Loss: 6.903115365505219\n",
      "Epoch 17, Batch 300, Loss: 6.90476761341095\n",
      "Epoch 17, Batch 600, Loss: 6.904230785369873\n",
      "Epoch 17, Batch 900, Loss: 6.905330619812012\n",
      "Epoch 18, Batch 300, Loss: 6.903966619968414\n",
      "Epoch 18, Batch 600, Loss: 6.903800513744354\n",
      "Epoch 18, Batch 900, Loss: 6.90471937417984\n",
      "Epoch 19, Batch 300, Loss: 6.904100198745727\n",
      "Epoch 19, Batch 600, Loss: 6.902827851772308\n",
      "Epoch 19, Batch 900, Loss: 6.90506582736969\n",
      "Epoch 20, Batch 300, Loss: 6.905136847496033\n",
      "Epoch 20, Batch 600, Loss: 6.903627531528473\n",
      "Epoch 20, Batch 900, Loss: 6.9051950693130495\n",
      "Epoch 21, Batch 300, Loss: 6.905340852737427\n",
      "Epoch 21, Batch 600, Loss: 6.903719675540924\n",
      "Epoch 21, Batch 900, Loss: 6.9045228672027585\n",
      "Epoch 22, Batch 300, Loss: 6.903334357738495\n",
      "Epoch 22, Batch 600, Loss: 6.905738966464996\n",
      "Epoch 22, Batch 900, Loss: 6.903517317771912\n",
      "Epoch 23, Batch 300, Loss: 6.905123574733734\n",
      "Epoch 23, Batch 600, Loss: 6.904601719379425\n",
      "Epoch 23, Batch 900, Loss: 6.904544749259949\n",
      "Epoch 24, Batch 300, Loss: 6.904857385158539\n",
      "Epoch 24, Batch 600, Loss: 6.904211995601654\n",
      "Epoch 24, Batch 900, Loss: 6.905365750789643\n",
      "Epoch 25, Batch 300, Loss: 6.904382565021515\n",
      "Epoch 25, Batch 600, Loss: 6.905776941776276\n",
      "Epoch 25, Batch 900, Loss: 6.9044725966453555\n",
      "Accuracy on test set: 0.1135%\n",
      "Fitting for combination 33\n",
      "784\n",
      "4\n",
      "10\n",
      "[30, 10, 10, 20, 10]\n",
      "False\n",
      "['tanh', 'sigmoid', 'relu', 'tanh']\n",
      "SGD\n",
      "0.003\n",
      "0.1\n",
      "CrossEntropyLoss\n",
      "100\n",
      "25\n",
      "0\n",
      "Epoch 1, Batch 100, Loss: 2.314321551322937\n",
      "Epoch 1, Batch 200, Loss: 2.309125657081604\n",
      "Epoch 1, Batch 300, Loss: 2.3048639607429506\n",
      "Epoch 1, Batch 400, Loss: 2.30392493724823\n",
      "Epoch 1, Batch 500, Loss: 2.3041231632232666\n",
      "Epoch 1, Batch 600, Loss: 2.304053418636322\n",
      "Epoch 1, Batch 700, Loss: 2.3023253655433655\n",
      "Epoch 1, Batch 800, Loss: 2.3045470452308656\n",
      "Epoch 1, Batch 900, Loss: 2.3019040489196776\n",
      "Epoch 2, Batch 100, Loss: 2.302683448791504\n",
      "Epoch 2, Batch 200, Loss: 2.302746584415436\n",
      "Epoch 2, Batch 300, Loss: 2.3024925708770754\n",
      "Epoch 2, Batch 400, Loss: 2.30125412940979\n",
      "Epoch 2, Batch 500, Loss: 2.3022219491004945\n",
      "Epoch 2, Batch 600, Loss: 2.3015031003952027\n",
      "Epoch 2, Batch 700, Loss: 2.301898694038391\n",
      "Epoch 2, Batch 800, Loss: 2.302039682865143\n",
      "Epoch 2, Batch 900, Loss: 2.3007028245925905\n",
      "Epoch 3, Batch 100, Loss: 2.3024974155426023\n",
      "Epoch 3, Batch 200, Loss: 2.301496410369873\n",
      "Epoch 3, Batch 300, Loss: 2.3010918378829954\n",
      "Epoch 3, Batch 400, Loss: 2.3022488808631896\n",
      "Epoch 3, Batch 500, Loss: 2.301370208263397\n",
      "Epoch 3, Batch 600, Loss: 2.302221977710724\n",
      "Epoch 3, Batch 700, Loss: 2.302288010120392\n",
      "Epoch 3, Batch 800, Loss: 2.301592833995819\n",
      "Epoch 3, Batch 900, Loss: 2.3007985377311706\n",
      "Epoch 4, Batch 100, Loss: 2.3014010763168335\n",
      "Epoch 4, Batch 200, Loss: 2.3013669657707214\n",
      "Epoch 4, Batch 300, Loss: 2.302241554260254\n",
      "Epoch 4, Batch 400, Loss: 2.301881091594696\n",
      "Epoch 4, Batch 500, Loss: 2.3016442894935607\n",
      "Epoch 4, Batch 600, Loss: 2.301701605319977\n",
      "Epoch 4, Batch 700, Loss: 2.301369550228119\n",
      "Epoch 4, Batch 800, Loss: 2.3013068294525145\n",
      "Epoch 4, Batch 900, Loss: 2.3019811701774597\n",
      "Epoch 5, Batch 100, Loss: 2.301844127178192\n",
      "Epoch 5, Batch 200, Loss: 2.3013161516189573\n",
      "Epoch 5, Batch 300, Loss: 2.301361925601959\n",
      "Epoch 5, Batch 400, Loss: 2.301656596660614\n",
      "Epoch 5, Batch 500, Loss: 2.301887283325195\n",
      "Epoch 5, Batch 600, Loss: 2.302076961994171\n",
      "Epoch 5, Batch 700, Loss: 2.3017040610313417\n",
      "Epoch 5, Batch 800, Loss: 2.301665186882019\n",
      "Epoch 5, Batch 900, Loss: 2.3012561297416685\n",
      "Epoch 6, Batch 100, Loss: 2.301866829395294\n",
      "Epoch 6, Batch 200, Loss: 2.3011603021621703\n",
      "Epoch 6, Batch 300, Loss: 2.301574192047119\n",
      "Epoch 6, Batch 400, Loss: 2.3017595481872557\n",
      "Epoch 6, Batch 500, Loss: 2.3010201168060305\n",
      "Epoch 6, Batch 600, Loss: 2.3013864016532897\n",
      "Epoch 6, Batch 700, Loss: 2.302515923976898\n",
      "Epoch 6, Batch 800, Loss: 2.3017018270492553\n",
      "Epoch 6, Batch 900, Loss: 2.301686110496521\n",
      "Epoch 7, Batch 100, Loss: 2.30120854139328\n",
      "Epoch 7, Batch 200, Loss: 2.301558213233948\n",
      "Epoch 7, Batch 300, Loss: 2.301895923614502\n",
      "Epoch 7, Batch 400, Loss: 2.3018934178352355\n",
      "Epoch 7, Batch 500, Loss: 2.301039943695068\n",
      "Epoch 7, Batch 600, Loss: 2.3015341091156007\n",
      "Epoch 7, Batch 700, Loss: 2.301496214866638\n",
      "Epoch 7, Batch 800, Loss: 2.3013876485824585\n",
      "Epoch 7, Batch 900, Loss: 2.3020269012451173\n",
      "Epoch 8, Batch 100, Loss: 2.3020207500457763\n",
      "Epoch 8, Batch 200, Loss: 2.301809513568878\n",
      "Epoch 8, Batch 300, Loss: 2.3015088415145875\n",
      "Epoch 8, Batch 400, Loss: 2.301317069530487\n",
      "Epoch 8, Batch 500, Loss: 2.301501815319061\n",
      "Epoch 8, Batch 600, Loss: 2.301534571647644\n",
      "Epoch 8, Batch 700, Loss: 2.3019518136978148\n",
      "Epoch 8, Batch 800, Loss: 2.3011481308937074\n",
      "Epoch 8, Batch 900, Loss: 2.3016353821754456\n",
      "Epoch 9, Batch 100, Loss: 2.301735508441925\n",
      "Epoch 9, Batch 200, Loss: 2.30175252199173\n",
      "Epoch 9, Batch 300, Loss: 2.3014591097831727\n",
      "Epoch 9, Batch 400, Loss: 2.30113041639328\n",
      "Epoch 9, Batch 500, Loss: 2.301248080730438\n",
      "Epoch 9, Batch 600, Loss: 2.301579656600952\n",
      "Epoch 9, Batch 700, Loss: 2.3013215351104734\n",
      "Epoch 9, Batch 800, Loss: 2.301579222679138\n",
      "Epoch 9, Batch 900, Loss: 2.301889507770538\n",
      "Epoch 10, Batch 100, Loss: 2.301055417060852\n",
      "Epoch 10, Batch 200, Loss: 2.301556167602539\n",
      "Epoch 10, Batch 300, Loss: 2.3010616660118104\n",
      "Epoch 10, Batch 400, Loss: 2.301909170150757\n",
      "Epoch 10, Batch 500, Loss: 2.3011942720413208\n",
      "Epoch 10, Batch 600, Loss: 2.3017498016357423\n",
      "Epoch 10, Batch 700, Loss: 2.3018911552429198\n",
      "Epoch 10, Batch 800, Loss: 2.3014807367324828\n",
      "Epoch 10, Batch 900, Loss: 2.3018005537986754\n",
      "Epoch 11, Batch 100, Loss: 2.3018530988693238\n",
      "Epoch 11, Batch 200, Loss: 2.301166639328003\n",
      "Epoch 11, Batch 300, Loss: 2.3016335201263427\n",
      "Epoch 11, Batch 400, Loss: 2.3013994693756104\n",
      "Epoch 11, Batch 500, Loss: 2.3017990398406982\n",
      "Epoch 11, Batch 600, Loss: 2.3016960763931276\n",
      "Epoch 11, Batch 700, Loss: 2.301143581867218\n",
      "Epoch 11, Batch 800, Loss: 2.3012570452690126\n",
      "Epoch 11, Batch 900, Loss: 2.3015647029876707\n",
      "Epoch 12, Batch 100, Loss: 2.301318120956421\n",
      "Epoch 12, Batch 200, Loss: 2.3012241172790526\n",
      "Epoch 12, Batch 300, Loss: 2.3015771293640137\n",
      "Epoch 12, Batch 400, Loss: 2.3016023230552674\n",
      "Epoch 12, Batch 500, Loss: 2.3016527462005616\n",
      "Epoch 12, Batch 600, Loss: 2.301141502857208\n",
      "Epoch 12, Batch 700, Loss: 2.301416380405426\n",
      "Epoch 12, Batch 800, Loss: 2.3011282420158388\n",
      "Epoch 12, Batch 900, Loss: 2.3025392198562624\n",
      "Epoch 13, Batch 100, Loss: 2.301210763454437\n",
      "Epoch 13, Batch 200, Loss: 2.3017277932167053\n",
      "Epoch 13, Batch 300, Loss: 2.301559510231018\n",
      "Epoch 13, Batch 400, Loss: 2.3015527176856994\n",
      "Epoch 13, Batch 500, Loss: 2.301458911895752\n",
      "Epoch 13, Batch 600, Loss: 2.3015968799591064\n",
      "Epoch 13, Batch 700, Loss: 2.3014293575286864\n",
      "Epoch 13, Batch 800, Loss: 2.3017280745506286\n",
      "Epoch 13, Batch 900, Loss: 2.301497461795807\n",
      "Epoch 14, Batch 100, Loss: 2.301066474914551\n",
      "Epoch 14, Batch 200, Loss: 2.301833539009094\n",
      "Epoch 14, Batch 300, Loss: 2.3014759707450865\n",
      "Epoch 14, Batch 400, Loss: 2.301608233451843\n",
      "Epoch 14, Batch 500, Loss: 2.3011125802993773\n",
      "Epoch 14, Batch 600, Loss: 2.3013154816627504\n",
      "Epoch 14, Batch 700, Loss: 2.301848690509796\n",
      "Epoch 14, Batch 800, Loss: 2.3020773363113403\n",
      "Epoch 14, Batch 900, Loss: 2.301508903503418\n",
      "Epoch 15, Batch 100, Loss: 2.301484017372131\n",
      "Epoch 15, Batch 200, Loss: 2.301798701286316\n",
      "Epoch 15, Batch 300, Loss: 2.30152330160141\n",
      "Epoch 15, Batch 400, Loss: 2.301102485656738\n",
      "Epoch 15, Batch 500, Loss: 2.301897506713867\n",
      "Epoch 15, Batch 600, Loss: 2.3016527485847473\n",
      "Epoch 15, Batch 700, Loss: 2.301463477611542\n",
      "Epoch 15, Batch 800, Loss: 2.3013397240638733\n",
      "Epoch 15, Batch 900, Loss: 2.3014390516281127\n",
      "Epoch 16, Batch 100, Loss: 2.3009862542152404\n",
      "Epoch 16, Batch 200, Loss: 2.3014339351654054\n",
      "Epoch 16, Batch 300, Loss: 2.301336872577667\n",
      "Epoch 16, Batch 400, Loss: 2.30175066947937\n",
      "Epoch 16, Batch 500, Loss: 2.3012099504470824\n",
      "Epoch 16, Batch 600, Loss: 2.302002890110016\n",
      "Epoch 16, Batch 700, Loss: 2.301992356777191\n",
      "Epoch 16, Batch 800, Loss: 2.3012797164916994\n",
      "Epoch 16, Batch 900, Loss: 2.301771328449249\n",
      "Epoch 17, Batch 100, Loss: 2.3010750579833985\n",
      "Epoch 17, Batch 200, Loss: 2.3014671349525453\n",
      "Epoch 17, Batch 300, Loss: 2.301628415584564\n",
      "Epoch 17, Batch 400, Loss: 2.3021094179153443\n",
      "Epoch 17, Batch 500, Loss: 2.3010389900207517\n",
      "Epoch 17, Batch 600, Loss: 2.301966371536255\n",
      "Epoch 17, Batch 700, Loss: 2.301235680580139\n",
      "Epoch 17, Batch 800, Loss: 2.3017823815345766\n",
      "Epoch 17, Batch 900, Loss: 2.3015553402900695\n",
      "Epoch 18, Batch 100, Loss: 2.301771094799042\n",
      "Epoch 18, Batch 200, Loss: 2.3017958641052245\n",
      "Epoch 18, Batch 300, Loss: 2.301245095729828\n",
      "Epoch 18, Batch 400, Loss: 2.301465303897858\n",
      "Epoch 18, Batch 500, Loss: 2.301543552875519\n",
      "Epoch 18, Batch 600, Loss: 2.3015571546554567\n",
      "Epoch 18, Batch 700, Loss: 2.3014686155319213\n",
      "Epoch 18, Batch 800, Loss: 2.3009448194503785\n",
      "Epoch 18, Batch 900, Loss: 2.3014885210990905\n",
      "Epoch 19, Batch 100, Loss: 2.301403784751892\n",
      "Epoch 19, Batch 200, Loss: 2.301091339588165\n",
      "Epoch 19, Batch 300, Loss: 2.3014118838310242\n",
      "Epoch 19, Batch 400, Loss: 2.3012823557853697\n",
      "Epoch 19, Batch 500, Loss: 2.3019485783576967\n",
      "Epoch 19, Batch 600, Loss: 2.301995074748993\n",
      "Epoch 19, Batch 700, Loss: 2.301301062107086\n",
      "Epoch 19, Batch 800, Loss: 2.3013993239402772\n",
      "Epoch 19, Batch 900, Loss: 2.301794376373291\n",
      "Epoch 20, Batch 100, Loss: 2.3014442896842957\n",
      "Epoch 20, Batch 200, Loss: 2.301493740081787\n",
      "Epoch 20, Batch 300, Loss: 2.3014805245399477\n",
      "Epoch 20, Batch 400, Loss: 2.3017119765281677\n",
      "Epoch 20, Batch 500, Loss: 2.301582591533661\n",
      "Epoch 20, Batch 600, Loss: 2.301877155303955\n",
      "Epoch 20, Batch 700, Loss: 2.30122873544693\n",
      "Epoch 20, Batch 800, Loss: 2.3014824748039246\n",
      "Epoch 20, Batch 900, Loss: 2.3014612793922424\n",
      "Epoch 21, Batch 100, Loss: 2.30128258228302\n",
      "Epoch 21, Batch 200, Loss: 2.3019941067695617\n",
      "Epoch 21, Batch 300, Loss: 2.3016483426094054\n",
      "Epoch 21, Batch 400, Loss: 2.3013539719581604\n",
      "Epoch 21, Batch 500, Loss: 2.301419837474823\n",
      "Epoch 21, Batch 600, Loss: 2.301290142536163\n",
      "Epoch 21, Batch 700, Loss: 2.3014490604400635\n",
      "Epoch 21, Batch 800, Loss: 2.301492121219635\n",
      "Epoch 21, Batch 900, Loss: 2.301472783088684\n",
      "Epoch 22, Batch 100, Loss: 2.3016324400901795\n",
      "Epoch 22, Batch 200, Loss: 2.301224331855774\n",
      "Epoch 22, Batch 300, Loss: 2.301840205192566\n",
      "Epoch 22, Batch 400, Loss: 2.3009999442100524\n",
      "Epoch 22, Batch 500, Loss: 2.3008591389656066\n",
      "Epoch 22, Batch 600, Loss: 2.3017080283164977\n",
      "Epoch 22, Batch 700, Loss: 2.301221718788147\n",
      "Epoch 22, Batch 800, Loss: 2.301881513595581\n",
      "Epoch 22, Batch 900, Loss: 2.3021284437179563\n",
      "Epoch 23, Batch 100, Loss: 2.3017746901512144\n",
      "Epoch 23, Batch 200, Loss: 2.3011045241355896\n",
      "Epoch 23, Batch 300, Loss: 2.30171986579895\n",
      "Epoch 23, Batch 400, Loss: 2.301851544380188\n",
      "Epoch 23, Batch 500, Loss: 2.3012450551986694\n",
      "Epoch 23, Batch 600, Loss: 2.3019504070281984\n",
      "Epoch 23, Batch 700, Loss: 2.3010688066482543\n",
      "Epoch 23, Batch 800, Loss: 2.302072513103485\n",
      "Epoch 23, Batch 900, Loss: 2.3008120465278625\n",
      "Epoch 24, Batch 100, Loss: 2.3014084458351136\n",
      "Epoch 24, Batch 200, Loss: 2.30152960062027\n",
      "Epoch 24, Batch 300, Loss: 2.3018691158294677\n",
      "Epoch 24, Batch 400, Loss: 2.3010713911056517\n",
      "Epoch 24, Batch 500, Loss: 2.301009249687195\n",
      "Epoch 24, Batch 600, Loss: 2.3017334222793577\n",
      "Epoch 24, Batch 700, Loss: 2.3018659448623655\n",
      "Epoch 24, Batch 800, Loss: 2.301424241065979\n",
      "Epoch 24, Batch 900, Loss: 2.301767840385437\n",
      "Epoch 25, Batch 100, Loss: 2.3014558959007263\n",
      "Epoch 25, Batch 200, Loss: 2.301859641075134\n",
      "Epoch 25, Batch 300, Loss: 2.3014186787605286\n",
      "Epoch 25, Batch 400, Loss: 2.3019168615341186\n",
      "Epoch 25, Batch 500, Loss: 2.302230637073517\n",
      "Epoch 25, Batch 600, Loss: 2.3015005135536195\n",
      "Epoch 25, Batch 700, Loss: 2.3017748737335206\n",
      "Epoch 25, Batch 800, Loss: 2.3009428453445433\n",
      "Epoch 25, Batch 900, Loss: 2.3011189031600954\n",
      "Accuracy on test set: 0.1135%\n",
      "Fitting for combination 34\n",
      "784\n",
      "4\n",
      "10\n",
      "[30, 10, 10, 30, 10]\n",
      "True\n",
      "['tanh', 'sigmoid', 'relu', 'sigmoid']\n",
      "Adam\n",
      "0.001\n",
      "0.01\n",
      "CrossEntropyLoss\n",
      "100\n",
      "25\n",
      "0\n",
      "Epoch 1, Batch 100, Loss: 2.305938699245453\n",
      "Epoch 1, Batch 200, Loss: 2.301544260978699\n",
      "Epoch 1, Batch 300, Loss: 2.303128626346588\n",
      "Epoch 1, Batch 400, Loss: 2.302918255329132\n",
      "Epoch 1, Batch 500, Loss: 2.3039186120033266\n",
      "Epoch 1, Batch 600, Loss: 2.302266619205475\n",
      "Epoch 1, Batch 700, Loss: 2.301258096694946\n",
      "Epoch 1, Batch 800, Loss: 2.3031967186927798\n",
      "Epoch 1, Batch 900, Loss: 2.302704842090607\n",
      "Epoch 2, Batch 100, Loss: 2.3025838351249694\n",
      "Epoch 2, Batch 200, Loss: 2.3020473504066468\n",
      "Epoch 2, Batch 300, Loss: 2.3023079109191893\n",
      "Epoch 2, Batch 400, Loss: 2.30369594335556\n",
      "Epoch 2, Batch 500, Loss: 2.302420530319214\n",
      "Epoch 2, Batch 600, Loss: 2.301303379535675\n",
      "Epoch 2, Batch 700, Loss: 2.3027740144729614\n",
      "Epoch 2, Batch 800, Loss: 2.303249382972717\n",
      "Epoch 2, Batch 900, Loss: 2.3030671072006226\n",
      "Epoch 3, Batch 100, Loss: 2.3029619526863097\n",
      "Epoch 3, Batch 200, Loss: 2.3032128047943115\n",
      "Epoch 3, Batch 300, Loss: 2.3027725291252135\n",
      "Epoch 3, Batch 400, Loss: 2.3024398636817933\n",
      "Epoch 3, Batch 500, Loss: 2.3027998852729796\n",
      "Epoch 3, Batch 600, Loss: 2.300891094207764\n",
      "Epoch 3, Batch 700, Loss: 2.302543914318085\n",
      "Epoch 3, Batch 800, Loss: 2.302282133102417\n",
      "Epoch 3, Batch 900, Loss: 2.303411054611206\n",
      "Epoch 4, Batch 100, Loss: 2.302614514827728\n",
      "Epoch 4, Batch 200, Loss: 2.303312404155731\n",
      "Epoch 4, Batch 300, Loss: 2.3022472047805786\n",
      "Epoch 4, Batch 400, Loss: 2.3034124541282655\n",
      "Epoch 4, Batch 500, Loss: 2.302146487236023\n",
      "Epoch 4, Batch 600, Loss: 2.30219806432724\n",
      "Epoch 4, Batch 700, Loss: 2.30368136882782\n",
      "Epoch 4, Batch 800, Loss: 2.3026361012458803\n",
      "Epoch 4, Batch 900, Loss: 2.3028239798545838\n",
      "Epoch 5, Batch 100, Loss: 2.302617623806\n",
      "Epoch 5, Batch 200, Loss: 2.304255955219269\n",
      "Epoch 5, Batch 300, Loss: 2.302687289714813\n",
      "Epoch 5, Batch 400, Loss: 2.30303124666214\n",
      "Epoch 5, Batch 500, Loss: 2.3016525387763975\n",
      "Epoch 5, Batch 600, Loss: 2.3026719760894774\n",
      "Epoch 5, Batch 700, Loss: 2.3032788228988648\n",
      "Epoch 5, Batch 800, Loss: 2.3029927825927734\n",
      "Epoch 5, Batch 900, Loss: 2.3033780908584593\n",
      "Epoch 6, Batch 100, Loss: 2.302667787075043\n",
      "Epoch 6, Batch 200, Loss: 2.3023541927337647\n",
      "Epoch 6, Batch 300, Loss: 2.30257363319397\n",
      "Epoch 6, Batch 400, Loss: 2.3031632590293882\n",
      "Epoch 6, Batch 500, Loss: 2.301548566818237\n",
      "Epoch 6, Batch 600, Loss: 2.3030261754989625\n",
      "Epoch 6, Batch 700, Loss: 2.3032727146148684\n",
      "Epoch 6, Batch 800, Loss: 2.303492908477783\n",
      "Epoch 6, Batch 900, Loss: 2.303271481990814\n",
      "Epoch 7, Batch 100, Loss: 2.3046746325492857\n",
      "Epoch 7, Batch 200, Loss: 2.3023459243774416\n",
      "Epoch 7, Batch 300, Loss: 2.301770145893097\n",
      "Epoch 7, Batch 400, Loss: 2.302374942302704\n",
      "Epoch 7, Batch 500, Loss: 2.3021926212310793\n",
      "Epoch 7, Batch 600, Loss: 2.302493088245392\n",
      "Epoch 7, Batch 700, Loss: 2.302311420440674\n",
      "Epoch 7, Batch 800, Loss: 2.304390025138855\n",
      "Epoch 7, Batch 900, Loss: 2.303099157810211\n",
      "Epoch 8, Batch 100, Loss: 2.3021360039711\n",
      "Epoch 8, Batch 200, Loss: 2.3027522444725035\n",
      "Epoch 8, Batch 300, Loss: 2.302610614299774\n",
      "Epoch 8, Batch 400, Loss: 2.302987370491028\n",
      "Epoch 8, Batch 500, Loss: 2.302493772506714\n",
      "Epoch 8, Batch 600, Loss: 2.3023857569694517\n",
      "Epoch 8, Batch 700, Loss: 2.3035726857185366\n",
      "Epoch 8, Batch 800, Loss: 2.3018847250938417\n",
      "Epoch 8, Batch 900, Loss: 2.301903476715088\n",
      "Epoch 9, Batch 100, Loss: 2.302938404083252\n",
      "Epoch 9, Batch 200, Loss: 2.3028943037986753\n",
      "Epoch 9, Batch 300, Loss: 2.3024253034591675\n",
      "Epoch 9, Batch 400, Loss: 2.3017628502845766\n",
      "Epoch 9, Batch 500, Loss: 2.303252086639404\n",
      "Epoch 9, Batch 600, Loss: 2.3028779578208924\n",
      "Epoch 9, Batch 700, Loss: 2.3011509656906126\n",
      "Epoch 9, Batch 800, Loss: 2.3027876019477844\n",
      "Epoch 9, Batch 900, Loss: 2.3029387044906615\n",
      "Epoch 10, Batch 100, Loss: 2.3034215426445006\n",
      "Epoch 10, Batch 200, Loss: 2.3024530625343322\n",
      "Epoch 10, Batch 300, Loss: 2.3020711231231687\n",
      "Epoch 10, Batch 400, Loss: 2.302924392223358\n",
      "Epoch 10, Batch 500, Loss: 2.3017151808738707\n",
      "Epoch 10, Batch 600, Loss: 2.302637147903442\n",
      "Epoch 10, Batch 700, Loss: 2.3018709874153136\n",
      "Epoch 10, Batch 800, Loss: 2.3023949790000917\n",
      "Epoch 10, Batch 900, Loss: 2.303157913684845\n",
      "Epoch 11, Batch 100, Loss: 2.3032465600967407\n",
      "Epoch 11, Batch 200, Loss: 2.301979355812073\n",
      "Epoch 11, Batch 300, Loss: 2.3020054936408996\n",
      "Epoch 11, Batch 400, Loss: 2.3027858901023865\n",
      "Epoch 11, Batch 500, Loss: 2.3026433396339416\n",
      "Epoch 11, Batch 600, Loss: 2.302153794765472\n",
      "Epoch 11, Batch 700, Loss: 2.3018988943099976\n",
      "Epoch 11, Batch 800, Loss: 2.301868622303009\n",
      "Epoch 11, Batch 900, Loss: 2.3036556911468504\n",
      "Epoch 12, Batch 100, Loss: 2.3035212779045104\n",
      "Epoch 12, Batch 200, Loss: 2.3030873131752014\n",
      "Epoch 12, Batch 300, Loss: 2.3021641087532045\n",
      "Epoch 12, Batch 400, Loss: 2.3024532699584963\n",
      "Epoch 12, Batch 500, Loss: 2.3032016372680664\n",
      "Epoch 12, Batch 600, Loss: 2.302801084518433\n",
      "Epoch 12, Batch 700, Loss: 2.3035533618927\n",
      "Epoch 12, Batch 800, Loss: 2.3028179574012757\n",
      "Epoch 12, Batch 900, Loss: 2.302141215801239\n",
      "Epoch 13, Batch 100, Loss: 2.302665410041809\n",
      "Epoch 13, Batch 200, Loss: 2.3027826976776122\n",
      "Epoch 13, Batch 300, Loss: 2.3022914361953735\n",
      "Epoch 13, Batch 400, Loss: 2.30110271692276\n",
      "Epoch 13, Batch 500, Loss: 2.3029320454597473\n",
      "Epoch 13, Batch 600, Loss: 2.302606508731842\n",
      "Epoch 13, Batch 700, Loss: 2.303526108264923\n",
      "Epoch 13, Batch 800, Loss: 2.302910552024841\n",
      "Epoch 13, Batch 900, Loss: 2.301589455604553\n",
      "Epoch 14, Batch 100, Loss: 2.3035597515106203\n",
      "Epoch 14, Batch 200, Loss: 2.302346429824829\n",
      "Epoch 14, Batch 300, Loss: 2.3038069462776183\n",
      "Epoch 14, Batch 400, Loss: 2.3035062336921692\n",
      "Epoch 14, Batch 500, Loss: 2.302630445957184\n",
      "Epoch 14, Batch 600, Loss: 2.303018100261688\n",
      "Epoch 14, Batch 700, Loss: 2.3006949186325074\n",
      "Epoch 14, Batch 800, Loss: 2.3020833563804626\n",
      "Epoch 14, Batch 900, Loss: 2.3019946432113647\n",
      "Epoch 15, Batch 100, Loss: 2.3024054765701294\n",
      "Epoch 15, Batch 200, Loss: 2.3031123089790344\n",
      "Epoch 15, Batch 300, Loss: 2.3022597885131835\n",
      "Epoch 15, Batch 400, Loss: 2.3030861139297487\n",
      "Epoch 15, Batch 500, Loss: 2.3029029059410093\n",
      "Epoch 15, Batch 600, Loss: 2.301062169075012\n",
      "Epoch 15, Batch 700, Loss: 2.3030673003196718\n",
      "Epoch 15, Batch 800, Loss: 2.303112905025482\n",
      "Epoch 15, Batch 900, Loss: 2.3029027485847475\n",
      "Epoch 16, Batch 100, Loss: 2.3032943654060363\n",
      "Epoch 16, Batch 200, Loss: 2.3022224521636963\n",
      "Epoch 16, Batch 300, Loss: 2.3021830534934997\n",
      "Epoch 16, Batch 400, Loss: 2.3023432374000548\n",
      "Epoch 16, Batch 500, Loss: 2.3044567275047303\n",
      "Epoch 16, Batch 600, Loss: 2.3037992119789124\n",
      "Epoch 16, Batch 700, Loss: 2.3025997161865233\n",
      "Epoch 16, Batch 800, Loss: 2.302560422420502\n",
      "Epoch 16, Batch 900, Loss: 2.3024648690223692\n",
      "Epoch 17, Batch 100, Loss: 2.3036708545684816\n",
      "Epoch 17, Batch 200, Loss: 2.301757929325104\n",
      "Epoch 17, Batch 300, Loss: 2.3028318977355955\n",
      "Epoch 17, Batch 400, Loss: 2.3024382972717286\n",
      "Epoch 17, Batch 500, Loss: 2.3028177046775817\n",
      "Epoch 17, Batch 600, Loss: 2.30302782535553\n",
      "Epoch 17, Batch 700, Loss: 2.3028894090652465\n",
      "Epoch 17, Batch 800, Loss: 2.3032899117469787\n",
      "Epoch 17, Batch 900, Loss: 2.3017114710807802\n",
      "Epoch 18, Batch 100, Loss: 2.3036931347846985\n",
      "Epoch 18, Batch 200, Loss: 2.303062515258789\n",
      "Epoch 18, Batch 300, Loss: 2.3008944439888\n",
      "Epoch 18, Batch 400, Loss: 2.3022970294952394\n",
      "Epoch 18, Batch 500, Loss: 2.3028976416587827\n",
      "Epoch 18, Batch 600, Loss: 2.3032563614845274\n",
      "Epoch 18, Batch 700, Loss: 2.3020250391960144\n",
      "Epoch 18, Batch 800, Loss: 2.3030754995346068\n",
      "Epoch 18, Batch 900, Loss: 2.303263521194458\n",
      "Epoch 19, Batch 100, Loss: 2.303061783313751\n",
      "Epoch 19, Batch 200, Loss: 2.302265582084656\n",
      "Epoch 19, Batch 300, Loss: 2.3036074090003966\n",
      "Epoch 19, Batch 400, Loss: 2.302075343132019\n",
      "Epoch 19, Batch 500, Loss: 2.3016204261779785\n",
      "Epoch 19, Batch 600, Loss: 2.302615189552307\n",
      "Epoch 19, Batch 700, Loss: 2.3015482449531555\n",
      "Epoch 19, Batch 800, Loss: 2.303608832359314\n",
      "Epoch 19, Batch 900, Loss: 2.3015712094306946\n",
      "Epoch 20, Batch 100, Loss: 2.302646152973175\n",
      "Epoch 20, Batch 200, Loss: 2.3024304699897766\n",
      "Epoch 20, Batch 300, Loss: 2.302521228790283\n",
      "Epoch 20, Batch 400, Loss: 2.3020359086990356\n",
      "Epoch 20, Batch 500, Loss: 2.302414891719818\n",
      "Epoch 20, Batch 600, Loss: 2.302976071834564\n",
      "Epoch 20, Batch 700, Loss: 2.3017784762382507\n",
      "Epoch 20, Batch 800, Loss: 2.303283944129944\n",
      "Epoch 20, Batch 900, Loss: 2.3019864320755006\n",
      "Epoch 21, Batch 100, Loss: 2.302586362361908\n",
      "Epoch 21, Batch 200, Loss: 2.3021464729309082\n",
      "Epoch 21, Batch 300, Loss: 2.3042870783805847\n",
      "Epoch 21, Batch 400, Loss: 2.301757426261902\n",
      "Epoch 21, Batch 500, Loss: 2.302995512485504\n",
      "Epoch 21, Batch 600, Loss: 2.302266194820404\n",
      "Epoch 21, Batch 700, Loss: 2.302631540298462\n",
      "Epoch 21, Batch 800, Loss: 2.3025663447380067\n",
      "Epoch 21, Batch 900, Loss: 2.3042341756820677\n",
      "Epoch 22, Batch 100, Loss: 2.3023524498939514\n",
      "Epoch 22, Batch 200, Loss: 2.303138785362244\n",
      "Epoch 22, Batch 300, Loss: 2.3011005520820618\n",
      "Epoch 22, Batch 400, Loss: 2.3037724566459654\n",
      "Epoch 22, Batch 500, Loss: 2.3023629331588746\n",
      "Epoch 22, Batch 600, Loss: 2.3032828998565673\n",
      "Epoch 22, Batch 700, Loss: 2.302829692363739\n",
      "Epoch 22, Batch 800, Loss: 2.300776431560516\n",
      "Epoch 22, Batch 900, Loss: 2.3038998556137087\n",
      "Epoch 23, Batch 100, Loss: 2.302261927127838\n",
      "Epoch 23, Batch 200, Loss: 2.301950249671936\n",
      "Epoch 23, Batch 300, Loss: 2.302552869319916\n",
      "Epoch 23, Batch 400, Loss: 2.303094868659973\n",
      "Epoch 23, Batch 500, Loss: 2.3025615096092222\n",
      "Epoch 23, Batch 600, Loss: 2.3031918668746947\n",
      "Epoch 23, Batch 700, Loss: 2.302599194049835\n",
      "Epoch 23, Batch 800, Loss: 2.3017654728889467\n",
      "Epoch 23, Batch 900, Loss: 2.302587020397186\n",
      "Epoch 24, Batch 100, Loss: 2.303485951423645\n",
      "Epoch 24, Batch 200, Loss: 2.3031337785720827\n",
      "Epoch 24, Batch 300, Loss: 2.303066003322601\n",
      "Epoch 24, Batch 400, Loss: 2.303178143501282\n",
      "Epoch 24, Batch 500, Loss: 2.301564221382141\n",
      "Epoch 24, Batch 600, Loss: 2.303744535446167\n",
      "Epoch 24, Batch 700, Loss: 2.3032643842697142\n",
      "Epoch 24, Batch 800, Loss: 2.3016786861419676\n",
      "Epoch 24, Batch 900, Loss: 2.301324107646942\n",
      "Epoch 25, Batch 100, Loss: 2.3030718636512755\n",
      "Epoch 25, Batch 200, Loss: 2.303328022956848\n",
      "Epoch 25, Batch 300, Loss: 2.303323087692261\n",
      "Epoch 25, Batch 400, Loss: 2.3032812738418578\n",
      "Epoch 25, Batch 500, Loss: 2.30252112865448\n",
      "Epoch 25, Batch 600, Loss: 2.3024591159820558\n",
      "Epoch 25, Batch 700, Loss: 2.3025404524803164\n",
      "Epoch 25, Batch 800, Loss: 2.301265470981598\n",
      "Epoch 25, Batch 900, Loss: 2.3028918409347536\n",
      "Accuracy on test set: 0.1135%\n",
      "Fitting for combination 35\n",
      "784\n",
      "4\n",
      "10\n",
      "[30, 10, 10, 30, 10]\n",
      "False\n",
      "['tanh', 'sigmoid', 'relu', 'tanh']\n",
      "SGD\n",
      "0.03\n",
      "0.3\n",
      "CrossEntropyLoss\n",
      "200\n",
      "25\n",
      "0\n",
      "Epoch 1, Batch 200, Loss: 4.610447826385498\n",
      "Epoch 1, Batch 400, Loss: 4.6042033743858335\n",
      "Epoch 1, Batch 600, Loss: 4.6043266940116885\n",
      "Epoch 1, Batch 800, Loss: 4.603881907463074\n",
      "Epoch 2, Batch 200, Loss: 4.6039322566986085\n",
      "Epoch 2, Batch 400, Loss: 4.603348271846771\n",
      "Epoch 2, Batch 600, Loss: 4.603826394081116\n",
      "Epoch 2, Batch 800, Loss: 4.60409414768219\n",
      "Epoch 3, Batch 200, Loss: 4.603932065963745\n",
      "Epoch 3, Batch 400, Loss: 4.603648943901062\n",
      "Epoch 3, Batch 600, Loss: 4.604436609745026\n",
      "Epoch 3, Batch 800, Loss: 4.604032680988312\n",
      "Epoch 4, Batch 200, Loss: 4.6033659934997555\n",
      "Epoch 4, Batch 400, Loss: 4.604358215332031\n",
      "Epoch 4, Batch 600, Loss: 4.60435334444046\n",
      "Epoch 4, Batch 800, Loss: 4.603833260536194\n",
      "Epoch 5, Batch 200, Loss: 4.604199719429016\n",
      "Epoch 5, Batch 400, Loss: 4.603889074325561\n",
      "Epoch 5, Batch 600, Loss: 4.603792421817779\n",
      "Epoch 5, Batch 800, Loss: 4.604325077533722\n",
      "Epoch 6, Batch 200, Loss: 4.604211394786835\n",
      "Epoch 6, Batch 400, Loss: 4.604085793495178\n",
      "Epoch 6, Batch 600, Loss: 4.603868618011474\n",
      "Epoch 6, Batch 800, Loss: 4.604089775085449\n",
      "Epoch 7, Batch 200, Loss: 4.604435157775879\n",
      "Epoch 7, Batch 400, Loss: 4.604335880279541\n",
      "Epoch 7, Batch 600, Loss: 4.603589949607849\n",
      "Epoch 7, Batch 800, Loss: 4.603931982517242\n",
      "Epoch 8, Batch 200, Loss: 4.603986611366272\n",
      "Epoch 8, Batch 400, Loss: 4.604093608856201\n",
      "Epoch 8, Batch 600, Loss: 4.6041974568367\n",
      "Epoch 8, Batch 800, Loss: 4.6043010783195495\n",
      "Epoch 9, Batch 200, Loss: 4.603529608249664\n",
      "Epoch 9, Batch 400, Loss: 4.604061942100525\n",
      "Epoch 9, Batch 600, Loss: 4.603979053497315\n",
      "Epoch 9, Batch 800, Loss: 4.604151027202606\n",
      "Epoch 10, Batch 200, Loss: 4.604466049671173\n",
      "Epoch 10, Batch 400, Loss: 4.603343250751496\n",
      "Epoch 10, Batch 600, Loss: 4.603813202381134\n",
      "Epoch 10, Batch 800, Loss: 4.604038655757904\n",
      "Epoch 11, Batch 200, Loss: 4.604141116142273\n",
      "Epoch 11, Batch 400, Loss: 4.60421481847763\n",
      "Epoch 11, Batch 600, Loss: 4.604194548130035\n",
      "Epoch 11, Batch 800, Loss: 4.604458668231964\n",
      "Epoch 12, Batch 200, Loss: 4.6041426181793215\n",
      "Epoch 12, Batch 400, Loss: 4.603921992778778\n",
      "Epoch 12, Batch 600, Loss: 4.604414491653443\n",
      "Epoch 12, Batch 800, Loss: 4.6040234065055845\n",
      "Epoch 13, Batch 200, Loss: 4.604340634346008\n",
      "Epoch 13, Batch 400, Loss: 4.603797678947449\n",
      "Epoch 13, Batch 600, Loss: 4.6043173956871035\n",
      "Epoch 13, Batch 800, Loss: 4.603837838172913\n",
      "Epoch 14, Batch 200, Loss: 4.604556536674499\n",
      "Epoch 14, Batch 400, Loss: 4.603756315708161\n",
      "Epoch 14, Batch 600, Loss: 4.603846991062165\n",
      "Epoch 14, Batch 800, Loss: 4.603921976089477\n",
      "Epoch 15, Batch 200, Loss: 4.604183120727539\n",
      "Epoch 15, Batch 400, Loss: 4.60426766872406\n",
      "Epoch 15, Batch 600, Loss: 4.603971812725067\n",
      "Epoch 15, Batch 800, Loss: 4.604042866230011\n",
      "Epoch 16, Batch 200, Loss: 4.604104056358337\n",
      "Epoch 16, Batch 400, Loss: 4.6043101191520694\n",
      "Epoch 16, Batch 600, Loss: 4.604028947353363\n",
      "Epoch 16, Batch 800, Loss: 4.604019806385041\n",
      "Epoch 17, Batch 200, Loss: 4.603658852577209\n",
      "Epoch 17, Batch 400, Loss: 4.6042338919639585\n",
      "Epoch 17, Batch 600, Loss: 4.60412181854248\n",
      "Epoch 17, Batch 800, Loss: 4.604397675991058\n",
      "Epoch 18, Batch 200, Loss: 4.604302434921265\n",
      "Epoch 18, Batch 400, Loss: 4.604409730434417\n",
      "Epoch 18, Batch 600, Loss: 4.603769176006317\n",
      "Epoch 18, Batch 800, Loss: 4.60358030796051\n",
      "Epoch 19, Batch 200, Loss: 4.6040583205223085\n",
      "Epoch 19, Batch 400, Loss: 4.603822183609009\n",
      "Epoch 19, Batch 600, Loss: 4.604348843097687\n",
      "Epoch 19, Batch 800, Loss: 4.603721613883972\n",
      "Epoch 20, Batch 200, Loss: 4.604445779323578\n",
      "Epoch 20, Batch 400, Loss: 4.6036410641670225\n",
      "Epoch 20, Batch 600, Loss: 4.603767647743225\n",
      "Epoch 20, Batch 800, Loss: 4.6040347695350645\n",
      "Epoch 21, Batch 200, Loss: 4.60409120798111\n",
      "Epoch 21, Batch 400, Loss: 4.604334576129913\n",
      "Epoch 21, Batch 600, Loss: 4.60403216123581\n",
      "Epoch 21, Batch 800, Loss: 4.604210596084595\n",
      "Epoch 22, Batch 200, Loss: 4.603883631229401\n",
      "Epoch 22, Batch 400, Loss: 4.604435086250305\n",
      "Epoch 22, Batch 600, Loss: 4.604126737117768\n",
      "Epoch 22, Batch 800, Loss: 4.603633422851562\n",
      "Epoch 23, Batch 200, Loss: 4.603770718574524\n",
      "Epoch 23, Batch 400, Loss: 4.6041491508483885\n",
      "Epoch 23, Batch 600, Loss: 4.604048807621002\n",
      "Epoch 23, Batch 800, Loss: 4.603872594833374\n",
      "Epoch 24, Batch 200, Loss: 4.604073057174682\n",
      "Epoch 24, Batch 400, Loss: 4.603898873329163\n",
      "Epoch 24, Batch 600, Loss: 4.604186854362488\n",
      "Epoch 24, Batch 800, Loss: 4.6043446516990665\n",
      "Epoch 25, Batch 200, Loss: 4.603655996322632\n",
      "Epoch 25, Batch 400, Loss: 4.604102032184601\n",
      "Epoch 25, Batch 600, Loss: 4.604233338832855\n",
      "Epoch 25, Batch 800, Loss: 4.6039529800415036\n",
      "Accuracy on test set: 0.1135%\n",
      "Fitting for combination 36\n",
      "784\n",
      "4\n",
      "10\n",
      "[30, 10, 10, 40, 10]\n",
      "False\n",
      "['tanh', 'sigmoid', 'relu', 'tanh']\n",
      "Adam\n",
      "0.001\n",
      "0.3\n",
      "CrossEntropyLoss\n",
      "200\n",
      "25\n",
      "0\n",
      "Epoch 1, Batch 200, Loss: 4.6052666497230526\n",
      "Epoch 1, Batch 400, Loss: 4.60454802274704\n",
      "Epoch 1, Batch 600, Loss: 4.60417555809021\n",
      "Epoch 1, Batch 800, Loss: 4.6041513180732725\n",
      "Epoch 2, Batch 200, Loss: 4.603744466304779\n",
      "Epoch 2, Batch 400, Loss: 4.603757214546204\n",
      "Epoch 2, Batch 600, Loss: 4.6039957165718075\n",
      "Epoch 2, Batch 800, Loss: 4.604070656299591\n",
      "Epoch 3, Batch 200, Loss: 4.604024538993835\n",
      "Epoch 3, Batch 400, Loss: 4.604968738555908\n",
      "Epoch 3, Batch 600, Loss: 4.603760888576508\n",
      "Epoch 3, Batch 800, Loss: 4.603575336933136\n",
      "Epoch 4, Batch 200, Loss: 4.60360265493393\n",
      "Epoch 4, Batch 400, Loss: 4.604099855422974\n",
      "Epoch 4, Batch 600, Loss: 4.604213061332703\n",
      "Epoch 4, Batch 800, Loss: 4.604497802257538\n",
      "Epoch 5, Batch 200, Loss: 4.6045015740394595\n",
      "Epoch 5, Batch 400, Loss: 4.603408591747284\n",
      "Epoch 5, Batch 600, Loss: 4.603619568347931\n",
      "Epoch 5, Batch 800, Loss: 4.60430330991745\n",
      "Epoch 6, Batch 200, Loss: 4.604225811958313\n",
      "Epoch 6, Batch 400, Loss: 4.604267637729645\n",
      "Epoch 6, Batch 600, Loss: 4.604011456966401\n",
      "Epoch 6, Batch 800, Loss: 4.604018785953522\n",
      "Epoch 7, Batch 200, Loss: 4.603680753707886\n",
      "Epoch 7, Batch 400, Loss: 4.603758296966553\n",
      "Epoch 7, Batch 600, Loss: 4.604667408466339\n",
      "Epoch 7, Batch 800, Loss: 4.604006586074829\n",
      "Epoch 8, Batch 200, Loss: 4.603733403682709\n",
      "Epoch 8, Batch 400, Loss: 4.603924925327301\n",
      "Epoch 8, Batch 600, Loss: 4.6044051599502565\n",
      "Epoch 8, Batch 800, Loss: 4.604148862361908\n",
      "Epoch 9, Batch 200, Loss: 4.603759241104126\n",
      "Epoch 9, Batch 400, Loss: 4.604042289257049\n",
      "Epoch 9, Batch 600, Loss: 4.603925511837006\n",
      "Epoch 9, Batch 800, Loss: 4.6038263273239135\n",
      "Epoch 10, Batch 200, Loss: 4.604057788848877\n",
      "Epoch 10, Batch 400, Loss: 4.603651394844055\n",
      "Epoch 10, Batch 600, Loss: 4.603921630382538\n",
      "Epoch 10, Batch 800, Loss: 4.604043340682983\n",
      "Epoch 11, Batch 200, Loss: 4.6040572786331175\n",
      "Epoch 11, Batch 400, Loss: 4.603386259078979\n",
      "Epoch 11, Batch 600, Loss: 4.604479248523712\n",
      "Epoch 11, Batch 800, Loss: 4.6041923213005065\n",
      "Epoch 12, Batch 200, Loss: 4.603853514194489\n",
      "Epoch 12, Batch 400, Loss: 4.604398310184479\n",
      "Epoch 12, Batch 600, Loss: 4.603848559856415\n",
      "Epoch 12, Batch 800, Loss: 4.603965146541595\n",
      "Epoch 13, Batch 200, Loss: 4.603984553813934\n",
      "Epoch 13, Batch 400, Loss: 4.604030282497406\n",
      "Epoch 13, Batch 600, Loss: 4.604255557060242\n",
      "Epoch 13, Batch 800, Loss: 4.603972973823548\n",
      "Epoch 14, Batch 200, Loss: 4.60388043642044\n",
      "Epoch 14, Batch 400, Loss: 4.604337656497956\n",
      "Epoch 14, Batch 600, Loss: 4.603912043571472\n",
      "Epoch 14, Batch 800, Loss: 4.603939688205719\n",
      "Epoch 15, Batch 200, Loss: 4.604356260299682\n",
      "Epoch 15, Batch 400, Loss: 4.604380578994751\n",
      "Epoch 15, Batch 600, Loss: 4.604086751937866\n",
      "Epoch 15, Batch 800, Loss: 4.6040759587287905\n",
      "Epoch 16, Batch 200, Loss: 4.604038774967194\n",
      "Epoch 16, Batch 400, Loss: 4.604171717166901\n",
      "Epoch 16, Batch 600, Loss: 4.60388160943985\n",
      "Epoch 16, Batch 800, Loss: 4.604443373680115\n",
      "Epoch 17, Batch 200, Loss: 4.604381837844849\n",
      "Epoch 17, Batch 400, Loss: 4.603720936775208\n",
      "Epoch 17, Batch 600, Loss: 4.603814809322357\n",
      "Epoch 17, Batch 800, Loss: 4.604169421195984\n",
      "Epoch 18, Batch 200, Loss: 4.603914744853974\n",
      "Epoch 18, Batch 400, Loss: 4.603901908397675\n",
      "Epoch 18, Batch 600, Loss: 4.604254341125488\n",
      "Epoch 18, Batch 800, Loss: 4.603736696243286\n",
      "Epoch 19, Batch 200, Loss: 4.604085862636566\n",
      "Epoch 19, Batch 400, Loss: 4.603632266521454\n",
      "Epoch 19, Batch 600, Loss: 4.60375061750412\n",
      "Epoch 19, Batch 800, Loss: 4.6043215131759645\n",
      "Epoch 20, Batch 200, Loss: 4.603793923854828\n",
      "Epoch 20, Batch 400, Loss: 4.604021351337433\n",
      "Epoch 20, Batch 600, Loss: 4.604130923748016\n",
      "Epoch 20, Batch 800, Loss: 4.604000024795532\n",
      "Epoch 21, Batch 200, Loss: 4.604217798709869\n",
      "Epoch 21, Batch 400, Loss: 4.604437303543091\n",
      "Epoch 21, Batch 600, Loss: 4.603392057418823\n",
      "Epoch 21, Batch 800, Loss: 4.603913598060608\n",
      "Epoch 22, Batch 200, Loss: 4.604161183834076\n",
      "Epoch 22, Batch 400, Loss: 4.604105758666992\n",
      "Epoch 22, Batch 600, Loss: 4.603929626941681\n",
      "Epoch 22, Batch 800, Loss: 4.604015040397644\n",
      "Epoch 23, Batch 200, Loss: 4.604070701599121\n",
      "Epoch 23, Batch 400, Loss: 4.6043311929702755\n",
      "Epoch 23, Batch 600, Loss: 4.604114463329315\n",
      "Epoch 23, Batch 800, Loss: 4.603752057552338\n",
      "Epoch 24, Batch 200, Loss: 4.604232013225555\n",
      "Epoch 24, Batch 400, Loss: 4.603832776546478\n",
      "Epoch 24, Batch 600, Loss: 4.604625160694122\n",
      "Epoch 24, Batch 800, Loss: 4.603710951805115\n",
      "Epoch 25, Batch 200, Loss: 4.603980145454407\n",
      "Epoch 25, Batch 400, Loss: 4.6043061351776124\n",
      "Epoch 25, Batch 600, Loss: 4.60407634973526\n",
      "Epoch 25, Batch 800, Loss: 4.604086308479309\n",
      "Accuracy on test set: 0.1135%\n",
      "Fitting for combination 37\n",
      "784\n",
      "4\n",
      "10\n",
      "[30, 10, 10, 40, 10]\n",
      "True\n",
      "['tanh', 'sigmoid', 'relu', 'sigmoid']\n",
      "SGD\n",
      "0.3\n",
      "1\n",
      "CrossEntropyLoss\n",
      "100\n",
      "25\n",
      "0\n",
      "Epoch 1, Batch 100, Loss: 2.309681053161621\n",
      "Epoch 1, Batch 200, Loss: 2.3102763628959657\n",
      "Epoch 1, Batch 300, Loss: 2.3096591949462892\n",
      "Epoch 1, Batch 400, Loss: 2.3113853836059572\n",
      "Epoch 1, Batch 500, Loss: 2.3113649344444274\n",
      "Epoch 1, Batch 600, Loss: 2.3084112906455996\n",
      "Epoch 1, Batch 700, Loss: 2.3101616525650024\n",
      "Epoch 1, Batch 800, Loss: 2.3125801062583924\n",
      "Epoch 1, Batch 900, Loss: 2.3109069418907167\n",
      "Epoch 2, Batch 100, Loss: 2.309477896690369\n",
      "Epoch 2, Batch 200, Loss: 2.312460570335388\n",
      "Epoch 2, Batch 300, Loss: 2.3065795159339904\n",
      "Epoch 2, Batch 400, Loss: 2.3097451162338256\n",
      "Epoch 2, Batch 500, Loss: 2.308436861038208\n",
      "Epoch 2, Batch 600, Loss: 2.3102497553825376\n",
      "Epoch 2, Batch 700, Loss: 2.3121407294273375\n",
      "Epoch 2, Batch 800, Loss: 2.3105796146392823\n",
      "Epoch 2, Batch 900, Loss: 2.3095953631401063\n",
      "Epoch 3, Batch 100, Loss: 2.3125524878501893\n",
      "Epoch 3, Batch 200, Loss: 2.312280638217926\n",
      "Epoch 3, Batch 300, Loss: 2.3108945226669313\n",
      "Epoch 3, Batch 400, Loss: 2.309115869998932\n",
      "Epoch 3, Batch 500, Loss: 2.3109449768066406\n",
      "Epoch 3, Batch 600, Loss: 2.309415545463562\n",
      "Epoch 3, Batch 700, Loss: 2.30934743642807\n",
      "Epoch 3, Batch 800, Loss: 2.3100715112686157\n",
      "Epoch 3, Batch 900, Loss: 2.3138164329528808\n",
      "Epoch 4, Batch 100, Loss: 2.3119003224372863\n",
      "Epoch 4, Batch 200, Loss: 2.3104835987091064\n",
      "Epoch 4, Batch 300, Loss: 2.309196000099182\n",
      "Epoch 4, Batch 400, Loss: 2.3070919489860535\n",
      "Epoch 4, Batch 500, Loss: 2.3103160786628725\n",
      "Epoch 4, Batch 600, Loss: 2.311388511657715\n",
      "Epoch 4, Batch 700, Loss: 2.3112086772918703\n",
      "Epoch 4, Batch 800, Loss: 2.3108171606063843\n",
      "Epoch 4, Batch 900, Loss: 2.3119416189193727\n",
      "Epoch 5, Batch 100, Loss: 2.313573820590973\n",
      "Epoch 5, Batch 200, Loss: 2.3082497048377992\n",
      "Epoch 5, Batch 300, Loss: 2.3123019766807555\n",
      "Epoch 5, Batch 400, Loss: 2.3111071681976316\n",
      "Epoch 5, Batch 500, Loss: 2.3092005157470705\n",
      "Epoch 5, Batch 600, Loss: 2.309531593322754\n",
      "Epoch 5, Batch 700, Loss: 2.3101383304595946\n",
      "Epoch 5, Batch 800, Loss: 2.3106693744659426\n",
      "Epoch 5, Batch 900, Loss: 2.3105290293693543\n",
      "Epoch 6, Batch 100, Loss: 2.3096562933921816\n",
      "Epoch 6, Batch 200, Loss: 2.309303872585297\n",
      "Epoch 6, Batch 300, Loss: 2.310157768726349\n",
      "Epoch 6, Batch 400, Loss: 2.3100382924079894\n",
      "Epoch 6, Batch 500, Loss: 2.3125649809837343\n",
      "Epoch 6, Batch 600, Loss: 2.310712766647339\n",
      "Epoch 6, Batch 700, Loss: 2.307582972049713\n",
      "Epoch 6, Batch 800, Loss: 2.3080450916290283\n",
      "Epoch 6, Batch 900, Loss: 2.3118072509765626\n",
      "Epoch 7, Batch 100, Loss: 2.3101080894470214\n",
      "Epoch 7, Batch 200, Loss: 2.310263605117798\n",
      "Epoch 7, Batch 300, Loss: 2.309316020011902\n",
      "Epoch 7, Batch 400, Loss: 2.3116637825965882\n",
      "Epoch 7, Batch 500, Loss: 2.308651337623596\n",
      "Epoch 7, Batch 600, Loss: 2.311432180404663\n",
      "Epoch 7, Batch 700, Loss: 2.3082779908180235\n",
      "Epoch 7, Batch 800, Loss: 2.310785188674927\n",
      "Epoch 7, Batch 900, Loss: 2.312613699436188\n",
      "Epoch 8, Batch 100, Loss: 2.309772973060608\n",
      "Epoch 8, Batch 200, Loss: 2.3090354871749876\n",
      "Epoch 8, Batch 300, Loss: 2.31132705450058\n",
      "Epoch 8, Batch 400, Loss: 2.309551272392273\n",
      "Epoch 8, Batch 500, Loss: 2.3122735714912412\n",
      "Epoch 8, Batch 600, Loss: 2.312217283248901\n",
      "Epoch 8, Batch 700, Loss: 2.3104479241371156\n",
      "Epoch 8, Batch 800, Loss: 2.3081373834609984\n",
      "Epoch 8, Batch 900, Loss: 2.3104247283935546\n",
      "Epoch 9, Batch 100, Loss: 2.3115442609786987\n",
      "Epoch 9, Batch 200, Loss: 2.3090427923202514\n",
      "Epoch 9, Batch 300, Loss: 2.312814450263977\n",
      "Epoch 9, Batch 400, Loss: 2.310373401641846\n",
      "Epoch 9, Batch 500, Loss: 2.3109659910202027\n",
      "Epoch 9, Batch 600, Loss: 2.312524416446686\n",
      "Epoch 9, Batch 700, Loss: 2.3066645789146425\n",
      "Epoch 9, Batch 800, Loss: 2.3107294607162476\n",
      "Epoch 9, Batch 900, Loss: 2.312346296310425\n",
      "Epoch 10, Batch 100, Loss: 2.310148184299469\n",
      "Epoch 10, Batch 200, Loss: 2.307683577537537\n",
      "Epoch 10, Batch 300, Loss: 2.3093254590034484\n",
      "Epoch 10, Batch 400, Loss: 2.309284574985504\n",
      "Epoch 10, Batch 500, Loss: 2.311176874637604\n",
      "Epoch 10, Batch 600, Loss: 2.3119329261779784\n",
      "Epoch 10, Batch 700, Loss: 2.310310916900635\n",
      "Epoch 10, Batch 800, Loss: 2.3137637424468993\n",
      "Epoch 10, Batch 900, Loss: 2.3096181774139404\n",
      "Epoch 11, Batch 100, Loss: 2.3086953496932985\n",
      "Epoch 11, Batch 200, Loss: 2.308804259300232\n",
      "Epoch 11, Batch 300, Loss: 2.3101882481575013\n",
      "Epoch 11, Batch 400, Loss: 2.3110012292861937\n",
      "Epoch 11, Batch 500, Loss: 2.3094252824783323\n",
      "Epoch 11, Batch 600, Loss: 2.311127715110779\n",
      "Epoch 11, Batch 700, Loss: 2.3114544558525085\n",
      "Epoch 11, Batch 800, Loss: 2.3111655616760256\n",
      "Epoch 11, Batch 900, Loss: 2.3092420220375063\n",
      "Epoch 12, Batch 100, Loss: 2.311901779174805\n",
      "Epoch 12, Batch 200, Loss: 2.313208231925964\n",
      "Epoch 12, Batch 300, Loss: 2.3080209350585936\n",
      "Epoch 12, Batch 400, Loss: 2.3104980635643004\n",
      "Epoch 12, Batch 500, Loss: 2.3102241468429567\n",
      "Epoch 12, Batch 600, Loss: 2.311412992477417\n",
      "Epoch 12, Batch 700, Loss: 2.310101273059845\n",
      "Epoch 12, Batch 800, Loss: 2.309244167804718\n",
      "Epoch 12, Batch 900, Loss: 2.309102773666382\n",
      "Epoch 13, Batch 100, Loss: 2.3115938830375673\n",
      "Epoch 13, Batch 200, Loss: 2.3105420374870302\n",
      "Epoch 13, Batch 300, Loss: 2.3087062311172484\n",
      "Epoch 13, Batch 400, Loss: 2.312254157066345\n",
      "Epoch 13, Batch 500, Loss: 2.309197795391083\n",
      "Epoch 13, Batch 600, Loss: 2.3114742851257324\n",
      "Epoch 13, Batch 700, Loss: 2.3103940296173096\n",
      "Epoch 13, Batch 800, Loss: 2.309107038974762\n",
      "Epoch 13, Batch 900, Loss: 2.3086651253700254\n",
      "Epoch 14, Batch 100, Loss: 2.3091313862800598\n",
      "Epoch 14, Batch 200, Loss: 2.3104715752601623\n",
      "Epoch 14, Batch 300, Loss: 2.3105204010009768\n",
      "Epoch 14, Batch 400, Loss: 2.308890779018402\n",
      "Epoch 14, Batch 500, Loss: 2.3106380534172057\n",
      "Epoch 14, Batch 600, Loss: 2.3090441823005676\n",
      "Epoch 14, Batch 700, Loss: 2.310332708358765\n",
      "Epoch 14, Batch 800, Loss: 2.3104879117012023\n",
      "Epoch 14, Batch 900, Loss: 2.308625063896179\n",
      "Epoch 15, Batch 100, Loss: 2.310436532497406\n",
      "Epoch 15, Batch 200, Loss: 2.3075689339637755\n",
      "Epoch 15, Batch 300, Loss: 2.3114189314842224\n",
      "Epoch 15, Batch 400, Loss: 2.3096537971496582\n",
      "Epoch 15, Batch 500, Loss: 2.3107195019721987\n",
      "Epoch 15, Batch 600, Loss: 2.3099188899993894\n",
      "Epoch 15, Batch 700, Loss: 2.3090661001205444\n",
      "Epoch 15, Batch 800, Loss: 2.3100189852714537\n",
      "Epoch 15, Batch 900, Loss: 2.313114721775055\n",
      "Epoch 16, Batch 100, Loss: 2.3114627480506895\n",
      "Epoch 16, Batch 200, Loss: 2.310585913658142\n",
      "Epoch 16, Batch 300, Loss: 2.3110127663612365\n",
      "Epoch 16, Batch 400, Loss: 2.3096349596977235\n",
      "Epoch 16, Batch 500, Loss: 2.3132650446891785\n",
      "Epoch 16, Batch 600, Loss: 2.308565196990967\n",
      "Epoch 16, Batch 700, Loss: 2.310922040939331\n",
      "Epoch 16, Batch 800, Loss: 2.3093200373649596\n",
      "Epoch 16, Batch 900, Loss: 2.310710711479187\n",
      "Epoch 17, Batch 100, Loss: 2.311423485279083\n",
      "Epoch 17, Batch 200, Loss: 2.3094558620452883\n",
      "Epoch 17, Batch 300, Loss: 2.3106987571716306\n",
      "Epoch 17, Batch 400, Loss: 2.311087384223938\n",
      "Epoch 17, Batch 500, Loss: 2.3110985469818117\n",
      "Epoch 17, Batch 600, Loss: 2.3093052911758423\n",
      "Epoch 17, Batch 700, Loss: 2.3107407093048096\n",
      "Epoch 17, Batch 800, Loss: 2.309552185535431\n",
      "Epoch 17, Batch 900, Loss: 2.3118571829795838\n",
      "Epoch 18, Batch 100, Loss: 2.310629036426544\n",
      "Epoch 18, Batch 200, Loss: 2.3117486667633056\n",
      "Epoch 18, Batch 300, Loss: 2.3129369473457335\n",
      "Epoch 18, Batch 400, Loss: 2.310195195674896\n",
      "Epoch 18, Batch 500, Loss: 2.3089540553092958\n",
      "Epoch 18, Batch 600, Loss: 2.3117694783210756\n",
      "Epoch 18, Batch 700, Loss: 2.3103723192214964\n",
      "Epoch 18, Batch 800, Loss: 2.311437633037567\n",
      "Epoch 18, Batch 900, Loss: 2.3117766857147215\n",
      "Epoch 19, Batch 100, Loss: 2.3104659390449522\n",
      "Epoch 19, Batch 200, Loss: 2.308218421936035\n",
      "Epoch 19, Batch 300, Loss: 2.308721871376038\n",
      "Epoch 19, Batch 400, Loss: 2.311194369792938\n",
      "Epoch 19, Batch 500, Loss: 2.3106714367866514\n",
      "Epoch 19, Batch 600, Loss: 2.311444892883301\n",
      "Epoch 19, Batch 700, Loss: 2.3105715584754942\n",
      "Epoch 19, Batch 800, Loss: 2.3108856201171877\n",
      "Epoch 19, Batch 900, Loss: 2.3114081478118895\n",
      "Epoch 20, Batch 100, Loss: 2.309948387145996\n",
      "Epoch 20, Batch 200, Loss: 2.30940238237381\n",
      "Epoch 20, Batch 300, Loss: 2.313036186695099\n",
      "Epoch 20, Batch 400, Loss: 2.3099812293052673\n",
      "Epoch 20, Batch 500, Loss: 2.3115707850456237\n",
      "Epoch 20, Batch 600, Loss: 2.3090923118591307\n",
      "Epoch 20, Batch 700, Loss: 2.309318199157715\n",
      "Epoch 20, Batch 800, Loss: 2.3103092670440675\n",
      "Epoch 20, Batch 900, Loss: 2.310176568031311\n",
      "Epoch 21, Batch 100, Loss: 2.310449471473694\n",
      "Epoch 21, Batch 200, Loss: 2.30992084980011\n",
      "Epoch 21, Batch 300, Loss: 2.3117041182518006\n",
      "Epoch 21, Batch 400, Loss: 2.3122495913505556\n",
      "Epoch 21, Batch 500, Loss: 2.3094872999191285\n",
      "Epoch 21, Batch 600, Loss: 2.309299874305725\n",
      "Epoch 21, Batch 700, Loss: 2.3121188402175905\n",
      "Epoch 21, Batch 800, Loss: 2.3074851775169374\n",
      "Epoch 21, Batch 900, Loss: 2.3099652624130247\n",
      "Epoch 22, Batch 100, Loss: 2.311040372848511\n",
      "Epoch 22, Batch 200, Loss: 2.309773020744324\n",
      "Epoch 22, Batch 300, Loss: 2.3118059134483335\n",
      "Epoch 22, Batch 400, Loss: 2.311558713912964\n",
      "Epoch 22, Batch 500, Loss: 2.3092453336715697\n",
      "Epoch 22, Batch 600, Loss: 2.311465380191803\n",
      "Epoch 22, Batch 700, Loss: 2.3105935716629027\n",
      "Epoch 22, Batch 800, Loss: 2.3066573309898377\n",
      "Epoch 22, Batch 900, Loss: 2.3095339798927306\n",
      "Epoch 23, Batch 100, Loss: 2.3112465953826904\n",
      "Epoch 23, Batch 200, Loss: 2.309789566993713\n",
      "Epoch 23, Batch 300, Loss: 2.308209819793701\n",
      "Epoch 23, Batch 400, Loss: 2.3126771378517152\n",
      "Epoch 23, Batch 500, Loss: 2.3098901987075804\n",
      "Epoch 23, Batch 600, Loss: 2.307699317932129\n",
      "Epoch 23, Batch 700, Loss: 2.306642518043518\n",
      "Epoch 23, Batch 800, Loss: 2.3121056246757505\n",
      "Epoch 23, Batch 900, Loss: 2.3115004992485044\n",
      "Epoch 24, Batch 100, Loss: 2.308359670639038\n",
      "Epoch 24, Batch 200, Loss: 2.3112530303001404\n",
      "Epoch 24, Batch 300, Loss: 2.3119194865226746\n",
      "Epoch 24, Batch 400, Loss: 2.309701907634735\n",
      "Epoch 24, Batch 500, Loss: 2.3090139412879944\n",
      "Epoch 24, Batch 600, Loss: 2.3122078585624695\n",
      "Epoch 24, Batch 700, Loss: 2.309284870624542\n",
      "Epoch 24, Batch 800, Loss: 2.310099198818207\n",
      "Epoch 24, Batch 900, Loss: 2.31155077457428\n",
      "Epoch 25, Batch 100, Loss: 2.3094475173950197\n",
      "Epoch 25, Batch 200, Loss: 2.3128276085853576\n",
      "Epoch 25, Batch 300, Loss: 2.310061902999878\n",
      "Epoch 25, Batch 400, Loss: 2.3080174803733824\n",
      "Epoch 25, Batch 500, Loss: 2.310708673000336\n",
      "Epoch 25, Batch 600, Loss: 2.309707283973694\n",
      "Epoch 25, Batch 700, Loss: 2.308785524368286\n",
      "Epoch 25, Batch 800, Loss: 2.3108927488327025\n",
      "Epoch 25, Batch 900, Loss: 2.306349523067474\n",
      "Accuracy on test set: 0.1032%\n",
      "Fitting for combination 38\n",
      "784\n",
      "4\n",
      "10\n",
      "[30, 10, 10, 50, 10]\n",
      "True\n",
      "['tanh', 'sigmoid', 'relu', 'relu']\n",
      "Adam\n",
      "0.001\n",
      "1\n",
      "CrossEntropyLoss\n",
      "300\n",
      "25\n",
      "0\n",
      "Epoch 1, Batch 300, Loss: 6.912492988109588\n",
      "Epoch 1, Batch 600, Loss: 6.907085809707642\n",
      "Epoch 1, Batch 900, Loss: 6.907402143478394\n",
      "Epoch 2, Batch 300, Loss: 6.906657638549805\n",
      "Epoch 2, Batch 600, Loss: 6.907141139507294\n",
      "Epoch 2, Batch 900, Loss: 6.907342963218689\n",
      "Epoch 3, Batch 300, Loss: 6.9071979284286495\n",
      "Epoch 3, Batch 600, Loss: 6.906904597282409\n",
      "Epoch 3, Batch 900, Loss: 6.907108469009399\n",
      "Epoch 4, Batch 300, Loss: 6.907126874923706\n",
      "Epoch 4, Batch 600, Loss: 6.907241926193238\n",
      "Epoch 4, Batch 900, Loss: 6.906780581474305\n",
      "Epoch 5, Batch 300, Loss: 6.9070760679245\n",
      "Epoch 5, Batch 600, Loss: 6.907177217006684\n",
      "Epoch 5, Batch 900, Loss: 6.906936750411988\n",
      "Epoch 6, Batch 300, Loss: 6.907170603275299\n",
      "Epoch 6, Batch 600, Loss: 6.907233397960663\n",
      "Epoch 6, Batch 900, Loss: 6.906737215518952\n",
      "Epoch 7, Batch 300, Loss: 6.906942183971405\n",
      "Epoch 7, Batch 600, Loss: 6.90703144788742\n",
      "Epoch 7, Batch 900, Loss: 6.907244980335236\n",
      "Epoch 8, Batch 300, Loss: 6.907147498130798\n",
      "Epoch 8, Batch 600, Loss: 6.90704785823822\n",
      "Epoch 8, Batch 900, Loss: 6.907226729393005\n",
      "Epoch 9, Batch 300, Loss: 6.906908648014069\n",
      "Epoch 9, Batch 600, Loss: 6.9071770286560055\n",
      "Epoch 9, Batch 900, Loss: 6.907262403964996\n",
      "Epoch 10, Batch 300, Loss: 6.906921656131744\n",
      "Epoch 10, Batch 600, Loss: 6.907292637825012\n",
      "Epoch 10, Batch 900, Loss: 6.906937041282654\n",
      "Epoch 11, Batch 300, Loss: 6.907075102329254\n",
      "Epoch 11, Batch 600, Loss: 6.907221717834473\n",
      "Epoch 11, Batch 900, Loss: 6.906840245723725\n",
      "Epoch 12, Batch 300, Loss: 6.9068343806266785\n",
      "Epoch 12, Batch 600, Loss: 6.906801326274872\n",
      "Epoch 12, Batch 900, Loss: 6.907323162555695\n",
      "Epoch 13, Batch 300, Loss: 6.906745929718017\n",
      "Epoch 13, Batch 600, Loss: 6.906895499229432\n",
      "Epoch 13, Batch 900, Loss: 6.907200253009796\n",
      "Epoch 14, Batch 300, Loss: 6.907235295772552\n",
      "Epoch 14, Batch 600, Loss: 6.907183048725128\n",
      "Epoch 14, Batch 900, Loss: 6.907080602645874\n",
      "Epoch 15, Batch 300, Loss: 6.907063505649567\n",
      "Epoch 15, Batch 600, Loss: 6.906904957294464\n",
      "Epoch 15, Batch 900, Loss: 6.907136468887329\n",
      "Epoch 16, Batch 300, Loss: 6.906977410316467\n",
      "Epoch 16, Batch 600, Loss: 6.907269327640534\n",
      "Epoch 16, Batch 900, Loss: 6.907169551849365\n",
      "Epoch 17, Batch 300, Loss: 6.9070860123634334\n",
      "Epoch 17, Batch 600, Loss: 6.9068511915206905\n",
      "Epoch 17, Batch 900, Loss: 6.907235758304596\n",
      "Epoch 18, Batch 300, Loss: 6.907318789958953\n",
      "Epoch 18, Batch 600, Loss: 6.906396853923797\n",
      "Epoch 18, Batch 900, Loss: 6.9073349213600155\n",
      "Epoch 19, Batch 300, Loss: 6.907090933322906\n",
      "Epoch 19, Batch 600, Loss: 6.9070723700523375\n",
      "Epoch 19, Batch 900, Loss: 6.907155275344849\n",
      "Epoch 20, Batch 300, Loss: 6.907213530540466\n",
      "Epoch 20, Batch 600, Loss: 6.9069791030883785\n",
      "Epoch 20, Batch 900, Loss: 6.90711320400238\n",
      "Epoch 21, Batch 300, Loss: 6.906832902431488\n",
      "Epoch 21, Batch 600, Loss: 6.907321317195892\n",
      "Epoch 21, Batch 900, Loss: 6.9069773292541505\n",
      "Epoch 22, Batch 300, Loss: 6.907267863750458\n",
      "Epoch 22, Batch 600, Loss: 6.907116503715515\n",
      "Epoch 22, Batch 900, Loss: 6.90685156583786\n",
      "Epoch 23, Batch 300, Loss: 6.907001113891601\n",
      "Epoch 23, Batch 600, Loss: 6.907150044441223\n",
      "Epoch 23, Batch 900, Loss: 6.90705638885498\n",
      "Epoch 24, Batch 300, Loss: 6.9072682619094845\n",
      "Epoch 24, Batch 600, Loss: 6.906925866603851\n",
      "Epoch 24, Batch 900, Loss: 6.907045922279358\n",
      "Epoch 25, Batch 300, Loss: 6.906831395626068\n",
      "Epoch 25, Batch 600, Loss: 6.907331745624543\n",
      "Epoch 25, Batch 900, Loss: 6.907169809341431\n",
      "Accuracy on test set: 0.1135%\n",
      "Fitting for combination 39\n",
      "784\n",
      "4\n",
      "10\n",
      "[30, 10, 10, 50, 10]\n",
      "False\n",
      "['tanh', 'sigmoid', 'relu', 'relu']\n",
      "SGD\n",
      "0.1\n",
      "0.3\n",
      "CrossEntropyLoss\n",
      "100\n",
      "25\n",
      "0\n",
      "Epoch 1, Batch 100, Loss: 2.3024667358398436\n",
      "Epoch 1, Batch 200, Loss: 2.3025220322608946\n",
      "Epoch 1, Batch 300, Loss: 2.3020969009399415\n",
      "Epoch 1, Batch 400, Loss: 2.301928286552429\n",
      "Epoch 1, Batch 500, Loss: 2.301694226264954\n",
      "Epoch 1, Batch 600, Loss: 2.3017459654808046\n",
      "Epoch 1, Batch 700, Loss: 2.302315564155579\n",
      "Epoch 1, Batch 800, Loss: 2.3020935463905334\n",
      "Epoch 1, Batch 900, Loss: 2.302092227935791\n",
      "Epoch 2, Batch 100, Loss: 2.302487807273865\n",
      "Epoch 2, Batch 200, Loss: 2.302003953456879\n",
      "Epoch 2, Batch 300, Loss: 2.301595468521118\n",
      "Epoch 2, Batch 400, Loss: 2.302257492542267\n",
      "Epoch 2, Batch 500, Loss: 2.30224746465683\n",
      "Epoch 2, Batch 600, Loss: 2.302388782501221\n",
      "Epoch 2, Batch 700, Loss: 2.30233558177948\n",
      "Epoch 2, Batch 800, Loss: 2.301956579685211\n",
      "Epoch 2, Batch 900, Loss: 2.3014677429199217\n",
      "Epoch 3, Batch 100, Loss: 2.301971790790558\n",
      "Epoch 3, Batch 200, Loss: 2.3021779751777647\n",
      "Epoch 3, Batch 300, Loss: 2.3017588591575624\n",
      "Epoch 3, Batch 400, Loss: 2.3017821860313417\n",
      "Epoch 3, Batch 500, Loss: 2.302278504371643\n",
      "Epoch 3, Batch 600, Loss: 2.3018112874031065\n",
      "Epoch 3, Batch 700, Loss: 2.302240557670593\n",
      "Epoch 3, Batch 800, Loss: 2.3022997999191284\n",
      "Epoch 3, Batch 900, Loss: 2.3017105150222776\n",
      "Epoch 4, Batch 100, Loss: 2.302424445152283\n",
      "Epoch 4, Batch 200, Loss: 2.3016771697998046\n",
      "Epoch 4, Batch 300, Loss: 2.301756160259247\n",
      "Epoch 4, Batch 400, Loss: 2.302517647743225\n",
      "Epoch 4, Batch 500, Loss: 2.302318413257599\n",
      "Epoch 4, Batch 600, Loss: 2.302106556892395\n",
      "Epoch 4, Batch 700, Loss: 2.3022964286804197\n",
      "Epoch 4, Batch 800, Loss: 2.3021129751205445\n",
      "Epoch 4, Batch 900, Loss: 2.302165410518646\n",
      "Epoch 5, Batch 100, Loss: 2.3021193671226503\n",
      "Epoch 5, Batch 200, Loss: 2.302320132255554\n",
      "Epoch 5, Batch 300, Loss: 2.30206538438797\n",
      "Epoch 5, Batch 400, Loss: 2.301840531826019\n",
      "Epoch 5, Batch 500, Loss: 2.3022362422943115\n",
      "Epoch 5, Batch 600, Loss: 2.3021270751953127\n",
      "Epoch 5, Batch 700, Loss: 2.301965765953064\n",
      "Epoch 5, Batch 800, Loss: 2.3019863295555116\n",
      "Epoch 5, Batch 900, Loss: 2.302393710613251\n",
      "Epoch 6, Batch 100, Loss: 2.3017996191978454\n",
      "Epoch 6, Batch 200, Loss: 2.3018456315994262\n",
      "Epoch 6, Batch 300, Loss: 2.3023088240623473\n",
      "Epoch 6, Batch 400, Loss: 2.3022663950920106\n",
      "Epoch 6, Batch 500, Loss: 2.3022045588493345\n",
      "Epoch 6, Batch 600, Loss: 2.3022417521476743\n",
      "Epoch 6, Batch 700, Loss: 2.301706533432007\n",
      "Epoch 6, Batch 800, Loss: 2.302297511100769\n",
      "Epoch 6, Batch 900, Loss: 2.302380530834198\n",
      "Epoch 7, Batch 100, Loss: 2.3021427488327024\n",
      "Epoch 7, Batch 200, Loss: 2.301887457370758\n",
      "Epoch 7, Batch 300, Loss: 2.3021040534973145\n",
      "Epoch 7, Batch 400, Loss: 2.3024276900291443\n",
      "Epoch 7, Batch 500, Loss: 2.3017484951019287\n",
      "Epoch 7, Batch 600, Loss: 2.301827702522278\n",
      "Epoch 7, Batch 700, Loss: 2.302341423034668\n",
      "Epoch 7, Batch 800, Loss: 2.302245543003082\n",
      "Epoch 7, Batch 900, Loss: 2.301978464126587\n",
      "Epoch 8, Batch 100, Loss: 2.301766357421875\n",
      "Epoch 8, Batch 200, Loss: 2.30248929977417\n",
      "Epoch 8, Batch 300, Loss: 2.30203533411026\n",
      "Epoch 8, Batch 400, Loss: 2.302140357494354\n",
      "Epoch 8, Batch 500, Loss: 2.302035217285156\n",
      "Epoch 8, Batch 600, Loss: 2.3014370012283325\n",
      "Epoch 8, Batch 700, Loss: 2.3020991468429566\n",
      "Epoch 8, Batch 800, Loss: 2.3022228860855103\n",
      "Epoch 8, Batch 900, Loss: 2.3019985485076906\n",
      "Epoch 9, Batch 100, Loss: 2.3022486400604247\n",
      "Epoch 9, Batch 200, Loss: 2.30241206407547\n",
      "Epoch 9, Batch 300, Loss: 2.3013867712020875\n",
      "Epoch 9, Batch 400, Loss: 2.302057704925537\n",
      "Epoch 9, Batch 500, Loss: 2.302135183811188\n",
      "Epoch 9, Batch 600, Loss: 2.301631090641022\n",
      "Epoch 9, Batch 700, Loss: 2.3020222973823548\n",
      "Epoch 9, Batch 800, Loss: 2.302367227077484\n",
      "Epoch 9, Batch 900, Loss: 2.3022946166992186\n",
      "Epoch 10, Batch 100, Loss: 2.3018795680999755\n",
      "Epoch 10, Batch 200, Loss: 2.3023822498321533\n",
      "Epoch 10, Batch 300, Loss: 2.3019728803634645\n",
      "Epoch 10, Batch 400, Loss: 2.3016006541252136\n",
      "Epoch 10, Batch 500, Loss: 2.302383830547333\n",
      "Epoch 10, Batch 600, Loss: 2.302438077926636\n",
      "Epoch 10, Batch 700, Loss: 2.301933779716492\n",
      "Epoch 10, Batch 800, Loss: 2.302160332202911\n",
      "Epoch 10, Batch 900, Loss: 2.3022105383872984\n",
      "Epoch 11, Batch 100, Loss: 2.3021438360214233\n",
      "Epoch 11, Batch 200, Loss: 2.302026753425598\n",
      "Epoch 11, Batch 300, Loss: 2.301682233810425\n",
      "Epoch 11, Batch 400, Loss: 2.3022576069831846\n",
      "Epoch 11, Batch 500, Loss: 2.302029700279236\n",
      "Epoch 11, Batch 600, Loss: 2.302416052818298\n",
      "Epoch 11, Batch 700, Loss: 2.301991322040558\n",
      "Epoch 11, Batch 800, Loss: 2.3018721771240234\n",
      "Epoch 11, Batch 900, Loss: 2.3027610564231873\n",
      "Epoch 12, Batch 100, Loss: 2.30206109046936\n",
      "Epoch 12, Batch 200, Loss: 2.3023362374305725\n",
      "Epoch 12, Batch 300, Loss: 2.3024328994750975\n",
      "Epoch 12, Batch 400, Loss: 2.3017466974258425\n",
      "Epoch 12, Batch 500, Loss: 2.3018575215339663\n",
      "Epoch 12, Batch 600, Loss: 2.3020679235458372\n",
      "Epoch 12, Batch 700, Loss: 2.30229101896286\n",
      "Epoch 12, Batch 800, Loss: 2.302154548168182\n",
      "Epoch 12, Batch 900, Loss: 2.301849892139435\n",
      "Epoch 13, Batch 100, Loss: 2.302015206813812\n",
      "Epoch 13, Batch 200, Loss: 2.3020513081550598\n",
      "Epoch 13, Batch 300, Loss: 2.3023202085494994\n",
      "Epoch 13, Batch 400, Loss: 2.3019593477249147\n",
      "Epoch 13, Batch 500, Loss: 2.3020587611198424\n",
      "Epoch 13, Batch 600, Loss: 2.301802625656128\n",
      "Epoch 13, Batch 700, Loss: 2.302679717540741\n",
      "Epoch 13, Batch 800, Loss: 2.3016592478752136\n",
      "Epoch 13, Batch 900, Loss: 2.302008101940155\n",
      "Epoch 14, Batch 100, Loss: 2.301938328742981\n",
      "Epoch 14, Batch 200, Loss: 2.301526358127594\n",
      "Epoch 14, Batch 300, Loss: 2.3013566660881044\n",
      "Epoch 14, Batch 400, Loss: 2.302222242355347\n",
      "Epoch 14, Batch 500, Loss: 2.3023414993286133\n",
      "Epoch 14, Batch 600, Loss: 2.3023218297958374\n",
      "Epoch 14, Batch 700, Loss: 2.302280592918396\n",
      "Epoch 14, Batch 800, Loss: 2.301816101074219\n",
      "Epoch 14, Batch 900, Loss: 2.302518892288208\n",
      "Epoch 15, Batch 100, Loss: 2.3021978831291197\n",
      "Epoch 15, Batch 200, Loss: 2.3024065852165223\n",
      "Epoch 15, Batch 300, Loss: 2.301913118362427\n",
      "Epoch 15, Batch 400, Loss: 2.3022107434272767\n",
      "Epoch 15, Batch 500, Loss: 2.301999156475067\n",
      "Epoch 15, Batch 600, Loss: 2.301995222568512\n",
      "Epoch 15, Batch 700, Loss: 2.3015232491493225\n",
      "Epoch 15, Batch 800, Loss: 2.3024906086921693\n",
      "Epoch 15, Batch 900, Loss: 2.3024578666687012\n",
      "Epoch 16, Batch 100, Loss: 2.302269847393036\n",
      "Epoch 16, Batch 200, Loss: 2.3020308995246888\n",
      "Epoch 16, Batch 300, Loss: 2.301555757522583\n",
      "Epoch 16, Batch 400, Loss: 2.3020335245132446\n",
      "Epoch 16, Batch 500, Loss: 2.302336962223053\n",
      "Epoch 16, Batch 600, Loss: 2.301818964481354\n",
      "Epoch 16, Batch 700, Loss: 2.3023737239837647\n",
      "Epoch 16, Batch 800, Loss: 2.3023293685913084\n",
      "Epoch 16, Batch 900, Loss: 2.3023645234107972\n",
      "Epoch 17, Batch 100, Loss: 2.3015599822998047\n",
      "Epoch 17, Batch 200, Loss: 2.3021286010742186\n",
      "Epoch 17, Batch 300, Loss: 2.3022089314460756\n",
      "Epoch 17, Batch 400, Loss: 2.302084550857544\n",
      "Epoch 17, Batch 500, Loss: 2.3020840954780577\n",
      "Epoch 17, Batch 600, Loss: 2.3023613810539247\n",
      "Epoch 17, Batch 700, Loss: 2.302404863834381\n",
      "Epoch 17, Batch 800, Loss: 2.301472051143646\n",
      "Epoch 17, Batch 900, Loss: 2.302236053943634\n",
      "Epoch 18, Batch 100, Loss: 2.302676348686218\n",
      "Epoch 18, Batch 200, Loss: 2.302429902553558\n",
      "Epoch 18, Batch 300, Loss: 2.3015760612487792\n",
      "Epoch 18, Batch 400, Loss: 2.3020877695083617\n",
      "Epoch 18, Batch 500, Loss: 2.3020212721824644\n",
      "Epoch 18, Batch 600, Loss: 2.302418897151947\n",
      "Epoch 18, Batch 700, Loss: 2.301556169986725\n",
      "Epoch 18, Batch 800, Loss: 2.3015207123756407\n",
      "Epoch 18, Batch 900, Loss: 2.3020167255401613\n",
      "Epoch 19, Batch 100, Loss: 2.302319302558899\n",
      "Epoch 19, Batch 200, Loss: 2.3019938707351684\n",
      "Epoch 19, Batch 300, Loss: 2.3020530414581297\n",
      "Epoch 19, Batch 400, Loss: 2.3017096495628357\n",
      "Epoch 19, Batch 500, Loss: 2.3021156454086302\n",
      "Epoch 19, Batch 600, Loss: 2.30230673789978\n",
      "Epoch 19, Batch 700, Loss: 2.301889805793762\n",
      "Epoch 19, Batch 800, Loss: 2.3021358823776246\n",
      "Epoch 19, Batch 900, Loss: 2.3020051550865173\n",
      "Epoch 20, Batch 100, Loss: 2.301887621879578\n",
      "Epoch 20, Batch 200, Loss: 2.302056851387024\n",
      "Epoch 20, Batch 300, Loss: 2.3020721650123597\n",
      "Epoch 20, Batch 400, Loss: 2.301595904827118\n",
      "Epoch 20, Batch 500, Loss: 2.30181955575943\n",
      "Epoch 20, Batch 600, Loss: 2.302405366897583\n",
      "Epoch 20, Batch 700, Loss: 2.30260223865509\n",
      "Epoch 20, Batch 800, Loss: 2.302230405807495\n",
      "Epoch 20, Batch 900, Loss: 2.302169563770294\n",
      "Epoch 21, Batch 100, Loss: 2.3021385860443115\n",
      "Epoch 21, Batch 200, Loss: 2.3021546030044555\n",
      "Epoch 21, Batch 300, Loss: 2.3017326331138612\n",
      "Epoch 21, Batch 400, Loss: 2.302204327583313\n",
      "Epoch 21, Batch 500, Loss: 2.3021334552764894\n",
      "Epoch 21, Batch 600, Loss: 2.3022045254707337\n",
      "Epoch 21, Batch 700, Loss: 2.302063574790955\n",
      "Epoch 21, Batch 800, Loss: 2.301908643245697\n",
      "Epoch 21, Batch 900, Loss: 2.302345864772797\n",
      "Epoch 22, Batch 100, Loss: 2.3023369312286377\n",
      "Epoch 22, Batch 200, Loss: 2.3016819190979003\n",
      "Epoch 22, Batch 300, Loss: 2.301442596912384\n",
      "Epoch 22, Batch 400, Loss: 2.3020722174644472\n",
      "Epoch 22, Batch 500, Loss: 2.3022839045524597\n",
      "Epoch 22, Batch 600, Loss: 2.3023068642616273\n",
      "Epoch 22, Batch 700, Loss: 2.3018205046653746\n",
      "Epoch 22, Batch 800, Loss: 2.3019391751289366\n",
      "Epoch 22, Batch 900, Loss: 2.3021546721458437\n",
      "Epoch 23, Batch 100, Loss: 2.3025917410850525\n",
      "Epoch 23, Batch 200, Loss: 2.302414243221283\n",
      "Epoch 23, Batch 300, Loss: 2.3018511605262755\n",
      "Epoch 23, Batch 400, Loss: 2.3025229477882387\n",
      "Epoch 23, Batch 500, Loss: 2.301993119716644\n",
      "Epoch 23, Batch 600, Loss: 2.3021955609321596\n",
      "Epoch 23, Batch 700, Loss: 2.3016666388511657\n",
      "Epoch 23, Batch 800, Loss: 2.301603877544403\n",
      "Epoch 23, Batch 900, Loss: 2.3019107747077943\n",
      "Epoch 24, Batch 100, Loss: 2.3022143912315367\n",
      "Epoch 24, Batch 200, Loss: 2.30137264251709\n",
      "Epoch 24, Batch 300, Loss: 2.3017705607414247\n",
      "Epoch 24, Batch 400, Loss: 2.3023123645782473\n",
      "Epoch 24, Batch 500, Loss: 2.3022403621673586\n",
      "Epoch 24, Batch 600, Loss: 2.3020389747619627\n",
      "Epoch 24, Batch 700, Loss: 2.301821358203888\n",
      "Epoch 24, Batch 800, Loss: 2.301948549747467\n",
      "Epoch 24, Batch 900, Loss: 2.30220360994339\n",
      "Epoch 25, Batch 100, Loss: 2.3018076729774477\n",
      "Epoch 25, Batch 200, Loss: 2.30230637550354\n",
      "Epoch 25, Batch 300, Loss: 2.3020364689826964\n",
      "Epoch 25, Batch 400, Loss: 2.3020544338226316\n",
      "Epoch 25, Batch 500, Loss: 2.3014628672599793\n",
      "Epoch 25, Batch 600, Loss: 2.3018904662132265\n",
      "Epoch 25, Batch 700, Loss: 2.3024657917022706\n",
      "Epoch 25, Batch 800, Loss: 2.302142996788025\n",
      "Epoch 25, Batch 900, Loss: 2.302258949279785\n",
      "Accuracy on test set: 0.1135%\n",
      "Best parameters: {'inputs': 784, 'number_of_layers': 1, 'outputs': 10, 'neurons_per_layer': [30, 10, 10, 10], 'dropout_layers': False, 'activation_functions': ['tanh', 'sigmoid', 'relu'], 'optimizers': 'Adam', 'learning_rates': 0.001, 'weight_decays': 0.01, 'loss_functions': 'CrossEntropyLoss', 'batches': 100, 'epochs': 25, 'score': 0.9315}\n"
     ]
    }
   ],
   "source": [
    "# Original Greedy Brute Force grid search that was infeasible because it would calculate ~108,000 different combination and take ~75 days, even on CUDA\n",
    "'''\n",
    "for number_of_layers in parameters['number_of_layers']:\n",
    "    for neurons_per_layer in parameters['neurons_per_layer']:\n",
    "        for dropout_layer in parameters['dropout_layers']:\n",
    "            for activation_function in parameters['activation_functions']:\n",
    "                for optimizer in parameters['optimizers']:\n",
    "                    for learning_rate in parameters['learning_rates']:\n",
    "                        for weight_decay in parameters['weight_decays']:\n",
    "                            for loss_function in parameters['loss_functions']:\n",
    "                                for batch in parameters['batches']:\n",
    "                                    for epochs in parameters['epochs']:\n",
    "                                        \n",
    "                                        neurons = []\n",
    "                                        if number_of_layers < 3:\n",
    "                                            neurons.append(neurons_per_layer)\n",
    "                                        else:\n",
    "                                            neurons += local_layers['neurons']\n",
    "                                            neurons.append(neurons_per_layer)\n",
    "                                        neurons.append(parameters['outputs'])\n",
    "    \n",
    "                                        activations = []\n",
    "                                        if number_of_layers < 3:\n",
    "                                            activations.append(activation_function)\n",
    "                                        else:\n",
    "                                            activations += local_layers['activation']\n",
    "                                            neurons.append(activation_function)\n",
    "    \n",
    "                                        print(f\"Fitting for combination {combination}\")\n",
    "                                        \n",
    "                                        model = MLP((parameters['inputs']), neurons, dropout_layer, activations, train_loader)\n",
    "                                        model.to(device)\n",
    "                                        model.fit(optimizer, loss_function, learning_rate, weight_decay, epochs)\n",
    "                                        \n",
    "                                        # Evaluate the model on the validation data\n",
    "                                        score = model.score(test_loader)\n",
    "    \n",
    "                                        results['number_of_layers'].append(number_of_layers)\n",
    "                                        results['neurons_per_layer'].append(neurons_per_layer)\n",
    "                                        results['dropout_layers'].append(dropout_layer)\n",
    "                                        results['activation_functions'].append(activation_function)\n",
    "                                        results['optimizers'].append(optimizer)\n",
    "                                        results['learning_rates'].append(learning_rate)\n",
    "                                        results['weight_decays'].append(weight_decay)\n",
    "                                        results['loss_functions'].append(loss_function)\n",
    "                                        results['batches'].append(batch)\n",
    "                                        results['epochs'].append(epochs)\n",
    "                                        results['inputs'].append(parameters['inputs'])\n",
    "                                        results['outputs'].append(parameters['outputs'])\n",
    "                                        results['score'].append(score)\n",
    "                                        \n",
    "                                        # If the current score is better than the best score, update the best score and best parameters\n",
    "                                        if score > best_parameters['score']:\n",
    "                                            best_score = score\n",
    "                                            for parameter in results:\n",
    "                                                best_parameters[parameter] = results[parameter][combination]\n",
    "                                            if len(local_layers['neurons']) < number_of_layers - 1:\n",
    "                                                print(f\"neurons {neurons_per_layer}\")\n",
    "                                                print(f\"activation {activation_function}\")\n",
    "                                                local_layers['neurons'].append(neurons_per_layer)\n",
    "                                                local_layers['activation'].append(activation_function)\n",
    "                                            else:\n",
    "                                                local_layers['neurons'][number_of_layers-2] = neurons_per_layer\n",
    "                                                local_layers['activation'][number_of_layers-2] = activation_function\n",
    "                                        elif score < worst_parameters['score']:\n",
    "                                            worst_score = score\n",
    "                                            for parameter in results:\n",
    "                                                worst_parameters[parameter] = results[parameter][combination]\n",
    "                                        \n",
    "                                        combination += 1\n",
    "'''\n",
    "\n",
    "# Much more reasonable Randomized Greedy grid search that only brute forces the number of layers, neurons per layer, and optimization functions,\n",
    "# which calculates ~40 different combination (~3.5 hours). All other parameters are chosen randomly during each iteration\n",
    "for number_of_layers in parameters['number_of_layers']:\n",
    "    for neurons_per_layer in parameters['neurons_per_layer']:\n",
    "        for optimizer in parameters['optimizers']:\n",
    "\n",
    "            # Randomly select the other hyperparameters\n",
    "            dropout_layer = random.choice(parameters['dropout_layers'])\n",
    "            activation_function = random.choice(parameters['activation_functions'])\n",
    "            learning_rate = random.choice(parameters['learning_rates'])\n",
    "            weight_decay = random.choice(parameters['weight_decays'])\n",
    "            loss_function = random.choice(parameters['loss_functions']) # I'm aware that there is only a single loss function, I'm just treating it the same as everything else\n",
    "            batch = random.choice(parameters['batches'])\n",
    "            epochs = random.choice(parameters['epochs'])\n",
    "            \n",
    "            # Create the network architecture                       \n",
    "            neurons = [] # Neurons array to store the neurons_per_layer\n",
    "            activations = [] # Activations array to store the activations per layer\n",
    "            if len(local_layers['neurons']) < number_of_layers: # If there aren't yet enough \"best performing\" neurons for the layer it's on, toss something in it\n",
    "                local_layers['neurons'].append(neurons_per_layer)\n",
    "                local_layers['activations'].append(activation_function)\n",
    "            for layer in range(0,number_of_layers-1): # Add in the \"best\" results from the previous layers, if any\n",
    "                neurons.append(local_layers['neurons'][layer])\n",
    "                activations.append(local_layers['activations'][layer])\n",
    "            neurons.append(neurons_per_layer) # Add the current number of neurons being tested for this layer\n",
    "            activations.append(activation_function) # Add the current (random) activation function being tested for this layer\n",
    "            neurons.append(parameters['outputs']) # For the final output layer, meaning we always have number_of_layers + 1 total layers\n",
    "            # PLEASE NOTE: the optimizer handles the softmax function on the output, so no softmax activation is added, meaning\n",
    "            # the activations array will always be one shorter than the neurons array\n",
    "    \n",
    "            print(f\"Fitting for combination {combination}\")\n",
    "\n",
    "            # Save the current set of hyperparameters into results\n",
    "            results['number_of_layers'].append(number_of_layers)\n",
    "            results['neurons_per_layer'].append(neurons)\n",
    "            results['dropout_layers'].append(dropout_layer)\n",
    "            results['activation_functions'].append(activations)\n",
    "            results['optimizers'].append(optimizer)\n",
    "            results['learning_rates'].append(learning_rate)\n",
    "            results['weight_decays'].append(weight_decay)\n",
    "            results['loss_functions'].append(loss_function)\n",
    "            results['batches'].append(batch)\n",
    "            results['epochs'].append(epochs)\n",
    "            results['inputs'].append(parameters['inputs'])\n",
    "            results['outputs'].append(parameters['outputs'])\n",
    "            results['score'].append(0)\n",
    "\n",
    "            for parameter in results:\n",
    "                print(results[parameter][combination])\n",
    "\n",
    "            # Define and fit the network\n",
    "            model = MLP((parameters['inputs']), neurons, dropout_layer, activations)\n",
    "            model.to(device)\n",
    "            model.fit(train_loader, optimizer, loss_function, learning_rate, weight_decay, batch, epochs)\n",
    "            \n",
    "            # Evaluate the model on the validation data\n",
    "            score = model.score(test_loader)\n",
    "            results['score'][combination] = score\n",
    "            \n",
    "            # If the current score is better than the best score, update the best score, best parameters, and save the architecture into local_layers\n",
    "            # This is what makes the algorithm \"greedy\" instead of brute force\n",
    "            if score > best_parameters['score']:\n",
    "                for parameter in results:\n",
    "                    best_parameters[parameter] = results[parameter][combination]\n",
    "                local_layers['neurons'] = neurons\n",
    "                local_layers['activations'] = activations\n",
    "            elif score < worst_parameters['score']:\n",
    "                for parameter in results:\n",
    "                    worst_parameters[parameter] = results[parameter][combination]\n",
    "            \n",
    "            combination += 1\n",
    "\n",
    "print('Best parameters:', best_parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'inputs': 784, 'number_of_layers': 1, 'outputs': 10, 'neurons_per_layer': [30, 10, 10, 10], 'dropout_layers': False, 'activation_functions': ['tanh', 'sigmoid', 'relu'], 'optimizers': 'Adam', 'learning_rates': 0.001, 'weight_decays': 0.01, 'loss_functions': 'CrossEntropyLoss', 'batches': 100, 'epochs': 25, 'score': 0.9315}\n",
      "{'inputs': 784, 'number_of_layers': 1, 'outputs': 10, 'neurons_per_layer': [20, 10], 'dropout_layers': False, 'activation_functions': ['sigmoid'], 'optimizers': 'Adam', 'learning_rates': 0.3, 'weight_decays': 1, 'loss_functions': 'CrossEntropyLoss', 'batches': 100, 'epochs': 25, 'score': 0.0892}\n"
     ]
    }
   ],
   "source": [
    "print(best_parameters) # THERE IS SOMETHING WRONG WITH HOW THIS IS BEING TRACKED AND I'M NOT ENTIRELY SURE WHAT\n",
    "print(worst_parameters) # THIS ONE SEEMS PRETTY RIGHT THOUGH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please note that there is something...  WRONG with the above.  The score, 0.9315, is correct.  The greedy algorithm is also seemingly working correctly, which will be shown below.  Just the tracking of the best parameters is messed up.  The REAL best parameters are these:\n",
    "\n",
    "784\n",
    "1\n",
    "10\n",
    "[30, 10]\n",
    "False\n",
    "['tanh']\n",
    "Adam\n",
    "0.001\n",
    "0.01\n",
    "CrossEntropyLoss\n",
    "100\n",
    "25\n",
    "0\n",
    "\n",
    "I believe there is some defect in the results and parameter dicts caused by the mutability of arrays in python.  Somehow.  It seems to have only impacted the best results of each model, and only the arrays (neurons_per_layer and activation_functions). I figured this out after training all of these hundreds of models and I don't want to spend hours re-running all of these tests when the only thing \"wrong\" is the results dict. The results will be manually fixed before calculating any metrics down below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"mnist_without_augments.csv\", \"w\", newline=\"\") as file:\n",
    "    writer = csv.DictWriter(file, results.keys())\n",
    "    writer.writeheader()\n",
    "    writer.writerow(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the Best Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are some results from a previous full run that had some iteration issues, but it shows us something interesting: 90% accuracy can be obtained with only a single hidden layer and an output layer, so 2 layers total:\n",
    "\n",
    "print(best_parameters)\n",
    "\n",
    "print(worst_parameters)\n",
    "\n",
    "{'inputs': 784, 'number_of_layers': 2, 'outputs': 10, 'neurons_per_layer': [10, 10], 'dropout_layers': False, 'activation_functions': ['tanh'], 'optimizers': 'SGD', 'learning_rates': 0.003, 'weight_decays': 0.01, 'loss_functions': 'CrossEntropyLoss', 'batches': 100, 'epochs': 25, 'score': 0.9066}\n",
    "\n",
    "{'inputs': 784, 'number_of_layers': 3, 'outputs': 10, 'neurons_per_layer': [10, 40, 10], 'dropout_layers': True, 'activation_functions': ['tanh', 'sigmoid'], 'optimizers': 'Adam', 'learning_rates': 0.1, 'weight_decays': 0.1, 'loss_functions': 'CrossEntropyLoss', 'batches': 300, 'epochs': 25, 'score': 0.0892}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Batch 100, Loss: 2.1683448660373688\n",
      "Epoch 1, Batch 200, Loss: 1.8907299280166625\n",
      "Epoch 1, Batch 300, Loss: 1.7443784546852112\n",
      "Epoch 1, Batch 400, Loss: 1.6490601587295532\n",
      "Epoch 1, Batch 500, Loss: 1.5699206805229187\n",
      "Epoch 1, Batch 600, Loss: 1.4962298274040222\n",
      "Epoch 1, Batch 700, Loss: 1.4510478830337525\n",
      "Epoch 1, Batch 800, Loss: 1.4044962656497955\n",
      "Epoch 1, Batch 900, Loss: 1.3459511113166809\n",
      "Epoch 2, Batch 100, Loss: 1.2887906754016876\n",
      "Epoch 2, Batch 200, Loss: 1.246523778438568\n",
      "Epoch 2, Batch 300, Loss: 1.2168618714809418\n",
      "Epoch 2, Batch 400, Loss: 1.1669938039779664\n",
      "Epoch 2, Batch 500, Loss: 1.1318346178531646\n",
      "Epoch 2, Batch 600, Loss: 1.11261554479599\n",
      "Epoch 2, Batch 700, Loss: 1.0856644201278687\n",
      "Epoch 2, Batch 800, Loss: 1.058673678636551\n",
      "Epoch 2, Batch 900, Loss: 1.0344437181949615\n",
      "Epoch 3, Batch 100, Loss: 1.001597905755043\n",
      "Epoch 3, Batch 200, Loss: 0.9744250214099884\n",
      "Epoch 3, Batch 300, Loss: 0.9567421394586563\n",
      "Epoch 3, Batch 400, Loss: 0.9149848026037216\n",
      "Epoch 3, Batch 500, Loss: 0.9116486668586731\n",
      "Epoch 3, Batch 600, Loss: 0.902739098072052\n",
      "Epoch 3, Batch 700, Loss: 0.8765326887369156\n",
      "Epoch 3, Batch 800, Loss: 0.8647979778051377\n",
      "Epoch 3, Batch 900, Loss: 0.8493481117486954\n",
      "Epoch 4, Batch 100, Loss: 0.8211150014400482\n",
      "Epoch 4, Batch 200, Loss: 0.8112613528966903\n",
      "Epoch 4, Batch 300, Loss: 0.7969390040636063\n",
      "Epoch 4, Batch 400, Loss: 0.7879145431518555\n",
      "Epoch 4, Batch 500, Loss: 0.7796124303340912\n",
      "Epoch 4, Batch 600, Loss: 0.7737205064296723\n",
      "Epoch 4, Batch 700, Loss: 0.7488846534490585\n",
      "Epoch 4, Batch 800, Loss: 0.742380108833313\n",
      "Epoch 4, Batch 900, Loss: 0.7323604071140289\n",
      "Epoch 5, Batch 100, Loss: 0.7233229541778564\n",
      "Epoch 5, Batch 200, Loss: 0.700810968875885\n",
      "Epoch 5, Batch 300, Loss: 0.7036721360683441\n",
      "Epoch 5, Batch 400, Loss: 0.687840320467949\n",
      "Epoch 5, Batch 500, Loss: 0.6740779149532318\n",
      "Epoch 5, Batch 600, Loss: 0.6819410866498947\n",
      "Epoch 5, Batch 700, Loss: 0.6727294933795929\n",
      "Epoch 5, Batch 800, Loss: 0.6606072670221329\n",
      "Epoch 5, Batch 900, Loss: 0.6600094375014305\n",
      "Epoch 6, Batch 100, Loss: 0.6513191705942154\n",
      "Epoch 6, Batch 200, Loss: 0.6489674714207649\n",
      "Epoch 6, Batch 300, Loss: 0.6424982652068139\n",
      "Epoch 6, Batch 400, Loss: 0.6224596792459488\n",
      "Epoch 6, Batch 500, Loss: 0.62490155428648\n",
      "Epoch 6, Batch 600, Loss: 0.6112331387400627\n",
      "Epoch 6, Batch 700, Loss: 0.6058012422919273\n",
      "Epoch 6, Batch 800, Loss: 0.5990972512960434\n",
      "Epoch 6, Batch 900, Loss: 0.5936191740632057\n",
      "Epoch 7, Batch 100, Loss: 0.5937437325716018\n",
      "Epoch 7, Batch 200, Loss: 0.5887588888406754\n",
      "Epoch 7, Batch 300, Loss: 0.5732683101296425\n",
      "Epoch 7, Batch 400, Loss: 0.5662779167294503\n",
      "Epoch 7, Batch 500, Loss: 0.583625834286213\n",
      "Epoch 7, Batch 600, Loss: 0.5767776933312416\n",
      "Epoch 7, Batch 700, Loss: 0.5790294232964516\n",
      "Epoch 7, Batch 800, Loss: 0.5751019942760468\n",
      "Epoch 7, Batch 900, Loss: 0.566089086830616\n",
      "Epoch 8, Batch 100, Loss: 0.5555891793966293\n",
      "Epoch 8, Batch 200, Loss: 0.553094684779644\n",
      "Epoch 8, Batch 300, Loss: 0.5496362516283989\n",
      "Epoch 8, Batch 400, Loss: 0.5597054734826088\n",
      "Epoch 8, Batch 500, Loss: 0.5415908896923065\n",
      "Epoch 8, Batch 600, Loss: 0.5360696673393249\n",
      "Epoch 8, Batch 700, Loss: 0.5422817912697792\n",
      "Epoch 8, Batch 800, Loss: 0.5398080661892891\n",
      "Epoch 8, Batch 900, Loss: 0.5286978486180306\n",
      "Epoch 9, Batch 100, Loss: 0.5279055002331734\n",
      "Epoch 9, Batch 200, Loss: 0.5319568902254105\n",
      "Epoch 9, Batch 300, Loss: 0.5362592774629593\n",
      "Epoch 9, Batch 400, Loss: 0.5124459797143937\n",
      "Epoch 9, Batch 500, Loss: 0.5086486652493477\n",
      "Epoch 9, Batch 600, Loss: 0.5162801831960678\n",
      "Epoch 9, Batch 700, Loss: 0.5114795729517937\n",
      "Epoch 9, Batch 800, Loss: 0.5245013302564621\n",
      "Epoch 9, Batch 900, Loss: 0.5110024347901344\n",
      "Epoch 10, Batch 100, Loss: 0.49068349838256836\n",
      "Epoch 10, Batch 200, Loss: 0.5033599621057511\n",
      "Epoch 10, Batch 300, Loss: 0.49276236206293106\n",
      "Epoch 10, Batch 400, Loss: 0.5020215621590615\n",
      "Epoch 10, Batch 500, Loss: 0.5032297456264496\n",
      "Epoch 10, Batch 600, Loss: 0.5016469645500183\n",
      "Epoch 10, Batch 700, Loss: 0.5007679945230484\n",
      "Epoch 10, Batch 800, Loss: 0.49332712769508363\n",
      "Epoch 10, Batch 900, Loss: 0.5047894325852395\n",
      "Epoch 11, Batch 100, Loss: 0.4972430047392845\n",
      "Epoch 11, Batch 200, Loss: 0.4811775866150856\n",
      "Epoch 11, Batch 300, Loss: 0.4821936002373695\n",
      "Epoch 11, Batch 400, Loss: 0.5000790718197823\n",
      "Epoch 11, Batch 500, Loss: 0.4785998305678368\n",
      "Epoch 11, Batch 600, Loss: 0.4920099422335625\n",
      "Epoch 11, Batch 700, Loss: 0.47670388758182525\n",
      "Epoch 11, Batch 800, Loss: 0.4749501270055771\n",
      "Epoch 11, Batch 900, Loss: 0.47373200953006744\n",
      "Epoch 12, Batch 100, Loss: 0.47321745872497556\n",
      "Epoch 12, Batch 200, Loss: 0.48156443566083906\n",
      "Epoch 12, Batch 300, Loss: 0.47491156101226806\n",
      "Epoch 12, Batch 400, Loss: 0.48129255533218385\n",
      "Epoch 12, Batch 500, Loss: 0.4680547320842743\n",
      "Epoch 12, Batch 600, Loss: 0.4620855402946472\n",
      "Epoch 12, Batch 700, Loss: 0.4713816404342651\n",
      "Epoch 12, Batch 800, Loss: 0.4617569580674171\n",
      "Epoch 12, Batch 900, Loss: 0.4652834165096283\n",
      "Epoch 13, Batch 100, Loss: 0.47056335300207136\n",
      "Epoch 13, Batch 200, Loss: 0.46216280162334444\n",
      "Epoch 13, Batch 300, Loss: 0.4815642738342285\n",
      "Epoch 13, Batch 400, Loss: 0.4602001827955246\n",
      "Epoch 13, Batch 500, Loss: 0.44297485500574113\n",
      "Epoch 13, Batch 600, Loss: 0.46094911903142927\n",
      "Epoch 13, Batch 700, Loss: 0.44842148274183274\n",
      "Epoch 13, Batch 800, Loss: 0.46112713605165484\n",
      "Epoch 13, Batch 900, Loss: 0.4563491281867027\n",
      "Epoch 14, Batch 100, Loss: 0.4470255243778229\n",
      "Epoch 14, Batch 200, Loss: 0.4569036287069321\n",
      "Epoch 14, Batch 300, Loss: 0.44156227201223375\n",
      "Epoch 14, Batch 400, Loss: 0.4451734361052513\n",
      "Epoch 14, Batch 500, Loss: 0.4577379015088081\n",
      "Epoch 14, Batch 600, Loss: 0.45611087679862977\n",
      "Epoch 14, Batch 700, Loss: 0.45746216386556626\n",
      "Epoch 14, Batch 800, Loss: 0.4553080561757088\n",
      "Epoch 14, Batch 900, Loss: 0.44821794241666796\n",
      "Epoch 15, Batch 100, Loss: 0.44965710818767546\n",
      "Epoch 15, Batch 200, Loss: 0.4423112779855728\n",
      "Epoch 15, Batch 300, Loss: 0.43908042639493944\n",
      "Epoch 15, Batch 400, Loss: 0.43736294090747835\n",
      "Epoch 15, Batch 500, Loss: 0.4387690508365631\n",
      "Epoch 15, Batch 600, Loss: 0.44384451627731325\n",
      "Epoch 15, Batch 700, Loss: 0.441554294526577\n",
      "Epoch 15, Batch 800, Loss: 0.44458657920360567\n",
      "Epoch 15, Batch 900, Loss: 0.4479394134879112\n",
      "Epoch 16, Batch 100, Loss: 0.43575206607580186\n",
      "Epoch 16, Batch 200, Loss: 0.44193112462759016\n",
      "Epoch 16, Batch 300, Loss: 0.44575057715177535\n",
      "Epoch 16, Batch 400, Loss: 0.4409059876203537\n",
      "Epoch 16, Batch 500, Loss: 0.42083024740219116\n",
      "Epoch 16, Batch 600, Loss: 0.45133272886276243\n",
      "Epoch 16, Batch 700, Loss: 0.42631244838237764\n",
      "Epoch 16, Batch 800, Loss: 0.44106811702251436\n",
      "Epoch 16, Batch 900, Loss: 0.43077988356351854\n",
      "Epoch 17, Batch 100, Loss: 0.4311788383126259\n",
      "Epoch 17, Batch 200, Loss: 0.4289709910750389\n",
      "Epoch 17, Batch 300, Loss: 0.435688296854496\n",
      "Epoch 17, Batch 400, Loss: 0.4340182963013649\n",
      "Epoch 17, Batch 500, Loss: 0.43582072407007216\n",
      "Epoch 17, Batch 600, Loss: 0.4372060425579548\n",
      "Epoch 17, Batch 700, Loss: 0.43723119229078294\n",
      "Epoch 17, Batch 800, Loss: 0.42050312042236326\n",
      "Epoch 17, Batch 900, Loss: 0.42201718777418135\n",
      "Epoch 18, Batch 100, Loss: 0.4290685111284256\n",
      "Epoch 18, Batch 200, Loss: 0.436537127494812\n",
      "Epoch 18, Batch 300, Loss: 0.435846351981163\n",
      "Epoch 18, Batch 400, Loss: 0.43456925481557845\n",
      "Epoch 18, Batch 500, Loss: 0.41572966396808625\n",
      "Epoch 18, Batch 600, Loss: 0.4197327271103859\n",
      "Epoch 18, Batch 700, Loss: 0.43128136813640594\n",
      "Epoch 18, Batch 800, Loss: 0.4148687964677811\n",
      "Epoch 18, Batch 900, Loss: 0.415044923722744\n",
      "Epoch 19, Batch 100, Loss: 0.41885689869523046\n",
      "Epoch 19, Batch 200, Loss: 0.4353275743126869\n",
      "Epoch 19, Batch 300, Loss: 0.43160512059926986\n",
      "Epoch 19, Batch 400, Loss: 0.41276328295469283\n",
      "Epoch 19, Batch 500, Loss: 0.41773207813501356\n",
      "Epoch 19, Batch 600, Loss: 0.42170680820941925\n",
      "Epoch 19, Batch 700, Loss: 0.4196070820093155\n",
      "Epoch 19, Batch 800, Loss: 0.4204434809088707\n",
      "Epoch 19, Batch 900, Loss: 0.4161510521173477\n",
      "Epoch 20, Batch 100, Loss: 0.4096391445398331\n",
      "Epoch 20, Batch 200, Loss: 0.41641820430755616\n",
      "Epoch 20, Batch 300, Loss: 0.42518160611391065\n",
      "Epoch 20, Batch 400, Loss: 0.41578699216246606\n",
      "Epoch 20, Batch 500, Loss: 0.4157721015810967\n",
      "Epoch 20, Batch 600, Loss: 0.43155165284872055\n",
      "Epoch 20, Batch 700, Loss: 0.41336374208331106\n",
      "Epoch 20, Batch 800, Loss: 0.4175617060065269\n",
      "Epoch 20, Batch 900, Loss: 0.4040031585097313\n",
      "Epoch 21, Batch 100, Loss: 0.40570886343717577\n",
      "Epoch 21, Batch 200, Loss: 0.4223542268574238\n",
      "Epoch 21, Batch 300, Loss: 0.4144339042901993\n",
      "Epoch 21, Batch 400, Loss: 0.4218767538666725\n",
      "Epoch 21, Batch 500, Loss: 0.4147991943359375\n",
      "Epoch 21, Batch 600, Loss: 0.42159827828407287\n",
      "Epoch 21, Batch 700, Loss: 0.41042051538825036\n",
      "Epoch 21, Batch 800, Loss: 0.40043333888053895\n",
      "Epoch 21, Batch 900, Loss: 0.4170255298912525\n",
      "Epoch 22, Batch 100, Loss: 0.40754544287919997\n",
      "Epoch 22, Batch 200, Loss: 0.4145868894457817\n",
      "Epoch 22, Batch 300, Loss: 0.4112010559439659\n",
      "Epoch 22, Batch 400, Loss: 0.4187831124663353\n",
      "Epoch 22, Batch 500, Loss: 0.4168018931150436\n",
      "Epoch 22, Batch 600, Loss: 0.41176446110010145\n",
      "Epoch 22, Batch 700, Loss: 0.4046323809027672\n",
      "Epoch 22, Batch 800, Loss: 0.41691951125860216\n",
      "Epoch 22, Batch 900, Loss: 0.40162621468305587\n",
      "Epoch 23, Batch 100, Loss: 0.3991581089794636\n",
      "Epoch 23, Batch 200, Loss: 0.4149786046147346\n",
      "Epoch 23, Batch 300, Loss: 0.4150220197439194\n",
      "Epoch 23, Batch 400, Loss: 0.40355480045080183\n",
      "Epoch 23, Batch 500, Loss: 0.4073930445313454\n",
      "Epoch 23, Batch 600, Loss: 0.4037324245274067\n",
      "Epoch 23, Batch 700, Loss: 0.3965423654019833\n",
      "Epoch 23, Batch 800, Loss: 0.41177611500024797\n",
      "Epoch 23, Batch 900, Loss: 0.42310207188129423\n",
      "Epoch 24, Batch 100, Loss: 0.3937268190085888\n",
      "Epoch 24, Batch 200, Loss: 0.41796601444482806\n",
      "Epoch 24, Batch 300, Loss: 0.41437910959124563\n",
      "Epoch 24, Batch 400, Loss: 0.3982764530181885\n",
      "Epoch 24, Batch 500, Loss: 0.4085423693060875\n",
      "Epoch 24, Batch 600, Loss: 0.39801679134368895\n",
      "Epoch 24, Batch 700, Loss: 0.40186510533094405\n",
      "Epoch 24, Batch 800, Loss: 0.42236069828271866\n",
      "Epoch 24, Batch 900, Loss: 0.40118171453475954\n",
      "Epoch 25, Batch 100, Loss: 0.39794172406196593\n",
      "Epoch 25, Batch 200, Loss: 0.4096869546175003\n",
      "Epoch 25, Batch 300, Loss: 0.4055464777350426\n",
      "Epoch 25, Batch 400, Loss: 0.4021562044322491\n",
      "Epoch 25, Batch 500, Loss: 0.3989961887896061\n",
      "Epoch 25, Batch 600, Loss: 0.4008499041199684\n",
      "Epoch 25, Batch 700, Loss: 0.39289137586951256\n",
      "Epoch 25, Batch 800, Loss: 0.40395038306713105\n",
      "Epoch 25, Batch 900, Loss: 0.4122780773043633\n",
      "Accuracy on test set: 0.9071%\n"
     ]
    }
   ],
   "source": [
    "# And here's the proof for the above\n",
    "\n",
    "model = MLP((28*28), [10, 10], False, ['tanh'])\n",
    "model.to(device)\n",
    "model.fit(train_loader, 'SGD', 'CrossEntropyLoss', 0.003, 0.01, 100, 25)\n",
    "score = model.score(test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Batch 100, Loss: 1.2703438222408294\n",
      "Epoch 1, Batch 200, Loss: 0.657022970020771\n",
      "Epoch 1, Batch 300, Loss: 0.5120882335305214\n",
      "Epoch 1, Batch 400, Loss: 0.4298510092496872\n",
      "Epoch 1, Batch 500, Loss: 0.4014650613069534\n",
      "Epoch 1, Batch 600, Loss: 0.373924899995327\n",
      "Epoch 1, Batch 700, Loss: 0.3787156601250172\n",
      "Epoch 1, Batch 800, Loss: 0.36641911819577216\n",
      "Epoch 1, Batch 900, Loss: 0.34734390795230863\n",
      "Epoch 2, Batch 100, Loss: 0.3373168142139912\n",
      "Epoch 2, Batch 200, Loss: 0.3329009784758091\n",
      "Epoch 2, Batch 300, Loss: 0.34941223323345183\n",
      "Epoch 2, Batch 400, Loss: 0.314088284522295\n",
      "Epoch 2, Batch 500, Loss: 0.3320115047693253\n",
      "Epoch 2, Batch 600, Loss: 0.3196147811412811\n",
      "Epoch 2, Batch 700, Loss: 0.3188136124610901\n",
      "Epoch 2, Batch 800, Loss: 0.3125146234035492\n",
      "Epoch 2, Batch 900, Loss: 0.31733709141612054\n",
      "Epoch 3, Batch 100, Loss: 0.3113005143404007\n",
      "Epoch 3, Batch 200, Loss: 0.31766519203782084\n",
      "Epoch 3, Batch 300, Loss: 0.2918596109747887\n",
      "Epoch 3, Batch 400, Loss: 0.30268812164664266\n",
      "Epoch 3, Batch 500, Loss: 0.3258713415265083\n",
      "Epoch 3, Batch 600, Loss: 0.29506819531321526\n",
      "Epoch 3, Batch 700, Loss: 0.3027909098565578\n",
      "Epoch 3, Batch 800, Loss: 0.3056508007645607\n",
      "Epoch 3, Batch 900, Loss: 0.30153734974563123\n",
      "Epoch 4, Batch 100, Loss: 0.2980291782319546\n",
      "Epoch 4, Batch 200, Loss: 0.2920131170749664\n",
      "Epoch 4, Batch 300, Loss: 0.3093721228837967\n",
      "Epoch 4, Batch 400, Loss: 0.3037105096131563\n",
      "Epoch 4, Batch 500, Loss: 0.29926115825772287\n",
      "Epoch 4, Batch 600, Loss: 0.29860312804579736\n",
      "Epoch 4, Batch 700, Loss: 0.2959014470875263\n",
      "Epoch 4, Batch 800, Loss: 0.318628086745739\n",
      "Epoch 4, Batch 900, Loss: 0.3063014131784439\n",
      "Epoch 5, Batch 100, Loss: 0.2933930683135986\n",
      "Epoch 5, Batch 200, Loss: 0.29717023521661756\n",
      "Epoch 5, Batch 300, Loss: 0.30317950002849103\n",
      "Epoch 5, Batch 400, Loss: 0.30032520517706873\n",
      "Epoch 5, Batch 500, Loss: 0.2880021597445011\n",
      "Epoch 5, Batch 600, Loss: 0.2999483093619347\n",
      "Epoch 5, Batch 700, Loss: 0.2863247174024582\n",
      "Epoch 5, Batch 800, Loss: 0.29591569796204564\n",
      "Epoch 5, Batch 900, Loss: 0.2784983275830746\n",
      "Epoch 6, Batch 100, Loss: 0.2957004208862781\n",
      "Epoch 6, Batch 200, Loss: 0.2968923997879028\n",
      "Epoch 6, Batch 300, Loss: 0.2942459037899971\n",
      "Epoch 6, Batch 400, Loss: 0.27710779160261156\n",
      "Epoch 6, Batch 500, Loss: 0.29524182066321375\n",
      "Epoch 6, Batch 600, Loss: 0.2925660848617554\n",
      "Epoch 6, Batch 700, Loss: 0.2822094379365444\n",
      "Epoch 6, Batch 800, Loss: 0.27841550797224046\n",
      "Epoch 6, Batch 900, Loss: 0.2933654601871967\n",
      "Epoch 7, Batch 100, Loss: 0.2880525480210781\n",
      "Epoch 7, Batch 200, Loss: 0.28995523616671565\n",
      "Epoch 7, Batch 300, Loss: 0.28524583637714385\n",
      "Epoch 7, Batch 400, Loss: 0.2791084612905979\n",
      "Epoch 7, Batch 500, Loss: 0.29849151223897935\n",
      "Epoch 7, Batch 600, Loss: 0.2861573792994022\n",
      "Epoch 7, Batch 700, Loss: 0.3019624102115631\n",
      "Epoch 7, Batch 800, Loss: 0.3021112070977688\n",
      "Epoch 7, Batch 900, Loss: 0.2796434634923935\n",
      "Epoch 8, Batch 100, Loss: 0.2819035394489765\n",
      "Epoch 8, Batch 200, Loss: 0.2877470533549786\n",
      "Epoch 8, Batch 300, Loss: 0.29435386404395103\n",
      "Epoch 8, Batch 400, Loss: 0.28711827859282496\n",
      "Epoch 8, Batch 500, Loss: 0.27467296838760374\n",
      "Epoch 8, Batch 600, Loss: 0.27689784109592436\n",
      "Epoch 8, Batch 700, Loss: 0.27326941177248953\n",
      "Epoch 8, Batch 800, Loss: 0.3094142338633537\n",
      "Epoch 8, Batch 900, Loss: 0.29891066446900366\n",
      "Epoch 9, Batch 100, Loss: 0.2799564632773399\n",
      "Epoch 9, Batch 200, Loss: 0.28456380173563955\n",
      "Epoch 9, Batch 300, Loss: 0.2925034290552139\n",
      "Epoch 9, Batch 400, Loss: 0.284509020447731\n",
      "Epoch 9, Batch 500, Loss: 0.2873040854930878\n",
      "Epoch 9, Batch 600, Loss: 0.2949142163991928\n",
      "Epoch 9, Batch 700, Loss: 0.2765858109295368\n",
      "Epoch 9, Batch 800, Loss: 0.29473208226263525\n",
      "Epoch 9, Batch 900, Loss: 0.29195095106959346\n",
      "Epoch 10, Batch 100, Loss: 0.29305908843874934\n",
      "Epoch 10, Batch 200, Loss: 0.2758795215189457\n",
      "Epoch 10, Batch 300, Loss: 0.27895511843264104\n",
      "Epoch 10, Batch 400, Loss: 0.2848364621400833\n",
      "Epoch 10, Batch 500, Loss: 0.28393199399113656\n",
      "Epoch 10, Batch 600, Loss: 0.28447588697075843\n",
      "Epoch 10, Batch 700, Loss: 0.28355235621333125\n",
      "Epoch 10, Batch 800, Loss: 0.28514356777071953\n",
      "Epoch 10, Batch 900, Loss: 0.2878238098323345\n",
      "Epoch 11, Batch 100, Loss: 0.2688942071795464\n",
      "Epoch 11, Batch 200, Loss: 0.2902006682753563\n",
      "Epoch 11, Batch 300, Loss: 0.28017725333571436\n",
      "Epoch 11, Batch 400, Loss: 0.2892945243418217\n",
      "Epoch 11, Batch 500, Loss: 0.2761461925506592\n",
      "Epoch 11, Batch 600, Loss: 0.2843369457125664\n",
      "Epoch 11, Batch 700, Loss: 0.3027731414139271\n",
      "Epoch 11, Batch 800, Loss: 0.2866284750401974\n",
      "Epoch 11, Batch 900, Loss: 0.2873884990811348\n",
      "Epoch 12, Batch 100, Loss: 0.28911730766296384\n",
      "Epoch 12, Batch 200, Loss: 0.2841318586468697\n",
      "Epoch 12, Batch 300, Loss: 0.2687912590801716\n",
      "Epoch 12, Batch 400, Loss: 0.2912788139283657\n",
      "Epoch 12, Batch 500, Loss: 0.2934736105799675\n",
      "Epoch 12, Batch 600, Loss: 0.2828548687696457\n",
      "Epoch 12, Batch 700, Loss: 0.29654019013047217\n",
      "Epoch 12, Batch 800, Loss: 0.2819668298959732\n",
      "Epoch 12, Batch 900, Loss: 0.2834647165238857\n",
      "Epoch 13, Batch 100, Loss: 0.2854487270116806\n",
      "Epoch 13, Batch 200, Loss: 0.2925183066725731\n",
      "Epoch 13, Batch 300, Loss: 0.29295539885759353\n",
      "Epoch 13, Batch 400, Loss: 0.28030758365988734\n",
      "Epoch 13, Batch 500, Loss: 0.267630136013031\n",
      "Epoch 13, Batch 600, Loss: 0.26944380134344104\n",
      "Epoch 13, Batch 700, Loss: 0.28675867661833765\n",
      "Epoch 13, Batch 800, Loss: 0.28483941331505774\n",
      "Epoch 13, Batch 900, Loss: 0.282278810441494\n",
      "Epoch 14, Batch 100, Loss: 0.28315268993377685\n",
      "Epoch 14, Batch 200, Loss: 0.282747433334589\n",
      "Epoch 14, Batch 300, Loss: 0.2752750088274479\n",
      "Epoch 14, Batch 400, Loss: 0.2897917807102203\n",
      "Epoch 14, Batch 500, Loss: 0.2962811627984047\n",
      "Epoch 14, Batch 600, Loss: 0.2729443034529686\n",
      "Epoch 14, Batch 700, Loss: 0.2841282650828362\n",
      "Epoch 14, Batch 800, Loss: 0.27514008089900016\n",
      "Epoch 14, Batch 900, Loss: 0.2777450217306614\n",
      "Epoch 15, Batch 100, Loss: 0.26878142975270747\n",
      "Epoch 15, Batch 200, Loss: 0.29918666392564774\n",
      "Epoch 15, Batch 300, Loss: 0.2858313834667206\n",
      "Epoch 15, Batch 400, Loss: 0.28157573074102404\n",
      "Epoch 15, Batch 500, Loss: 0.2884479720890522\n",
      "Epoch 15, Batch 600, Loss: 0.2895513066649437\n",
      "Epoch 15, Batch 700, Loss: 0.27795864567160605\n",
      "Epoch 15, Batch 800, Loss: 0.2898428426682949\n",
      "Epoch 15, Batch 900, Loss: 0.28356422781944274\n",
      "Epoch 16, Batch 100, Loss: 0.2659329783916473\n",
      "Epoch 16, Batch 200, Loss: 0.2726769057661295\n",
      "Epoch 16, Batch 300, Loss: 0.2731785669922829\n",
      "Epoch 16, Batch 400, Loss: 0.2786114723980427\n",
      "Epoch 16, Batch 500, Loss: 0.2892815202474594\n",
      "Epoch 16, Batch 600, Loss: 0.30250988468527795\n",
      "Epoch 16, Batch 700, Loss: 0.2763246588408947\n",
      "Epoch 16, Batch 800, Loss: 0.2943327708542347\n",
      "Epoch 16, Batch 900, Loss: 0.29243933111429216\n",
      "Epoch 17, Batch 100, Loss: 0.27114487931132314\n",
      "Epoch 17, Batch 200, Loss: 0.26293014869093895\n",
      "Epoch 17, Batch 300, Loss: 0.27077885538339613\n",
      "Epoch 17, Batch 400, Loss: 0.2781437020003796\n",
      "Epoch 17, Batch 500, Loss: 0.27831122174859046\n",
      "Epoch 17, Batch 600, Loss: 0.2847350713610649\n",
      "Epoch 17, Batch 700, Loss: 0.29220171973109244\n",
      "Epoch 17, Batch 800, Loss: 0.2821069982647896\n",
      "Epoch 17, Batch 900, Loss: 0.29601247027516364\n",
      "Epoch 18, Batch 100, Loss: 0.2737107926607132\n",
      "Epoch 18, Batch 200, Loss: 0.2853663267195225\n",
      "Epoch 18, Batch 300, Loss: 0.28627290710806846\n",
      "Epoch 18, Batch 400, Loss: 0.2864664015173912\n",
      "Epoch 18, Batch 500, Loss: 0.2810601472854614\n",
      "Epoch 18, Batch 600, Loss: 0.2717366849631071\n",
      "Epoch 18, Batch 700, Loss: 0.2791225050389767\n",
      "Epoch 18, Batch 800, Loss: 0.2720915213227272\n",
      "Epoch 18, Batch 900, Loss: 0.28757252112030984\n",
      "Epoch 19, Batch 100, Loss: 0.2886545404791832\n",
      "Epoch 19, Batch 200, Loss: 0.2727163617312908\n",
      "Epoch 19, Batch 300, Loss: 0.29225451678037645\n",
      "Epoch 19, Batch 400, Loss: 0.2801751419901848\n",
      "Epoch 19, Batch 500, Loss: 0.2773225185275078\n",
      "Epoch 19, Batch 600, Loss: 0.28387260586023333\n",
      "Epoch 19, Batch 700, Loss: 0.276816143989563\n",
      "Epoch 19, Batch 800, Loss: 0.28005932599306105\n",
      "Epoch 19, Batch 900, Loss: 0.28816464081406595\n",
      "Epoch 20, Batch 100, Loss: 0.2754633808135986\n",
      "Epoch 20, Batch 200, Loss: 0.270830045491457\n",
      "Epoch 20, Batch 300, Loss: 0.2771803915500641\n",
      "Epoch 20, Batch 400, Loss: 0.2926746092736721\n",
      "Epoch 20, Batch 500, Loss: 0.2774073179066181\n",
      "Epoch 20, Batch 600, Loss: 0.28525624111294745\n",
      "Epoch 20, Batch 700, Loss: 0.2731419284641743\n",
      "Epoch 20, Batch 800, Loss: 0.2921938121318817\n",
      "Epoch 20, Batch 900, Loss: 0.2889736731350422\n",
      "Epoch 21, Batch 100, Loss: 0.288407493531704\n",
      "Epoch 21, Batch 200, Loss: 0.27914718896150587\n",
      "Epoch 21, Batch 300, Loss: 0.2834553378820419\n",
      "Epoch 21, Batch 400, Loss: 0.26685601979494095\n",
      "Epoch 21, Batch 500, Loss: 0.27950875729322433\n",
      "Epoch 21, Batch 600, Loss: 0.2779263693094254\n",
      "Epoch 21, Batch 700, Loss: 0.2850539952516556\n",
      "Epoch 21, Batch 800, Loss: 0.28876945123076436\n",
      "Epoch 21, Batch 900, Loss: 0.28422098480165003\n",
      "Epoch 22, Batch 100, Loss: 0.27389745265245435\n",
      "Epoch 22, Batch 200, Loss: 0.2909285870194435\n",
      "Epoch 22, Batch 300, Loss: 0.28134407341480255\n",
      "Epoch 22, Batch 400, Loss: 0.28920317903161047\n",
      "Epoch 22, Batch 500, Loss: 0.28748834438621995\n",
      "Epoch 22, Batch 600, Loss: 0.292507847994566\n",
      "Epoch 22, Batch 700, Loss: 0.2786299343407154\n",
      "Epoch 22, Batch 800, Loss: 0.27617879524827005\n",
      "Epoch 22, Batch 900, Loss: 0.26017615213990214\n",
      "Epoch 23, Batch 100, Loss: 0.2892115218192339\n",
      "Epoch 23, Batch 200, Loss: 0.2734393388032913\n",
      "Epoch 23, Batch 300, Loss: 0.2760520486533642\n",
      "Epoch 23, Batch 400, Loss: 0.28213797360658643\n",
      "Epoch 23, Batch 500, Loss: 0.2748683485388756\n",
      "Epoch 23, Batch 600, Loss: 0.2771019048988819\n",
      "Epoch 23, Batch 700, Loss: 0.2920504704117775\n",
      "Epoch 23, Batch 800, Loss: 0.2834454956650734\n",
      "Epoch 23, Batch 900, Loss: 0.277475922703743\n",
      "Epoch 24, Batch 100, Loss: 0.2765570756793022\n",
      "Epoch 24, Batch 200, Loss: 0.2801434536278248\n",
      "Epoch 24, Batch 300, Loss: 0.28212892681360247\n",
      "Epoch 24, Batch 400, Loss: 0.27420290842652323\n",
      "Epoch 24, Batch 500, Loss: 0.2845497506856918\n",
      "Epoch 24, Batch 600, Loss: 0.2854844385385513\n",
      "Epoch 24, Batch 700, Loss: 0.28328108951449393\n",
      "Epoch 24, Batch 800, Loss: 0.2906604099273682\n",
      "Epoch 24, Batch 900, Loss: 0.281742650270462\n",
      "Epoch 25, Batch 100, Loss: 0.28057955741882323\n",
      "Epoch 25, Batch 200, Loss: 0.27549466855823995\n",
      "Epoch 25, Batch 300, Loss: 0.2889581745862961\n",
      "Epoch 25, Batch 400, Loss: 0.28223464384675023\n",
      "Epoch 25, Batch 500, Loss: 0.29758174538612364\n",
      "Epoch 25, Batch 600, Loss: 0.27428429290652273\n",
      "Epoch 25, Batch 700, Loss: 0.2722892932593822\n",
      "Epoch 25, Batch 800, Loss: 0.2629396056383848\n",
      "Epoch 25, Batch 900, Loss: 0.272427731603384\n",
      "Accuracy on test set: 0.9289%\n"
     ]
    }
   ],
   "source": [
    "# But now for the best model from the \"most successful\" test:\n",
    "\n",
    "model = MLP((28*28), [30, 10], False, ['tanh'])\n",
    "model.to(device)\n",
    "model.fit(train_loader, 'Adam', 'CrossEntropyLoss', 0.001, 0.01, 100, 25)\n",
    "score = model.score(test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 428
    },
    "id": "kNM8-ijzUE9w",
    "outputId": "a07af0ed-e731-461f-d8c1-35750b29394c"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "image_index = 27\n",
    "test_image, test_label = test_dataset[image_index]\n",
    "\n",
    "cpu_image = test_image\n",
    "test_image = torch.Tensor(test_image).to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    output = model(test_image.unsqueeze(0))\n",
    "    _, predicted_label = torch.max(output, 1)\n",
    "\n",
    "test_image_numpy = cpu_image.squeeze().numpy()\n",
    "\n",
    "plt.imshow(test_image_numpy, cmap='gray')\n",
    "plt.title(f'Predicted Label: {predicted_label.item()}, Actual Label: {test_label}')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L3IIK5kzHGH0"
   },
   "source": [
    "## Part 2\n",
    "\n",
    "### PyTorch FC ANN FMNIST Implementation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "jOCzfnBvB_bQ"
   },
   "outputs": [],
   "source": [
    "# Transformations --> this is a \"pre-processing step\" that's typical for image processing methods\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),  # Convert PIL image or numpy.ndarray to tensor\n",
    "    transforms.Normalize((0.5,), (0.5,))  # Normalize data to range [-1, 1]\n",
    "])\n",
    "# This dataset is already \"sorted\" as part of the import method, but no \"validation\" set has been selected in this case\n",
    "# Loading the FashionMNIST dataset\n",
    "train_dataset = torchvision.datasets.FashionMNIST(root='./data', train=True, download=True, transform=transform)\n",
    "test_dataset = torchvision.datasets.FashionMNIST(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "# Training and Testing loaders\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "jZk_FS9JCLOH"
   },
   "outputs": [],
   "source": [
    "# Mapping the labels for the MNIST dataset -- later we'll see that this using the \"keras to_categorical\" method as discussed in class\n",
    "labels_map = {\n",
    "    0: \"0\", 1: \"1\", 2: \"2\", 3: \"3\", 4: \"4\",\n",
    "    5: \"5\", 6: \"6\", 7: \"7\", 8: \"8\", 9: \"9\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 675
    },
    "id": "ykrRIGSdCMu5",
    "outputId": "a6737577-19d7-459e-aa83-ccebed56246c"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x800 with 9 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#This cell is designed to display a few images from the dataset\n",
    "#It isn't necessary to run this, but it can help give a better idea of the challanges your model will face\n",
    "figure = plt.figure(figsize=(8, 8))\n",
    "cols, rows = 3, 3\n",
    "\n",
    "# Displaying figures from the dataset randomly\n",
    "for i in range(1, cols * rows + 1):\n",
    "    sample_idx = torch.randint(len(train_dataset), size=(1,)).item()\n",
    "    img, label = train_dataset[sample_idx]\n",
    "    figure.add_subplot(rows, cols, i)\n",
    "    plt.title(labels_map[label])\n",
    "    plt.axis(\"off\")\n",
    "    plt.imshow(img.squeeze(), cmap=\"gray\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is identical to the Part 1 Model\n",
    "class MLP(nn.Module): #MLP stands for \"Multi-Layer Perceptron\"\n",
    "    def __init__(self, inputs, neurons, dropout, activation): #this initializes the structure of the network\n",
    "    #def __init__(self): #this initializes the structure of the network\n",
    "        super(MLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(inputs, neurons[0]) ## First fully connected linear layer, 28*28 input features and 32 outputs\n",
    "        for layer in range(2,len(neurons)): # Create the remaining fully connected layers, based on the given size of the outputs\n",
    "            setattr(self, f'fc{layer}', nn.Linear(neurons[layer-2], neurons[layer-1])) # Of course, the input is the size of the previous output\n",
    "            if (dropout == True) and (layer % 2 == 0): # If the dropout flag is set, add a dropout layer every 2 layers\n",
    "                self.dropout = torch.nn.Dropout(0.2)\n",
    "        setattr(self, f'fc{len(neurons)}', nn.Linear(neurons[len(neurons)-2], neurons[len(neurons)-1])) ## 10 output features because MNIST has 10 target classes\n",
    "        self.size = len(neurons)\n",
    "        self.activation = activation # Define the array of activation functions\n",
    "    \n",
    "    def forward(self, train_data): #this modifies the elements of the intial structure defined above\n",
    "        train_data = train_data.view(-1, 28 * 28) #the array is sent in as a vector\n",
    "        for layer in range(1,self.size):\n",
    "            train_data = getattr(torch, f'{self.activation[layer-1]}')(getattr(self, f'fc{layer}')(train_data)) # Set the activation functions for each layer, except the output layer\n",
    "        train_data = getattr(self, f'fc{self.size}')(train_data) # No modifications to the activation of the output layer, that's handled by the optimizer\n",
    "        return train_data\n",
    "\n",
    "    def fit(self, train_loader, optimizer, loss_function, learning_rate, weight_decay, batch_size, epochs): # Aside from the tensor changes, this was left alone\n",
    "        criterion = getattr(nn, f'{loss_function}')()\n",
    "        optimizer = getattr(optim, f'{optimizer}')(self.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "        for epoch in range(epochs):\n",
    "            self.train()\n",
    "            running_loss = 0.0\n",
    "            for i, data in enumerate(train_loader, 0):\n",
    "                inputs, labels = data\n",
    "                inputs = torch.Tensor(inputs).to(device) # These both have to be cast to GPU tensors or it implodes\n",
    "                labels = torch.Tensor(labels).to(device)\n",
    "                optimizer.zero_grad()\n",
    "                outputs = self(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                running_loss += loss.item()\n",
    "                if i % batch_size == (batch_size - 1):  # print every 100 mini-batches\n",
    "                    print(f'Epoch {epoch + 1}, Batch {i + 1}, Loss: {running_loss / 100}')\n",
    "                    running_loss = 0.0\n",
    "\n",
    "    def score(self, test_data): # Like the above method, no changes except to the GPU tensors\n",
    "        self.eval()\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        with torch.no_grad():\n",
    "            for data in test_data:\n",
    "                images, labels = data\n",
    "                images = torch.Tensor(images).to(device) # These.  Still have to be cast to GPU\n",
    "                labels = torch.Tensor(labels).to(device)\n",
    "                outputs = model(images)\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "        score = correct / total\n",
    "        print(f'Accuracy on test set: {score}%')\n",
    "        return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Batch 100, Loss: 2.307286038398743\n",
      "Epoch 1, Batch 200, Loss: 2.3088797521591187\n",
      "Epoch 1, Batch 300, Loss: 2.3039369440078734\n",
      "Epoch 1, Batch 400, Loss: 2.2976331067085267\n",
      "Epoch 1, Batch 500, Loss: 2.297868027687073\n",
      "Epoch 1, Batch 600, Loss: 2.2919821763038635\n",
      "Epoch 1, Batch 700, Loss: 2.289518640041351\n",
      "Epoch 1, Batch 800, Loss: 2.2844664359092715\n",
      "Epoch 1, Batch 900, Loss: 2.2793776321411134\n",
      "Epoch 2, Batch 100, Loss: 2.272441165447235\n",
      "Epoch 2, Batch 200, Loss: 2.268027946949005\n",
      "Epoch 2, Batch 300, Loss: 2.2616769790649416\n",
      "Epoch 2, Batch 400, Loss: 2.25360337972641\n",
      "Epoch 2, Batch 500, Loss: 2.247072057723999\n",
      "Epoch 2, Batch 600, Loss: 2.2446549129486084\n",
      "Epoch 2, Batch 700, Loss: 2.234690775871277\n",
      "Epoch 2, Batch 800, Loss: 2.2243023347854614\n",
      "Epoch 2, Batch 900, Loss: 2.2157741498947146\n",
      "Epoch 3, Batch 100, Loss: 2.202964475154877\n",
      "Epoch 3, Batch 200, Loss: 2.18995888710022\n",
      "Epoch 3, Batch 300, Loss: 2.183020496368408\n",
      "Epoch 3, Batch 400, Loss: 2.169453191757202\n",
      "Epoch 3, Batch 500, Loss: 2.1597390723228456\n",
      "Epoch 3, Batch 600, Loss: 2.144479887485504\n",
      "Epoch 3, Batch 700, Loss: 2.127907419204712\n",
      "Epoch 3, Batch 800, Loss: 2.116716692447662\n",
      "Epoch 3, Batch 900, Loss: 2.1032728147506714\n"
     ]
    }
   ],
   "source": [
    "model = MLP((28*28), [30, 10, 10], True, ['sigmoid', 'relu'])\n",
    "model.to(device)\n",
    "model.fit(train_loader, 'SGD', 'CrossEntropyLoss', 0.002, 0, 100, 3)\n",
    "#print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0.1858%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.1858"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.score(test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the range of parameters for the grid search\n",
    "parameters = {\n",
    "    'inputs': 28*28,\n",
    "    'number_of_layers': [1, 2, 3, 4],\n",
    "    'outputs': 10,\n",
    "    'neurons_per_layer': [10, 10, 20, 20, 30, 30, 40, 40, 50, 50],\n",
    "    'dropout_layers': [False, True],\n",
    "    'activation_functions': ['relu', 'tanh', 'sigmoid'],\n",
    "    'optimizers': ['Adam', 'SGD'],\n",
    "    'learning_rates': [0.01, 0.03, 0.1, 0.3],\n",
    "    'weight_decays': [0, 0.01, 0.03, 0.1, 0.3],\n",
    "    'loss_functions': ['CrossEntropyLoss'],\n",
    "    'batches': [50, 100, 200],\n",
    "    'epochs': [25] # This is going to take so long that I decided to keep it short\n",
    "}\n",
    "\n",
    "results = {}\n",
    "best_parameters = {}\n",
    "worst_parameters = {}\n",
    "for parameter in parameters:\n",
    "    results[parameter] = []\n",
    "    best_parameters[parameter] = None\n",
    "    worst_parameters[parameter] = None\n",
    "\n",
    "results['score'] = []\n",
    "best_parameters['score'] = -1\n",
    "worst_parameters['score'] = 1\n",
    "combination = 0\n",
    "local_layers = {'neurons':[],'activations':[]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting for combination 0\n",
      "784\n",
      "1\n",
      "10\n",
      "[10, 10]\n",
      "True\n",
      "['relu']\n",
      "Adam\n",
      "0.1\n",
      "0.1\n",
      "CrossEntropyLoss\n",
      "200\n",
      "25\n",
      "0\n",
      "Epoch 1, Batch 200, Loss: 4.147810602188111\n",
      "Epoch 1, Batch 400, Loss: 4.05073757648468\n",
      "Epoch 1, Batch 600, Loss: 3.742501095533371\n",
      "Epoch 1, Batch 800, Loss: 4.0115821838378904\n",
      "Epoch 2, Batch 200, Loss: 3.633005305528641\n",
      "Epoch 2, Batch 400, Loss: 3.831987998485565\n",
      "Epoch 2, Batch 600, Loss: 4.102848808765412\n",
      "Epoch 2, Batch 800, Loss: 3.938613874912262\n",
      "Epoch 3, Batch 200, Loss: 4.264614531993866\n",
      "Epoch 3, Batch 400, Loss: 3.814323523044586\n",
      "Epoch 3, Batch 600, Loss: 4.157729009389877\n",
      "Epoch 3, Batch 800, Loss: 4.0674411642551425\n",
      "Epoch 4, Batch 200, Loss: 3.6967059659957884\n",
      "Epoch 4, Batch 400, Loss: 4.317996252775192\n",
      "Epoch 4, Batch 600, Loss: 4.101454836130142\n",
      "Epoch 4, Batch 800, Loss: 4.687651757001877\n",
      "Epoch 5, Batch 200, Loss: 3.33524951338768\n",
      "Epoch 5, Batch 400, Loss: 3.5590685093402863\n",
      "Epoch 5, Batch 600, Loss: 3.6729763174057006\n",
      "Epoch 5, Batch 800, Loss: 3.4878467071056365\n",
      "Epoch 6, Batch 200, Loss: 3.8137072455883025\n",
      "Epoch 6, Batch 400, Loss: 3.536195216178894\n",
      "Epoch 6, Batch 600, Loss: 4.00516728758812\n",
      "Epoch 6, Batch 800, Loss: 3.9078872632980346\n",
      "Epoch 7, Batch 200, Loss: 3.713465210199356\n",
      "Epoch 7, Batch 400, Loss: 3.748645145893097\n",
      "Epoch 7, Batch 600, Loss: 3.928956662416458\n",
      "Epoch 7, Batch 800, Loss: 4.128662605285644\n",
      "Epoch 8, Batch 200, Loss: 4.066524380445481\n",
      "Epoch 8, Batch 400, Loss: 4.200993212461472\n",
      "Epoch 8, Batch 600, Loss: 3.571926016807556\n",
      "Epoch 8, Batch 800, Loss: 4.253860490322113\n",
      "Epoch 9, Batch 200, Loss: 4.320192573070526\n",
      "Epoch 9, Batch 400, Loss: 3.5165308856964113\n",
      "Epoch 9, Batch 600, Loss: 3.826609559059143\n",
      "Epoch 9, Batch 800, Loss: 3.5956756258010865\n",
      "Epoch 10, Batch 200, Loss: 4.060194146633148\n",
      "Epoch 10, Batch 400, Loss: 4.897694278955459\n",
      "Epoch 10, Batch 600, Loss: 3.567955288887024\n",
      "Epoch 10, Batch 800, Loss: 3.764783828854561\n",
      "Epoch 11, Batch 200, Loss: 3.648527591228485\n",
      "Epoch 11, Batch 400, Loss: 3.331111110448837\n",
      "Epoch 11, Batch 600, Loss: 4.196006407737732\n",
      "Epoch 11, Batch 800, Loss: 3.8443897104263307\n",
      "Epoch 12, Batch 200, Loss: 3.793433736562729\n",
      "Epoch 12, Batch 400, Loss: 4.241024030447006\n",
      "Epoch 12, Batch 600, Loss: 4.1189382302761075\n",
      "Epoch 12, Batch 800, Loss: 3.826011027097702\n",
      "Epoch 13, Batch 200, Loss: 3.596892017126083\n",
      "Epoch 13, Batch 400, Loss: 3.996995289325714\n",
      "Epoch 13, Batch 600, Loss: 3.8626214337348936\n",
      "Epoch 13, Batch 800, Loss: 3.944511773586273\n",
      "Epoch 14, Batch 200, Loss: 3.8285235965251925\n",
      "Epoch 14, Batch 400, Loss: 3.8589287877082823\n",
      "Epoch 14, Batch 600, Loss: 3.7287895560264586\n",
      "Epoch 14, Batch 800, Loss: 4.157381639480591\n",
      "Epoch 15, Batch 200, Loss: 3.754065717458725\n",
      "Epoch 15, Batch 400, Loss: 3.930566476583481\n",
      "Epoch 15, Batch 600, Loss: 4.3366044330596925\n",
      "Epoch 15, Batch 800, Loss: 3.811422884464264\n",
      "Epoch 16, Batch 200, Loss: 3.912297412157059\n",
      "Epoch 16, Batch 400, Loss: 3.8377994883060453\n",
      "Epoch 16, Batch 600, Loss: 4.673823980093002\n",
      "Epoch 16, Batch 800, Loss: 3.488224592208862\n",
      "Epoch 17, Batch 200, Loss: 3.8759735453128816\n",
      "Epoch 17, Batch 400, Loss: 4.256413502693176\n",
      "Epoch 17, Batch 600, Loss: 3.6162778508663176\n",
      "Epoch 17, Batch 800, Loss: 3.333392699956894\n",
      "Epoch 18, Batch 200, Loss: 4.257337825298309\n",
      "Epoch 18, Batch 400, Loss: 3.7831359845399857\n",
      "Epoch 18, Batch 600, Loss: 4.463421187400818\n",
      "Epoch 18, Batch 800, Loss: 3.537066125869751\n",
      "Epoch 19, Batch 200, Loss: 3.9436569011211393\n",
      "Epoch 19, Batch 400, Loss: 3.5802035439014435\n",
      "Epoch 19, Batch 600, Loss: 3.968055534362793\n",
      "Epoch 19, Batch 800, Loss: 3.9626543152332308\n",
      "Epoch 20, Batch 200, Loss: 4.019282573461533\n",
      "Epoch 20, Batch 400, Loss: 4.017972379922867\n",
      "Epoch 20, Batch 600, Loss: 3.971189149618149\n",
      "Epoch 20, Batch 800, Loss: 3.8782535779476164\n",
      "Epoch 21, Batch 200, Loss: 3.8429930567741395\n",
      "Epoch 21, Batch 400, Loss: 3.70173043012619\n",
      "Epoch 21, Batch 600, Loss: 3.9045932841300965\n",
      "Epoch 21, Batch 800, Loss: 4.027274154424667\n",
      "Epoch 22, Batch 200, Loss: 3.840508587360382\n",
      "Epoch 22, Batch 400, Loss: 4.093271629810333\n",
      "Epoch 22, Batch 600, Loss: 3.8792409539222716\n",
      "Epoch 22, Batch 800, Loss: 3.9147266221046446\n",
      "Epoch 23, Batch 200, Loss: 4.3670257484912876\n",
      "Epoch 23, Batch 400, Loss: 3.776997995376587\n",
      "Epoch 23, Batch 600, Loss: 3.7324445724487303\n",
      "Epoch 23, Batch 800, Loss: 3.9758958542346954\n",
      "Epoch 24, Batch 200, Loss: 3.3964046692848204\n",
      "Epoch 24, Batch 400, Loss: 4.0299700272083285\n",
      "Epoch 24, Batch 600, Loss: 4.187110117673874\n",
      "Epoch 24, Batch 800, Loss: 3.5218076395988462\n",
      "Epoch 25, Batch 200, Loss: 4.116812369823456\n",
      "Epoch 25, Batch 400, Loss: 4.1873580420017245\n",
      "Epoch 25, Batch 600, Loss: 3.642970757484436\n",
      "Epoch 25, Batch 800, Loss: 3.604679650068283\n",
      "Accuracy on test set: 0.1502%\n",
      "Fitting for combination 1\n",
      "784\n",
      "1\n",
      "10\n",
      "[10, 10]\n",
      "True\n",
      "['tanh']\n",
      "SGD\n",
      "0.01\n",
      "0\n",
      "CrossEntropyLoss\n",
      "200\n",
      "25\n",
      "0\n",
      "Epoch 1, Batch 200, Loss: 3.5568535995483397\n",
      "Epoch 1, Batch 400, Loss: 2.659884892702103\n",
      "Epoch 1, Batch 600, Loss: 2.2035903257131575\n",
      "Epoch 1, Batch 800, Loss: 1.9106485950946808\n",
      "Epoch 2, Batch 200, Loss: 1.6136109912395478\n",
      "Epoch 2, Batch 400, Loss: 1.5097976905107497\n",
      "Epoch 2, Batch 600, Loss: 1.4702420237660407\n",
      "Epoch 2, Batch 800, Loss: 1.3762679997086524\n",
      "Epoch 3, Batch 200, Loss: 1.2743526256084443\n",
      "Epoch 3, Batch 400, Loss: 1.2625274658203125\n",
      "Epoch 3, Batch 600, Loss: 1.20377904266119\n",
      "Epoch 3, Batch 800, Loss: 1.1864854553341866\n",
      "Epoch 4, Batch 200, Loss: 1.1302381366491319\n",
      "Epoch 4, Batch 400, Loss: 1.1167902725934982\n",
      "Epoch 4, Batch 600, Loss: 1.0918701076507569\n",
      "Epoch 4, Batch 800, Loss: 1.0953851440548896\n",
      "Epoch 5, Batch 200, Loss: 1.0420343080163001\n",
      "Epoch 5, Batch 400, Loss: 1.06118459969759\n",
      "Epoch 5, Batch 600, Loss: 1.0191468214988708\n",
      "Epoch 5, Batch 800, Loss: 1.0378675523400307\n",
      "Epoch 6, Batch 200, Loss: 0.994766679406166\n",
      "Epoch 6, Batch 400, Loss: 1.0055096092820168\n",
      "Epoch 6, Batch 600, Loss: 0.9674361589550972\n",
      "Epoch 6, Batch 800, Loss: 0.9959297129511833\n",
      "Epoch 7, Batch 200, Loss: 0.9711378848552704\n",
      "Epoch 7, Batch 400, Loss: 0.9538154011964798\n",
      "Epoch 7, Batch 600, Loss: 0.9496377366781235\n",
      "Epoch 7, Batch 800, Loss: 0.9487523135542869\n",
      "Epoch 8, Batch 200, Loss: 0.9259926569461823\n",
      "Epoch 8, Batch 400, Loss: 0.926910188794136\n",
      "Epoch 8, Batch 600, Loss: 0.921029821485281\n",
      "Epoch 8, Batch 800, Loss: 0.911396059691906\n",
      "Epoch 9, Batch 200, Loss: 0.9068626283109188\n",
      "Epoch 9, Batch 400, Loss: 0.9076881378889083\n",
      "Epoch 9, Batch 600, Loss: 0.8938231383264065\n",
      "Epoch 9, Batch 800, Loss: 0.9017396168410778\n",
      "Epoch 10, Batch 200, Loss: 0.8983444948494435\n",
      "Epoch 10, Batch 400, Loss: 0.883829184025526\n",
      "Epoch 10, Batch 600, Loss: 0.8836607179045677\n",
      "Epoch 10, Batch 800, Loss: 0.8641343189775944\n",
      "Epoch 11, Batch 200, Loss: 0.8561112532019615\n",
      "Epoch 11, Batch 400, Loss: 0.868613313883543\n",
      "Epoch 11, Batch 600, Loss: 0.8828962847590447\n",
      "Epoch 11, Batch 800, Loss: 0.8623040789365768\n",
      "Epoch 12, Batch 200, Loss: 0.8479848895967007\n",
      "Epoch 12, Batch 400, Loss: 0.8776108902692795\n",
      "Epoch 12, Batch 600, Loss: 0.8403209801018238\n",
      "Epoch 12, Batch 800, Loss: 0.8620131596922874\n",
      "Epoch 13, Batch 200, Loss: 0.8181994053721428\n",
      "Epoch 13, Batch 400, Loss: 0.8410993303358555\n",
      "Epoch 13, Batch 600, Loss: 0.8456741398572922\n",
      "Epoch 13, Batch 800, Loss: 0.8602186663448811\n",
      "Epoch 14, Batch 200, Loss: 0.8071226830780506\n",
      "Epoch 14, Batch 400, Loss: 0.8422043478488922\n",
      "Epoch 14, Batch 600, Loss: 0.8440334960818291\n",
      "Epoch 14, Batch 800, Loss: 0.8288642799854279\n",
      "Epoch 15, Batch 200, Loss: 0.8312914867699146\n",
      "Epoch 15, Batch 400, Loss: 0.811168961673975\n",
      "Epoch 15, Batch 600, Loss: 0.8379491500556469\n",
      "Epoch 15, Batch 800, Loss: 0.8090033605694771\n",
      "Epoch 16, Batch 200, Loss: 0.8043527522683144\n",
      "Epoch 16, Batch 400, Loss: 0.8089667271077633\n",
      "Epoch 16, Batch 600, Loss: 0.8201254610717297\n",
      "Epoch 16, Batch 800, Loss: 0.8088787208497524\n",
      "Epoch 17, Batch 200, Loss: 0.8043727420270443\n",
      "Epoch 17, Batch 400, Loss: 0.7791348400712014\n",
      "Epoch 17, Batch 600, Loss: 0.8469558568298816\n",
      "Epoch 17, Batch 800, Loss: 0.7876060742139817\n",
      "Epoch 18, Batch 200, Loss: 0.7833582484722137\n",
      "Epoch 18, Batch 400, Loss: 0.8123790946602821\n",
      "Epoch 18, Batch 600, Loss: 0.7905453349649906\n",
      "Epoch 18, Batch 800, Loss: 0.8160845869779587\n",
      "Epoch 19, Batch 200, Loss: 0.795977950245142\n",
      "Epoch 19, Batch 400, Loss: 0.7967114965617657\n",
      "Epoch 19, Batch 600, Loss: 0.7869202736020088\n",
      "Epoch 19, Batch 800, Loss: 0.8207945874333382\n",
      "Epoch 20, Batch 200, Loss: 0.7934806206822396\n",
      "Epoch 20, Batch 400, Loss: 0.8072988718748093\n",
      "Epoch 20, Batch 600, Loss: 0.7863858330249787\n",
      "Epoch 20, Batch 800, Loss: 0.7898650355637074\n",
      "Epoch 21, Batch 200, Loss: 0.7861982709169388\n",
      "Epoch 21, Batch 400, Loss: 0.775459750443697\n",
      "Epoch 21, Batch 600, Loss: 0.7662235035002232\n",
      "Epoch 21, Batch 800, Loss: 0.7947383916378021\n",
      "Epoch 22, Batch 200, Loss: 0.7818587970733643\n",
      "Epoch 22, Batch 400, Loss: 0.8065070208907127\n",
      "Epoch 22, Batch 600, Loss: 0.7586887711286545\n",
      "Epoch 22, Batch 800, Loss: 0.77493528008461\n",
      "Epoch 23, Batch 200, Loss: 0.786695057451725\n",
      "Epoch 23, Batch 400, Loss: 0.791090382039547\n",
      "Epoch 23, Batch 600, Loss: 0.7764835187792778\n",
      "Epoch 23, Batch 800, Loss: 0.7508487100899219\n",
      "Epoch 24, Batch 200, Loss: 0.7659744533896446\n",
      "Epoch 24, Batch 400, Loss: 0.7586573812365532\n",
      "Epoch 24, Batch 600, Loss: 0.7647360995411873\n",
      "Epoch 24, Batch 800, Loss: 0.7765517313778401\n",
      "Epoch 25, Batch 200, Loss: 0.746060781031847\n",
      "Epoch 25, Batch 400, Loss: 0.7528118471801281\n",
      "Epoch 25, Batch 600, Loss: 0.7836431710422039\n",
      "Epoch 25, Batch 800, Loss: 0.7659607066214085\n",
      "Accuracy on test set: 0.8457%\n",
      "Fitting for combination 2\n",
      "784\n",
      "1\n",
      "10\n",
      "[10, 10]\n",
      "False\n",
      "['sigmoid']\n",
      "Adam\n",
      "0.03\n",
      "0.3\n",
      "CrossEntropyLoss\n",
      "50\n",
      "25\n",
      "0\n",
      "Epoch 1, Batch 50, Loss: 1.1107260632514953\n",
      "Epoch 1, Batch 100, Loss: 1.0871582865715026\n",
      "Epoch 1, Batch 150, Loss: 1.0832746362686156\n",
      "Epoch 1, Batch 200, Loss: 1.0957622122764588\n",
      "Epoch 1, Batch 250, Loss: 1.1097194719314576\n",
      "Epoch 1, Batch 300, Loss: 1.0950857377052308\n",
      "Epoch 1, Batch 350, Loss: 1.0888292455673219\n",
      "Epoch 1, Batch 400, Loss: 1.0859638333320618\n",
      "Epoch 1, Batch 450, Loss: 1.0872472620010376\n",
      "Epoch 1, Batch 500, Loss: 1.0935704946517943\n",
      "Epoch 1, Batch 550, Loss: 1.1076930379867553\n",
      "Epoch 1, Batch 600, Loss: 1.0994302797317506\n",
      "Epoch 1, Batch 650, Loss: 1.0833316135406494\n",
      "Epoch 1, Batch 700, Loss: 1.092181179523468\n",
      "Epoch 1, Batch 750, Loss: 1.1010398435592652\n",
      "Epoch 1, Batch 800, Loss: 1.0871663498878479\n",
      "Epoch 1, Batch 850, Loss: 1.0936974596977234\n",
      "Epoch 1, Batch 900, Loss: 1.084694936275482\n",
      "Epoch 2, Batch 50, Loss: 1.0922088813781738\n",
      "Epoch 2, Batch 100, Loss: 1.0958598804473878\n",
      "Epoch 2, Batch 150, Loss: 1.0974014353752137\n",
      "Epoch 2, Batch 200, Loss: 1.0932412266731262\n",
      "Epoch 2, Batch 250, Loss: 1.088213074207306\n",
      "Epoch 2, Batch 300, Loss: 1.0913839101791383\n",
      "Epoch 2, Batch 350, Loss: 1.0991272377967833\n",
      "Epoch 2, Batch 400, Loss: 1.095794062614441\n",
      "Epoch 2, Batch 450, Loss: 1.0860576963424682\n",
      "Epoch 2, Batch 500, Loss: 1.1190079879760741\n",
      "Epoch 2, Batch 550, Loss: 1.090023217201233\n",
      "Epoch 2, Batch 600, Loss: 1.095801022052765\n",
      "Epoch 2, Batch 650, Loss: 1.1112367820739746\n",
      "Epoch 2, Batch 700, Loss: 1.0891548299789429\n",
      "Epoch 2, Batch 750, Loss: 1.09885324716568\n",
      "Epoch 2, Batch 800, Loss: 1.1078627800941467\n",
      "Epoch 2, Batch 850, Loss: 1.1016502070426941\n",
      "Epoch 2, Batch 900, Loss: 1.0989644050598144\n",
      "Epoch 3, Batch 50, Loss: 1.0966394019126893\n",
      "Epoch 3, Batch 100, Loss: 1.0973528695106507\n",
      "Epoch 3, Batch 150, Loss: 1.095764274597168\n",
      "Epoch 3, Batch 200, Loss: 1.0933470249176025\n",
      "Epoch 3, Batch 250, Loss: 1.09840083360672\n",
      "Epoch 3, Batch 300, Loss: 1.0861188077926636\n",
      "Epoch 3, Batch 350, Loss: 1.0898871612548828\n",
      "Epoch 3, Batch 400, Loss: 1.0933391761779785\n",
      "Epoch 3, Batch 450, Loss: 1.1020488333702088\n",
      "Epoch 3, Batch 500, Loss: 1.0873276853561402\n",
      "Epoch 3, Batch 550, Loss: 1.1067727708816528\n",
      "Epoch 3, Batch 600, Loss: 1.0999908685684203\n",
      "Epoch 3, Batch 650, Loss: 1.099468080997467\n",
      "Epoch 3, Batch 700, Loss: 1.0955385231971742\n",
      "Epoch 3, Batch 750, Loss: 1.097029802799225\n",
      "Epoch 3, Batch 800, Loss: 1.084001796245575\n",
      "Epoch 3, Batch 850, Loss: 1.1025060772895814\n",
      "Epoch 3, Batch 900, Loss: 1.0917340159416198\n",
      "Epoch 4, Batch 50, Loss: 1.0895280742645264\n",
      "Epoch 4, Batch 100, Loss: 1.097174801826477\n",
      "Epoch 4, Batch 150, Loss: 1.095410747528076\n",
      "Epoch 4, Batch 200, Loss: 1.0873061776161195\n",
      "Epoch 4, Batch 250, Loss: 1.0954887223243714\n",
      "Epoch 4, Batch 300, Loss: 1.0900661420822144\n",
      "Epoch 4, Batch 350, Loss: 1.0893812489509582\n",
      "Epoch 4, Batch 400, Loss: 1.101449909210205\n",
      "Epoch 4, Batch 450, Loss: 1.091014597415924\n",
      "Epoch 4, Batch 500, Loss: 1.09422331571579\n",
      "Epoch 4, Batch 550, Loss: 1.0924218368530274\n",
      "Epoch 4, Batch 600, Loss: 1.0900858736038208\n",
      "Epoch 4, Batch 650, Loss: 1.090269877910614\n",
      "Epoch 4, Batch 700, Loss: 1.090283703804016\n",
      "Epoch 4, Batch 750, Loss: 1.102944371700287\n",
      "Epoch 4, Batch 800, Loss: 1.103466453552246\n",
      "Epoch 4, Batch 850, Loss: 1.084874415397644\n",
      "Epoch 4, Batch 900, Loss: 1.0970080995559692\n",
      "Epoch 5, Batch 50, Loss: 1.0992165756225587\n",
      "Epoch 5, Batch 100, Loss: 1.091593897342682\n",
      "Epoch 5, Batch 150, Loss: 1.087326579093933\n",
      "Epoch 5, Batch 200, Loss: 1.1056507873535155\n",
      "Epoch 5, Batch 250, Loss: 1.091450753211975\n",
      "Epoch 5, Batch 300, Loss: 1.0859838771820067\n",
      "Epoch 5, Batch 350, Loss: 1.1026947808265686\n",
      "Epoch 5, Batch 400, Loss: 1.0847258067131043\n",
      "Epoch 5, Batch 450, Loss: 1.0900819087028504\n",
      "Epoch 5, Batch 500, Loss: 1.1176201462745667\n",
      "Epoch 5, Batch 550, Loss: 1.0871949768066407\n",
      "Epoch 5, Batch 600, Loss: 1.1152758622169494\n",
      "Epoch 5, Batch 650, Loss: 1.1120511937141417\n",
      "Epoch 5, Batch 700, Loss: 1.097170352935791\n",
      "Epoch 5, Batch 750, Loss: 1.0940786957740785\n",
      "Epoch 5, Batch 800, Loss: 1.099279932975769\n",
      "Epoch 5, Batch 850, Loss: 1.1005954051017761\n",
      "Epoch 5, Batch 900, Loss: 1.0941198301315307\n",
      "Epoch 6, Batch 50, Loss: 1.110631115436554\n",
      "Epoch 6, Batch 100, Loss: 1.0910285878181458\n",
      "Epoch 6, Batch 150, Loss: 1.09153981924057\n",
      "Epoch 6, Batch 200, Loss: 1.090842409133911\n",
      "Epoch 6, Batch 250, Loss: 1.092864751815796\n",
      "Epoch 6, Batch 300, Loss: 1.0924078369140624\n",
      "Epoch 6, Batch 350, Loss: 1.0930543279647826\n",
      "Epoch 6, Batch 400, Loss: 1.094367628097534\n",
      "Epoch 6, Batch 450, Loss: 1.0959720587730408\n",
      "Epoch 6, Batch 500, Loss: 1.0947141313552857\n",
      "Epoch 6, Batch 550, Loss: 1.1000368618965148\n",
      "Epoch 6, Batch 600, Loss: 1.0886690735816955\n",
      "Epoch 6, Batch 650, Loss: 1.0945924639701843\n",
      "Epoch 6, Batch 700, Loss: 1.0997208499908446\n",
      "Epoch 6, Batch 750, Loss: 1.0982741594314576\n",
      "Epoch 6, Batch 800, Loss: 1.092782232761383\n",
      "Epoch 6, Batch 850, Loss: 1.087477834224701\n",
      "Epoch 6, Batch 900, Loss: 1.0994100499153137\n",
      "Epoch 7, Batch 50, Loss: 1.1133097553253173\n",
      "Epoch 7, Batch 100, Loss: 1.0983527040481567\n",
      "Epoch 7, Batch 150, Loss: 1.1021116614341735\n",
      "Epoch 7, Batch 200, Loss: 1.1034109044075011\n",
      "Epoch 7, Batch 250, Loss: 1.100631022453308\n",
      "Epoch 7, Batch 300, Loss: 1.096497368812561\n",
      "Epoch 7, Batch 350, Loss: 1.1062578082084655\n",
      "Epoch 7, Batch 400, Loss: 1.1127456402778626\n",
      "Epoch 7, Batch 450, Loss: 1.0941154670715332\n",
      "Epoch 7, Batch 500, Loss: 1.1035472440719605\n",
      "Epoch 7, Batch 550, Loss: 1.0869302153587341\n",
      "Epoch 7, Batch 600, Loss: 1.1107167410850525\n",
      "Epoch 7, Batch 650, Loss: 1.0920622420310975\n",
      "Epoch 7, Batch 700, Loss: 1.0962077116966247\n",
      "Epoch 7, Batch 750, Loss: 1.1121259951591491\n",
      "Epoch 7, Batch 800, Loss: 1.1002775192260743\n",
      "Epoch 7, Batch 850, Loss: 1.0916205477714538\n",
      "Epoch 7, Batch 900, Loss: 1.0876031064987182\n",
      "Epoch 8, Batch 50, Loss: 1.08625572681427\n",
      "Epoch 8, Batch 100, Loss: 1.087622308731079\n",
      "Epoch 8, Batch 150, Loss: 1.0942743611335755\n",
      "Epoch 8, Batch 200, Loss: 1.0979502606391907\n",
      "Epoch 8, Batch 250, Loss: 1.089858317375183\n",
      "Epoch 8, Batch 300, Loss: 1.114421741962433\n",
      "Epoch 8, Batch 350, Loss: 1.09283429145813\n",
      "Epoch 8, Batch 400, Loss: 1.1082253193855285\n",
      "Epoch 8, Batch 450, Loss: 1.099731550216675\n",
      "Epoch 8, Batch 500, Loss: 1.0946758031845092\n",
      "Epoch 8, Batch 550, Loss: 1.097887291908264\n",
      "Epoch 8, Batch 600, Loss: 1.1011478638648986\n",
      "Epoch 8, Batch 650, Loss: 1.1025384974479675\n",
      "Epoch 8, Batch 700, Loss: 1.0795443892478942\n",
      "Epoch 8, Batch 750, Loss: 1.0947535252571106\n",
      "Epoch 8, Batch 800, Loss: 1.092362711429596\n",
      "Epoch 8, Batch 850, Loss: 1.1007258653640748\n",
      "Epoch 8, Batch 900, Loss: 1.0972443199157715\n",
      "Epoch 9, Batch 50, Loss: 1.0955378103256226\n",
      "Epoch 9, Batch 100, Loss: 1.0949151420593262\n",
      "Epoch 9, Batch 150, Loss: 1.0978225684165954\n",
      "Epoch 9, Batch 200, Loss: 1.098277997970581\n",
      "Epoch 9, Batch 250, Loss: 1.1064798903465272\n",
      "Epoch 9, Batch 300, Loss: 1.0980136466026307\n",
      "Epoch 9, Batch 350, Loss: 1.0881397438049316\n",
      "Epoch 9, Batch 400, Loss: 1.104173913002014\n",
      "Epoch 9, Batch 450, Loss: 1.0923157906532288\n",
      "Epoch 9, Batch 500, Loss: 1.1205611038208008\n",
      "Epoch 9, Batch 550, Loss: 1.1047587275505066\n",
      "Epoch 9, Batch 600, Loss: 1.0928237342834473\n",
      "Epoch 9, Batch 650, Loss: 1.0894971084594727\n",
      "Epoch 9, Batch 700, Loss: 1.1033084869384766\n",
      "Epoch 9, Batch 750, Loss: 1.087255175113678\n",
      "Epoch 9, Batch 800, Loss: 1.089941861629486\n",
      "Epoch 9, Batch 850, Loss: 1.1078325939178466\n",
      "Epoch 9, Batch 900, Loss: 1.1170406413078309\n",
      "Epoch 10, Batch 50, Loss: 1.0910202407836913\n",
      "Epoch 10, Batch 100, Loss: 1.0983731317520142\n",
      "Epoch 10, Batch 150, Loss: 1.0917366814613343\n",
      "Epoch 10, Batch 200, Loss: 1.094469883441925\n",
      "Epoch 10, Batch 250, Loss: 1.1032194137573241\n",
      "Epoch 10, Batch 300, Loss: 1.0884036111831665\n",
      "Epoch 10, Batch 350, Loss: 1.0894994521141053\n",
      "Epoch 10, Batch 400, Loss: 1.1065198826789855\n",
      "Epoch 10, Batch 450, Loss: 1.0823723149299622\n",
      "Epoch 10, Batch 500, Loss: 1.1023954081535339\n",
      "Epoch 10, Batch 550, Loss: 1.0894605565071105\n",
      "Epoch 10, Batch 600, Loss: 1.0901440072059632\n",
      "Epoch 10, Batch 650, Loss: 1.090804078578949\n",
      "Epoch 10, Batch 700, Loss: 1.0843904900550843\n",
      "Epoch 10, Batch 750, Loss: 1.0857873749732971\n",
      "Epoch 10, Batch 800, Loss: 1.0869897723197937\n",
      "Epoch 10, Batch 850, Loss: 1.0933537364006043\n",
      "Epoch 10, Batch 900, Loss: 1.084427011013031\n",
      "Epoch 11, Batch 50, Loss: 1.0971007943153381\n",
      "Epoch 11, Batch 100, Loss: 1.1106032752990722\n",
      "Epoch 11, Batch 150, Loss: 1.0966294169425965\n",
      "Epoch 11, Batch 200, Loss: 1.088537974357605\n",
      "Epoch 11, Batch 250, Loss: 1.102436420917511\n",
      "Epoch 11, Batch 300, Loss: 1.0944502997398375\n",
      "Epoch 11, Batch 350, Loss: 1.095184757709503\n",
      "Epoch 11, Batch 400, Loss: 1.0985434675216674\n",
      "Epoch 11, Batch 450, Loss: 1.0966836166381837\n",
      "Epoch 11, Batch 500, Loss: 1.0941486930847169\n",
      "Epoch 11, Batch 550, Loss: 1.0831151413917541\n",
      "Epoch 11, Batch 600, Loss: 1.0928405737876892\n",
      "Epoch 11, Batch 650, Loss: 1.0950133514404297\n",
      "Epoch 11, Batch 700, Loss: 1.0883861422538756\n",
      "Epoch 11, Batch 750, Loss: 1.1006566381454468\n",
      "Epoch 11, Batch 800, Loss: 1.0945381474494935\n",
      "Epoch 11, Batch 850, Loss: 1.1013087916374207\n",
      "Epoch 11, Batch 900, Loss: 1.0884758639335632\n",
      "Epoch 12, Batch 50, Loss: 1.0916953372955323\n",
      "Epoch 12, Batch 100, Loss: 1.1045556545257569\n",
      "Epoch 12, Batch 150, Loss: 1.0913145971298217\n",
      "Epoch 12, Batch 200, Loss: 1.0929215788841247\n",
      "Epoch 12, Batch 250, Loss: 1.083871877193451\n",
      "Epoch 12, Batch 300, Loss: 1.0907494473457335\n",
      "Epoch 12, Batch 350, Loss: 1.080625524520874\n",
      "Epoch 12, Batch 400, Loss: 1.1107897877693176\n",
      "Epoch 12, Batch 450, Loss: 1.1023772835731507\n",
      "Epoch 12, Batch 500, Loss: 1.1063682508468629\n",
      "Epoch 12, Batch 550, Loss: 1.0904220604896546\n",
      "Epoch 12, Batch 600, Loss: 1.0976257801055909\n",
      "Epoch 12, Batch 650, Loss: 1.1029489016532898\n",
      "Epoch 12, Batch 700, Loss: 1.0903291368484498\n",
      "Epoch 12, Batch 750, Loss: 1.0996755814552308\n",
      "Epoch 12, Batch 800, Loss: 1.106113188266754\n",
      "Epoch 12, Batch 850, Loss: 1.0986797738075256\n",
      "Epoch 12, Batch 900, Loss: 1.088688108921051\n",
      "Epoch 13, Batch 50, Loss: 1.098254542350769\n",
      "Epoch 13, Batch 100, Loss: 1.0912967467308043\n",
      "Epoch 13, Batch 150, Loss: 1.0939635944366455\n",
      "Epoch 13, Batch 200, Loss: 1.10886607170105\n",
      "Epoch 13, Batch 250, Loss: 1.1093494534492492\n",
      "Epoch 13, Batch 300, Loss: 1.1130256962776184\n",
      "Epoch 13, Batch 350, Loss: 1.0918275618553162\n",
      "Epoch 13, Batch 400, Loss: 1.1008332085609436\n",
      "Epoch 13, Batch 450, Loss: 1.0913268089294434\n",
      "Epoch 13, Batch 500, Loss: 1.1051484274864196\n",
      "Epoch 13, Batch 550, Loss: 1.094655656814575\n",
      "Epoch 13, Batch 600, Loss: 1.102600905895233\n",
      "Epoch 13, Batch 650, Loss: 1.0934259366989136\n",
      "Epoch 13, Batch 700, Loss: 1.0952881836891175\n",
      "Epoch 13, Batch 750, Loss: 1.095970206260681\n",
      "Epoch 13, Batch 800, Loss: 1.0877016043663026\n",
      "Epoch 13, Batch 850, Loss: 1.102196125984192\n",
      "Epoch 13, Batch 900, Loss: 1.0999367117881775\n",
      "Epoch 14, Batch 50, Loss: 1.0992720031738281\n",
      "Epoch 14, Batch 100, Loss: 1.1053311419487\n",
      "Epoch 14, Batch 150, Loss: 1.0954958534240722\n",
      "Epoch 14, Batch 200, Loss: 1.103918299674988\n",
      "Epoch 14, Batch 250, Loss: 1.0876785802841187\n",
      "Epoch 14, Batch 300, Loss: 1.089153220653534\n",
      "Epoch 14, Batch 350, Loss: 1.0977157711982728\n",
      "Epoch 14, Batch 400, Loss: 1.0961088943481445\n",
      "Epoch 14, Batch 450, Loss: 1.1021358394622802\n",
      "Epoch 14, Batch 500, Loss: 1.0899956369400023\n",
      "Epoch 14, Batch 550, Loss: 1.0986960339546203\n",
      "Epoch 14, Batch 600, Loss: 1.1010208177566527\n",
      "Epoch 14, Batch 650, Loss: 1.1046067070960999\n",
      "Epoch 14, Batch 700, Loss: 1.0948473477363587\n",
      "Epoch 14, Batch 750, Loss: 1.1075292587280274\n",
      "Epoch 14, Batch 800, Loss: 1.0926371669769288\n",
      "Epoch 14, Batch 850, Loss: 1.0907629370689391\n",
      "Epoch 14, Batch 900, Loss: 1.0969855046272279\n",
      "Epoch 15, Batch 50, Loss: 1.1050341963768004\n",
      "Epoch 15, Batch 100, Loss: 1.0972623801231385\n",
      "Epoch 15, Batch 150, Loss: 1.1018176412582397\n",
      "Epoch 15, Batch 200, Loss: 1.098600435256958\n",
      "Epoch 15, Batch 250, Loss: 1.0997092366218566\n",
      "Epoch 15, Batch 300, Loss: 1.0929493117332458\n",
      "Epoch 15, Batch 350, Loss: 1.0996745872497558\n",
      "Epoch 15, Batch 400, Loss: 1.0967269706726075\n",
      "Epoch 15, Batch 450, Loss: 1.085462052822113\n",
      "Epoch 15, Batch 500, Loss: 1.0956389570236207\n",
      "Epoch 15, Batch 550, Loss: 1.1003158164024354\n",
      "Epoch 15, Batch 600, Loss: 1.0861601495742799\n",
      "Epoch 15, Batch 650, Loss: 1.0885615253448486\n",
      "Epoch 15, Batch 700, Loss: 1.0932868695259095\n",
      "Epoch 15, Batch 750, Loss: 1.1008493661880494\n",
      "Epoch 15, Batch 800, Loss: 1.085514497756958\n",
      "Epoch 15, Batch 850, Loss: 1.1246808242797852\n",
      "Epoch 15, Batch 900, Loss: 1.0948325157165528\n",
      "Epoch 16, Batch 50, Loss: 1.098745620250702\n",
      "Epoch 16, Batch 100, Loss: 1.0997193479537963\n",
      "Epoch 16, Batch 150, Loss: 1.0920859289169311\n",
      "Epoch 16, Batch 200, Loss: 1.0966685438156127\n",
      "Epoch 16, Batch 250, Loss: 1.0847353339195251\n",
      "Epoch 16, Batch 300, Loss: 1.0894407844543457\n",
      "Epoch 16, Batch 350, Loss: 1.0901473259925842\n",
      "Epoch 16, Batch 400, Loss: 1.092001757621765\n",
      "Epoch 16, Batch 450, Loss: 1.094706165790558\n",
      "Epoch 16, Batch 500, Loss: 1.0930955243110656\n",
      "Epoch 16, Batch 550, Loss: 1.0814142727851868\n",
      "Epoch 16, Batch 600, Loss: 1.0891569542884827\n",
      "Epoch 16, Batch 650, Loss: 1.0921925830841064\n",
      "Epoch 16, Batch 700, Loss: 1.0986828708648682\n",
      "Epoch 16, Batch 750, Loss: 1.0892766070365907\n",
      "Epoch 16, Batch 800, Loss: 1.1054847431182862\n",
      "Epoch 16, Batch 850, Loss: 1.09609028339386\n",
      "Epoch 16, Batch 900, Loss: 1.0945224928855897\n",
      "Epoch 17, Batch 50, Loss: 1.1010988903045655\n",
      "Epoch 17, Batch 100, Loss: 1.0951400542259215\n",
      "Epoch 17, Batch 150, Loss: 1.099607729911804\n",
      "Epoch 17, Batch 200, Loss: 1.0902161359786988\n",
      "Epoch 17, Batch 250, Loss: 1.0823691368103028\n",
      "Epoch 17, Batch 300, Loss: 1.1006360363960266\n",
      "Epoch 17, Batch 350, Loss: 1.0991068410873412\n",
      "Epoch 17, Batch 400, Loss: 1.0966924405097962\n",
      "Epoch 17, Batch 450, Loss: 1.0839691305160521\n",
      "Epoch 17, Batch 500, Loss: 1.0825804781913757\n",
      "Epoch 17, Batch 550, Loss: 1.088525846004486\n",
      "Epoch 17, Batch 600, Loss: 1.1023412775993346\n",
      "Epoch 17, Batch 650, Loss: 1.0936456775665284\n",
      "Epoch 17, Batch 700, Loss: 1.090908043384552\n",
      "Epoch 17, Batch 750, Loss: 1.090717396736145\n",
      "Epoch 17, Batch 800, Loss: 1.0912781429290772\n",
      "Epoch 17, Batch 850, Loss: 1.0956922459602356\n",
      "Epoch 17, Batch 900, Loss: 1.086716206073761\n",
      "Epoch 18, Batch 50, Loss: 1.1085584783554077\n",
      "Epoch 18, Batch 100, Loss: 1.0900375962257385\n",
      "Epoch 18, Batch 150, Loss: 1.0994283866882324\n",
      "Epoch 18, Batch 200, Loss: 1.0887285828590394\n",
      "Epoch 18, Batch 250, Loss: 1.0963182663917541\n",
      "Epoch 18, Batch 300, Loss: 1.093518660068512\n",
      "Epoch 18, Batch 350, Loss: 1.085068302154541\n",
      "Epoch 18, Batch 400, Loss: 1.0990010690689087\n",
      "Epoch 18, Batch 450, Loss: 1.1096735620498657\n",
      "Epoch 18, Batch 500, Loss: 1.0957862997055055\n",
      "Epoch 18, Batch 550, Loss: 1.0904904794692993\n",
      "Epoch 18, Batch 600, Loss: 1.0892873859405519\n",
      "Epoch 18, Batch 650, Loss: 1.0859842896461487\n",
      "Epoch 18, Batch 700, Loss: 1.1001173758506775\n",
      "Epoch 18, Batch 750, Loss: 1.1122242140769958\n",
      "Epoch 18, Batch 800, Loss: 1.096418845653534\n",
      "Epoch 18, Batch 850, Loss: 1.0877743101119994\n",
      "Epoch 18, Batch 900, Loss: 1.0872443389892579\n",
      "Epoch 19, Batch 50, Loss: 1.0967869830131531\n",
      "Epoch 19, Batch 100, Loss: 1.094885311126709\n",
      "Epoch 19, Batch 150, Loss: 1.0987756943702698\n",
      "Epoch 19, Batch 200, Loss: 1.1223641443252563\n",
      "Epoch 19, Batch 250, Loss: 1.1083915328979492\n",
      "Epoch 19, Batch 300, Loss: 1.116781702041626\n",
      "Epoch 19, Batch 350, Loss: 1.1015456628799438\n",
      "Epoch 19, Batch 400, Loss: 1.095551528930664\n",
      "Epoch 19, Batch 450, Loss: 1.0980513978004456\n",
      "Epoch 19, Batch 500, Loss: 1.0975207424163818\n",
      "Epoch 19, Batch 550, Loss: 1.0912956047058104\n",
      "Epoch 19, Batch 600, Loss: 1.0832479810714721\n",
      "Epoch 19, Batch 650, Loss: 1.0973732113838195\n",
      "Epoch 19, Batch 700, Loss: 1.1003411984443665\n",
      "Epoch 19, Batch 750, Loss: 1.084423179626465\n",
      "Epoch 19, Batch 800, Loss: 1.0978515195846557\n",
      "Epoch 19, Batch 850, Loss: 1.096508092880249\n",
      "Epoch 19, Batch 900, Loss: 1.095247631072998\n",
      "Epoch 20, Batch 50, Loss: 1.0992762494087218\n",
      "Epoch 20, Batch 100, Loss: 1.089892671108246\n",
      "Epoch 20, Batch 150, Loss: 1.0900768661499023\n",
      "Epoch 20, Batch 200, Loss: 1.101462459564209\n",
      "Epoch 20, Batch 250, Loss: 1.103556809425354\n",
      "Epoch 20, Batch 300, Loss: 1.1017740154266358\n",
      "Epoch 20, Batch 350, Loss: 1.1107180523872375\n",
      "Epoch 20, Batch 400, Loss: 1.0891908764839173\n",
      "Epoch 20, Batch 450, Loss: 1.1055451703071595\n",
      "Epoch 20, Batch 500, Loss: 1.088107132911682\n",
      "Epoch 20, Batch 550, Loss: 1.1103633499145509\n",
      "Epoch 20, Batch 600, Loss: 1.116441924571991\n",
      "Epoch 20, Batch 650, Loss: 1.1001136469841004\n",
      "Epoch 20, Batch 700, Loss: 1.0857364702224732\n",
      "Epoch 20, Batch 750, Loss: 1.0906324505805969\n",
      "Epoch 20, Batch 800, Loss: 1.0884928059577943\n",
      "Epoch 20, Batch 850, Loss: 1.091107156276703\n",
      "Epoch 20, Batch 900, Loss: 1.1046322250366212\n",
      "Epoch 21, Batch 50, Loss: 1.0965477442741394\n",
      "Epoch 21, Batch 100, Loss: 1.1042650938034058\n",
      "Epoch 21, Batch 150, Loss: 1.1007322263717652\n",
      "Epoch 21, Batch 200, Loss: 1.0978127646446227\n",
      "Epoch 21, Batch 250, Loss: 1.095147089958191\n",
      "Epoch 21, Batch 300, Loss: 1.0979433345794678\n",
      "Epoch 21, Batch 350, Loss: 1.0954361009597777\n",
      "Epoch 21, Batch 400, Loss: 1.0947506356239318\n",
      "Epoch 21, Batch 450, Loss: 1.099127492904663\n",
      "Epoch 21, Batch 500, Loss: 1.113217794895172\n",
      "Epoch 21, Batch 550, Loss: 1.0997560906410218\n",
      "Epoch 21, Batch 600, Loss: 1.0901792526245118\n",
      "Epoch 21, Batch 650, Loss: 1.1024234795570373\n",
      "Epoch 21, Batch 700, Loss: 1.0952632379531861\n",
      "Epoch 21, Batch 750, Loss: 1.1070701098442077\n",
      "Epoch 21, Batch 800, Loss: 1.1145385932922363\n",
      "Epoch 21, Batch 850, Loss: 1.087014572620392\n",
      "Epoch 21, Batch 900, Loss: 1.0922233319282533\n",
      "Epoch 22, Batch 50, Loss: 1.1021218276023865\n",
      "Epoch 22, Batch 100, Loss: 1.0997353172302247\n",
      "Epoch 22, Batch 150, Loss: 1.0959208178520203\n",
      "Epoch 22, Batch 200, Loss: 1.0914242625236512\n",
      "Epoch 22, Batch 250, Loss: 1.0915758419036865\n",
      "Epoch 22, Batch 300, Loss: 1.0920146799087525\n",
      "Epoch 22, Batch 350, Loss: 1.0929189944267272\n",
      "Epoch 22, Batch 400, Loss: 1.0971961450576782\n",
      "Epoch 22, Batch 450, Loss: 1.0965143752098083\n",
      "Epoch 22, Batch 500, Loss: 1.0876070713996888\n",
      "Epoch 22, Batch 550, Loss: 1.0850103521347045\n",
      "Epoch 22, Batch 600, Loss: 1.0935637044906616\n",
      "Epoch 22, Batch 650, Loss: 1.108907287120819\n",
      "Epoch 22, Batch 700, Loss: 1.0980117511749268\n",
      "Epoch 22, Batch 750, Loss: 1.089351511001587\n",
      "Epoch 22, Batch 800, Loss: 1.1091644287109375\n",
      "Epoch 22, Batch 850, Loss: 1.1185371923446654\n",
      "Epoch 22, Batch 900, Loss: 1.105641405582428\n",
      "Epoch 23, Batch 50, Loss: 1.1062583971023559\n",
      "Epoch 23, Batch 100, Loss: 1.0941608214378358\n",
      "Epoch 23, Batch 150, Loss: 1.0881921768188476\n",
      "Epoch 23, Batch 200, Loss: 1.0988620805740357\n",
      "Epoch 23, Batch 250, Loss: 1.0858577680587769\n",
      "Epoch 23, Batch 300, Loss: 1.09157466173172\n",
      "Epoch 23, Batch 350, Loss: 1.097240421772003\n",
      "Epoch 23, Batch 400, Loss: 1.0929557418823241\n",
      "Epoch 23, Batch 450, Loss: 1.0889251184463502\n",
      "Epoch 23, Batch 500, Loss: 1.0904175209999085\n",
      "Epoch 23, Batch 550, Loss: 1.0943094849586488\n",
      "Epoch 23, Batch 600, Loss: 1.105266182422638\n",
      "Epoch 23, Batch 650, Loss: 1.0863579940795898\n",
      "Epoch 23, Batch 700, Loss: 1.0964271092414857\n",
      "Epoch 23, Batch 750, Loss: 1.109589765071869\n",
      "Epoch 23, Batch 800, Loss: 1.0966314888000488\n",
      "Epoch 23, Batch 850, Loss: 1.0928855800628663\n",
      "Epoch 23, Batch 900, Loss: 1.1094504594802856\n",
      "Epoch 24, Batch 50, Loss: 1.1075772333145142\n",
      "Epoch 24, Batch 100, Loss: 1.0859988141059875\n",
      "Epoch 24, Batch 150, Loss: 1.0960021567344667\n",
      "Epoch 24, Batch 200, Loss: 1.0931739258766173\n",
      "Epoch 24, Batch 250, Loss: 1.0905172061920165\n",
      "Epoch 24, Batch 300, Loss: 1.0864365220069885\n",
      "Epoch 24, Batch 350, Loss: 1.0878113794326782\n",
      "Epoch 24, Batch 400, Loss: 1.092884328365326\n",
      "Epoch 24, Batch 450, Loss: 1.0976802492141724\n",
      "Epoch 24, Batch 500, Loss: 1.0948740029335022\n",
      "Epoch 24, Batch 550, Loss: 1.1031288838386535\n",
      "Epoch 24, Batch 600, Loss: 1.0949992394447328\n",
      "Epoch 24, Batch 650, Loss: 1.0873344326019287\n",
      "Epoch 24, Batch 700, Loss: 1.0957533121109009\n",
      "Epoch 24, Batch 750, Loss: 1.095666081905365\n",
      "Epoch 24, Batch 800, Loss: 1.0909707713127137\n",
      "Epoch 24, Batch 850, Loss: 1.1074224352836608\n",
      "Epoch 24, Batch 900, Loss: 1.1166490960121154\n",
      "Epoch 25, Batch 50, Loss: 1.1008603358268738\n",
      "Epoch 25, Batch 100, Loss: 1.0827279281616211\n",
      "Epoch 25, Batch 150, Loss: 1.0815193724632264\n",
      "Epoch 25, Batch 200, Loss: 1.0925827813148499\n",
      "Epoch 25, Batch 250, Loss: 1.0906647515296937\n",
      "Epoch 25, Batch 300, Loss: 1.0944757318496705\n",
      "Epoch 25, Batch 350, Loss: 1.0961806416511535\n",
      "Epoch 25, Batch 400, Loss: 1.102113699913025\n",
      "Epoch 25, Batch 450, Loss: 1.1138004088401794\n",
      "Epoch 25, Batch 500, Loss: 1.0962602400779724\n",
      "Epoch 25, Batch 550, Loss: 1.0812759828567504\n",
      "Epoch 25, Batch 600, Loss: 1.0859829783439636\n",
      "Epoch 25, Batch 650, Loss: 1.0966944813728332\n",
      "Epoch 25, Batch 700, Loss: 1.1090320777893066\n",
      "Epoch 25, Batch 750, Loss: 1.096567997932434\n",
      "Epoch 25, Batch 800, Loss: 1.089491057395935\n",
      "Epoch 25, Batch 850, Loss: 1.1056049346923829\n",
      "Epoch 25, Batch 900, Loss: 1.1065603828430175\n",
      "Accuracy on test set: 0.2163%\n",
      "Fitting for combination 3\n",
      "784\n",
      "1\n",
      "10\n",
      "[10, 10]\n",
      "False\n",
      "['relu']\n",
      "SGD\n",
      "0.1\n",
      "0.03\n",
      "CrossEntropyLoss\n",
      "100\n",
      "25\n",
      "0\n",
      "Epoch 1, Batch 100, Loss: 1.1903513631224631\n",
      "Epoch 1, Batch 200, Loss: 0.6882381942868233\n",
      "Epoch 1, Batch 300, Loss: 0.6481164199113846\n",
      "Epoch 1, Batch 400, Loss: 0.6384565880894661\n",
      "Epoch 1, Batch 500, Loss: 0.623297530412674\n",
      "Epoch 1, Batch 600, Loss: 0.6271082401275635\n",
      "Epoch 1, Batch 700, Loss: 0.5948870599269866\n",
      "Epoch 1, Batch 800, Loss: 0.6091437152028084\n",
      "Epoch 1, Batch 900, Loss: 0.5949543443322182\n",
      "Epoch 2, Batch 100, Loss: 0.6199072989821434\n",
      "Epoch 2, Batch 200, Loss: 0.5898324459791183\n",
      "Epoch 2, Batch 300, Loss: 0.5945136588811875\n",
      "Epoch 2, Batch 400, Loss: 0.604129042327404\n",
      "Epoch 2, Batch 500, Loss: 0.6219427219033241\n",
      "Epoch 2, Batch 600, Loss: 0.5701570251584053\n",
      "Epoch 2, Batch 700, Loss: 0.5968577101826668\n",
      "Epoch 2, Batch 800, Loss: 0.6199505561590195\n",
      "Epoch 2, Batch 900, Loss: 0.5905949410796165\n",
      "Epoch 3, Batch 100, Loss: 0.5935548493266105\n",
      "Epoch 3, Batch 200, Loss: 0.6148792305588722\n",
      "Epoch 3, Batch 300, Loss: 0.5837793979048729\n",
      "Epoch 3, Batch 400, Loss: 0.5893498584628105\n",
      "Epoch 3, Batch 500, Loss: 0.5913516718149185\n",
      "Epoch 3, Batch 600, Loss: 0.5995855614542961\n",
      "Epoch 3, Batch 700, Loss: 0.6002520263195038\n",
      "Epoch 3, Batch 800, Loss: 0.6014304250478745\n",
      "Epoch 3, Batch 900, Loss: 0.5879698246717453\n",
      "Epoch 4, Batch 100, Loss: 0.5985992112755776\n",
      "Epoch 4, Batch 200, Loss: 0.5897680884599685\n",
      "Epoch 4, Batch 300, Loss: 0.6049284356832504\n",
      "Epoch 4, Batch 400, Loss: 0.5957220703363418\n",
      "Epoch 4, Batch 500, Loss: 0.592545930147171\n",
      "Epoch 4, Batch 600, Loss: 0.5911100143194199\n",
      "Epoch 4, Batch 700, Loss: 0.5828825679421424\n",
      "Epoch 4, Batch 800, Loss: 0.5808367758989335\n",
      "Epoch 4, Batch 900, Loss: 0.5941015964746476\n",
      "Epoch 5, Batch 100, Loss: 0.5896594300866127\n",
      "Epoch 5, Batch 200, Loss: 0.5899785310029984\n",
      "Epoch 5, Batch 300, Loss: 0.5890664607286453\n",
      "Epoch 5, Batch 400, Loss: 0.57826455950737\n",
      "Epoch 5, Batch 500, Loss: 0.5830493533611297\n",
      "Epoch 5, Batch 600, Loss: 0.5890867066383362\n",
      "Epoch 5, Batch 700, Loss: 0.6037738335132599\n",
      "Epoch 5, Batch 800, Loss: 0.6052871185541153\n",
      "Epoch 5, Batch 900, Loss: 0.5960934343934059\n",
      "Epoch 6, Batch 100, Loss: 0.5892635193467141\n",
      "Epoch 6, Batch 200, Loss: 0.5723694276809692\n",
      "Epoch 6, Batch 300, Loss: 0.5836854615807533\n",
      "Epoch 6, Batch 400, Loss: 0.5920462241768837\n",
      "Epoch 6, Batch 500, Loss: 0.599116616845131\n",
      "Epoch 6, Batch 600, Loss: 0.5978983110189438\n",
      "Epoch 6, Batch 700, Loss: 0.5861806327104568\n",
      "Epoch 6, Batch 800, Loss: 0.5996664929389953\n",
      "Epoch 6, Batch 900, Loss: 0.5669423541426659\n",
      "Epoch 7, Batch 100, Loss: 0.5788964423537254\n",
      "Epoch 7, Batch 200, Loss: 0.6010363641381263\n",
      "Epoch 7, Batch 300, Loss: 0.5710903996229172\n",
      "Epoch 7, Batch 400, Loss: 0.5905858337879181\n",
      "Epoch 7, Batch 500, Loss: 0.5989893990755081\n",
      "Epoch 7, Batch 600, Loss: 0.5815483650565147\n",
      "Epoch 7, Batch 700, Loss: 0.6176746493577957\n",
      "Epoch 7, Batch 800, Loss: 0.6096549874544144\n",
      "Epoch 7, Batch 900, Loss: 0.5894804936647415\n",
      "Epoch 8, Batch 100, Loss: 0.5982502293586731\n",
      "Epoch 8, Batch 200, Loss: 0.6066823020577431\n",
      "Epoch 8, Batch 300, Loss: 0.585356947183609\n",
      "Epoch 8, Batch 400, Loss: 0.5972718983888626\n",
      "Epoch 8, Batch 500, Loss: 0.6016532933712005\n",
      "Epoch 8, Batch 600, Loss: 0.602524825334549\n",
      "Epoch 8, Batch 700, Loss: 0.5831402859091759\n",
      "Epoch 8, Batch 800, Loss: 0.5944940108060837\n",
      "Epoch 8, Batch 900, Loss: 0.5892049261927604\n",
      "Epoch 9, Batch 100, Loss: 0.5726902109384536\n",
      "Epoch 9, Batch 200, Loss: 0.5722218245267868\n",
      "Epoch 9, Batch 300, Loss: 0.5830428972840309\n",
      "Epoch 9, Batch 400, Loss: 0.5845099696516991\n",
      "Epoch 9, Batch 500, Loss: 0.5944609361886978\n",
      "Epoch 9, Batch 600, Loss: 0.5893317008018494\n",
      "Epoch 9, Batch 700, Loss: 0.6055732080340386\n",
      "Epoch 9, Batch 800, Loss: 0.6050728023052215\n",
      "Epoch 9, Batch 900, Loss: 0.605345253944397\n",
      "Epoch 10, Batch 100, Loss: 0.5973692566156388\n",
      "Epoch 10, Batch 200, Loss: 0.5827556639909744\n",
      "Epoch 10, Batch 300, Loss: 0.5865227088332177\n",
      "Epoch 10, Batch 400, Loss: 0.6039917081594467\n",
      "Epoch 10, Batch 500, Loss: 0.6220556750893593\n",
      "Epoch 10, Batch 600, Loss: 0.5720484736561775\n",
      "Epoch 10, Batch 700, Loss: 0.5971470934152603\n",
      "Epoch 10, Batch 800, Loss: 0.5863131159543991\n",
      "Epoch 10, Batch 900, Loss: 0.5929725643992424\n",
      "Epoch 11, Batch 100, Loss: 0.5783341068029404\n",
      "Epoch 11, Batch 200, Loss: 0.5873133221268654\n",
      "Epoch 11, Batch 300, Loss: 0.5830866429209709\n",
      "Epoch 11, Batch 400, Loss: 0.5951809406280517\n",
      "Epoch 11, Batch 500, Loss: 0.5848115479946137\n",
      "Epoch 11, Batch 600, Loss: 0.5757964417338371\n",
      "Epoch 11, Batch 700, Loss: 0.5994734027981758\n",
      "Epoch 11, Batch 800, Loss: 0.5950669646263123\n",
      "Epoch 11, Batch 900, Loss: 0.6103416064381599\n",
      "Epoch 12, Batch 100, Loss: 0.5855945256352425\n",
      "Epoch 12, Batch 200, Loss: 0.5823079866170883\n",
      "Epoch 12, Batch 300, Loss: 0.5758146497607232\n",
      "Epoch 12, Batch 400, Loss: 0.5825189837813377\n",
      "Epoch 12, Batch 500, Loss: 0.5980225548148155\n",
      "Epoch 12, Batch 600, Loss: 0.6039263236522675\n",
      "Epoch 12, Batch 700, Loss: 0.611363389492035\n",
      "Epoch 12, Batch 800, Loss: 0.5688591778278351\n",
      "Epoch 12, Batch 900, Loss: 0.5864310419559479\n",
      "Epoch 13, Batch 100, Loss: 0.5934225311875343\n",
      "Epoch 13, Batch 200, Loss: 0.5852329564094544\n",
      "Epoch 13, Batch 300, Loss: 0.5791600739955902\n",
      "Epoch 13, Batch 400, Loss: 0.591010674238205\n",
      "Epoch 13, Batch 500, Loss: 0.5831753143668175\n",
      "Epoch 13, Batch 600, Loss: 0.5849261468648911\n",
      "Epoch 13, Batch 700, Loss: 0.5826409712433815\n",
      "Epoch 13, Batch 800, Loss: 0.5867112550139427\n",
      "Epoch 13, Batch 900, Loss: 0.5953678494691849\n",
      "Epoch 14, Batch 100, Loss: 0.5881268373131752\n",
      "Epoch 14, Batch 200, Loss: 0.5875657412409783\n",
      "Epoch 14, Batch 300, Loss: 0.5800222977995872\n",
      "Epoch 14, Batch 400, Loss: 0.594098609983921\n",
      "Epoch 14, Batch 500, Loss: 0.580223039984703\n",
      "Epoch 14, Batch 600, Loss: 0.5848856434226036\n",
      "Epoch 14, Batch 700, Loss: 0.5964114132523537\n",
      "Epoch 14, Batch 800, Loss: 0.567975831925869\n",
      "Epoch 14, Batch 900, Loss: 0.561755608022213\n",
      "Epoch 15, Batch 100, Loss: 0.5810528334975242\n",
      "Epoch 15, Batch 200, Loss: 0.567057529091835\n",
      "Epoch 15, Batch 300, Loss: 0.5856250381469726\n",
      "Epoch 15, Batch 400, Loss: 0.5968600335717201\n",
      "Epoch 15, Batch 500, Loss: 0.5691636002063751\n",
      "Epoch 15, Batch 600, Loss: 0.5902786734700203\n",
      "Epoch 15, Batch 700, Loss: 0.5860748878121376\n",
      "Epoch 15, Batch 800, Loss: 0.5833611181378364\n",
      "Epoch 15, Batch 900, Loss: 0.594699267745018\n",
      "Epoch 16, Batch 100, Loss: 0.5537124821543693\n",
      "Epoch 16, Batch 200, Loss: 0.5848404252529145\n",
      "Epoch 16, Batch 300, Loss: 0.5912986531853676\n",
      "Epoch 16, Batch 400, Loss: 0.5940498721599579\n",
      "Epoch 16, Batch 500, Loss: 0.5774914088845253\n",
      "Epoch 16, Batch 600, Loss: 0.5813706198334694\n",
      "Epoch 16, Batch 700, Loss: 0.5804593917727471\n",
      "Epoch 16, Batch 800, Loss: 0.5873132327198982\n",
      "Epoch 16, Batch 900, Loss: 0.5980201771855355\n",
      "Epoch 17, Batch 100, Loss: 0.5753534552454949\n",
      "Epoch 17, Batch 200, Loss: 0.6003755351901054\n",
      "Epoch 17, Batch 300, Loss: 0.580583847463131\n",
      "Epoch 17, Batch 400, Loss: 0.5736630925536156\n",
      "Epoch 17, Batch 500, Loss: 0.5677901110053063\n",
      "Epoch 17, Batch 600, Loss: 0.5822449648380279\n",
      "Epoch 17, Batch 700, Loss: 0.5840339589118958\n",
      "Epoch 17, Batch 800, Loss: 0.5965393447875976\n",
      "Epoch 17, Batch 900, Loss: 0.5961580413579941\n",
      "Epoch 18, Batch 100, Loss: 0.5798207381367684\n",
      "Epoch 18, Batch 200, Loss: 0.6091156452894211\n",
      "Epoch 18, Batch 300, Loss: 0.593483314216137\n",
      "Epoch 18, Batch 400, Loss: 0.5911977884173393\n",
      "Epoch 18, Batch 500, Loss: 0.566862952709198\n",
      "Epoch 18, Batch 600, Loss: 0.58609893232584\n",
      "Epoch 18, Batch 700, Loss: 0.5910286325216293\n",
      "Epoch 18, Batch 800, Loss: 0.5683983290195465\n",
      "Epoch 18, Batch 900, Loss: 0.5817177683115006\n",
      "Epoch 19, Batch 100, Loss: 0.5994102391600609\n",
      "Epoch 19, Batch 200, Loss: 0.5713786783814431\n",
      "Epoch 19, Batch 300, Loss: 0.5767768985033035\n",
      "Epoch 19, Batch 400, Loss: 0.5921126726269722\n",
      "Epoch 19, Batch 500, Loss: 0.5738760709762574\n",
      "Epoch 19, Batch 600, Loss: 0.5777005270123482\n",
      "Epoch 19, Batch 700, Loss: 0.574292677640915\n",
      "Epoch 19, Batch 800, Loss: 0.5653202736377716\n",
      "Epoch 19, Batch 900, Loss: 0.5885364380478859\n",
      "Epoch 20, Batch 100, Loss: 0.5622644287347793\n",
      "Epoch 20, Batch 200, Loss: 0.5915605872869492\n",
      "Epoch 20, Batch 300, Loss: 0.5881476497650147\n",
      "Epoch 20, Batch 400, Loss: 0.5887404894828796\n",
      "Epoch 20, Batch 500, Loss: 0.575337282717228\n",
      "Epoch 20, Batch 600, Loss: 0.5621251860260963\n",
      "Epoch 20, Batch 700, Loss: 0.5762437200546264\n",
      "Epoch 20, Batch 800, Loss: 0.5710970985889435\n",
      "Epoch 20, Batch 900, Loss: 0.5882248523831367\n",
      "Epoch 21, Batch 100, Loss: 0.5955009889602662\n",
      "Epoch 21, Batch 200, Loss: 0.5658484074473381\n",
      "Epoch 21, Batch 300, Loss: 0.5787073192000389\n",
      "Epoch 21, Batch 400, Loss: 0.5865111437439918\n",
      "Epoch 21, Batch 500, Loss: 0.5849697634577751\n",
      "Epoch 21, Batch 600, Loss: 0.573941683769226\n",
      "Epoch 21, Batch 700, Loss: 0.5784563934803009\n",
      "Epoch 21, Batch 800, Loss: 0.5937121137976646\n",
      "Epoch 21, Batch 900, Loss: 0.5754848870635033\n",
      "Epoch 22, Batch 100, Loss: 0.5631402704119682\n",
      "Epoch 22, Batch 200, Loss: 0.5686893227696419\n",
      "Epoch 22, Batch 300, Loss: 0.6006277903914452\n",
      "Epoch 22, Batch 400, Loss: 0.5805876407027245\n",
      "Epoch 22, Batch 500, Loss: 0.5890194052457809\n",
      "Epoch 22, Batch 600, Loss: 0.5821845442056656\n",
      "Epoch 22, Batch 700, Loss: 0.5766853013634682\n",
      "Epoch 22, Batch 800, Loss: 0.5735718673467636\n",
      "Epoch 22, Batch 900, Loss: 0.5648293179273606\n",
      "Epoch 23, Batch 100, Loss: 0.5709290993213654\n",
      "Epoch 23, Batch 200, Loss: 0.5856519252061844\n",
      "Epoch 23, Batch 300, Loss: 0.5697426077723503\n",
      "Epoch 23, Batch 400, Loss: 0.5759295809268952\n",
      "Epoch 23, Batch 500, Loss: 0.5686858853697777\n",
      "Epoch 23, Batch 600, Loss: 0.5859632068872451\n",
      "Epoch 23, Batch 700, Loss: 0.5769460135698319\n",
      "Epoch 23, Batch 800, Loss: 0.6008666679263115\n",
      "Epoch 23, Batch 900, Loss: 0.5871065160632134\n",
      "Epoch 24, Batch 100, Loss: 0.5678823637962341\n",
      "Epoch 24, Batch 200, Loss: 0.5638311272859573\n",
      "Epoch 24, Batch 300, Loss: 0.5988860046863556\n",
      "Epoch 24, Batch 400, Loss: 0.5966170141100884\n",
      "Epoch 24, Batch 500, Loss: 0.5841576582193375\n",
      "Epoch 24, Batch 600, Loss: 0.562000286579132\n",
      "Epoch 24, Batch 700, Loss: 0.5775450223684311\n",
      "Epoch 24, Batch 800, Loss: 0.5671437177062034\n",
      "Epoch 24, Batch 900, Loss: 0.581245208978653\n",
      "Epoch 25, Batch 100, Loss: 0.5874983900785447\n",
      "Epoch 25, Batch 200, Loss: 0.567764880657196\n",
      "Epoch 25, Batch 300, Loss: 0.5700984588265419\n",
      "Epoch 25, Batch 400, Loss: 0.5603968754410744\n",
      "Epoch 25, Batch 500, Loss: 0.5721194049715996\n",
      "Epoch 25, Batch 600, Loss: 0.5950504124164582\n",
      "Epoch 25, Batch 700, Loss: 0.5859949335455894\n",
      "Epoch 25, Batch 800, Loss: 0.5762385982275009\n",
      "Epoch 25, Batch 900, Loss: 0.5781158223748207\n",
      "Accuracy on test set: 0.7733%\n",
      "Fitting for combination 4\n",
      "784\n",
      "1\n",
      "10\n",
      "[20, 10]\n",
      "False\n",
      "['sigmoid']\n",
      "Adam\n",
      "0.1\n",
      "0\n",
      "CrossEntropyLoss\n",
      "100\n",
      "25\n",
      "0\n",
      "Epoch 1, Batch 100, Loss: 1.3480539011955261\n",
      "Epoch 1, Batch 200, Loss: 1.055583851337433\n",
      "Epoch 1, Batch 300, Loss: 0.9895628386735916\n",
      "Epoch 1, Batch 400, Loss: 0.9755148088932037\n",
      "Epoch 1, Batch 500, Loss: 0.9762508720159531\n",
      "Epoch 1, Batch 600, Loss: 0.9500494718551635\n",
      "Epoch 1, Batch 700, Loss: 1.0262483036518097\n",
      "Epoch 1, Batch 800, Loss: 0.9804546737670898\n",
      "Epoch 1, Batch 900, Loss: 0.9566191631555557\n",
      "Epoch 2, Batch 100, Loss: 0.9361758655309678\n",
      "Epoch 2, Batch 200, Loss: 0.9851059854030609\n",
      "Epoch 2, Batch 300, Loss: 0.9583227813243866\n",
      "Epoch 2, Batch 400, Loss: 0.9549036151170731\n",
      "Epoch 2, Batch 500, Loss: 0.8684534880518914\n",
      "Epoch 2, Batch 600, Loss: 0.9301361006498337\n",
      "Epoch 2, Batch 700, Loss: 0.9842169833183289\n",
      "Epoch 2, Batch 800, Loss: 0.9942667675018311\n",
      "Epoch 2, Batch 900, Loss: 0.9045918589830398\n",
      "Epoch 3, Batch 100, Loss: 0.9247489738464355\n",
      "Epoch 3, Batch 200, Loss: 0.9968483090400696\n",
      "Epoch 3, Batch 300, Loss: 0.9813103890419006\n",
      "Epoch 3, Batch 400, Loss: 1.001780925989151\n",
      "Epoch 3, Batch 500, Loss: 1.0068923318386078\n",
      "Epoch 3, Batch 600, Loss: 0.9385346305370331\n",
      "Epoch 3, Batch 700, Loss: 0.9679240030050278\n",
      "Epoch 3, Batch 800, Loss: 0.8959304863214492\n",
      "Epoch 3, Batch 900, Loss: 0.8892789077758789\n",
      "Epoch 4, Batch 100, Loss: 0.8946774429082871\n",
      "Epoch 4, Batch 200, Loss: 0.9604340225458146\n",
      "Epoch 4, Batch 300, Loss: 0.9231773728132248\n",
      "Epoch 4, Batch 400, Loss: 0.9629305136203766\n",
      "Epoch 4, Batch 500, Loss: 0.8841431802511215\n",
      "Epoch 4, Batch 600, Loss: 0.8511729300022125\n",
      "Epoch 4, Batch 700, Loss: 0.8810077357292175\n",
      "Epoch 4, Batch 800, Loss: 0.8567921245098113\n",
      "Epoch 4, Batch 900, Loss: 0.8267653489112854\n",
      "Epoch 5, Batch 100, Loss: 0.7906446307897568\n",
      "Epoch 5, Batch 200, Loss: 0.815893469452858\n",
      "Epoch 5, Batch 300, Loss: 0.8516913211345672\n",
      "Epoch 5, Batch 400, Loss: 0.8144033640623093\n",
      "Epoch 5, Batch 500, Loss: 0.9074265921115875\n",
      "Epoch 5, Batch 600, Loss: 0.8178671127557755\n",
      "Epoch 5, Batch 700, Loss: 0.8449149200320244\n",
      "Epoch 5, Batch 800, Loss: 0.8421955996751785\n",
      "Epoch 5, Batch 900, Loss: 0.805619335770607\n",
      "Epoch 6, Batch 100, Loss: 0.8004874521493912\n",
      "Epoch 6, Batch 200, Loss: 0.8122549661993981\n",
      "Epoch 6, Batch 300, Loss: 0.7919131895899773\n",
      "Epoch 6, Batch 400, Loss: 0.8050167495012284\n",
      "Epoch 6, Batch 500, Loss: 0.931707955300808\n",
      "Epoch 6, Batch 600, Loss: 0.8941327401995659\n",
      "Epoch 6, Batch 700, Loss: 0.9245754915475846\n",
      "Epoch 6, Batch 800, Loss: 0.8752752429246903\n",
      "Epoch 6, Batch 900, Loss: 0.8861646008491516\n",
      "Epoch 7, Batch 100, Loss: 0.8661684772372246\n",
      "Epoch 7, Batch 200, Loss: 0.8726391559839248\n",
      "Epoch 7, Batch 300, Loss: 0.8694608592987061\n",
      "Epoch 7, Batch 400, Loss: 0.9259798657894135\n",
      "Epoch 7, Batch 500, Loss: 0.9146901750564576\n",
      "Epoch 7, Batch 600, Loss: 0.8808868706226349\n",
      "Epoch 7, Batch 700, Loss: 0.8496002930402756\n",
      "Epoch 7, Batch 800, Loss: 0.8423887723684311\n",
      "Epoch 7, Batch 900, Loss: 0.8611099886894226\n",
      "Epoch 8, Batch 100, Loss: 0.8040054166316986\n",
      "Epoch 8, Batch 200, Loss: 0.8268316143751144\n",
      "Epoch 8, Batch 300, Loss: 0.8004488018155098\n",
      "Epoch 8, Batch 400, Loss: 0.8534036552906037\n",
      "Epoch 8, Batch 500, Loss: 0.812738156914711\n",
      "Epoch 8, Batch 600, Loss: 0.7959583684802055\n",
      "Epoch 8, Batch 700, Loss: 0.8442133456468582\n",
      "Epoch 8, Batch 800, Loss: 0.8385236525535583\n",
      "Epoch 8, Batch 900, Loss: 0.7999626064300537\n",
      "Epoch 9, Batch 100, Loss: 0.7888767325878143\n",
      "Epoch 9, Batch 200, Loss: 0.7788585054874421\n",
      "Epoch 9, Batch 300, Loss: 0.7813245216012001\n",
      "Epoch 9, Batch 400, Loss: 0.7475104793906212\n",
      "Epoch 9, Batch 500, Loss: 0.7926158428192138\n",
      "Epoch 9, Batch 600, Loss: 0.7887066602706909\n",
      "Epoch 9, Batch 700, Loss: 0.7871647945046425\n",
      "Epoch 9, Batch 800, Loss: 0.8465000596642495\n",
      "Epoch 9, Batch 900, Loss: 0.8697571450471878\n",
      "Epoch 10, Batch 100, Loss: 0.8373207080364228\n",
      "Epoch 10, Batch 200, Loss: 0.8299370524287224\n",
      "Epoch 10, Batch 300, Loss: 0.8079898506402969\n",
      "Epoch 10, Batch 400, Loss: 0.8351782077550888\n",
      "Epoch 10, Batch 500, Loss: 0.8516420531272888\n",
      "Epoch 10, Batch 600, Loss: 0.8648167365789413\n",
      "Epoch 10, Batch 700, Loss: 0.8479247418045998\n",
      "Epoch 10, Batch 800, Loss: 0.8317133617401123\n",
      "Epoch 10, Batch 900, Loss: 0.7854221817851067\n",
      "Epoch 11, Batch 100, Loss: 0.8012948423624039\n",
      "Epoch 11, Batch 200, Loss: 0.8240111070871353\n",
      "Epoch 11, Batch 300, Loss: 0.8533517497777939\n",
      "Epoch 11, Batch 400, Loss: 0.8132524675130844\n",
      "Epoch 11, Batch 500, Loss: 0.7943343839049339\n",
      "Epoch 11, Batch 600, Loss: 0.7916175454854966\n",
      "Epoch 11, Batch 700, Loss: 0.8060085701942444\n",
      "Epoch 11, Batch 800, Loss: 0.7369336259365081\n",
      "Epoch 11, Batch 900, Loss: 0.7393764224648476\n",
      "Epoch 12, Batch 100, Loss: 0.7504256868362427\n",
      "Epoch 12, Batch 200, Loss: 0.7670988267660142\n",
      "Epoch 12, Batch 300, Loss: 0.755250011086464\n",
      "Epoch 12, Batch 400, Loss: 0.770484399497509\n",
      "Epoch 12, Batch 500, Loss: 0.7716588616371155\n",
      "Epoch 12, Batch 600, Loss: 0.7442853969335556\n",
      "Epoch 12, Batch 700, Loss: 0.7573269319534301\n",
      "Epoch 12, Batch 800, Loss: 0.7620284351706504\n",
      "Epoch 12, Batch 900, Loss: 0.7625011384487153\n",
      "Epoch 13, Batch 100, Loss: 0.7724284079670906\n",
      "Epoch 13, Batch 200, Loss: 0.7429018667340279\n",
      "Epoch 13, Batch 300, Loss: 0.7282231992483139\n",
      "Epoch 13, Batch 400, Loss: 0.7717835286259651\n",
      "Epoch 13, Batch 500, Loss: 0.77933529317379\n",
      "Epoch 13, Batch 600, Loss: 0.7593807634711266\n",
      "Epoch 13, Batch 700, Loss: 0.7838444870710373\n",
      "Epoch 13, Batch 800, Loss: 0.7810171070694923\n",
      "Epoch 13, Batch 900, Loss: 0.8071645712852478\n",
      "Epoch 14, Batch 100, Loss: 0.8072040030360221\n",
      "Epoch 14, Batch 200, Loss: 0.8190234184265137\n",
      "Epoch 14, Batch 300, Loss: 0.8304991573095322\n",
      "Epoch 14, Batch 400, Loss: 0.8088538539409638\n",
      "Epoch 14, Batch 500, Loss: 0.7893273055553436\n",
      "Epoch 14, Batch 600, Loss: 0.8116869431734085\n",
      "Epoch 14, Batch 700, Loss: 0.7974395969510079\n",
      "Epoch 14, Batch 800, Loss: 0.7707736724615097\n",
      "Epoch 14, Batch 900, Loss: 0.7776615226268768\n",
      "Epoch 15, Batch 100, Loss: 0.7858772173523902\n",
      "Epoch 15, Batch 200, Loss: 0.859841565489769\n",
      "Epoch 15, Batch 300, Loss: 0.7812409925460816\n",
      "Epoch 15, Batch 400, Loss: 0.7920179125666619\n",
      "Epoch 15, Batch 500, Loss: 0.7704943135380745\n",
      "Epoch 15, Batch 600, Loss: 0.7519285777211189\n",
      "Epoch 15, Batch 700, Loss: 0.8009340226650238\n",
      "Epoch 15, Batch 800, Loss: 0.8622184550762176\n",
      "Epoch 15, Batch 900, Loss: 0.8309396296739578\n",
      "Epoch 16, Batch 100, Loss: 0.8345030921697617\n",
      "Epoch 16, Batch 200, Loss: 0.880922035574913\n",
      "Epoch 16, Batch 300, Loss: 0.8219472521543503\n",
      "Epoch 16, Batch 400, Loss: 0.7433944663405418\n",
      "Epoch 16, Batch 500, Loss: 0.7460359814763069\n",
      "Epoch 16, Batch 600, Loss: 0.7418178793787956\n",
      "Epoch 16, Batch 700, Loss: 0.7479847967624664\n",
      "Epoch 16, Batch 800, Loss: 0.7932497009634971\n",
      "Epoch 16, Batch 900, Loss: 0.7190112072229385\n",
      "Epoch 17, Batch 100, Loss: 0.7206694394350052\n",
      "Epoch 17, Batch 200, Loss: 0.7329512014985085\n",
      "Epoch 17, Batch 300, Loss: 0.7510887709259987\n",
      "Epoch 17, Batch 400, Loss: 0.7520266282558441\n",
      "Epoch 17, Batch 500, Loss: 0.7356932011246681\n",
      "Epoch 17, Batch 600, Loss: 0.7104892706871033\n",
      "Epoch 17, Batch 700, Loss: 0.7236519944667816\n",
      "Epoch 17, Batch 800, Loss: 0.740971885919571\n",
      "Epoch 17, Batch 900, Loss: 0.718321159183979\n",
      "Epoch 18, Batch 100, Loss: 0.7314373695850372\n",
      "Epoch 18, Batch 200, Loss: 0.7859816807508468\n",
      "Epoch 18, Batch 300, Loss: 0.79086081802845\n",
      "Epoch 18, Batch 400, Loss: 0.7399602270126343\n",
      "Epoch 18, Batch 500, Loss: 0.7341282683610916\n",
      "Epoch 18, Batch 600, Loss: 0.7819215446710587\n",
      "Epoch 18, Batch 700, Loss: 0.8114648431539535\n",
      "Epoch 18, Batch 800, Loss: 0.7989080747961999\n",
      "Epoch 18, Batch 900, Loss: 0.7488286662101745\n",
      "Epoch 19, Batch 100, Loss: 0.7335913860797882\n",
      "Epoch 19, Batch 200, Loss: 0.72365424066782\n",
      "Epoch 19, Batch 300, Loss: 0.7056879737973213\n",
      "Epoch 19, Batch 400, Loss: 0.7355204439163208\n",
      "Epoch 19, Batch 500, Loss: 0.7193264126777649\n",
      "Epoch 19, Batch 600, Loss: 0.6913282617926597\n",
      "Epoch 19, Batch 700, Loss: 0.7627366608381272\n",
      "Epoch 19, Batch 800, Loss: 0.7742307555675506\n",
      "Epoch 19, Batch 900, Loss: 0.6949525770545005\n",
      "Epoch 20, Batch 100, Loss: 0.7159201022982598\n",
      "Epoch 20, Batch 200, Loss: 0.7674017155170441\n",
      "Epoch 20, Batch 300, Loss: 0.7427415043115616\n",
      "Epoch 20, Batch 400, Loss: 0.7024848791956901\n",
      "Epoch 20, Batch 500, Loss: 0.7041987317800522\n",
      "Epoch 20, Batch 600, Loss: 0.6964727410674095\n",
      "Epoch 20, Batch 700, Loss: 0.726112579703331\n",
      "Epoch 20, Batch 800, Loss: 0.7039527466893196\n",
      "Epoch 20, Batch 900, Loss: 0.7159821027517319\n",
      "Epoch 21, Batch 100, Loss: 0.7195702722668648\n",
      "Epoch 21, Batch 200, Loss: 0.7402626174688339\n",
      "Epoch 21, Batch 300, Loss: 0.7201470416784287\n",
      "Epoch 21, Batch 400, Loss: 0.7191544735431671\n",
      "Epoch 21, Batch 500, Loss: 0.749248499572277\n",
      "Epoch 21, Batch 600, Loss: 0.7010871815681458\n",
      "Epoch 21, Batch 700, Loss: 0.6831871852278709\n",
      "Epoch 21, Batch 800, Loss: 0.6869684666395187\n",
      "Epoch 21, Batch 900, Loss: 0.6859981098771095\n",
      "Epoch 22, Batch 100, Loss: 0.7794110724329948\n",
      "Epoch 22, Batch 200, Loss: 0.7985360276699066\n",
      "Epoch 22, Batch 300, Loss: 0.832870437502861\n",
      "Epoch 22, Batch 400, Loss: 0.7811573851108551\n",
      "Epoch 22, Batch 500, Loss: 0.8212082332372665\n",
      "Epoch 22, Batch 600, Loss: 0.803687881231308\n",
      "Epoch 22, Batch 700, Loss: 0.8271049457788467\n",
      "Epoch 22, Batch 800, Loss: 0.7952503314614296\n",
      "Epoch 22, Batch 900, Loss: 0.7252759644389153\n",
      "Epoch 23, Batch 100, Loss: 0.7127053913474083\n",
      "Epoch 23, Batch 200, Loss: 0.7057022395730018\n",
      "Epoch 23, Batch 300, Loss: 0.7079653632640839\n",
      "Epoch 23, Batch 400, Loss: 0.7299462831020356\n",
      "Epoch 23, Batch 500, Loss: 0.7243317607045173\n",
      "Epoch 23, Batch 600, Loss: 0.7566546869277954\n",
      "Epoch 23, Batch 700, Loss: 0.7474870532751083\n",
      "Epoch 23, Batch 800, Loss: 0.7319406595826149\n",
      "Epoch 23, Batch 900, Loss: 0.6908951568603515\n",
      "Epoch 24, Batch 100, Loss: 0.7098943200707436\n",
      "Epoch 24, Batch 200, Loss: 0.7445848643779754\n",
      "Epoch 24, Batch 300, Loss: 0.7252488648891449\n",
      "Epoch 24, Batch 400, Loss: 0.7223432317376137\n",
      "Epoch 24, Batch 500, Loss: 0.747198819220066\n",
      "Epoch 24, Batch 600, Loss: 0.7474361172318459\n",
      "Epoch 24, Batch 700, Loss: 0.750447626709938\n",
      "Epoch 24, Batch 800, Loss: 0.7396607699990273\n",
      "Epoch 24, Batch 900, Loss: 0.7361802861094475\n",
      "Epoch 25, Batch 100, Loss: 0.8117175883054734\n",
      "Epoch 25, Batch 200, Loss: 0.7967187869548797\n",
      "Epoch 25, Batch 300, Loss: 0.7908385589718818\n",
      "Epoch 25, Batch 400, Loss: 0.7944153386354447\n",
      "Epoch 25, Batch 500, Loss: 0.835100467801094\n",
      "Epoch 25, Batch 600, Loss: 0.7977760347723961\n",
      "Epoch 25, Batch 700, Loss: 0.7621909689903259\n",
      "Epoch 25, Batch 800, Loss: 0.7467093735933303\n",
      "Epoch 25, Batch 900, Loss: 0.8393986839056015\n",
      "Accuracy on test set: 0.6983%\n",
      "Fitting for combination 5\n",
      "784\n",
      "1\n",
      "10\n",
      "[20, 10]\n",
      "False\n",
      "['sigmoid']\n",
      "SGD\n",
      "0.03\n",
      "0.03\n",
      "CrossEntropyLoss\n",
      "200\n",
      "25\n",
      "0\n",
      "Epoch 1, Batch 200, Loss: 4.066484705209732\n",
      "Epoch 1, Batch 400, Loss: 3.221502823829651\n",
      "Epoch 1, Batch 600, Loss: 2.737336114645004\n",
      "Epoch 1, Batch 800, Loss: 2.4750488555431365\n",
      "Epoch 2, Batch 200, Loss: 2.2226581597328186\n",
      "Epoch 2, Batch 400, Loss: 2.140673485994339\n",
      "Epoch 2, Batch 600, Loss: 2.072137361764908\n",
      "Epoch 2, Batch 800, Loss: 2.0342258089780807\n",
      "Epoch 3, Batch 200, Loss: 1.9885634237527847\n",
      "Epoch 3, Batch 400, Loss: 1.9776436114311218\n",
      "Epoch 3, Batch 600, Loss: 1.9326414662599563\n",
      "Epoch 3, Batch 800, Loss: 1.948446461558342\n",
      "Epoch 4, Batch 200, Loss: 1.9301776427030564\n",
      "Epoch 4, Batch 400, Loss: 1.931890351176262\n",
      "Epoch 4, Batch 600, Loss: 1.9099219405651093\n",
      "Epoch 4, Batch 800, Loss: 1.9150251299142838\n",
      "Epoch 5, Batch 200, Loss: 1.902416706085205\n",
      "Epoch 5, Batch 400, Loss: 1.9066471499204636\n",
      "Epoch 5, Batch 600, Loss: 1.8881153547763825\n",
      "Epoch 5, Batch 800, Loss: 1.9014961034059525\n",
      "Epoch 6, Batch 200, Loss: 1.8943501216173173\n",
      "Epoch 6, Batch 400, Loss: 1.891080374121666\n",
      "Epoch 6, Batch 600, Loss: 1.901242755651474\n",
      "Epoch 6, Batch 800, Loss: 1.8920996868610382\n",
      "Epoch 7, Batch 200, Loss: 1.877079320549965\n",
      "Epoch 7, Batch 400, Loss: 1.8909058523178102\n",
      "Epoch 7, Batch 600, Loss: 1.8747520816326142\n",
      "Epoch 7, Batch 800, Loss: 1.9033346432447433\n",
      "Epoch 8, Batch 200, Loss: 1.8768470501899719\n",
      "Epoch 8, Batch 400, Loss: 1.8887315905094146\n",
      "Epoch 8, Batch 600, Loss: 1.9001122242212296\n",
      "Epoch 8, Batch 800, Loss: 1.8872089457511902\n",
      "Epoch 9, Batch 200, Loss: 1.8792677372694016\n",
      "Epoch 9, Batch 400, Loss: 1.8754718440771103\n",
      "Epoch 9, Batch 600, Loss: 1.8739387673139571\n",
      "Epoch 9, Batch 800, Loss: 1.8787511152029037\n",
      "Epoch 10, Batch 200, Loss: 1.8790148216485978\n",
      "Epoch 10, Batch 400, Loss: 1.8688478112220763\n",
      "Epoch 10, Batch 600, Loss: 1.8797572672367096\n",
      "Epoch 10, Batch 800, Loss: 1.8789984619617461\n",
      "Epoch 11, Batch 200, Loss: 1.8683213233947753\n",
      "Epoch 11, Batch 400, Loss: 1.864062175154686\n",
      "Epoch 11, Batch 600, Loss: 1.878860542178154\n",
      "Epoch 11, Batch 800, Loss: 1.8591630417108536\n",
      "Epoch 12, Batch 200, Loss: 1.8732533729076386\n",
      "Epoch 12, Batch 400, Loss: 1.8772384357452392\n",
      "Epoch 12, Batch 600, Loss: 1.8683116412162781\n",
      "Epoch 12, Batch 800, Loss: 1.8591775047779082\n",
      "Epoch 13, Batch 200, Loss: 1.8722408217191697\n",
      "Epoch 13, Batch 400, Loss: 1.8644899362325669\n",
      "Epoch 13, Batch 600, Loss: 1.864246495962143\n",
      "Epoch 13, Batch 800, Loss: 1.8658753794431686\n",
      "Epoch 14, Batch 200, Loss: 1.8658789384365082\n",
      "Epoch 14, Batch 400, Loss: 1.8758314597606658\n",
      "Epoch 14, Batch 600, Loss: 1.8674702912569046\n",
      "Epoch 14, Batch 800, Loss: 1.8491480082273484\n",
      "Epoch 15, Batch 200, Loss: 1.8566275978088378\n",
      "Epoch 15, Batch 400, Loss: 1.8658763778209686\n",
      "Epoch 15, Batch 600, Loss: 1.872609819173813\n",
      "Epoch 15, Batch 800, Loss: 1.8668181902170182\n",
      "Epoch 16, Batch 200, Loss: 1.8677200257778168\n",
      "Epoch 16, Batch 400, Loss: 1.8668938893079758\n",
      "Epoch 16, Batch 600, Loss: 1.8716349560022354\n",
      "Epoch 16, Batch 800, Loss: 1.8584115481376648\n",
      "Epoch 17, Batch 200, Loss: 1.8463938802480697\n",
      "Epoch 17, Batch 400, Loss: 1.8648862308263778\n",
      "Epoch 17, Batch 600, Loss: 1.8678386831283569\n",
      "Epoch 17, Batch 800, Loss: 1.869428153038025\n",
      "Epoch 18, Batch 200, Loss: 1.869905081987381\n",
      "Epoch 18, Batch 400, Loss: 1.8689652568101882\n",
      "Epoch 18, Batch 600, Loss: 1.8586761116981507\n",
      "Epoch 18, Batch 800, Loss: 1.8476188844442367\n",
      "Epoch 19, Batch 200, Loss: 1.8541814255714417\n",
      "Epoch 19, Batch 400, Loss: 1.8523985797166824\n",
      "Epoch 19, Batch 600, Loss: 1.882556023001671\n",
      "Epoch 19, Batch 800, Loss: 1.8604037427902222\n",
      "Epoch 20, Batch 200, Loss: 1.8579206442832947\n",
      "Epoch 20, Batch 400, Loss: 1.8860749453306198\n",
      "Epoch 20, Batch 600, Loss: 1.852283131480217\n",
      "Epoch 20, Batch 800, Loss: 1.858899101614952\n",
      "Epoch 21, Batch 200, Loss: 1.8722979003190994\n",
      "Epoch 21, Batch 400, Loss: 1.8568019181489945\n",
      "Epoch 21, Batch 600, Loss: 1.8700307291746139\n",
      "Epoch 21, Batch 800, Loss: 1.8539477729797362\n",
      "Epoch 22, Batch 200, Loss: 1.8682620984315872\n",
      "Epoch 22, Batch 400, Loss: 1.8604972779750824\n",
      "Epoch 22, Batch 600, Loss: 1.8643619388341903\n",
      "Epoch 22, Batch 800, Loss: 1.8605802881717681\n",
      "Epoch 23, Batch 200, Loss: 1.8737919163703918\n",
      "Epoch 23, Batch 400, Loss: 1.852142914533615\n",
      "Epoch 23, Batch 600, Loss: 1.8600718808174133\n",
      "Epoch 23, Batch 800, Loss: 1.8596279317140578\n",
      "Epoch 24, Batch 200, Loss: 1.859447154402733\n",
      "Epoch 24, Batch 400, Loss: 1.853735652565956\n",
      "Epoch 24, Batch 600, Loss: 1.8603354340791702\n",
      "Epoch 24, Batch 800, Loss: 1.880713750720024\n",
      "Epoch 25, Batch 200, Loss: 1.8673723649978637\n",
      "Epoch 25, Batch 400, Loss: 1.8607508623600006\n",
      "Epoch 25, Batch 600, Loss: 1.8482073593139647\n",
      "Epoch 25, Batch 800, Loss: 1.8547889745235444\n",
      "Accuracy on test set: 0.7625%\n",
      "Fitting for combination 6\n",
      "784\n",
      "1\n",
      "10\n",
      "[20, 10]\n",
      "True\n",
      "['relu']\n",
      "Adam\n",
      "0.1\n",
      "0.3\n",
      "CrossEntropyLoss\n",
      "100\n",
      "25\n",
      "0\n",
      "Epoch 1, Batch 100, Loss: 2.149243947267532\n",
      "Epoch 1, Batch 200, Loss: 2.072004916667938\n",
      "Epoch 1, Batch 300, Loss: 2.847457844018936\n",
      "Epoch 1, Batch 400, Loss: 1.8803594422340393\n",
      "Epoch 1, Batch 500, Loss: 1.869934858083725\n",
      "Epoch 1, Batch 600, Loss: 3.0184800362586977\n",
      "Epoch 1, Batch 700, Loss: 3.5795218443870542\n",
      "Epoch 1, Batch 800, Loss: 1.8992490553855896\n",
      "Epoch 1, Batch 900, Loss: 1.5072976803779603\n",
      "Epoch 2, Batch 100, Loss: 1.574684191942215\n",
      "Epoch 2, Batch 200, Loss: 1.8285552203655242\n",
      "Epoch 2, Batch 300, Loss: 1.9517815971374513\n",
      "Epoch 2, Batch 400, Loss: 4.675935208797455\n",
      "Epoch 2, Batch 500, Loss: 2.169944202899933\n",
      "Epoch 2, Batch 600, Loss: 1.6299691200256348\n",
      "Epoch 2, Batch 700, Loss: 1.5740218317508698\n",
      "Epoch 2, Batch 800, Loss: 1.6709275686740874\n",
      "Epoch 2, Batch 900, Loss: 1.724606465101242\n",
      "Epoch 3, Batch 100, Loss: 2.187154760360718\n",
      "Epoch 3, Batch 200, Loss: 2.9004370379447937\n",
      "Epoch 3, Batch 300, Loss: 2.710180643796921\n",
      "Epoch 3, Batch 400, Loss: 2.914173871278763\n",
      "Epoch 3, Batch 500, Loss: 3.631801017522812\n",
      "Epoch 3, Batch 600, Loss: 1.6220318746566773\n",
      "Epoch 3, Batch 700, Loss: 1.552501199245453\n",
      "Epoch 3, Batch 800, Loss: 1.580223799943924\n",
      "Epoch 3, Batch 900, Loss: 1.6349598479270935\n",
      "Epoch 4, Batch 100, Loss: 1.9345497345924378\n",
      "Epoch 4, Batch 200, Loss: 3.5437107610702516\n",
      "Epoch 4, Batch 300, Loss: 2.654351975917816\n",
      "Epoch 4, Batch 400, Loss: 1.9953359055519104\n",
      "Epoch 4, Batch 500, Loss: 1.7368846321105957\n",
      "Epoch 4, Batch 600, Loss: 1.911402302980423\n",
      "Epoch 4, Batch 700, Loss: 3.1767276227474213\n",
      "Epoch 4, Batch 800, Loss: 2.339491730928421\n",
      "Epoch 4, Batch 900, Loss: 1.7167797231674193\n",
      "Epoch 5, Batch 100, Loss: 1.860771906375885\n",
      "Epoch 5, Batch 200, Loss: 2.1776050412654877\n",
      "Epoch 5, Batch 300, Loss: 2.448346894979477\n",
      "Epoch 5, Batch 400, Loss: 2.872084845304489\n",
      "Epoch 5, Batch 500, Loss: 2.8384606873989107\n",
      "Epoch 5, Batch 600, Loss: 1.9841917085647582\n",
      "Epoch 5, Batch 700, Loss: 1.8672567749023437\n",
      "Epoch 5, Batch 800, Loss: 1.7181994938850402\n",
      "Epoch 5, Batch 900, Loss: 1.778959835767746\n",
      "Epoch 6, Batch 100, Loss: 2.227909792661667\n",
      "Epoch 6, Batch 200, Loss: 1.9122759926319122\n",
      "Epoch 6, Batch 300, Loss: 2.420810942649841\n",
      "Epoch 6, Batch 400, Loss: 3.6989045667648317\n",
      "Epoch 6, Batch 500, Loss: 3.1417100870609285\n",
      "Epoch 6, Batch 600, Loss: 1.9074998819828033\n",
      "Epoch 6, Batch 700, Loss: 1.5430578684806824\n",
      "Epoch 6, Batch 800, Loss: 1.6170529735088348\n",
      "Epoch 6, Batch 900, Loss: 1.6786306190490723\n",
      "Epoch 7, Batch 100, Loss: 1.9901593852043151\n",
      "Epoch 7, Batch 200, Loss: 3.7200631606578827\n",
      "Epoch 7, Batch 300, Loss: 2.4264235985279083\n",
      "Epoch 7, Batch 400, Loss: 1.696220577955246\n",
      "Epoch 7, Batch 500, Loss: 1.8296373963356019\n",
      "Epoch 7, Batch 600, Loss: 1.777834621667862\n",
      "Epoch 7, Batch 700, Loss: 2.7312753975391386\n",
      "Epoch 7, Batch 800, Loss: 3.3076727735996245\n",
      "Epoch 7, Batch 900, Loss: 2.4307118320465086\n",
      "Epoch 8, Batch 100, Loss: 1.6249191522598267\n",
      "Epoch 8, Batch 200, Loss: 1.6182032215595246\n",
      "Epoch 8, Batch 300, Loss: 1.5992849135398866\n",
      "Epoch 8, Batch 400, Loss: 1.8147505617141724\n",
      "Epoch 8, Batch 500, Loss: 2.3920274746417998\n",
      "Epoch 8, Batch 600, Loss: 2.8650561177730562\n",
      "Epoch 8, Batch 700, Loss: 2.858795428276062\n",
      "Epoch 8, Batch 800, Loss: 3.0645913684368136\n",
      "Epoch 8, Batch 900, Loss: 2.5105733489990234\n",
      "Epoch 9, Batch 100, Loss: 1.714373688697815\n",
      "Epoch 9, Batch 200, Loss: 1.637810037136078\n",
      "Epoch 9, Batch 300, Loss: 1.7522290277481078\n",
      "Epoch 9, Batch 400, Loss: 2.1843854117393495\n",
      "Epoch 9, Batch 500, Loss: 1.8353489828109741\n",
      "Epoch 9, Batch 600, Loss: 2.4595503866672517\n",
      "Epoch 9, Batch 700, Loss: 3.38174164891243\n",
      "Epoch 9, Batch 800, Loss: 2.0599780189990997\n",
      "Epoch 9, Batch 900, Loss: 1.9451850414276124\n",
      "Epoch 10, Batch 100, Loss: 2.5299568712711333\n",
      "Epoch 10, Batch 200, Loss: 1.9652754998207091\n",
      "Epoch 10, Batch 300, Loss: 3.033181689977646\n",
      "Epoch 10, Batch 400, Loss: 3.4202142190933227\n",
      "Epoch 10, Batch 500, Loss: 4.161543408632278\n",
      "Epoch 10, Batch 600, Loss: 1.5334746062755584\n",
      "Epoch 10, Batch 700, Loss: 1.5027652645111085\n",
      "Epoch 10, Batch 800, Loss: 1.512443996667862\n",
      "Epoch 10, Batch 900, Loss: 1.6126904451847077\n",
      "Epoch 11, Batch 100, Loss: 1.5668506264686584\n",
      "Epoch 11, Batch 200, Loss: 1.6785563004016877\n",
      "Epoch 11, Batch 300, Loss: 3.0273002648353575\n",
      "Epoch 11, Batch 400, Loss: 3.999127675294876\n",
      "Epoch 11, Batch 500, Loss: 1.6803280520439148\n",
      "Epoch 11, Batch 600, Loss: 1.6609117186069489\n",
      "Epoch 11, Batch 700, Loss: 1.6956439292430878\n",
      "Epoch 11, Batch 800, Loss: 1.8275959992408752\n",
      "Epoch 11, Batch 900, Loss: 1.9073653256893157\n",
      "Epoch 12, Batch 100, Loss: 2.7667873775959015\n",
      "Epoch 12, Batch 200, Loss: 2.1710172474384306\n",
      "Epoch 12, Batch 300, Loss: 2.9948879301548006\n",
      "Epoch 12, Batch 400, Loss: 2.007259213924408\n",
      "Epoch 12, Batch 500, Loss: 1.591736397743225\n",
      "Epoch 12, Batch 600, Loss: 1.7115918469429017\n",
      "Epoch 12, Batch 700, Loss: 2.3576473808288574\n",
      "Epoch 12, Batch 800, Loss: 2.95582200884819\n",
      "Epoch 12, Batch 900, Loss: 3.6026809298992157\n",
      "Epoch 13, Batch 100, Loss: 1.775428729057312\n",
      "Epoch 13, Batch 200, Loss: 1.729311660528183\n",
      "Epoch 13, Batch 300, Loss: 1.7125810956954957\n",
      "Epoch 13, Batch 400, Loss: 1.6540335929393768\n",
      "Epoch 13, Batch 500, Loss: 3.9153499138355254\n",
      "Epoch 13, Batch 600, Loss: 3.1849409639835358\n",
      "Epoch 13, Batch 700, Loss: 1.5542207562923431\n",
      "Epoch 13, Batch 800, Loss: 1.4894965076446534\n",
      "Epoch 13, Batch 900, Loss: 1.4850382840633392\n",
      "Epoch 14, Batch 100, Loss: 1.5969769644737244\n",
      "Epoch 14, Batch 200, Loss: 1.7572886860370636\n",
      "Epoch 14, Batch 300, Loss: 1.8015945827960969\n",
      "Epoch 14, Batch 400, Loss: 2.3467389619350434\n",
      "Epoch 14, Batch 500, Loss: 2.582498518228531\n",
      "Epoch 14, Batch 600, Loss: 2.98263156414032\n",
      "Epoch 14, Batch 700, Loss: 4.154419773817063\n",
      "Epoch 14, Batch 800, Loss: 1.652218495607376\n",
      "Epoch 14, Batch 900, Loss: 1.7116455042362213\n",
      "Epoch 15, Batch 100, Loss: 1.8559901201725006\n",
      "Epoch 15, Batch 200, Loss: 2.364491239786148\n",
      "Epoch 15, Batch 300, Loss: 2.085208059549332\n",
      "Epoch 15, Batch 400, Loss: 2.2204740858078003\n",
      "Epoch 15, Batch 500, Loss: 2.0252808356285095\n",
      "Epoch 15, Batch 600, Loss: 1.639388644695282\n",
      "Epoch 15, Batch 700, Loss: 1.7379444551467895\n",
      "Epoch 15, Batch 800, Loss: 2.341896766424179\n",
      "Epoch 15, Batch 900, Loss: 2.2994387483596803\n",
      "Epoch 16, Batch 100, Loss: 2.867718243598938\n",
      "Epoch 16, Batch 200, Loss: 2.788658457994461\n",
      "Epoch 16, Batch 300, Loss: 2.2608838284015658\n",
      "Epoch 16, Batch 400, Loss: 1.7943322348594666\n",
      "Epoch 16, Batch 500, Loss: 1.8312476754188538\n",
      "Epoch 16, Batch 600, Loss: 2.0704761183261873\n",
      "Epoch 16, Batch 700, Loss: 3.0699956393241883\n",
      "Epoch 16, Batch 800, Loss: 3.196970854997635\n",
      "Epoch 16, Batch 900, Loss: 2.5357780718803404\n",
      "Epoch 17, Batch 100, Loss: 1.7549549126625061\n",
      "Epoch 17, Batch 200, Loss: 1.6258116197586059\n",
      "Epoch 17, Batch 300, Loss: 1.7657800149917602\n",
      "Epoch 17, Batch 400, Loss: 1.9175335848331452\n",
      "Epoch 17, Batch 500, Loss: 2.832695459127426\n",
      "Epoch 17, Batch 600, Loss: 3.723958122730255\n",
      "Epoch 17, Batch 700, Loss: 1.9639768528938293\n",
      "Epoch 17, Batch 800, Loss: 1.7255119681358337\n",
      "Epoch 17, Batch 900, Loss: 1.6033272886276244\n",
      "Epoch 18, Batch 100, Loss: 2.4268167078495027\n",
      "Epoch 18, Batch 200, Loss: 2.5022546887397765\n",
      "Epoch 18, Batch 300, Loss: 1.8693570744991304\n",
      "Epoch 18, Batch 400, Loss: 2.377854061126709\n",
      "Epoch 18, Batch 500, Loss: 3.22579957485199\n",
      "Epoch 18, Batch 600, Loss: 2.089802385568619\n",
      "Epoch 18, Batch 700, Loss: 1.8971211075782777\n",
      "Epoch 18, Batch 800, Loss: 1.9272009015083313\n",
      "Epoch 18, Batch 900, Loss: 2.170254830121994\n",
      "Epoch 19, Batch 100, Loss: 2.72109139919281\n",
      "Epoch 19, Batch 200, Loss: 2.1455813360214235\n",
      "Epoch 19, Batch 300, Loss: 2.2663101315498353\n",
      "Epoch 19, Batch 400, Loss: 2.893259892463684\n",
      "Epoch 19, Batch 500, Loss: 1.7914688050746919\n",
      "Epoch 19, Batch 600, Loss: 1.8522149240970611\n",
      "Epoch 19, Batch 700, Loss: 1.6715528106689452\n",
      "Epoch 19, Batch 800, Loss: 2.8406260907649994\n",
      "Epoch 19, Batch 900, Loss: 2.412612761259079\n",
      "Epoch 20, Batch 100, Loss: 2.7845190465450287\n",
      "Epoch 20, Batch 200, Loss: 1.948827406167984\n",
      "Epoch 20, Batch 300, Loss: 1.7036753141880034\n",
      "Epoch 20, Batch 400, Loss: 1.861700451374054\n",
      "Epoch 20, Batch 500, Loss: 1.9428985738754272\n",
      "Epoch 20, Batch 600, Loss: 3.611206977367401\n",
      "Epoch 20, Batch 700, Loss: 2.091700837612152\n",
      "Epoch 20, Batch 800, Loss: 1.8663566040992736\n",
      "Epoch 20, Batch 900, Loss: 2.0173868894577027\n",
      "Epoch 21, Batch 100, Loss: 1.9458477151393891\n",
      "Epoch 21, Batch 200, Loss: 1.8610098671913147\n",
      "Epoch 21, Batch 300, Loss: 3.03446537733078\n",
      "Epoch 21, Batch 400, Loss: 1.9518605077266693\n",
      "Epoch 21, Batch 500, Loss: 2.185727251768112\n",
      "Epoch 21, Batch 600, Loss: 2.7346204888820647\n",
      "Epoch 21, Batch 700, Loss: 1.6991121757030487\n",
      "Epoch 21, Batch 800, Loss: 2.293484479188919\n",
      "Epoch 21, Batch 900, Loss: 1.8425068843364716\n",
      "Epoch 22, Batch 100, Loss: 1.8882844924926758\n",
      "Epoch 22, Batch 200, Loss: 4.308949936628341\n",
      "Epoch 22, Batch 300, Loss: 2.0901853573322295\n",
      "Epoch 22, Batch 400, Loss: 1.6794824779033661\n",
      "Epoch 22, Batch 500, Loss: 2.0602225184440615\n",
      "Epoch 22, Batch 600, Loss: 1.737850822210312\n",
      "Epoch 22, Batch 700, Loss: 3.5034267032146453\n",
      "Epoch 22, Batch 800, Loss: 3.390842264890671\n",
      "Epoch 22, Batch 900, Loss: 2.3288471806049347\n",
      "Epoch 23, Batch 100, Loss: 1.6299965643882752\n",
      "Epoch 23, Batch 200, Loss: 1.6490876877307892\n",
      "Epoch 23, Batch 300, Loss: 1.9030017483234405\n",
      "Epoch 23, Batch 400, Loss: 2.417816371917725\n",
      "Epoch 23, Batch 500, Loss: 1.7269958686828613\n",
      "Epoch 23, Batch 600, Loss: 1.8229886507987976\n",
      "Epoch 23, Batch 700, Loss: 2.1754973697662354\n",
      "Epoch 23, Batch 800, Loss: 1.7909475946426392\n",
      "Epoch 23, Batch 900, Loss: 2.2745730555057526\n",
      "Epoch 24, Batch 100, Loss: 4.251784101724625\n",
      "Epoch 24, Batch 200, Loss: 2.505919338464737\n",
      "Epoch 24, Batch 300, Loss: 1.545859124660492\n",
      "Epoch 24, Batch 400, Loss: 1.6733781135082244\n",
      "Epoch 24, Batch 500, Loss: 1.5852581036090851\n",
      "Epoch 24, Batch 600, Loss: 1.7671772348880768\n",
      "Epoch 24, Batch 700, Loss: 4.116317723989487\n",
      "Epoch 24, Batch 800, Loss: 2.9929757726192476\n",
      "Epoch 24, Batch 900, Loss: 1.5790155231952667\n",
      "Epoch 25, Batch 100, Loss: 1.5956419730186462\n",
      "Epoch 25, Batch 200, Loss: 1.8679819226264953\n",
      "Epoch 25, Batch 300, Loss: 1.632045613527298\n",
      "Epoch 25, Batch 400, Loss: 1.745298194885254\n",
      "Epoch 25, Batch 500, Loss: 1.9516289019584656\n",
      "Epoch 25, Batch 600, Loss: 3.3381932377815247\n",
      "Epoch 25, Batch 700, Loss: 3.1058483493328093\n",
      "Epoch 25, Batch 800, Loss: 2.242658815383911\n",
      "Epoch 25, Batch 900, Loss: 1.7756395196914674\n",
      "Accuracy on test set: 0.3688%\n",
      "Fitting for combination 7\n",
      "784\n",
      "1\n",
      "10\n",
      "[20, 10]\n",
      "False\n",
      "['sigmoid']\n",
      "SGD\n",
      "0.3\n",
      "0.03\n",
      "CrossEntropyLoss\n",
      "50\n",
      "25\n",
      "0\n",
      "Epoch 1, Batch 50, Loss: 0.8510638916492462\n",
      "Epoch 1, Batch 100, Loss: 0.6184507036209106\n",
      "Epoch 1, Batch 150, Loss: 0.5478316873311997\n",
      "Epoch 1, Batch 200, Loss: 0.5212664675712585\n",
      "Epoch 1, Batch 250, Loss: 0.5152295696735382\n",
      "Epoch 1, Batch 300, Loss: 0.5061577343940735\n",
      "Epoch 1, Batch 350, Loss: 0.49547343552112577\n",
      "Epoch 1, Batch 400, Loss: 0.49276557087898254\n",
      "Epoch 1, Batch 450, Loss: 0.49481951057910917\n",
      "Epoch 1, Batch 500, Loss: 0.4848622626066208\n",
      "Epoch 1, Batch 550, Loss: 0.48863131999969484\n",
      "Epoch 1, Batch 600, Loss: 0.4861738634109497\n",
      "Epoch 1, Batch 650, Loss: 0.4889481675624847\n",
      "Epoch 1, Batch 700, Loss: 0.48341447234153745\n",
      "Epoch 1, Batch 750, Loss: 0.4901068550348282\n",
      "Epoch 1, Batch 800, Loss: 0.48971942722797396\n",
      "Epoch 1, Batch 850, Loss: 0.48513498187065124\n",
      "Epoch 1, Batch 900, Loss: 0.4795065689086914\n",
      "Epoch 2, Batch 50, Loss: 0.4751198238134384\n",
      "Epoch 2, Batch 100, Loss: 0.49182086110115053\n",
      "Epoch 2, Batch 150, Loss: 0.48886067152023316\n",
      "Epoch 2, Batch 200, Loss: 0.4957876318693161\n",
      "Epoch 2, Batch 250, Loss: 0.4810545361042023\n",
      "Epoch 2, Batch 300, Loss: 0.48203197956085203\n",
      "Epoch 2, Batch 350, Loss: 0.4815923202037811\n",
      "Epoch 2, Batch 400, Loss: 0.4795942795276642\n",
      "Epoch 2, Batch 450, Loss: 0.4843289017677307\n",
      "Epoch 2, Batch 500, Loss: 0.4874792969226837\n",
      "Epoch 2, Batch 550, Loss: 0.47388351142406465\n",
      "Epoch 2, Batch 600, Loss: 0.49079369962215424\n",
      "Epoch 2, Batch 650, Loss: 0.4804349637031555\n",
      "Epoch 2, Batch 700, Loss: 0.4809199649095535\n",
      "Epoch 2, Batch 750, Loss: 0.48179747104644777\n",
      "Epoch 2, Batch 800, Loss: 0.4957219135761261\n",
      "Epoch 2, Batch 850, Loss: 0.4803354781866074\n",
      "Epoch 2, Batch 900, Loss: 0.48330089628696443\n",
      "Epoch 3, Batch 50, Loss: 0.4814284747838974\n",
      "Epoch 3, Batch 100, Loss: 0.4806892490386963\n",
      "Epoch 3, Batch 150, Loss: 0.4902717822790146\n",
      "Epoch 3, Batch 200, Loss: 0.48044546484947204\n",
      "Epoch 3, Batch 250, Loss: 0.48434052884578704\n",
      "Epoch 3, Batch 300, Loss: 0.48300705790519716\n",
      "Epoch 3, Batch 350, Loss: 0.47608108818531036\n",
      "Epoch 3, Batch 400, Loss: 0.47691388726234435\n",
      "Epoch 3, Batch 450, Loss: 0.47747758150100705\n",
      "Epoch 3, Batch 500, Loss: 0.48408174455165864\n",
      "Epoch 3, Batch 550, Loss: 0.4833517974615097\n",
      "Epoch 3, Batch 600, Loss: 0.48813691377639773\n",
      "Epoch 3, Batch 650, Loss: 0.48071330666542056\n",
      "Epoch 3, Batch 700, Loss: 0.48352301120758057\n",
      "Epoch 3, Batch 750, Loss: 0.4770592629909515\n",
      "Epoch 3, Batch 800, Loss: 0.480744571685791\n",
      "Epoch 3, Batch 850, Loss: 0.4809195375442505\n",
      "Epoch 3, Batch 900, Loss: 0.48277524173259734\n",
      "Epoch 4, Batch 50, Loss: 0.47695395529270174\n",
      "Epoch 4, Batch 100, Loss: 0.4794122987985611\n",
      "Epoch 4, Batch 150, Loss: 0.4847477912902832\n",
      "Epoch 4, Batch 200, Loss: 0.4852010422945023\n",
      "Epoch 4, Batch 250, Loss: 0.4841158390045166\n",
      "Epoch 4, Batch 300, Loss: 0.48432776689529417\n",
      "Epoch 4, Batch 350, Loss: 0.4819007658958435\n",
      "Epoch 4, Batch 400, Loss: 0.4873443388938904\n",
      "Epoch 4, Batch 450, Loss: 0.4791646337509155\n",
      "Epoch 4, Batch 500, Loss: 0.4823837792873383\n",
      "Epoch 4, Batch 550, Loss: 0.4772348266839981\n",
      "Epoch 4, Batch 600, Loss: 0.4853625649213791\n",
      "Epoch 4, Batch 650, Loss: 0.47340902507305144\n",
      "Epoch 4, Batch 700, Loss: 0.489671089053154\n",
      "Epoch 4, Batch 750, Loss: 0.4742747151851654\n",
      "Epoch 4, Batch 800, Loss: 0.4797453612089157\n",
      "Epoch 4, Batch 850, Loss: 0.47400372207164765\n",
      "Epoch 4, Batch 900, Loss: 0.4800520646572113\n",
      "Epoch 5, Batch 50, Loss: 0.4792600286006927\n",
      "Epoch 5, Batch 100, Loss: 0.48646078824996947\n",
      "Epoch 5, Batch 150, Loss: 0.4767467999458313\n",
      "Epoch 5, Batch 200, Loss: 0.48909711122512817\n",
      "Epoch 5, Batch 250, Loss: 0.4825085824728012\n",
      "Epoch 5, Batch 300, Loss: 0.4852575886249542\n",
      "Epoch 5, Batch 350, Loss: 0.4860203266143799\n",
      "Epoch 5, Batch 400, Loss: 0.47894555747509004\n",
      "Epoch 5, Batch 450, Loss: 0.47077175736427307\n",
      "Epoch 5, Batch 500, Loss: 0.47775812208652496\n",
      "Epoch 5, Batch 550, Loss: 0.4680830144882202\n",
      "Epoch 5, Batch 600, Loss: 0.4711588364839554\n",
      "Epoch 5, Batch 650, Loss: 0.4709270280599594\n",
      "Epoch 5, Batch 700, Loss: 0.4805238205194473\n",
      "Epoch 5, Batch 750, Loss: 0.4779204976558685\n",
      "Epoch 5, Batch 800, Loss: 0.4758284872770309\n",
      "Epoch 5, Batch 850, Loss: 0.49044930696487427\n",
      "Epoch 5, Batch 900, Loss: 0.49463989198207853\n",
      "Epoch 6, Batch 50, Loss: 0.4825465697050095\n",
      "Epoch 6, Batch 100, Loss: 0.4750295197963715\n",
      "Epoch 6, Batch 150, Loss: 0.4831827110052109\n",
      "Epoch 6, Batch 200, Loss: 0.478129523396492\n",
      "Epoch 6, Batch 250, Loss: 0.48675931990146637\n",
      "Epoch 6, Batch 300, Loss: 0.4806339693069458\n",
      "Epoch 6, Batch 350, Loss: 0.47228471159935\n",
      "Epoch 6, Batch 400, Loss: 0.48210886418819426\n",
      "Epoch 6, Batch 450, Loss: 0.47727423667907715\n",
      "Epoch 6, Batch 500, Loss: 0.47857013404369353\n",
      "Epoch 6, Batch 550, Loss: 0.4825181001424789\n",
      "Epoch 6, Batch 600, Loss: 0.4863597416877747\n",
      "Epoch 6, Batch 650, Loss: 0.470254989862442\n",
      "Epoch 6, Batch 700, Loss: 0.4909980094432831\n",
      "Epoch 6, Batch 750, Loss: 0.482726149559021\n",
      "Epoch 6, Batch 800, Loss: 0.4750503790378571\n",
      "Epoch 6, Batch 850, Loss: 0.47654959082603454\n",
      "Epoch 6, Batch 900, Loss: 0.4832796996831894\n",
      "Epoch 7, Batch 50, Loss: 0.4860933691263199\n",
      "Epoch 7, Batch 100, Loss: 0.4748037415742874\n",
      "Epoch 7, Batch 150, Loss: 0.476814706325531\n",
      "Epoch 7, Batch 200, Loss: 0.4787113720178604\n",
      "Epoch 7, Batch 250, Loss: 0.47464892745018006\n",
      "Epoch 7, Batch 300, Loss: 0.46700622141361237\n",
      "Epoch 7, Batch 350, Loss: 0.48017428755760194\n",
      "Epoch 7, Batch 400, Loss: 0.485542910695076\n",
      "Epoch 7, Batch 450, Loss: 0.47839755594730377\n",
      "Epoch 7, Batch 500, Loss: 0.4753509384393692\n",
      "Epoch 7, Batch 550, Loss: 0.4837932735681534\n",
      "Epoch 7, Batch 600, Loss: 0.48150279343128205\n",
      "Epoch 7, Batch 650, Loss: 0.4861171013116837\n",
      "Epoch 7, Batch 700, Loss: 0.4827859741449356\n",
      "Epoch 7, Batch 750, Loss: 0.48633606791496276\n",
      "Epoch 7, Batch 800, Loss: 0.48534292340278623\n",
      "Epoch 7, Batch 850, Loss: 0.4729805338382721\n",
      "Epoch 7, Batch 900, Loss: 0.4912128186225891\n",
      "Epoch 8, Batch 50, Loss: 0.47740846157073974\n",
      "Epoch 8, Batch 100, Loss: 0.477436717748642\n",
      "Epoch 8, Batch 150, Loss: 0.4779923796653748\n",
      "Epoch 8, Batch 200, Loss: 0.475635729432106\n",
      "Epoch 8, Batch 250, Loss: 0.4733993190526962\n",
      "Epoch 8, Batch 300, Loss: 0.47832579255104063\n",
      "Epoch 8, Batch 350, Loss: 0.4952733558416367\n",
      "Epoch 8, Batch 400, Loss: 0.4881385934352875\n",
      "Epoch 8, Batch 450, Loss: 0.4853660202026367\n",
      "Epoch 8, Batch 500, Loss: 0.4786404377222061\n",
      "Epoch 8, Batch 550, Loss: 0.48793288886547087\n",
      "Epoch 8, Batch 600, Loss: 0.4819996577501297\n",
      "Epoch 8, Batch 650, Loss: 0.47621108531951906\n",
      "Epoch 8, Batch 700, Loss: 0.48470729231834414\n",
      "Epoch 8, Batch 750, Loss: 0.4808746272325516\n",
      "Epoch 8, Batch 800, Loss: 0.47028601050376895\n",
      "Epoch 8, Batch 850, Loss: 0.4780683332681656\n",
      "Epoch 8, Batch 900, Loss: 0.48009138941764834\n",
      "Epoch 9, Batch 50, Loss: 0.4802543169260025\n",
      "Epoch 9, Batch 100, Loss: 0.47222999095916746\n",
      "Epoch 9, Batch 150, Loss: 0.48465147852897644\n",
      "Epoch 9, Batch 200, Loss: 0.48267681777477267\n",
      "Epoch 9, Batch 250, Loss: 0.4830499744415283\n",
      "Epoch 9, Batch 300, Loss: 0.48925924897193906\n",
      "Epoch 9, Batch 350, Loss: 0.4738136702775955\n",
      "Epoch 9, Batch 400, Loss: 0.4737537783384323\n",
      "Epoch 9, Batch 450, Loss: 0.4840138953924179\n",
      "Epoch 9, Batch 500, Loss: 0.47900488674640657\n",
      "Epoch 9, Batch 550, Loss: 0.48196806132793424\n",
      "Epoch 9, Batch 600, Loss: 0.4762362587451935\n",
      "Epoch 9, Batch 650, Loss: 0.4884129178524017\n",
      "Epoch 9, Batch 700, Loss: 0.48038746893405915\n",
      "Epoch 9, Batch 750, Loss: 0.477901092171669\n",
      "Epoch 9, Batch 800, Loss: 0.48316556930541993\n",
      "Epoch 9, Batch 850, Loss: 0.4781357204914093\n",
      "Epoch 9, Batch 900, Loss: 0.4755771732330322\n",
      "Epoch 10, Batch 50, Loss: 0.4795796447992325\n",
      "Epoch 10, Batch 100, Loss: 0.4880962699651718\n",
      "Epoch 10, Batch 150, Loss: 0.4821076875925064\n",
      "Epoch 10, Batch 200, Loss: 0.48230106353759766\n",
      "Epoch 10, Batch 250, Loss: 0.47852125704288484\n",
      "Epoch 10, Batch 300, Loss: 0.47908092617988585\n",
      "Epoch 10, Batch 350, Loss: 0.4763134628534317\n",
      "Epoch 10, Batch 400, Loss: 0.4778300642967224\n",
      "Epoch 10, Batch 450, Loss: 0.4919497978687286\n",
      "Epoch 10, Batch 500, Loss: 0.4755182802677155\n",
      "Epoch 10, Batch 550, Loss: 0.47188139855861666\n",
      "Epoch 10, Batch 600, Loss: 0.47772379100322726\n",
      "Epoch 10, Batch 650, Loss: 0.46796521127223967\n",
      "Epoch 10, Batch 700, Loss: 0.4772734123468399\n",
      "Epoch 10, Batch 750, Loss: 0.48052555084228515\n",
      "Epoch 10, Batch 800, Loss: 0.4830003035068512\n",
      "Epoch 10, Batch 850, Loss: 0.48422032594680786\n",
      "Epoch 10, Batch 900, Loss: 0.48306327402591703\n",
      "Epoch 11, Batch 50, Loss: 0.4843880021572113\n",
      "Epoch 11, Batch 100, Loss: 0.48493467807769775\n",
      "Epoch 11, Batch 150, Loss: 0.47559774935245513\n",
      "Epoch 11, Batch 200, Loss: 0.48739448249340056\n",
      "Epoch 11, Batch 250, Loss: 0.47649137616157533\n",
      "Epoch 11, Batch 300, Loss: 0.47947457551956174\n",
      "Epoch 11, Batch 350, Loss: 0.4825920432806015\n",
      "Epoch 11, Batch 400, Loss: 0.4703878962993622\n",
      "Epoch 11, Batch 450, Loss: 0.48676549673080444\n",
      "Epoch 11, Batch 500, Loss: 0.4808052313327789\n",
      "Epoch 11, Batch 550, Loss: 0.480044304728508\n",
      "Epoch 11, Batch 600, Loss: 0.47397188901901244\n",
      "Epoch 11, Batch 650, Loss: 0.4806839483976364\n",
      "Epoch 11, Batch 700, Loss: 0.4794259035587311\n",
      "Epoch 11, Batch 750, Loss: 0.4746500247716904\n",
      "Epoch 11, Batch 800, Loss: 0.48296910405159\n",
      "Epoch 11, Batch 850, Loss: 0.47887431919574736\n",
      "Epoch 11, Batch 900, Loss: 0.48534857869148257\n",
      "Epoch 12, Batch 50, Loss: 0.4784106558561325\n",
      "Epoch 12, Batch 100, Loss: 0.48030388474464414\n",
      "Epoch 12, Batch 150, Loss: 0.4830317449569702\n",
      "Epoch 12, Batch 200, Loss: 0.4743304491043091\n",
      "Epoch 12, Batch 250, Loss: 0.4846331173181534\n",
      "Epoch 12, Batch 300, Loss: 0.4843083369731903\n",
      "Epoch 12, Batch 350, Loss: 0.4848016321659088\n",
      "Epoch 12, Batch 400, Loss: 0.4739241999387741\n",
      "Epoch 12, Batch 450, Loss: 0.48347096920013427\n",
      "Epoch 12, Batch 500, Loss: 0.4753821504116058\n",
      "Epoch 12, Batch 550, Loss: 0.4742199635505676\n",
      "Epoch 12, Batch 600, Loss: 0.4878952693939209\n",
      "Epoch 12, Batch 650, Loss: 0.4858108526468277\n",
      "Epoch 12, Batch 700, Loss: 0.474693575501442\n",
      "Epoch 12, Batch 750, Loss: 0.4846358197927475\n",
      "Epoch 12, Batch 800, Loss: 0.4873605626821518\n",
      "Epoch 12, Batch 850, Loss: 0.4789088863134384\n",
      "Epoch 12, Batch 900, Loss: 0.4719634115695953\n",
      "Epoch 13, Batch 50, Loss: 0.4751879948377609\n",
      "Epoch 13, Batch 100, Loss: 0.46996700584888457\n",
      "Epoch 13, Batch 150, Loss: 0.4827953404188156\n",
      "Epoch 13, Batch 200, Loss: 0.48622172117233275\n",
      "Epoch 13, Batch 250, Loss: 0.4753098511695862\n",
      "Epoch 13, Batch 300, Loss: 0.48211590766906737\n",
      "Epoch 13, Batch 350, Loss: 0.4912115472555161\n",
      "Epoch 13, Batch 400, Loss: 0.48258050680160525\n",
      "Epoch 13, Batch 450, Loss: 0.4691081011295319\n",
      "Epoch 13, Batch 500, Loss: 0.48363164603710174\n",
      "Epoch 13, Batch 550, Loss: 0.49076140820980074\n",
      "Epoch 13, Batch 600, Loss: 0.480827015042305\n",
      "Epoch 13, Batch 650, Loss: 0.4734836041927338\n",
      "Epoch 13, Batch 700, Loss: 0.47717343270778656\n",
      "Epoch 13, Batch 750, Loss: 0.47343312740325927\n",
      "Epoch 13, Batch 800, Loss: 0.48904940247535705\n",
      "Epoch 13, Batch 850, Loss: 0.4880299913883209\n",
      "Epoch 13, Batch 900, Loss: 0.47588712751865386\n",
      "Epoch 14, Batch 50, Loss: 0.487040855884552\n",
      "Epoch 14, Batch 100, Loss: 0.48345622956752776\n",
      "Epoch 14, Batch 150, Loss: 0.470258127450943\n",
      "Epoch 14, Batch 200, Loss: 0.4801157009601593\n",
      "Epoch 14, Batch 250, Loss: 0.46795893311500547\n",
      "Epoch 14, Batch 300, Loss: 0.4782056826353073\n",
      "Epoch 14, Batch 350, Loss: 0.485079625248909\n",
      "Epoch 14, Batch 400, Loss: 0.47775117814540863\n",
      "Epoch 14, Batch 450, Loss: 0.4784502083063126\n",
      "Epoch 14, Batch 500, Loss: 0.4868994998931885\n",
      "Epoch 14, Batch 550, Loss: 0.4763698434829712\n",
      "Epoch 14, Batch 600, Loss: 0.47625775456428526\n",
      "Epoch 14, Batch 650, Loss: 0.4854292565584183\n",
      "Epoch 14, Batch 700, Loss: 0.48079403042793273\n",
      "Epoch 14, Batch 750, Loss: 0.4872294217348099\n",
      "Epoch 14, Batch 800, Loss: 0.4931305307149887\n",
      "Epoch 14, Batch 850, Loss: 0.473760684132576\n",
      "Epoch 14, Batch 900, Loss: 0.4775097489356995\n",
      "Epoch 15, Batch 50, Loss: 0.48068956911563876\n",
      "Epoch 15, Batch 100, Loss: 0.48295943200588226\n",
      "Epoch 15, Batch 150, Loss: 0.47532669961452484\n",
      "Epoch 15, Batch 200, Loss: 0.4780159443616867\n",
      "Epoch 15, Batch 250, Loss: 0.48572025179862977\n",
      "Epoch 15, Batch 300, Loss: 0.47843361914157867\n",
      "Epoch 15, Batch 350, Loss: 0.4726833921670914\n",
      "Epoch 15, Batch 400, Loss: 0.4811822384595871\n",
      "Epoch 15, Batch 450, Loss: 0.4768936812877655\n",
      "Epoch 15, Batch 500, Loss: 0.479068346619606\n",
      "Epoch 15, Batch 550, Loss: 0.47473312556743624\n",
      "Epoch 15, Batch 600, Loss: 0.4853328460454941\n",
      "Epoch 15, Batch 650, Loss: 0.4820873707532883\n",
      "Epoch 15, Batch 700, Loss: 0.48215039253234865\n",
      "Epoch 15, Batch 750, Loss: 0.4812937635183334\n",
      "Epoch 15, Batch 800, Loss: 0.48551495671272277\n",
      "Epoch 15, Batch 850, Loss: 0.4823001378774643\n",
      "Epoch 15, Batch 900, Loss: 0.4836828792095184\n",
      "Epoch 16, Batch 50, Loss: 0.48996837496757506\n",
      "Epoch 16, Batch 100, Loss: 0.49063833951950075\n",
      "Epoch 16, Batch 150, Loss: 0.477027884721756\n",
      "Epoch 16, Batch 200, Loss: 0.4783119088411331\n",
      "Epoch 16, Batch 250, Loss: 0.48557608962059023\n",
      "Epoch 16, Batch 300, Loss: 0.4896387851238251\n",
      "Epoch 16, Batch 350, Loss: 0.4791640990972519\n",
      "Epoch 16, Batch 400, Loss: 0.4767729538679123\n",
      "Epoch 16, Batch 450, Loss: 0.48370319068431855\n",
      "Epoch 16, Batch 500, Loss: 0.4819664698839188\n",
      "Epoch 16, Batch 550, Loss: 0.48050609946250916\n",
      "Epoch 16, Batch 600, Loss: 0.4686902451515198\n",
      "Epoch 16, Batch 650, Loss: 0.4709335869550705\n",
      "Epoch 16, Batch 700, Loss: 0.47966964066028595\n",
      "Epoch 16, Batch 750, Loss: 0.4667693591117859\n",
      "Epoch 16, Batch 800, Loss: 0.4755057466030121\n",
      "Epoch 16, Batch 850, Loss: 0.48040888011455535\n",
      "Epoch 16, Batch 900, Loss: 0.47989190578460694\n",
      "Epoch 17, Batch 50, Loss: 0.4910227906703949\n",
      "Epoch 17, Batch 100, Loss: 0.48152334153652193\n",
      "Epoch 17, Batch 150, Loss: 0.4849740904569626\n",
      "Epoch 17, Batch 200, Loss: 0.4716162669658661\n",
      "Epoch 17, Batch 250, Loss: 0.47276899695396424\n",
      "Epoch 17, Batch 300, Loss: 0.480713369846344\n",
      "Epoch 17, Batch 350, Loss: 0.4779547560214996\n",
      "Epoch 17, Batch 400, Loss: 0.47992522895336154\n",
      "Epoch 17, Batch 450, Loss: 0.4815018630027771\n",
      "Epoch 17, Batch 500, Loss: 0.4829330962896347\n",
      "Epoch 17, Batch 550, Loss: 0.4761460113525391\n",
      "Epoch 17, Batch 600, Loss: 0.48281647443771364\n",
      "Epoch 17, Batch 650, Loss: 0.47291663348674773\n",
      "Epoch 17, Batch 700, Loss: 0.4922630661725998\n",
      "Epoch 17, Batch 750, Loss: 0.48277906656265257\n",
      "Epoch 17, Batch 800, Loss: 0.46935887694358824\n",
      "Epoch 17, Batch 850, Loss: 0.4732180601358414\n",
      "Epoch 17, Batch 900, Loss: 0.4792682671546936\n",
      "Epoch 18, Batch 50, Loss: 0.48203277826309204\n",
      "Epoch 18, Batch 100, Loss: 0.4736160659790039\n",
      "Epoch 18, Batch 150, Loss: 0.4816670799255371\n",
      "Epoch 18, Batch 200, Loss: 0.47705875694751737\n",
      "Epoch 18, Batch 250, Loss: 0.4784645861387253\n",
      "Epoch 18, Batch 300, Loss: 0.4791974830627441\n",
      "Epoch 18, Batch 350, Loss: 0.4715817159414291\n",
      "Epoch 18, Batch 400, Loss: 0.48122392416000365\n",
      "Epoch 18, Batch 450, Loss: 0.47844979107379915\n",
      "Epoch 18, Batch 500, Loss: 0.47031789243221284\n",
      "Epoch 18, Batch 550, Loss: 0.4814541757106781\n",
      "Epoch 18, Batch 600, Loss: 0.48467881202697755\n",
      "Epoch 18, Batch 650, Loss: 0.48024505138397217\n",
      "Epoch 18, Batch 700, Loss: 0.49455327689647677\n",
      "Epoch 18, Batch 750, Loss: 0.49022121012210845\n",
      "Epoch 18, Batch 800, Loss: 0.4823481327295303\n",
      "Epoch 18, Batch 850, Loss: 0.48142375707626345\n",
      "Epoch 18, Batch 900, Loss: 0.4795334243774414\n",
      "Epoch 19, Batch 50, Loss: 0.4847017937898636\n",
      "Epoch 19, Batch 100, Loss: 0.4840778690576553\n",
      "Epoch 19, Batch 150, Loss: 0.477669233083725\n",
      "Epoch 19, Batch 200, Loss: 0.48894031703472135\n",
      "Epoch 19, Batch 250, Loss: 0.4736713582277298\n",
      "Epoch 19, Batch 300, Loss: 0.46720973193645476\n",
      "Epoch 19, Batch 350, Loss: 0.477604643702507\n",
      "Epoch 19, Batch 400, Loss: 0.4753513538837433\n",
      "Epoch 19, Batch 450, Loss: 0.46779875695705414\n",
      "Epoch 19, Batch 500, Loss: 0.48715573668479917\n",
      "Epoch 19, Batch 550, Loss: 0.47816439509391784\n",
      "Epoch 19, Batch 600, Loss: 0.47434844136238097\n",
      "Epoch 19, Batch 650, Loss: 0.47521796464920046\n",
      "Epoch 19, Batch 700, Loss: 0.4804502785205841\n",
      "Epoch 19, Batch 750, Loss: 0.48461862504482267\n",
      "Epoch 19, Batch 800, Loss: 0.4774579811096191\n",
      "Epoch 19, Batch 850, Loss: 0.482829629778862\n",
      "Epoch 19, Batch 900, Loss: 0.4930652964115143\n",
      "Epoch 20, Batch 50, Loss: 0.4740349280834198\n",
      "Epoch 20, Batch 100, Loss: 0.475137312412262\n",
      "Epoch 20, Batch 150, Loss: 0.4854793906211853\n",
      "Epoch 20, Batch 200, Loss: 0.4792139959335327\n",
      "Epoch 20, Batch 250, Loss: 0.49146962523460386\n",
      "Epoch 20, Batch 300, Loss: 0.4843429297208786\n",
      "Epoch 20, Batch 350, Loss: 0.4835411417484283\n",
      "Epoch 20, Batch 400, Loss: 0.48023608446121213\n",
      "Epoch 20, Batch 450, Loss: 0.4862238681316376\n",
      "Epoch 20, Batch 500, Loss: 0.47215900421142576\n",
      "Epoch 20, Batch 550, Loss: 0.4756478029489517\n",
      "Epoch 20, Batch 600, Loss: 0.4880619579553604\n",
      "Epoch 20, Batch 650, Loss: 0.4718219894170761\n",
      "Epoch 20, Batch 700, Loss: 0.48664541482925416\n",
      "Epoch 20, Batch 750, Loss: 0.4820921742916107\n",
      "Epoch 20, Batch 800, Loss: 0.4719676047563553\n",
      "Epoch 20, Batch 850, Loss: 0.4774359673261642\n",
      "Epoch 20, Batch 900, Loss: 0.4759954261779785\n",
      "Epoch 21, Batch 50, Loss: 0.48830455660820005\n",
      "Epoch 21, Batch 100, Loss: 0.4737911397218704\n",
      "Epoch 21, Batch 150, Loss: 0.47828672885894774\n",
      "Epoch 21, Batch 200, Loss: 0.47328597426414487\n",
      "Epoch 21, Batch 250, Loss: 0.4745172268152237\n",
      "Epoch 21, Batch 300, Loss: 0.4822692042589188\n",
      "Epoch 21, Batch 350, Loss: 0.4834532105922699\n",
      "Epoch 21, Batch 400, Loss: 0.4841246646642685\n",
      "Epoch 21, Batch 450, Loss: 0.4792063277959824\n",
      "Epoch 21, Batch 500, Loss: 0.4828773128986359\n",
      "Epoch 21, Batch 550, Loss: 0.4836064237356186\n",
      "Epoch 21, Batch 600, Loss: 0.4854905027151108\n",
      "Epoch 21, Batch 650, Loss: 0.4781266713142395\n",
      "Epoch 21, Batch 700, Loss: 0.46546567499637603\n",
      "Epoch 21, Batch 750, Loss: 0.4896024227142334\n",
      "Epoch 21, Batch 800, Loss: 0.4887463676929474\n",
      "Epoch 21, Batch 850, Loss: 0.4714386034011841\n",
      "Epoch 21, Batch 900, Loss: 0.481793093085289\n",
      "Epoch 22, Batch 50, Loss: 0.4843314516544342\n",
      "Epoch 22, Batch 100, Loss: 0.48708066284656526\n",
      "Epoch 22, Batch 150, Loss: 0.47318019688129426\n",
      "Epoch 22, Batch 200, Loss: 0.48801846981048586\n",
      "Epoch 22, Batch 250, Loss: 0.4849697494506836\n",
      "Epoch 22, Batch 300, Loss: 0.4762046027183533\n",
      "Epoch 22, Batch 350, Loss: 0.47520882487297056\n",
      "Epoch 22, Batch 400, Loss: 0.47984842598438265\n",
      "Epoch 22, Batch 450, Loss: 0.4880016154050827\n",
      "Epoch 22, Batch 500, Loss: 0.4787095701694489\n",
      "Epoch 22, Batch 550, Loss: 0.4736703896522522\n",
      "Epoch 22, Batch 600, Loss: 0.48622663497924806\n",
      "Epoch 22, Batch 650, Loss: 0.4783637273311615\n",
      "Epoch 22, Batch 700, Loss: 0.4763126587867737\n",
      "Epoch 22, Batch 750, Loss: 0.4808425182104111\n",
      "Epoch 22, Batch 800, Loss: 0.48119025111198427\n",
      "Epoch 22, Batch 850, Loss: 0.4690406268835068\n",
      "Epoch 22, Batch 900, Loss: 0.4763671261072159\n",
      "Epoch 23, Batch 50, Loss: 0.48349453270435333\n",
      "Epoch 23, Batch 100, Loss: 0.4834146398305893\n",
      "Epoch 23, Batch 150, Loss: 0.48619851648807527\n",
      "Epoch 23, Batch 200, Loss: 0.4970212894678116\n",
      "Epoch 23, Batch 250, Loss: 0.47704197227954864\n",
      "Epoch 23, Batch 300, Loss: 0.47770859360694884\n",
      "Epoch 23, Batch 350, Loss: 0.47437079191207887\n",
      "Epoch 23, Batch 400, Loss: 0.47707400262355804\n",
      "Epoch 23, Batch 450, Loss: 0.4761297309398651\n",
      "Epoch 23, Batch 500, Loss: 0.46970897674560547\n",
      "Epoch 23, Batch 550, Loss: 0.4760291928052902\n",
      "Epoch 23, Batch 600, Loss: 0.4820103120803833\n",
      "Epoch 23, Batch 650, Loss: 0.48670476257801054\n",
      "Epoch 23, Batch 700, Loss: 0.4698690408468246\n",
      "Epoch 23, Batch 750, Loss: 0.49290029108524325\n",
      "Epoch 23, Batch 800, Loss: 0.48017858147621156\n",
      "Epoch 23, Batch 850, Loss: 0.4780265790224075\n",
      "Epoch 23, Batch 900, Loss: 0.4786489397287369\n",
      "Epoch 24, Batch 50, Loss: 0.4839277935028076\n",
      "Epoch 24, Batch 100, Loss: 0.4672857451438904\n",
      "Epoch 24, Batch 150, Loss: 0.47835861444473265\n",
      "Epoch 24, Batch 200, Loss: 0.4762063467502594\n",
      "Epoch 24, Batch 250, Loss: 0.4778676211833954\n",
      "Epoch 24, Batch 300, Loss: 0.4682597893476486\n",
      "Epoch 24, Batch 350, Loss: 0.47809514582157137\n",
      "Epoch 24, Batch 400, Loss: 0.48275837659835813\n",
      "Epoch 24, Batch 450, Loss: 0.4827213722467423\n",
      "Epoch 24, Batch 500, Loss: 0.48833278954029086\n",
      "Epoch 24, Batch 550, Loss: 0.4794605255126953\n",
      "Epoch 24, Batch 600, Loss: 0.48210653245449064\n",
      "Epoch 24, Batch 650, Loss: 0.4825207883119583\n",
      "Epoch 24, Batch 700, Loss: 0.4903239518404007\n",
      "Epoch 24, Batch 750, Loss: 0.47498974978923797\n",
      "Epoch 24, Batch 800, Loss: 0.48087397873401644\n",
      "Epoch 24, Batch 850, Loss: 0.48477657973766325\n",
      "Epoch 24, Batch 900, Loss: 0.48588081777095793\n",
      "Epoch 25, Batch 50, Loss: 0.47599162757396696\n",
      "Epoch 25, Batch 100, Loss: 0.47661350548267367\n",
      "Epoch 25, Batch 150, Loss: 0.4829848492145538\n",
      "Epoch 25, Batch 200, Loss: 0.4850242793560028\n",
      "Epoch 25, Batch 250, Loss: 0.47454668283462526\n",
      "Epoch 25, Batch 300, Loss: 0.4757722753286362\n",
      "Epoch 25, Batch 350, Loss: 0.4812863963842392\n",
      "Epoch 25, Batch 400, Loss: 0.49035804271697997\n",
      "Epoch 25, Batch 450, Loss: 0.48487401723861695\n",
      "Epoch 25, Batch 500, Loss: 0.4764281767606735\n",
      "Epoch 25, Batch 550, Loss: 0.48356946408748624\n",
      "Epoch 25, Batch 600, Loss: 0.4748204904794693\n",
      "Epoch 25, Batch 650, Loss: 0.4778167486190796\n",
      "Epoch 25, Batch 700, Loss: 0.48275244116783145\n",
      "Epoch 25, Batch 750, Loss: 0.47493859112262726\n",
      "Epoch 25, Batch 800, Loss: 0.48360067725181577\n",
      "Epoch 25, Batch 850, Loss: 0.4844208711385727\n",
      "Epoch 25, Batch 900, Loss: 0.4757679110765457\n",
      "Accuracy on test set: 0.7391%\n",
      "Fitting for combination 8\n",
      "784\n",
      "1\n",
      "10\n",
      "[30, 10]\n",
      "False\n",
      "['sigmoid']\n",
      "Adam\n",
      "0.1\n",
      "0.01\n",
      "CrossEntropyLoss\n",
      "200\n",
      "25\n",
      "0\n",
      "Epoch 1, Batch 200, Loss: 2.8857376527786256\n",
      "Epoch 1, Batch 400, Loss: 2.8477650463581083\n",
      "Epoch 1, Batch 600, Loss: 2.8433075833320616\n",
      "Epoch 1, Batch 800, Loss: 2.866293088197708\n",
      "Epoch 2, Batch 200, Loss: 2.8186560559272764\n",
      "Epoch 2, Batch 400, Loss: 2.8602776992321015\n",
      "Epoch 2, Batch 600, Loss: 2.855921667814255\n",
      "Epoch 2, Batch 800, Loss: 2.8844600665569304\n",
      "Epoch 3, Batch 200, Loss: 2.8399231815338135\n",
      "Epoch 3, Batch 400, Loss: 2.9191910040378573\n",
      "Epoch 3, Batch 600, Loss: 2.809478883743286\n",
      "Epoch 3, Batch 800, Loss: 2.8260376393795013\n",
      "Epoch 4, Batch 200, Loss: 2.887772581577301\n",
      "Epoch 4, Batch 400, Loss: 2.918473483324051\n",
      "Epoch 4, Batch 600, Loss: 2.9017912125587464\n",
      "Epoch 4, Batch 800, Loss: 2.909904658794403\n",
      "Epoch 5, Batch 200, Loss: 2.8681742763519287\n",
      "Epoch 5, Batch 400, Loss: 2.8413495683670043\n",
      "Epoch 5, Batch 600, Loss: 2.7840207505226133\n",
      "Epoch 5, Batch 800, Loss: 2.8561926925182344\n",
      "Epoch 6, Batch 200, Loss: 2.9100008034706115\n",
      "Epoch 6, Batch 400, Loss: 2.8334580290317537\n",
      "Epoch 6, Batch 600, Loss: 2.892949231863022\n",
      "Epoch 6, Batch 800, Loss: 2.9046798825263975\n",
      "Epoch 7, Batch 200, Loss: 2.823224595785141\n",
      "Epoch 7, Batch 400, Loss: 2.8568104219436647\n",
      "Epoch 7, Batch 600, Loss: 2.8768105399608612\n",
      "Epoch 7, Batch 800, Loss: 2.9099435305595396\n",
      "Epoch 8, Batch 200, Loss: 2.8508343958854674\n",
      "Epoch 8, Batch 400, Loss: 2.906155959367752\n",
      "Epoch 8, Batch 600, Loss: 2.7793988728523256\n",
      "Epoch 8, Batch 800, Loss: 2.788114849328995\n",
      "Epoch 9, Batch 200, Loss: 2.902148209810257\n",
      "Epoch 9, Batch 400, Loss: 2.787520493268967\n",
      "Epoch 9, Batch 600, Loss: 2.892343546152115\n",
      "Epoch 9, Batch 800, Loss: 2.932001371383667\n",
      "Epoch 10, Batch 200, Loss: 2.8262962329387666\n",
      "Epoch 10, Batch 400, Loss: 2.8310753655433656\n",
      "Epoch 10, Batch 600, Loss: 2.853074204325676\n",
      "Epoch 10, Batch 800, Loss: 2.8872127401828767\n",
      "Epoch 11, Batch 200, Loss: 2.886323720216751\n",
      "Epoch 11, Batch 400, Loss: 2.7931532430648804\n",
      "Epoch 11, Batch 600, Loss: 2.758881227374077\n",
      "Epoch 11, Batch 800, Loss: 2.8918125998973845\n",
      "Epoch 12, Batch 200, Loss: 2.86068573474884\n",
      "Epoch 12, Batch 400, Loss: 2.888403772115707\n",
      "Epoch 12, Batch 600, Loss: 2.8168666017055513\n",
      "Epoch 12, Batch 800, Loss: 2.838951413631439\n",
      "Epoch 13, Batch 200, Loss: 2.813038604259491\n",
      "Epoch 13, Batch 400, Loss: 2.8715367436409\n",
      "Epoch 13, Batch 600, Loss: 2.857644990682602\n",
      "Epoch 13, Batch 800, Loss: 2.8837671089172363\n",
      "Epoch 14, Batch 200, Loss: 2.843883361816406\n",
      "Epoch 14, Batch 400, Loss: 2.9059551548957825\n",
      "Epoch 14, Batch 600, Loss: 2.8013179540634154\n",
      "Epoch 14, Batch 800, Loss: 2.8796921598911287\n",
      "Epoch 15, Batch 200, Loss: 2.858331386446953\n",
      "Epoch 15, Batch 400, Loss: 2.805462462902069\n",
      "Epoch 15, Batch 600, Loss: 2.8277910530567167\n",
      "Epoch 15, Batch 800, Loss: 2.830194548368454\n",
      "Epoch 16, Batch 200, Loss: 2.912371096611023\n",
      "Epoch 16, Batch 400, Loss: 2.811886225938797\n",
      "Epoch 16, Batch 600, Loss: 2.878801805973053\n",
      "Epoch 16, Batch 800, Loss: 2.866872311830521\n",
      "Epoch 17, Batch 200, Loss: 2.8702582359313964\n",
      "Epoch 17, Batch 400, Loss: 3.0022565722465515\n",
      "Epoch 17, Batch 600, Loss: 2.8300050032138824\n",
      "Epoch 17, Batch 800, Loss: 2.8107783818244934\n",
      "Epoch 18, Batch 200, Loss: 2.8243887960910796\n",
      "Epoch 18, Batch 400, Loss: 2.8110497760772706\n",
      "Epoch 18, Batch 600, Loss: 2.9229525184631346\n",
      "Epoch 18, Batch 800, Loss: 2.7783515191078187\n",
      "Epoch 19, Batch 200, Loss: 2.83091472029686\n",
      "Epoch 19, Batch 400, Loss: 2.8120983707904816\n",
      "Epoch 19, Batch 600, Loss: 2.9270346689224245\n",
      "Epoch 19, Batch 800, Loss: 2.895125946998596\n",
      "Epoch 20, Batch 200, Loss: 2.836012406349182\n",
      "Epoch 20, Batch 400, Loss: 2.8979804944992065\n",
      "Epoch 20, Batch 600, Loss: 2.850993709564209\n",
      "Epoch 20, Batch 800, Loss: 2.79709476351738\n",
      "Epoch 21, Batch 200, Loss: 2.780766123533249\n",
      "Epoch 21, Batch 400, Loss: 2.834631561040878\n",
      "Epoch 21, Batch 600, Loss: 2.846388291120529\n",
      "Epoch 21, Batch 800, Loss: 2.8723643624782564\n",
      "Epoch 22, Batch 200, Loss: 2.8905830466747284\n",
      "Epoch 22, Batch 400, Loss: 2.82474951505661\n",
      "Epoch 22, Batch 600, Loss: 2.8451536285877226\n",
      "Epoch 22, Batch 800, Loss: 2.8144965624809264\n",
      "Epoch 23, Batch 200, Loss: 2.823512862920761\n",
      "Epoch 23, Batch 400, Loss: 2.8362027275562287\n",
      "Epoch 23, Batch 600, Loss: 2.83137109875679\n",
      "Epoch 23, Batch 800, Loss: 2.884550359249115\n",
      "Epoch 24, Batch 200, Loss: 2.758202439546585\n",
      "Epoch 24, Batch 400, Loss: 2.9925046944618225\n",
      "Epoch 24, Batch 600, Loss: 2.8509160113334655\n",
      "Epoch 24, Batch 800, Loss: 2.804491465091705\n",
      "Epoch 25, Batch 200, Loss: 2.8736608350276946\n",
      "Epoch 25, Batch 400, Loss: 2.8658421635627747\n",
      "Epoch 25, Batch 600, Loss: 2.9374581837654112\n",
      "Epoch 25, Batch 800, Loss: 2.8799008524417875\n",
      "Accuracy on test set: 0.4276%\n",
      "Fitting for combination 9\n",
      "784\n",
      "1\n",
      "10\n",
      "[30, 10]\n",
      "False\n",
      "['relu']\n",
      "SGD\n",
      "0.01\n",
      "0.01\n",
      "CrossEntropyLoss\n",
      "100\n",
      "25\n",
      "0\n",
      "Epoch 1, Batch 100, Loss: 1.7065825414657594\n",
      "Epoch 1, Batch 200, Loss: 1.0517338436841965\n",
      "Epoch 1, Batch 300, Loss: 0.8303906655311585\n",
      "Epoch 1, Batch 400, Loss: 0.7366327685117722\n",
      "Epoch 1, Batch 500, Loss: 0.682699248790741\n",
      "Epoch 1, Batch 600, Loss: 0.6473564279079437\n",
      "Epoch 1, Batch 700, Loss: 0.6371810787916183\n",
      "Epoch 1, Batch 800, Loss: 0.6008711391687394\n",
      "Epoch 1, Batch 900, Loss: 0.584615889787674\n",
      "Epoch 2, Batch 100, Loss: 0.5825823584198951\n",
      "Epoch 2, Batch 200, Loss: 0.5602064210176468\n",
      "Epoch 2, Batch 300, Loss: 0.5639653286337852\n",
      "Epoch 2, Batch 400, Loss: 0.554790925681591\n",
      "Epoch 2, Batch 500, Loss: 0.5295672869682312\n",
      "Epoch 2, Batch 600, Loss: 0.5314016896486282\n",
      "Epoch 2, Batch 700, Loss: 0.519526641368866\n",
      "Epoch 2, Batch 800, Loss: 0.5418444874882699\n",
      "Epoch 2, Batch 900, Loss: 0.5206016206741333\n",
      "Epoch 3, Batch 100, Loss: 0.5202773642539978\n",
      "Epoch 3, Batch 200, Loss: 0.5177092665433883\n",
      "Epoch 3, Batch 300, Loss: 0.5238470470905304\n",
      "Epoch 3, Batch 400, Loss: 0.497345634996891\n",
      "Epoch 3, Batch 500, Loss: 0.5040617644786834\n",
      "Epoch 3, Batch 600, Loss: 0.4877665615081787\n",
      "Epoch 3, Batch 700, Loss: 0.4993820780515671\n",
      "Epoch 3, Batch 800, Loss: 0.48830624580383303\n",
      "Epoch 3, Batch 900, Loss: 0.47543443024158477\n",
      "Epoch 4, Batch 100, Loss: 0.47636103242635724\n",
      "Epoch 4, Batch 200, Loss: 0.4911694858968258\n",
      "Epoch 4, Batch 300, Loss: 0.4825681123137474\n",
      "Epoch 4, Batch 400, Loss: 0.4817416459321976\n",
      "Epoch 4, Batch 500, Loss: 0.4858888277411461\n",
      "Epoch 4, Batch 600, Loss: 0.4760219183564186\n",
      "Epoch 4, Batch 700, Loss: 0.4800181004405022\n",
      "Epoch 4, Batch 800, Loss: 0.47303331360220907\n",
      "Epoch 4, Batch 900, Loss: 0.47758439511060713\n",
      "Epoch 5, Batch 100, Loss: 0.467658126950264\n",
      "Epoch 5, Batch 200, Loss: 0.45333026111125946\n",
      "Epoch 5, Batch 300, Loss: 0.479078216701746\n",
      "Epoch 5, Batch 400, Loss: 0.4912260702252388\n",
      "Epoch 5, Batch 500, Loss: 0.4617776674032211\n",
      "Epoch 5, Batch 600, Loss: 0.4584229642152786\n",
      "Epoch 5, Batch 700, Loss: 0.4554917451739311\n",
      "Epoch 5, Batch 800, Loss: 0.4512617394328117\n",
      "Epoch 5, Batch 900, Loss: 0.47118661016225816\n",
      "Epoch 6, Batch 100, Loss: 0.4532503056526184\n",
      "Epoch 6, Batch 200, Loss: 0.46269069105386734\n",
      "Epoch 6, Batch 300, Loss: 0.43731634885072707\n",
      "Epoch 6, Batch 400, Loss: 0.45667018115520475\n",
      "Epoch 6, Batch 500, Loss: 0.47654888659715655\n",
      "Epoch 6, Batch 600, Loss: 0.4576746308803558\n",
      "Epoch 6, Batch 700, Loss: 0.44734994679689405\n",
      "Epoch 6, Batch 800, Loss: 0.46287681102752687\n",
      "Epoch 6, Batch 900, Loss: 0.46228198558092115\n",
      "Epoch 7, Batch 100, Loss: 0.44307556092739103\n",
      "Epoch 7, Batch 200, Loss: 0.4548723208904266\n",
      "Epoch 7, Batch 300, Loss: 0.4522950667142868\n",
      "Epoch 7, Batch 400, Loss: 0.4555668570101261\n",
      "Epoch 7, Batch 500, Loss: 0.46724159717559816\n",
      "Epoch 7, Batch 600, Loss: 0.44638817608356474\n",
      "Epoch 7, Batch 700, Loss: 0.44252432629466054\n",
      "Epoch 7, Batch 800, Loss: 0.45300758570432664\n",
      "Epoch 7, Batch 900, Loss: 0.4490749874711037\n",
      "Epoch 8, Batch 100, Loss: 0.4466933028399944\n",
      "Epoch 8, Batch 200, Loss: 0.45403364092111587\n",
      "Epoch 8, Batch 300, Loss: 0.44493514150381086\n",
      "Epoch 8, Batch 400, Loss: 0.421207083016634\n",
      "Epoch 8, Batch 500, Loss: 0.4430123661458492\n",
      "Epoch 8, Batch 600, Loss: 0.44497536480426786\n",
      "Epoch 8, Batch 700, Loss: 0.45922932893037793\n",
      "Epoch 8, Batch 800, Loss: 0.4409376364946365\n",
      "Epoch 8, Batch 900, Loss: 0.4455004097521305\n",
      "Epoch 9, Batch 100, Loss: 0.4534024794399738\n",
      "Epoch 9, Batch 200, Loss: 0.4383983117341995\n",
      "Epoch 9, Batch 300, Loss: 0.4405002278089523\n",
      "Epoch 9, Batch 400, Loss: 0.449618222117424\n",
      "Epoch 9, Batch 500, Loss: 0.41797358393669126\n",
      "Epoch 9, Batch 600, Loss: 0.42563292682170867\n",
      "Epoch 9, Batch 700, Loss: 0.4464789408445358\n",
      "Epoch 9, Batch 800, Loss: 0.44671094000339506\n",
      "Epoch 9, Batch 900, Loss: 0.4505945208668709\n",
      "Epoch 10, Batch 100, Loss: 0.42634016171097755\n",
      "Epoch 10, Batch 200, Loss: 0.44160298198461534\n",
      "Epoch 10, Batch 300, Loss: 0.44818240582942964\n",
      "Epoch 10, Batch 400, Loss: 0.4465930500626564\n",
      "Epoch 10, Batch 500, Loss: 0.4391025178134441\n",
      "Epoch 10, Batch 600, Loss: 0.43731672167778013\n",
      "Epoch 10, Batch 700, Loss: 0.41751268446445466\n",
      "Epoch 10, Batch 800, Loss: 0.4330964049696922\n",
      "Epoch 10, Batch 900, Loss: 0.43189994126558306\n",
      "Epoch 11, Batch 100, Loss: 0.4370544876158238\n",
      "Epoch 11, Batch 200, Loss: 0.43804755091667175\n",
      "Epoch 11, Batch 300, Loss: 0.430785648226738\n",
      "Epoch 11, Batch 400, Loss: 0.4187163308262825\n",
      "Epoch 11, Batch 500, Loss: 0.42164108008146284\n",
      "Epoch 11, Batch 600, Loss: 0.44458587348461154\n",
      "Epoch 11, Batch 700, Loss: 0.43201658576726915\n",
      "Epoch 11, Batch 800, Loss: 0.4345154774188995\n",
      "Epoch 11, Batch 900, Loss: 0.43512565702199935\n",
      "Epoch 12, Batch 100, Loss: 0.4197986701130867\n",
      "Epoch 12, Batch 200, Loss: 0.42601760387420656\n",
      "Epoch 12, Batch 300, Loss: 0.4359554970264435\n",
      "Epoch 12, Batch 400, Loss: 0.4274297569692135\n",
      "Epoch 12, Batch 500, Loss: 0.4199742220342159\n",
      "Epoch 12, Batch 600, Loss: 0.42384683832526204\n",
      "Epoch 12, Batch 700, Loss: 0.4425992366671562\n",
      "Epoch 12, Batch 800, Loss: 0.4275243881344795\n",
      "Epoch 12, Batch 900, Loss: 0.4359767286479473\n",
      "Epoch 13, Batch 100, Loss: 0.4360825072228909\n",
      "Epoch 13, Batch 200, Loss: 0.41891892671585085\n",
      "Epoch 13, Batch 300, Loss: 0.4386899775266647\n",
      "Epoch 13, Batch 400, Loss: 0.4162646721303463\n",
      "Epoch 13, Batch 500, Loss: 0.4287745168805122\n",
      "Epoch 13, Batch 600, Loss: 0.4320675891637802\n",
      "Epoch 13, Batch 700, Loss: 0.4289129799604416\n",
      "Epoch 13, Batch 800, Loss: 0.4204986777901649\n",
      "Epoch 13, Batch 900, Loss: 0.4276659958064556\n",
      "Epoch 14, Batch 100, Loss: 0.4328134399652481\n",
      "Epoch 14, Batch 200, Loss: 0.43110188633203506\n",
      "Epoch 14, Batch 300, Loss: 0.4218828059732914\n",
      "Epoch 14, Batch 400, Loss: 0.41737749353051184\n",
      "Epoch 14, Batch 500, Loss: 0.4240251810848713\n",
      "Epoch 14, Batch 600, Loss: 0.42069475740194323\n",
      "Epoch 14, Batch 700, Loss: 0.4222244158387184\n",
      "Epoch 14, Batch 800, Loss: 0.42283608838915826\n",
      "Epoch 14, Batch 900, Loss: 0.42902079582214353\n",
      "Epoch 15, Batch 100, Loss: 0.4174092973768711\n",
      "Epoch 15, Batch 200, Loss: 0.4442005880177021\n",
      "Epoch 15, Batch 300, Loss: 0.42718810364604\n",
      "Epoch 15, Batch 400, Loss: 0.41570185124874115\n",
      "Epoch 15, Batch 500, Loss: 0.42476588308811186\n",
      "Epoch 15, Batch 600, Loss: 0.4235734355449676\n",
      "Epoch 15, Batch 700, Loss: 0.43229553401470183\n",
      "Epoch 15, Batch 800, Loss: 0.40719901233911515\n",
      "Epoch 15, Batch 900, Loss: 0.41168024852871893\n",
      "Epoch 16, Batch 100, Loss: 0.41030223697423934\n",
      "Epoch 16, Batch 200, Loss: 0.4129994857311249\n",
      "Epoch 16, Batch 300, Loss: 0.43086034268140794\n",
      "Epoch 16, Batch 400, Loss: 0.4296366220712662\n",
      "Epoch 16, Batch 500, Loss: 0.4233684012293816\n",
      "Epoch 16, Batch 600, Loss: 0.4267939381301403\n",
      "Epoch 16, Batch 700, Loss: 0.4180066046118736\n",
      "Epoch 16, Batch 800, Loss: 0.4130119256675243\n",
      "Epoch 16, Batch 900, Loss: 0.42473303750157354\n",
      "Epoch 17, Batch 100, Loss: 0.41259644627571107\n",
      "Epoch 17, Batch 200, Loss: 0.41249369621276855\n",
      "Epoch 17, Batch 300, Loss: 0.4303004559874535\n",
      "Epoch 17, Batch 400, Loss: 0.41967533171176913\n",
      "Epoch 17, Batch 500, Loss: 0.4133816693723202\n",
      "Epoch 17, Batch 600, Loss: 0.4290135759115219\n",
      "Epoch 17, Batch 700, Loss: 0.4209002356231213\n",
      "Epoch 17, Batch 800, Loss: 0.41112828597426415\n",
      "Epoch 17, Batch 900, Loss: 0.420517355799675\n",
      "Epoch 18, Batch 100, Loss: 0.4093085078895092\n",
      "Epoch 18, Batch 200, Loss: 0.4403437739610672\n",
      "Epoch 18, Batch 300, Loss: 0.4101335854828358\n",
      "Epoch 18, Batch 400, Loss: 0.41625016525387765\n",
      "Epoch 18, Batch 500, Loss: 0.42606977075338365\n",
      "Epoch 18, Batch 600, Loss: 0.4012847185134888\n",
      "Epoch 18, Batch 700, Loss: 0.433011604398489\n",
      "Epoch 18, Batch 800, Loss: 0.4104481925070286\n",
      "Epoch 18, Batch 900, Loss: 0.41577809885144235\n",
      "Epoch 19, Batch 100, Loss: 0.40520775005221366\n",
      "Epoch 19, Batch 200, Loss: 0.417219310104847\n",
      "Epoch 19, Batch 300, Loss: 0.4099585796892643\n",
      "Epoch 19, Batch 400, Loss: 0.4223447224497795\n",
      "Epoch 19, Batch 500, Loss: 0.4356975156068802\n",
      "Epoch 19, Batch 600, Loss: 0.4039109417796135\n",
      "Epoch 19, Batch 700, Loss: 0.41888390958309174\n",
      "Epoch 19, Batch 800, Loss: 0.4262288963794708\n",
      "Epoch 19, Batch 900, Loss: 0.4102150216698647\n",
      "Epoch 20, Batch 100, Loss: 0.4100540496408939\n",
      "Epoch 20, Batch 200, Loss: 0.4055746881663799\n",
      "Epoch 20, Batch 300, Loss: 0.4229179485142231\n",
      "Epoch 20, Batch 400, Loss: 0.4244502779841423\n",
      "Epoch 20, Batch 500, Loss: 0.4143542286753654\n",
      "Epoch 20, Batch 600, Loss: 0.41220682322978974\n",
      "Epoch 20, Batch 700, Loss: 0.4302306707203388\n",
      "Epoch 20, Batch 800, Loss: 0.4091682334244251\n",
      "Epoch 20, Batch 900, Loss: 0.41454340040683746\n",
      "Epoch 21, Batch 100, Loss: 0.40977800950407983\n",
      "Epoch 21, Batch 200, Loss: 0.40951203599572183\n",
      "Epoch 21, Batch 300, Loss: 0.40946292608976365\n",
      "Epoch 21, Batch 400, Loss: 0.4292597049474716\n",
      "Epoch 21, Batch 500, Loss: 0.4170103733241558\n",
      "Epoch 21, Batch 600, Loss: 0.41970855221152303\n",
      "Epoch 21, Batch 700, Loss: 0.4010116674005985\n",
      "Epoch 21, Batch 800, Loss: 0.4186308094859123\n",
      "Epoch 21, Batch 900, Loss: 0.4197780239582062\n",
      "Epoch 22, Batch 100, Loss: 0.4139991965889931\n",
      "Epoch 22, Batch 200, Loss: 0.4075200213491917\n",
      "Epoch 22, Batch 300, Loss: 0.412488479167223\n",
      "Epoch 22, Batch 400, Loss: 0.40803044036030767\n",
      "Epoch 22, Batch 500, Loss: 0.407229477763176\n",
      "Epoch 22, Batch 600, Loss: 0.41745105758309364\n",
      "Epoch 22, Batch 700, Loss: 0.41777044013142584\n",
      "Epoch 22, Batch 800, Loss: 0.4238467065989971\n",
      "Epoch 22, Batch 900, Loss: 0.4105129037797451\n",
      "Epoch 23, Batch 100, Loss: 0.4158139215409756\n",
      "Epoch 23, Batch 200, Loss: 0.40044907182455064\n",
      "Epoch 23, Batch 300, Loss: 0.4168165864050388\n",
      "Epoch 23, Batch 400, Loss: 0.3979341043531895\n",
      "Epoch 23, Batch 500, Loss: 0.41582525938749315\n",
      "Epoch 23, Batch 600, Loss: 0.4207957972586155\n",
      "Epoch 23, Batch 700, Loss: 0.41869484826922415\n",
      "Epoch 23, Batch 800, Loss: 0.43429088428616525\n",
      "Epoch 23, Batch 900, Loss: 0.3942229537665844\n",
      "Epoch 24, Batch 100, Loss: 0.4227934794127941\n",
      "Epoch 24, Batch 200, Loss: 0.3969471126794815\n",
      "Epoch 24, Batch 300, Loss: 0.42557184159755707\n",
      "Epoch 24, Batch 400, Loss: 0.4056574538350105\n",
      "Epoch 24, Batch 500, Loss: 0.40504387110471723\n",
      "Epoch 24, Batch 600, Loss: 0.41653150543570516\n",
      "Epoch 24, Batch 700, Loss: 0.40200046986341476\n",
      "Epoch 24, Batch 800, Loss: 0.4074522362649441\n",
      "Epoch 24, Batch 900, Loss: 0.421716633439064\n",
      "Epoch 25, Batch 100, Loss: 0.414105666577816\n",
      "Epoch 25, Batch 200, Loss: 0.4122862875461578\n",
      "Epoch 25, Batch 300, Loss: 0.39067080974578855\n",
      "Epoch 25, Batch 400, Loss: 0.39878223463892937\n",
      "Epoch 25, Batch 500, Loss: 0.41885707318782806\n",
      "Epoch 25, Batch 600, Loss: 0.4103917697072029\n",
      "Epoch 25, Batch 700, Loss: 0.43864847779273985\n",
      "Epoch 25, Batch 800, Loss: 0.4192712216079235\n",
      "Epoch 25, Batch 900, Loss: 0.4066443361341953\n",
      "Accuracy on test set: 0.8417%\n",
      "Fitting for combination 10\n",
      "784\n",
      "1\n",
      "10\n",
      "[30, 10]\n",
      "True\n",
      "['relu']\n",
      "Adam\n",
      "0.01\n",
      "0.03\n",
      "CrossEntropyLoss\n",
      "50\n",
      "25\n",
      "0\n",
      "Epoch 1, Batch 50, Loss: 0.46821879923343657\n",
      "Epoch 1, Batch 100, Loss: 0.3563706386089325\n",
      "Epoch 1, Batch 150, Loss: 0.36636632204055786\n",
      "Epoch 1, Batch 200, Loss: 0.3625104314088821\n",
      "Epoch 1, Batch 250, Loss: 0.34995083808898925\n",
      "Epoch 1, Batch 300, Loss: 0.3570308291912079\n",
      "Epoch 1, Batch 350, Loss: 0.3564094197750092\n",
      "Epoch 1, Batch 400, Loss: 0.3267481181025505\n",
      "Epoch 1, Batch 450, Loss: 0.3341457012295723\n",
      "Epoch 1, Batch 500, Loss: 0.3342454636096954\n",
      "Epoch 1, Batch 550, Loss: 0.3454151055216789\n",
      "Epoch 1, Batch 600, Loss: 0.3304180806875229\n",
      "Epoch 1, Batch 650, Loss: 0.35006745398044586\n",
      "Epoch 1, Batch 700, Loss: 0.3459015288949013\n",
      "Epoch 1, Batch 750, Loss: 0.3324912413954735\n",
      "Epoch 1, Batch 800, Loss: 0.3277449357509613\n",
      "Epoch 1, Batch 850, Loss: 0.3425552576780319\n",
      "Epoch 1, Batch 900, Loss: 0.3209697571396828\n",
      "Epoch 2, Batch 50, Loss: 0.3459963282942772\n",
      "Epoch 2, Batch 100, Loss: 0.3265694382786751\n",
      "Epoch 2, Batch 150, Loss: 0.33467045694589614\n",
      "Epoch 2, Batch 200, Loss: 0.33708087116479873\n",
      "Epoch 2, Batch 250, Loss: 0.3194242206215858\n",
      "Epoch 2, Batch 300, Loss: 0.3217975291609764\n",
      "Epoch 2, Batch 350, Loss: 0.32939701318740844\n",
      "Epoch 2, Batch 400, Loss: 0.3434315299987793\n",
      "Epoch 2, Batch 450, Loss: 0.346686986386776\n",
      "Epoch 2, Batch 500, Loss: 0.3288898837566376\n",
      "Epoch 2, Batch 550, Loss: 0.3258836230635643\n",
      "Epoch 2, Batch 600, Loss: 0.3288318559527397\n",
      "Epoch 2, Batch 650, Loss: 0.3269298303127289\n",
      "Epoch 2, Batch 700, Loss: 0.341910520195961\n",
      "Epoch 2, Batch 750, Loss: 0.32327019721269606\n",
      "Epoch 2, Batch 800, Loss: 0.3266218677163124\n",
      "Epoch 2, Batch 850, Loss: 0.32765106707811353\n",
      "Epoch 2, Batch 900, Loss: 0.326713590323925\n",
      "Epoch 3, Batch 50, Loss: 0.3312471705675125\n",
      "Epoch 3, Batch 100, Loss: 0.3262052357196808\n",
      "Epoch 3, Batch 150, Loss: 0.3350249457359314\n",
      "Epoch 3, Batch 200, Loss: 0.3141147318482399\n",
      "Epoch 3, Batch 250, Loss: 0.3220791473984718\n",
      "Epoch 3, Batch 300, Loss: 0.32728279680013656\n",
      "Epoch 3, Batch 350, Loss: 0.33044157683849334\n",
      "Epoch 3, Batch 400, Loss: 0.3192847311496735\n",
      "Epoch 3, Batch 450, Loss: 0.3253320413827896\n",
      "Epoch 3, Batch 500, Loss: 0.33465355217456816\n",
      "Epoch 3, Batch 550, Loss: 0.32235833048820495\n",
      "Epoch 3, Batch 600, Loss: 0.3258815535902977\n",
      "Epoch 3, Batch 650, Loss: 0.33790572345256803\n",
      "Epoch 3, Batch 700, Loss: 0.33369344890117647\n",
      "Epoch 3, Batch 750, Loss: 0.33312040984630586\n",
      "Epoch 3, Batch 800, Loss: 0.32841853410005567\n",
      "Epoch 3, Batch 850, Loss: 0.31883190870285033\n",
      "Epoch 3, Batch 900, Loss: 0.3281167149543762\n",
      "Epoch 4, Batch 50, Loss: 0.3274104446172714\n",
      "Epoch 4, Batch 100, Loss: 0.32439765125513076\n",
      "Epoch 4, Batch 150, Loss: 0.3247662627696991\n",
      "Epoch 4, Batch 200, Loss: 0.32048763155937193\n",
      "Epoch 4, Batch 250, Loss: 0.3422137024998665\n",
      "Epoch 4, Batch 300, Loss: 0.31752088040113446\n",
      "Epoch 4, Batch 350, Loss: 0.33100667387247085\n",
      "Epoch 4, Batch 400, Loss: 0.3157473737001419\n",
      "Epoch 4, Batch 450, Loss: 0.3206814715266228\n",
      "Epoch 4, Batch 500, Loss: 0.335670148730278\n",
      "Epoch 4, Batch 550, Loss: 0.3276033729314804\n",
      "Epoch 4, Batch 600, Loss: 0.3105990183353424\n",
      "Epoch 4, Batch 650, Loss: 0.3412203231453896\n",
      "Epoch 4, Batch 700, Loss: 0.33724142372608185\n",
      "Epoch 4, Batch 750, Loss: 0.3267401012778282\n",
      "Epoch 4, Batch 800, Loss: 0.32273586124181747\n",
      "Epoch 4, Batch 850, Loss: 0.315944190621376\n",
      "Epoch 4, Batch 900, Loss: 0.3287543389201164\n",
      "Epoch 5, Batch 50, Loss: 0.325391052365303\n",
      "Epoch 5, Batch 100, Loss: 0.3392761033773422\n",
      "Epoch 5, Batch 150, Loss: 0.3384204477071762\n",
      "Epoch 5, Batch 200, Loss: 0.3166076651215553\n",
      "Epoch 5, Batch 250, Loss: 0.31972984224557877\n",
      "Epoch 5, Batch 300, Loss: 0.3378749543428421\n",
      "Epoch 5, Batch 350, Loss: 0.3222270375490189\n",
      "Epoch 5, Batch 400, Loss: 0.32015951961278916\n",
      "Epoch 5, Batch 450, Loss: 0.31918302059173587\n",
      "Epoch 5, Batch 500, Loss: 0.3196824789047241\n",
      "Epoch 5, Batch 550, Loss: 0.311990512907505\n",
      "Epoch 5, Batch 600, Loss: 0.328839139342308\n",
      "Epoch 5, Batch 650, Loss: 0.3213026013970375\n",
      "Epoch 5, Batch 700, Loss: 0.33059153258800505\n",
      "Epoch 5, Batch 750, Loss: 0.32455157846212385\n",
      "Epoch 5, Batch 800, Loss: 0.33672546058893205\n",
      "Epoch 5, Batch 850, Loss: 0.34848266452550886\n",
      "Epoch 5, Batch 900, Loss: 0.3320665222406387\n",
      "Epoch 6, Batch 50, Loss: 0.3294565662741661\n",
      "Epoch 6, Batch 100, Loss: 0.3156083610653877\n",
      "Epoch 6, Batch 150, Loss: 0.3262686944007874\n",
      "Epoch 6, Batch 200, Loss: 0.32445815980434417\n",
      "Epoch 6, Batch 250, Loss: 0.3226770901679993\n",
      "Epoch 6, Batch 300, Loss: 0.31982132971286775\n",
      "Epoch 6, Batch 350, Loss: 0.32154676377773284\n",
      "Epoch 6, Batch 400, Loss: 0.3194721746444702\n",
      "Epoch 6, Batch 450, Loss: 0.3158040452003479\n",
      "Epoch 6, Batch 500, Loss: 0.3298872384428978\n",
      "Epoch 6, Batch 550, Loss: 0.335764656662941\n",
      "Epoch 6, Batch 600, Loss: 0.3243472945690155\n",
      "Epoch 6, Batch 650, Loss: 0.32013622611761094\n",
      "Epoch 6, Batch 700, Loss: 0.3315268415212631\n",
      "Epoch 6, Batch 750, Loss: 0.3438959762454033\n",
      "Epoch 6, Batch 800, Loss: 0.32238141149282457\n",
      "Epoch 6, Batch 850, Loss: 0.3191096982359886\n",
      "Epoch 6, Batch 900, Loss: 0.33685105890035627\n",
      "Epoch 7, Batch 50, Loss: 0.32138540625572204\n",
      "Epoch 7, Batch 100, Loss: 0.3059419333934784\n",
      "Epoch 7, Batch 150, Loss: 0.32694451868534086\n",
      "Epoch 7, Batch 200, Loss: 0.3273409000039101\n",
      "Epoch 7, Batch 250, Loss: 0.34001851111650466\n",
      "Epoch 7, Batch 300, Loss: 0.3150643742084503\n",
      "Epoch 7, Batch 350, Loss: 0.34100565165281294\n",
      "Epoch 7, Batch 400, Loss: 0.3253147426247597\n",
      "Epoch 7, Batch 450, Loss: 0.31704417705535887\n",
      "Epoch 7, Batch 500, Loss: 0.3235470122098923\n",
      "Epoch 7, Batch 550, Loss: 0.3094161641597748\n",
      "Epoch 7, Batch 600, Loss: 0.3387036207318306\n",
      "Epoch 7, Batch 650, Loss: 0.32151282727718355\n",
      "Epoch 7, Batch 700, Loss: 0.34615111380815505\n",
      "Epoch 7, Batch 750, Loss: 0.3340157932043076\n",
      "Epoch 7, Batch 800, Loss: 0.3241186088323593\n",
      "Epoch 7, Batch 850, Loss: 0.3404975119233131\n",
      "Epoch 7, Batch 900, Loss: 0.3155547246336937\n",
      "Epoch 8, Batch 50, Loss: 0.3244123211503029\n",
      "Epoch 8, Batch 100, Loss: 0.31833865255117416\n",
      "Epoch 8, Batch 150, Loss: 0.3340736490488052\n",
      "Epoch 8, Batch 200, Loss: 0.33827889651060106\n",
      "Epoch 8, Batch 250, Loss: 0.3417576548457146\n",
      "Epoch 8, Batch 300, Loss: 0.31913747400045395\n",
      "Epoch 8, Batch 350, Loss: 0.30285064548254015\n",
      "Epoch 8, Batch 400, Loss: 0.33403410136699674\n",
      "Epoch 8, Batch 450, Loss: 0.340250198841095\n",
      "Epoch 8, Batch 500, Loss: 0.3063859423995018\n",
      "Epoch 8, Batch 550, Loss: 0.3361470329761505\n",
      "Epoch 8, Batch 600, Loss: 0.33686107128858567\n",
      "Epoch 8, Batch 650, Loss: 0.3263686534762382\n",
      "Epoch 8, Batch 700, Loss: 0.33556954890489576\n",
      "Epoch 8, Batch 750, Loss: 0.33746927738189697\n",
      "Epoch 8, Batch 800, Loss: 0.31567994683980943\n",
      "Epoch 8, Batch 850, Loss: 0.3228523576259613\n",
      "Epoch 8, Batch 900, Loss: 0.3234167259931564\n",
      "Epoch 9, Batch 50, Loss: 0.3207405635714531\n",
      "Epoch 9, Batch 100, Loss: 0.33250357925891877\n",
      "Epoch 9, Batch 150, Loss: 0.3125139564275742\n",
      "Epoch 9, Batch 200, Loss: 0.3420636233687401\n",
      "Epoch 9, Batch 250, Loss: 0.31331751614809034\n",
      "Epoch 9, Batch 300, Loss: 0.3367955130338669\n",
      "Epoch 9, Batch 350, Loss: 0.3210354942083359\n",
      "Epoch 9, Batch 400, Loss: 0.3267788338661194\n",
      "Epoch 9, Batch 450, Loss: 0.3302041956782341\n",
      "Epoch 9, Batch 500, Loss: 0.33181817770004274\n",
      "Epoch 9, Batch 550, Loss: 0.3327611580491066\n",
      "Epoch 9, Batch 600, Loss: 0.32946117103099826\n",
      "Epoch 9, Batch 650, Loss: 0.34120800405740737\n",
      "Epoch 9, Batch 700, Loss: 0.33926975935697556\n",
      "Epoch 9, Batch 750, Loss: 0.3284265747666359\n",
      "Epoch 9, Batch 800, Loss: 0.34873128950595855\n",
      "Epoch 9, Batch 850, Loss: 0.31471372008323667\n",
      "Epoch 9, Batch 900, Loss: 0.3190897238254547\n",
      "Epoch 10, Batch 50, Loss: 0.32291020154953004\n",
      "Epoch 10, Batch 100, Loss: 0.335698636174202\n",
      "Epoch 10, Batch 150, Loss: 0.32766087979078296\n",
      "Epoch 10, Batch 200, Loss: 0.32551434487104414\n",
      "Epoch 10, Batch 250, Loss: 0.3422436481714249\n",
      "Epoch 10, Batch 300, Loss: 0.338149730861187\n",
      "Epoch 10, Batch 350, Loss: 0.3518338003754616\n",
      "Epoch 10, Batch 400, Loss: 0.3170056337118149\n",
      "Epoch 10, Batch 450, Loss: 0.3453796923160553\n",
      "Epoch 10, Batch 500, Loss: 0.34223745971918107\n",
      "Epoch 10, Batch 550, Loss: 0.3281686055660248\n",
      "Epoch 10, Batch 600, Loss: 0.33473475366830824\n",
      "Epoch 10, Batch 650, Loss: 0.3379576396942139\n",
      "Epoch 10, Batch 700, Loss: 0.31394554048776624\n",
      "Epoch 10, Batch 750, Loss: 0.3124044123291969\n",
      "Epoch 10, Batch 800, Loss: 0.3331218862533569\n",
      "Epoch 10, Batch 850, Loss: 0.3261397761106491\n",
      "Epoch 10, Batch 900, Loss: 0.31310813218355177\n",
      "Epoch 11, Batch 50, Loss: 0.3287752088904381\n",
      "Epoch 11, Batch 100, Loss: 0.31464522689580915\n",
      "Epoch 11, Batch 150, Loss: 0.31300692319869994\n",
      "Epoch 11, Batch 200, Loss: 0.32873923033475877\n",
      "Epoch 11, Batch 250, Loss: 0.3166849932074547\n",
      "Epoch 11, Batch 300, Loss: 0.3379632118344307\n",
      "Epoch 11, Batch 350, Loss: 0.33700830340385435\n",
      "Epoch 11, Batch 400, Loss: 0.3372384417057037\n",
      "Epoch 11, Batch 450, Loss: 0.31745230555534365\n",
      "Epoch 11, Batch 500, Loss: 0.3202032342553139\n",
      "Epoch 11, Batch 550, Loss: 0.3309098407626152\n",
      "Epoch 11, Batch 600, Loss: 0.32441700279712676\n",
      "Epoch 11, Batch 650, Loss: 0.31066200882196426\n",
      "Epoch 11, Batch 700, Loss: 0.33672175675630567\n",
      "Epoch 11, Batch 750, Loss: 0.3133283427357674\n",
      "Epoch 11, Batch 800, Loss: 0.3312502461671829\n",
      "Epoch 11, Batch 850, Loss: 0.3236231008172035\n",
      "Epoch 11, Batch 900, Loss: 0.3369417721033096\n",
      "Epoch 12, Batch 50, Loss: 0.32638432502746584\n",
      "Epoch 12, Batch 100, Loss: 0.3168469735980034\n",
      "Epoch 12, Batch 150, Loss: 0.3280101212859154\n",
      "Epoch 12, Batch 200, Loss: 0.3086506778001785\n",
      "Epoch 12, Batch 250, Loss: 0.32561338126659395\n",
      "Epoch 12, Batch 300, Loss: 0.32053165674209594\n",
      "Epoch 12, Batch 350, Loss: 0.32593558728694916\n",
      "Epoch 12, Batch 400, Loss: 0.3322889631986618\n",
      "Epoch 12, Batch 450, Loss: 0.3217385405302048\n",
      "Epoch 12, Batch 500, Loss: 0.3242133170366287\n",
      "Epoch 12, Batch 550, Loss: 0.3302401551604271\n",
      "Epoch 12, Batch 600, Loss: 0.34502167016267776\n",
      "Epoch 12, Batch 650, Loss: 0.3315719404816628\n",
      "Epoch 12, Batch 700, Loss: 0.32819545686244966\n",
      "Epoch 12, Batch 750, Loss: 0.32360482156276704\n",
      "Epoch 12, Batch 800, Loss: 0.3396799963712692\n",
      "Epoch 12, Batch 850, Loss: 0.3184817090630531\n",
      "Epoch 12, Batch 900, Loss: 0.32914926469326017\n",
      "Epoch 13, Batch 50, Loss: 0.31937167048454285\n",
      "Epoch 13, Batch 100, Loss: 0.307554102241993\n",
      "Epoch 13, Batch 150, Loss: 0.3345485129952431\n",
      "Epoch 13, Batch 200, Loss: 0.3252816760540009\n",
      "Epoch 13, Batch 250, Loss: 0.32678453892469406\n",
      "Epoch 13, Batch 300, Loss: 0.3522655808925629\n",
      "Epoch 13, Batch 350, Loss: 0.33028116255998613\n",
      "Epoch 13, Batch 400, Loss: 0.34690037071704866\n",
      "Epoch 13, Batch 450, Loss: 0.3326453161239624\n",
      "Epoch 13, Batch 500, Loss: 0.3036175328493118\n",
      "Epoch 13, Batch 550, Loss: 0.3213541194796562\n",
      "Epoch 13, Batch 600, Loss: 0.32648915797472\n",
      "Epoch 13, Batch 650, Loss: 0.31228577077388764\n",
      "Epoch 13, Batch 700, Loss: 0.3380764907598495\n",
      "Epoch 13, Batch 750, Loss: 0.33882015764713286\n",
      "Epoch 13, Batch 800, Loss: 0.34346204906702044\n",
      "Epoch 13, Batch 850, Loss: 0.3236862251162529\n",
      "Epoch 13, Batch 900, Loss: 0.31967643171548843\n",
      "Epoch 14, Batch 50, Loss: 0.3175822904706001\n",
      "Epoch 14, Batch 100, Loss: 0.33683309853076937\n",
      "Epoch 14, Batch 150, Loss: 0.3356659001111984\n",
      "Epoch 14, Batch 200, Loss: 0.3118532064557076\n",
      "Epoch 14, Batch 250, Loss: 0.3295168313384056\n",
      "Epoch 14, Batch 300, Loss: 0.3273926702141762\n",
      "Epoch 14, Batch 350, Loss: 0.31515688329935077\n",
      "Epoch 14, Batch 400, Loss: 0.33581680059432983\n",
      "Epoch 14, Batch 450, Loss: 0.33587371081113815\n",
      "Epoch 14, Batch 500, Loss: 0.32673450142145155\n",
      "Epoch 14, Batch 550, Loss: 0.3213983929157257\n",
      "Epoch 14, Batch 600, Loss: 0.3273910692334175\n",
      "Epoch 14, Batch 650, Loss: 0.3412229347229004\n",
      "Epoch 14, Batch 700, Loss: 0.3221919333934784\n",
      "Epoch 14, Batch 750, Loss: 0.32825487285852434\n",
      "Epoch 14, Batch 800, Loss: 0.31484473824501036\n",
      "Epoch 14, Batch 850, Loss: 0.3367274248600006\n",
      "Epoch 14, Batch 900, Loss: 0.3175557377934456\n",
      "Epoch 15, Batch 50, Loss: 0.3243401989340782\n",
      "Epoch 15, Batch 100, Loss: 0.3330349960923195\n",
      "Epoch 15, Batch 150, Loss: 0.33362043619155884\n",
      "Epoch 15, Batch 200, Loss: 0.3161994591355324\n",
      "Epoch 15, Batch 250, Loss: 0.3095913779735565\n",
      "Epoch 15, Batch 300, Loss: 0.3149344456195831\n",
      "Epoch 15, Batch 350, Loss: 0.3081937047839165\n",
      "Epoch 15, Batch 400, Loss: 0.3433040043711662\n",
      "Epoch 15, Batch 450, Loss: 0.32262091726064684\n",
      "Epoch 15, Batch 500, Loss: 0.33841693550348284\n",
      "Epoch 15, Batch 550, Loss: 0.33722328543663027\n",
      "Epoch 15, Batch 600, Loss: 0.33420776695013044\n",
      "Epoch 15, Batch 650, Loss: 0.32966251015663145\n",
      "Epoch 15, Batch 700, Loss: 0.3260918053984642\n",
      "Epoch 15, Batch 750, Loss: 0.32538519471883776\n",
      "Epoch 15, Batch 800, Loss: 0.3178380316495895\n",
      "Epoch 15, Batch 850, Loss: 0.3128326445817947\n",
      "Epoch 15, Batch 900, Loss: 0.35098759531974794\n",
      "Epoch 16, Batch 50, Loss: 0.342174896299839\n",
      "Epoch 16, Batch 100, Loss: 0.3140355822443962\n",
      "Epoch 16, Batch 150, Loss: 0.32553349792957303\n",
      "Epoch 16, Batch 200, Loss: 0.33728567630052564\n",
      "Epoch 16, Batch 250, Loss: 0.34754938542842867\n",
      "Epoch 16, Batch 300, Loss: 0.3201369944214821\n",
      "Epoch 16, Batch 350, Loss: 0.30937449604272843\n",
      "Epoch 16, Batch 400, Loss: 0.30421977669000627\n",
      "Epoch 16, Batch 450, Loss: 0.33915819466114044\n",
      "Epoch 16, Batch 500, Loss: 0.31231202572584155\n",
      "Epoch 16, Batch 550, Loss: 0.3384062445163727\n",
      "Epoch 16, Batch 600, Loss: 0.32271106213331224\n",
      "Epoch 16, Batch 650, Loss: 0.3176672822237015\n",
      "Epoch 16, Batch 700, Loss: 0.35010090947151185\n",
      "Epoch 16, Batch 750, Loss: 0.3358004370331764\n",
      "Epoch 16, Batch 800, Loss: 0.3299462440609932\n",
      "Epoch 16, Batch 850, Loss: 0.3334500214457512\n",
      "Epoch 16, Batch 900, Loss: 0.32765357226133346\n",
      "Epoch 17, Batch 50, Loss: 0.32203398406505584\n",
      "Epoch 17, Batch 100, Loss: 0.31603488326072693\n",
      "Epoch 17, Batch 150, Loss: 0.3402301499247551\n",
      "Epoch 17, Batch 200, Loss: 0.32030119150877\n",
      "Epoch 17, Batch 250, Loss: 0.3350375634431839\n",
      "Epoch 17, Batch 300, Loss: 0.32816466718912124\n",
      "Epoch 17, Batch 350, Loss: 0.32175128370523454\n",
      "Epoch 17, Batch 400, Loss: 0.33025114983320236\n",
      "Epoch 17, Batch 450, Loss: 0.34596793860197067\n",
      "Epoch 17, Batch 500, Loss: 0.3225178337097168\n",
      "Epoch 17, Batch 550, Loss: 0.335208585858345\n",
      "Epoch 17, Batch 600, Loss: 0.3136887064576149\n",
      "Epoch 17, Batch 650, Loss: 0.31035605907440184\n",
      "Epoch 17, Batch 700, Loss: 0.3234644415974617\n",
      "Epoch 17, Batch 750, Loss: 0.3401035109162331\n",
      "Epoch 17, Batch 800, Loss: 0.32206859171390534\n",
      "Epoch 17, Batch 850, Loss: 0.3312694400548935\n",
      "Epoch 17, Batch 900, Loss: 0.3367837119102478\n",
      "Epoch 18, Batch 50, Loss: 0.34082115679979325\n",
      "Epoch 18, Batch 100, Loss: 0.3187912982702255\n",
      "Epoch 18, Batch 150, Loss: 0.3299912443757057\n",
      "Epoch 18, Batch 200, Loss: 0.3108263137936592\n",
      "Epoch 18, Batch 250, Loss: 0.34130218118429184\n",
      "Epoch 18, Batch 300, Loss: 0.3264197862148285\n",
      "Epoch 18, Batch 350, Loss: 0.3373317807912827\n",
      "Epoch 18, Batch 400, Loss: 0.31762478411197664\n",
      "Epoch 18, Batch 450, Loss: 0.32674468249082567\n",
      "Epoch 18, Batch 500, Loss: 0.3288040325045586\n",
      "Epoch 18, Batch 550, Loss: 0.33005364030599593\n",
      "Epoch 18, Batch 600, Loss: 0.3286111283302307\n",
      "Epoch 18, Batch 650, Loss: 0.3241401547193527\n",
      "Epoch 18, Batch 700, Loss: 0.33438899904489516\n",
      "Epoch 18, Batch 750, Loss: 0.34098613262176514\n",
      "Epoch 18, Batch 800, Loss: 0.3362865728139877\n",
      "Epoch 18, Batch 850, Loss: 0.31862648963928225\n",
      "Epoch 18, Batch 900, Loss: 0.33874291211366653\n",
      "Epoch 19, Batch 50, Loss: 0.3701011100411415\n",
      "Epoch 19, Batch 100, Loss: 0.31941002696752546\n",
      "Epoch 19, Batch 150, Loss: 0.3151846566796303\n",
      "Epoch 19, Batch 200, Loss: 0.32771777242422107\n",
      "Epoch 19, Batch 250, Loss: 0.3086251664161682\n",
      "Epoch 19, Batch 300, Loss: 0.32925460368394854\n",
      "Epoch 19, Batch 350, Loss: 0.325497210919857\n",
      "Epoch 19, Batch 400, Loss: 0.33494606912136077\n",
      "Epoch 19, Batch 450, Loss: 0.30690979450941086\n",
      "Epoch 19, Batch 500, Loss: 0.3265357720851898\n",
      "Epoch 19, Batch 550, Loss: 0.3226432934403419\n",
      "Epoch 19, Batch 600, Loss: 0.3288363119959831\n",
      "Epoch 19, Batch 650, Loss: 0.333449624478817\n",
      "Epoch 19, Batch 700, Loss: 0.3336798173189163\n",
      "Epoch 19, Batch 750, Loss: 0.3305268406867981\n",
      "Epoch 19, Batch 800, Loss: 0.331002037525177\n",
      "Epoch 19, Batch 850, Loss: 0.33551269918680193\n",
      "Epoch 19, Batch 900, Loss: 0.33429869651794436\n",
      "Epoch 20, Batch 50, Loss: 0.3265602257847786\n",
      "Epoch 20, Batch 100, Loss: 0.3176798510551453\n",
      "Epoch 20, Batch 150, Loss: 0.3257893145084381\n",
      "Epoch 20, Batch 200, Loss: 0.31895715564489363\n",
      "Epoch 20, Batch 250, Loss: 0.3214811387658119\n",
      "Epoch 20, Batch 300, Loss: 0.3434238341450691\n",
      "Epoch 20, Batch 350, Loss: 0.3243119442462921\n",
      "Epoch 20, Batch 400, Loss: 0.341654354929924\n",
      "Epoch 20, Batch 450, Loss: 0.3265760937333107\n",
      "Epoch 20, Batch 500, Loss: 0.3233782222867012\n",
      "Epoch 20, Batch 550, Loss: 0.314907554090023\n",
      "Epoch 20, Batch 600, Loss: 0.30849291026592257\n",
      "Epoch 20, Batch 650, Loss: 0.3336618372797966\n",
      "Epoch 20, Batch 700, Loss: 0.32773722350597384\n",
      "Epoch 20, Batch 750, Loss: 0.3220991650223732\n",
      "Epoch 20, Batch 800, Loss: 0.31916102796792983\n",
      "Epoch 20, Batch 850, Loss: 0.320996521115303\n",
      "Epoch 20, Batch 900, Loss: 0.3336214208602905\n",
      "Epoch 21, Batch 50, Loss: 0.33208919495344164\n",
      "Epoch 21, Batch 100, Loss: 0.3258442470431328\n",
      "Epoch 21, Batch 150, Loss: 0.3446726828813553\n",
      "Epoch 21, Batch 200, Loss: 0.31585975050926207\n",
      "Epoch 21, Batch 250, Loss: 0.3192241778969765\n",
      "Epoch 21, Batch 300, Loss: 0.32103546351194384\n",
      "Epoch 21, Batch 350, Loss: 0.3149302196502686\n",
      "Epoch 21, Batch 400, Loss: 0.3284887135028839\n",
      "Epoch 21, Batch 450, Loss: 0.32174810975790025\n",
      "Epoch 21, Batch 500, Loss: 0.32098862260580063\n",
      "Epoch 21, Batch 550, Loss: 0.30321032196283343\n",
      "Epoch 21, Batch 600, Loss: 0.3269894915819168\n",
      "Epoch 21, Batch 650, Loss: 0.3451285007596016\n",
      "Epoch 21, Batch 700, Loss: 0.33668081849813464\n",
      "Epoch 21, Batch 750, Loss: 0.3255149406194687\n",
      "Epoch 21, Batch 800, Loss: 0.324722501039505\n",
      "Epoch 21, Batch 850, Loss: 0.32920343220233916\n",
      "Epoch 21, Batch 900, Loss: 0.3183460786938667\n",
      "Epoch 22, Batch 50, Loss: 0.339023705124855\n",
      "Epoch 22, Batch 100, Loss: 0.31882101386785505\n",
      "Epoch 22, Batch 150, Loss: 0.337541740834713\n",
      "Epoch 22, Batch 200, Loss: 0.3207930979132652\n",
      "Epoch 22, Batch 250, Loss: 0.3228070676326752\n",
      "Epoch 22, Batch 300, Loss: 0.33611653476953507\n",
      "Epoch 22, Batch 350, Loss: 0.3033101490139961\n",
      "Epoch 22, Batch 400, Loss: 0.31405734181404116\n",
      "Epoch 22, Batch 450, Loss: 0.3242784196138382\n",
      "Epoch 22, Batch 500, Loss: 0.3189127969741821\n",
      "Epoch 22, Batch 550, Loss: 0.3430528378486633\n",
      "Epoch 22, Batch 600, Loss: 0.33359094202518463\n",
      "Epoch 22, Batch 650, Loss: 0.3277964508533478\n",
      "Epoch 22, Batch 700, Loss: 0.3058266094326973\n",
      "Epoch 22, Batch 750, Loss: 0.329025094807148\n",
      "Epoch 22, Batch 800, Loss: 0.32063228636980057\n",
      "Epoch 22, Batch 850, Loss: 0.31549379259347915\n",
      "Epoch 22, Batch 900, Loss: 0.3300821325182915\n",
      "Epoch 23, Batch 50, Loss: 0.34477027237415314\n",
      "Epoch 23, Batch 100, Loss: 0.32382433116436005\n",
      "Epoch 23, Batch 150, Loss: 0.32083533346652987\n",
      "Epoch 23, Batch 200, Loss: 0.3233388352394104\n",
      "Epoch 23, Batch 250, Loss: 0.32448605328798297\n",
      "Epoch 23, Batch 300, Loss: 0.32756114304065703\n",
      "Epoch 23, Batch 350, Loss: 0.322627936899662\n",
      "Epoch 23, Batch 400, Loss: 0.31113513380289076\n",
      "Epoch 23, Batch 450, Loss: 0.3346559789776802\n",
      "Epoch 23, Batch 500, Loss: 0.32156501650810243\n",
      "Epoch 23, Batch 550, Loss: 0.3156618121266365\n",
      "Epoch 23, Batch 600, Loss: 0.3227757868170738\n",
      "Epoch 23, Batch 650, Loss: 0.32517712473869326\n",
      "Epoch 23, Batch 700, Loss: 0.33862623006105425\n",
      "Epoch 23, Batch 750, Loss: 0.3136441460251808\n",
      "Epoch 23, Batch 800, Loss: 0.33315791726112365\n",
      "Epoch 23, Batch 850, Loss: 0.3343772414326668\n",
      "Epoch 23, Batch 900, Loss: 0.3134066900610924\n",
      "Epoch 24, Batch 50, Loss: 0.32103302359580993\n",
      "Epoch 24, Batch 100, Loss: 0.3234123006463051\n",
      "Epoch 24, Batch 150, Loss: 0.32848622620105744\n",
      "Epoch 24, Batch 200, Loss: 0.3314700573682785\n",
      "Epoch 24, Batch 250, Loss: 0.31064647167921067\n",
      "Epoch 24, Batch 300, Loss: 0.31734679818153383\n",
      "Epoch 24, Batch 350, Loss: 0.32207985520362853\n",
      "Epoch 24, Batch 400, Loss: 0.33117544651031494\n",
      "Epoch 24, Batch 450, Loss: 0.32671624898910523\n",
      "Epoch 24, Batch 500, Loss: 0.3054605141282082\n",
      "Epoch 24, Batch 550, Loss: 0.31831420212984085\n",
      "Epoch 24, Batch 600, Loss: 0.3376757273077965\n",
      "Epoch 24, Batch 650, Loss: 0.32404376566410065\n",
      "Epoch 24, Batch 700, Loss: 0.3318253725767136\n",
      "Epoch 24, Batch 750, Loss: 0.3462864062190056\n",
      "Epoch 24, Batch 800, Loss: 0.32301551759243013\n",
      "Epoch 24, Batch 850, Loss: 0.32552276372909544\n",
      "Epoch 24, Batch 900, Loss: 0.3289939859509468\n",
      "Epoch 25, Batch 50, Loss: 0.33752074182033537\n",
      "Epoch 25, Batch 100, Loss: 0.3074280759692192\n",
      "Epoch 25, Batch 150, Loss: 0.31103760361671445\n",
      "Epoch 25, Batch 200, Loss: 0.32025898635387423\n",
      "Epoch 25, Batch 250, Loss: 0.3251867064833641\n",
      "Epoch 25, Batch 300, Loss: 0.3358437943458557\n",
      "Epoch 25, Batch 350, Loss: 0.3173864081501961\n",
      "Epoch 25, Batch 400, Loss: 0.3296009427309036\n",
      "Epoch 25, Batch 450, Loss: 0.323535298705101\n",
      "Epoch 25, Batch 500, Loss: 0.3409911134839058\n",
      "Epoch 25, Batch 550, Loss: 0.3078989338874817\n",
      "Epoch 25, Batch 600, Loss: 0.3389925473928452\n",
      "Epoch 25, Batch 650, Loss: 0.3298216965794563\n",
      "Epoch 25, Batch 700, Loss: 0.34028595954179763\n",
      "Epoch 25, Batch 750, Loss: 0.3152791565656662\n",
      "Epoch 25, Batch 800, Loss: 0.3287146273255348\n",
      "Epoch 25, Batch 850, Loss: 0.3195226642489433\n",
      "Epoch 25, Batch 900, Loss: 0.30863701432943347\n",
      "Accuracy on test set: 0.7627%\n",
      "Fitting for combination 11\n",
      "784\n",
      "1\n",
      "10\n",
      "[30, 10]\n",
      "False\n",
      "['sigmoid']\n",
      "SGD\n",
      "0.03\n",
      "0\n",
      "CrossEntropyLoss\n",
      "200\n",
      "25\n",
      "0\n",
      "Epoch 1, Batch 200, Loss: 3.6702413058280943\n",
      "Epoch 1, Batch 400, Loss: 2.5824795293807985\n",
      "Epoch 1, Batch 600, Loss: 2.0669516783952715\n",
      "Epoch 1, Batch 800, Loss: 1.780036581158638\n",
      "Epoch 2, Batch 200, Loss: 1.522035698890686\n",
      "Epoch 2, Batch 400, Loss: 1.4168178111314773\n",
      "Epoch 2, Batch 600, Loss: 1.3666280901432037\n",
      "Epoch 2, Batch 800, Loss: 1.293496957719326\n",
      "Epoch 3, Batch 200, Loss: 1.250760880112648\n",
      "Epoch 3, Batch 400, Loss: 1.1836892688274383\n",
      "Epoch 3, Batch 600, Loss: 1.1517771631479263\n",
      "Epoch 3, Batch 800, Loss: 1.113562646508217\n",
      "Epoch 4, Batch 200, Loss: 1.0972706392407416\n",
      "Epoch 4, Batch 400, Loss: 1.0400570338964463\n",
      "Epoch 4, Batch 600, Loss: 1.0609966918826104\n",
      "Epoch 4, Batch 800, Loss: 1.0266090106964112\n",
      "Epoch 5, Batch 200, Loss: 1.017449399828911\n",
      "Epoch 5, Batch 400, Loss: 0.9794182257354259\n",
      "Epoch 5, Batch 600, Loss: 0.9820681524276733\n",
      "Epoch 5, Batch 800, Loss: 0.9422062653303146\n",
      "Epoch 6, Batch 200, Loss: 0.94597575455904\n",
      "Epoch 6, Batch 400, Loss: 0.9363812291622162\n",
      "Epoch 6, Batch 600, Loss: 0.9241396364569664\n",
      "Epoch 6, Batch 800, Loss: 0.9134033891558647\n",
      "Epoch 7, Batch 200, Loss: 0.8921161556243896\n",
      "Epoch 7, Batch 400, Loss: 0.887642765045166\n",
      "Epoch 7, Batch 600, Loss: 0.8948301626741886\n",
      "Epoch 7, Batch 800, Loss: 0.8924949404597282\n",
      "Epoch 8, Batch 200, Loss: 0.8689251238107681\n",
      "Epoch 8, Batch 400, Loss: 0.8692033770680427\n",
      "Epoch 8, Batch 600, Loss: 0.8722137260437012\n",
      "Epoch 8, Batch 800, Loss: 0.8417032340168953\n",
      "Epoch 9, Batch 200, Loss: 0.8466944190859794\n",
      "Epoch 9, Batch 400, Loss: 0.8484824441373349\n",
      "Epoch 9, Batch 600, Loss: 0.8403319703042507\n",
      "Epoch 9, Batch 800, Loss: 0.8153860929608345\n",
      "Epoch 10, Batch 200, Loss: 0.8099517340958119\n",
      "Epoch 10, Batch 400, Loss: 0.8230112260580063\n",
      "Epoch 10, Batch 600, Loss: 0.8266542935371399\n",
      "Epoch 10, Batch 800, Loss: 0.8164474254846573\n",
      "Epoch 11, Batch 200, Loss: 0.8104345534741878\n",
      "Epoch 11, Batch 400, Loss: 0.8038722229003906\n",
      "Epoch 11, Batch 600, Loss: 0.8143057034909725\n",
      "Epoch 11, Batch 800, Loss: 0.7977559058368207\n",
      "Epoch 12, Batch 200, Loss: 0.805981750190258\n",
      "Epoch 12, Batch 400, Loss: 0.7866289032995701\n",
      "Epoch 12, Batch 600, Loss: 0.7807929202914238\n",
      "Epoch 12, Batch 800, Loss: 0.7818574592471123\n",
      "Epoch 13, Batch 200, Loss: 0.7643441903591156\n",
      "Epoch 13, Batch 400, Loss: 0.7724602490663528\n",
      "Epoch 13, Batch 600, Loss: 0.7935701695084572\n",
      "Epoch 13, Batch 800, Loss: 0.7839075089991092\n",
      "Epoch 14, Batch 200, Loss: 0.7682467499375343\n",
      "Epoch 14, Batch 400, Loss: 0.7724885952472687\n",
      "Epoch 14, Batch 600, Loss: 0.7678010752797126\n",
      "Epoch 14, Batch 800, Loss: 0.7754130417108536\n",
      "Epoch 15, Batch 200, Loss: 0.7496468742191792\n",
      "Epoch 15, Batch 400, Loss: 0.7646100154519081\n",
      "Epoch 15, Batch 600, Loss: 0.7601405791938305\n",
      "Epoch 15, Batch 800, Loss: 0.7483648179471493\n",
      "Epoch 16, Batch 200, Loss: 0.7559581139683723\n",
      "Epoch 16, Batch 400, Loss: 0.7363907554745674\n",
      "Epoch 16, Batch 600, Loss: 0.7687942917644978\n",
      "Epoch 16, Batch 800, Loss: 0.7600647304952145\n",
      "Epoch 17, Batch 200, Loss: 0.7503821195662022\n",
      "Epoch 17, Batch 400, Loss: 0.7472728049755096\n",
      "Epoch 17, Batch 600, Loss: 0.7432913067936897\n",
      "Epoch 17, Batch 800, Loss: 0.724872812628746\n",
      "Epoch 18, Batch 200, Loss: 0.7412854287028313\n",
      "Epoch 18, Batch 400, Loss: 0.7482138538360595\n",
      "Epoch 18, Batch 600, Loss: 0.7324693529307842\n",
      "Epoch 18, Batch 800, Loss: 0.724184935837984\n",
      "Epoch 19, Batch 200, Loss: 0.7223198986053467\n",
      "Epoch 19, Batch 400, Loss: 0.7245706370472909\n",
      "Epoch 19, Batch 600, Loss: 0.7340553168952465\n",
      "Epoch 19, Batch 800, Loss: 0.7303744851052761\n",
      "Epoch 20, Batch 200, Loss: 0.7298336167633533\n",
      "Epoch 20, Batch 400, Loss: 0.7177735917270184\n",
      "Epoch 20, Batch 600, Loss: 0.717485980540514\n",
      "Epoch 20, Batch 800, Loss: 0.7269272699952125\n",
      "Epoch 21, Batch 200, Loss: 0.7158376869559288\n",
      "Epoch 21, Batch 400, Loss: 0.7206788900494575\n",
      "Epoch 21, Batch 600, Loss: 0.7031988367438317\n",
      "Epoch 21, Batch 800, Loss: 0.7266083115339279\n",
      "Epoch 22, Batch 200, Loss: 0.7010570272803307\n",
      "Epoch 22, Batch 400, Loss: 0.718945930302143\n",
      "Epoch 22, Batch 600, Loss: 0.6906195752322674\n",
      "Epoch 22, Batch 800, Loss: 0.7209411661326885\n",
      "Epoch 23, Batch 200, Loss: 0.7074938797950745\n",
      "Epoch 23, Batch 400, Loss: 0.7138798998296261\n",
      "Epoch 23, Batch 600, Loss: 0.6991050557047128\n",
      "Epoch 23, Batch 800, Loss: 0.7015185263752938\n",
      "Epoch 24, Batch 200, Loss: 0.6951335974037647\n",
      "Epoch 24, Batch 400, Loss: 0.721619388461113\n",
      "Epoch 24, Batch 600, Loss: 0.6895253300666809\n",
      "Epoch 24, Batch 800, Loss: 0.6941266672313213\n",
      "Epoch 25, Batch 200, Loss: 0.6931214481592178\n",
      "Epoch 25, Batch 400, Loss: 0.7090172332525253\n",
      "Epoch 25, Batch 600, Loss: 0.6918254975974559\n",
      "Epoch 25, Batch 800, Loss: 0.6866357842087746\n",
      "Accuracy on test set: 0.8579%\n",
      "Fitting for combination 12\n",
      "784\n",
      "1\n",
      "10\n",
      "[40, 10]\n",
      "False\n",
      "['relu']\n",
      "Adam\n",
      "0.03\n",
      "0.01\n",
      "CrossEntropyLoss\n",
      "50\n",
      "25\n",
      "0\n",
      "Epoch 1, Batch 50, Loss: 0.6535202842950821\n",
      "Epoch 1, Batch 100, Loss: 0.3990969783067703\n",
      "Epoch 1, Batch 150, Loss: 0.409198243021965\n",
      "Epoch 1, Batch 200, Loss: 0.4021907752752304\n",
      "Epoch 1, Batch 250, Loss: 0.40877306640148164\n",
      "Epoch 1, Batch 300, Loss: 0.41648665606975555\n",
      "Epoch 1, Batch 350, Loss: 0.3664331924915314\n",
      "Epoch 1, Batch 400, Loss: 0.39052104353904726\n",
      "Epoch 1, Batch 450, Loss: 0.46722067713737486\n",
      "Epoch 1, Batch 500, Loss: 0.37137911677360536\n",
      "Epoch 1, Batch 550, Loss: 0.4263303533196449\n",
      "Epoch 1, Batch 600, Loss: 0.4019952628016472\n",
      "Epoch 1, Batch 650, Loss: 0.36579918980598447\n",
      "Epoch 1, Batch 700, Loss: 0.3934607779979706\n",
      "Epoch 1, Batch 750, Loss: 0.3645596534013748\n",
      "Epoch 1, Batch 800, Loss: 0.36118441462516787\n",
      "Epoch 1, Batch 850, Loss: 0.42476551949977875\n",
      "Epoch 1, Batch 900, Loss: 0.36112038224935533\n",
      "Epoch 2, Batch 50, Loss: 0.400628300011158\n",
      "Epoch 2, Batch 100, Loss: 0.38082529842853546\n",
      "Epoch 2, Batch 150, Loss: 0.4195589256286621\n",
      "Epoch 2, Batch 200, Loss: 0.3772708347439766\n",
      "Epoch 2, Batch 250, Loss: 0.4074254697561264\n",
      "Epoch 2, Batch 300, Loss: 0.4012342095375061\n",
      "Epoch 2, Batch 350, Loss: 0.4330965742468834\n",
      "Epoch 2, Batch 400, Loss: 0.4345621699094772\n",
      "Epoch 2, Batch 450, Loss: 0.45373093724250796\n",
      "Epoch 2, Batch 500, Loss: 0.45474603682756426\n",
      "Epoch 2, Batch 550, Loss: 0.37528404474258426\n",
      "Epoch 2, Batch 600, Loss: 0.4192722284793854\n",
      "Epoch 2, Batch 650, Loss: 0.35630642890930175\n",
      "Epoch 2, Batch 700, Loss: 0.4325485217571259\n",
      "Epoch 2, Batch 750, Loss: 0.34742056131362914\n",
      "Epoch 2, Batch 800, Loss: 0.35535711705684664\n",
      "Epoch 2, Batch 850, Loss: 0.404966216981411\n",
      "Epoch 2, Batch 900, Loss: 0.3597241747379303\n",
      "Epoch 3, Batch 50, Loss: 0.40219917714595793\n",
      "Epoch 3, Batch 100, Loss: 0.40629749357700345\n",
      "Epoch 3, Batch 150, Loss: 0.44845317453145983\n",
      "Epoch 3, Batch 200, Loss: 0.3967388314008713\n",
      "Epoch 3, Batch 250, Loss: 0.4488880354166031\n",
      "Epoch 3, Batch 300, Loss: 0.4546831706166267\n",
      "Epoch 3, Batch 350, Loss: 0.4003276574611664\n",
      "Epoch 3, Batch 400, Loss: 0.4457227835059166\n",
      "Epoch 3, Batch 450, Loss: 0.3819824278354645\n",
      "Epoch 3, Batch 500, Loss: 0.3840292653441429\n",
      "Epoch 3, Batch 550, Loss: 0.42485265612602235\n",
      "Epoch 3, Batch 600, Loss: 0.4086639904975891\n",
      "Epoch 3, Batch 650, Loss: 0.38037730634212497\n",
      "Epoch 3, Batch 700, Loss: 0.35602030128240586\n",
      "Epoch 3, Batch 750, Loss: 0.39521292597055435\n",
      "Epoch 3, Batch 800, Loss: 0.40573856234550476\n",
      "Epoch 3, Batch 850, Loss: 0.39517180442810057\n",
      "Epoch 3, Batch 900, Loss: 0.4460869818925858\n",
      "Epoch 4, Batch 50, Loss: 0.4207729375362396\n",
      "Epoch 4, Batch 100, Loss: 0.38374763369560244\n",
      "Epoch 4, Batch 150, Loss: 0.4016449695825577\n",
      "Epoch 4, Batch 200, Loss: 0.37469223767519\n",
      "Epoch 4, Batch 250, Loss: 0.4144310075044632\n",
      "Epoch 4, Batch 300, Loss: 0.38455533742904663\n",
      "Epoch 4, Batch 350, Loss: 0.37741184175014497\n",
      "Epoch 4, Batch 400, Loss: 0.369795001745224\n",
      "Epoch 4, Batch 450, Loss: 0.3830465406179428\n",
      "Epoch 4, Batch 500, Loss: 0.4568195927143097\n",
      "Epoch 4, Batch 550, Loss: 0.4277432149648666\n",
      "Epoch 4, Batch 600, Loss: 0.4035772657394409\n",
      "Epoch 4, Batch 650, Loss: 0.4092716440558434\n",
      "Epoch 4, Batch 700, Loss: 0.3859569996595383\n",
      "Epoch 4, Batch 750, Loss: 0.4239294618368149\n",
      "Epoch 4, Batch 800, Loss: 0.3934710857272148\n",
      "Epoch 4, Batch 850, Loss: 0.42467989593744276\n",
      "Epoch 4, Batch 900, Loss: 0.4152027291059494\n",
      "Epoch 5, Batch 50, Loss: 0.3895589792728424\n",
      "Epoch 5, Batch 100, Loss: 0.40987312734127046\n",
      "Epoch 5, Batch 150, Loss: 0.39349030375480654\n",
      "Epoch 5, Batch 200, Loss: 0.3954219031333923\n",
      "Epoch 5, Batch 250, Loss: 0.36335348188877103\n",
      "Epoch 5, Batch 300, Loss: 0.3818559443950653\n",
      "Epoch 5, Batch 350, Loss: 0.39327722012996674\n",
      "Epoch 5, Batch 400, Loss: 0.3619417443871498\n",
      "Epoch 5, Batch 450, Loss: 0.3743233588337898\n",
      "Epoch 5, Batch 500, Loss: 0.376836998462677\n",
      "Epoch 5, Batch 550, Loss: 0.40931437522172925\n",
      "Epoch 5, Batch 600, Loss: 0.38966007202863695\n",
      "Epoch 5, Batch 650, Loss: 0.44938207030296323\n",
      "Epoch 5, Batch 700, Loss: 0.4463061225414276\n",
      "Epoch 5, Batch 750, Loss: 0.39839145720005037\n",
      "Epoch 5, Batch 800, Loss: 0.41304931640625\n",
      "Epoch 5, Batch 850, Loss: 0.4171601840853691\n",
      "Epoch 5, Batch 900, Loss: 0.376600481569767\n",
      "Epoch 6, Batch 50, Loss: 0.39120718121528625\n",
      "Epoch 6, Batch 100, Loss: 0.4042717057466507\n",
      "Epoch 6, Batch 150, Loss: 0.37445548206567764\n",
      "Epoch 6, Batch 200, Loss: 0.3615242639183998\n",
      "Epoch 6, Batch 250, Loss: 0.3563018500804901\n",
      "Epoch 6, Batch 300, Loss: 0.4245994049310684\n",
      "Epoch 6, Batch 350, Loss: 0.4461912912130356\n",
      "Epoch 6, Batch 400, Loss: 0.3834575194120407\n",
      "Epoch 6, Batch 450, Loss: 0.37920972138643266\n",
      "Epoch 6, Batch 500, Loss: 0.4502513417601585\n",
      "Epoch 6, Batch 550, Loss: 0.37235460460186004\n",
      "Epoch 6, Batch 600, Loss: 0.3697753608226776\n",
      "Epoch 6, Batch 650, Loss: 0.3960069891810417\n",
      "Epoch 6, Batch 700, Loss: 0.506209517121315\n",
      "Epoch 6, Batch 750, Loss: 0.4435882923007011\n",
      "Epoch 6, Batch 800, Loss: 0.42700901985168455\n",
      "Epoch 6, Batch 850, Loss: 0.39378781378269195\n",
      "Epoch 6, Batch 900, Loss: 0.3956311720609665\n",
      "Epoch 7, Batch 50, Loss: 0.3827020338177681\n",
      "Epoch 7, Batch 100, Loss: 0.4176767259836197\n",
      "Epoch 7, Batch 150, Loss: 0.4596830779314041\n",
      "Epoch 7, Batch 200, Loss: 0.45537776172161104\n",
      "Epoch 7, Batch 250, Loss: 0.4225235593318939\n",
      "Epoch 7, Batch 300, Loss: 0.37915101647377014\n",
      "Epoch 7, Batch 350, Loss: 0.38034398764371874\n",
      "Epoch 7, Batch 400, Loss: 0.35987652659416197\n",
      "Epoch 7, Batch 450, Loss: 0.36127486646175383\n",
      "Epoch 7, Batch 500, Loss: 0.3769268560409546\n",
      "Epoch 7, Batch 550, Loss: 0.4111348533630371\n",
      "Epoch 7, Batch 600, Loss: 0.4161986017227173\n",
      "Epoch 7, Batch 650, Loss: 0.3679290372133255\n",
      "Epoch 7, Batch 700, Loss: 0.4069346386194229\n",
      "Epoch 7, Batch 750, Loss: 0.3618981096148491\n",
      "Epoch 7, Batch 800, Loss: 0.3659589695930481\n",
      "Epoch 7, Batch 850, Loss: 0.37997326523065567\n",
      "Epoch 7, Batch 900, Loss: 0.364251346886158\n",
      "Epoch 8, Batch 50, Loss: 0.34383634984493255\n",
      "Epoch 8, Batch 100, Loss: 0.43479537695646286\n",
      "Epoch 8, Batch 150, Loss: 0.40970152616500854\n",
      "Epoch 8, Batch 200, Loss: 0.4550877571105957\n",
      "Epoch 8, Batch 250, Loss: 0.4213203835487366\n",
      "Epoch 8, Batch 300, Loss: 0.4235625410079956\n",
      "Epoch 8, Batch 350, Loss: 0.3815826851129532\n",
      "Epoch 8, Batch 400, Loss: 0.37407889306545256\n",
      "Epoch 8, Batch 450, Loss: 0.396614333987236\n",
      "Epoch 8, Batch 500, Loss: 0.36801296323537824\n",
      "Epoch 8, Batch 550, Loss: 0.40635801702737806\n",
      "Epoch 8, Batch 600, Loss: 0.39237620383501054\n",
      "Epoch 8, Batch 650, Loss: 0.4317109867930412\n",
      "Epoch 8, Batch 700, Loss: 0.39698702812194825\n",
      "Epoch 8, Batch 750, Loss: 0.39159885078668594\n",
      "Epoch 8, Batch 800, Loss: 0.4569738662242889\n",
      "Epoch 8, Batch 850, Loss: 0.40685222506523133\n",
      "Epoch 8, Batch 900, Loss: 0.4285134363174439\n",
      "Epoch 9, Batch 50, Loss: 0.39878317683935166\n",
      "Epoch 9, Batch 100, Loss: 0.3964167854189873\n",
      "Epoch 9, Batch 150, Loss: 0.38350420624017717\n",
      "Epoch 9, Batch 200, Loss: 0.36661300659179685\n",
      "Epoch 9, Batch 250, Loss: 0.393489765226841\n",
      "Epoch 9, Batch 300, Loss: 0.3781211268901825\n",
      "Epoch 9, Batch 350, Loss: 0.3757329535484314\n",
      "Epoch 9, Batch 400, Loss: 0.3867544624209404\n",
      "Epoch 9, Batch 450, Loss: 0.38012463361024856\n",
      "Epoch 9, Batch 500, Loss: 0.42394189596176146\n",
      "Epoch 9, Batch 550, Loss: 0.3650170356035233\n",
      "Epoch 9, Batch 600, Loss: 0.3788580268621445\n",
      "Epoch 9, Batch 650, Loss: 0.3936721047759056\n",
      "Epoch 9, Batch 700, Loss: 0.3937980407476425\n",
      "Epoch 9, Batch 750, Loss: 0.3993457990884781\n",
      "Epoch 9, Batch 800, Loss: 0.4223721888661385\n",
      "Epoch 9, Batch 850, Loss: 0.4015348708629608\n",
      "Epoch 9, Batch 900, Loss: 0.4141115635633469\n",
      "Epoch 10, Batch 50, Loss: 0.42295114278793333\n",
      "Epoch 10, Batch 100, Loss: 0.39414939522743225\n",
      "Epoch 10, Batch 150, Loss: 0.3888801580667496\n",
      "Epoch 10, Batch 200, Loss: 0.3847326636314392\n",
      "Epoch 10, Batch 250, Loss: 0.3717501163482666\n",
      "Epoch 10, Batch 300, Loss: 0.38819253355264666\n",
      "Epoch 10, Batch 350, Loss: 0.38787175685167313\n",
      "Epoch 10, Batch 400, Loss: 0.41145955473184587\n",
      "Epoch 10, Batch 450, Loss: 0.42607715249061584\n",
      "Epoch 10, Batch 500, Loss: 0.4237969899177551\n",
      "Epoch 10, Batch 550, Loss: 0.3846056443452835\n",
      "Epoch 10, Batch 600, Loss: 0.3998034745454788\n",
      "Epoch 10, Batch 650, Loss: 0.38395474255084994\n",
      "Epoch 10, Batch 700, Loss: 0.42169744193553926\n",
      "Epoch 10, Batch 750, Loss: 0.3824822545051575\n",
      "Epoch 10, Batch 800, Loss: 0.3896347391605377\n",
      "Epoch 10, Batch 850, Loss: 0.4183108472824097\n",
      "Epoch 10, Batch 900, Loss: 0.4200670379400253\n",
      "Epoch 11, Batch 50, Loss: 0.4021219515800476\n",
      "Epoch 11, Batch 100, Loss: 0.39793252110481264\n",
      "Epoch 11, Batch 150, Loss: 0.41605798691511153\n",
      "Epoch 11, Batch 200, Loss: 0.3580883938074112\n",
      "Epoch 11, Batch 250, Loss: 0.4448463797569275\n",
      "Epoch 11, Batch 300, Loss: 0.42621059596538546\n",
      "Epoch 11, Batch 350, Loss: 0.48397343993186953\n",
      "Epoch 11, Batch 400, Loss: 0.4254556342959404\n",
      "Epoch 11, Batch 450, Loss: 0.35562881231307986\n",
      "Epoch 11, Batch 500, Loss: 0.3595547813177109\n",
      "Epoch 11, Batch 550, Loss: 0.4025603592395782\n",
      "Epoch 11, Batch 600, Loss: 0.38538849413394927\n",
      "Epoch 11, Batch 650, Loss: 0.4009740400314331\n",
      "Epoch 11, Batch 700, Loss: 0.45245530545711515\n",
      "Epoch 11, Batch 750, Loss: 0.3634673136472702\n",
      "Epoch 11, Batch 800, Loss: 0.3923496747016907\n",
      "Epoch 11, Batch 850, Loss: 0.3661815923452377\n",
      "Epoch 11, Batch 900, Loss: 0.4239149785041809\n",
      "Epoch 12, Batch 50, Loss: 0.41246467500925066\n",
      "Epoch 12, Batch 100, Loss: 0.37571607053279876\n",
      "Epoch 12, Batch 150, Loss: 0.3829290220141411\n",
      "Epoch 12, Batch 200, Loss: 0.3888877159357071\n",
      "Epoch 12, Batch 250, Loss: 0.37254261791706084\n",
      "Epoch 12, Batch 300, Loss: 0.4047863408923149\n",
      "Epoch 12, Batch 350, Loss: 0.4218187898397446\n",
      "Epoch 12, Batch 400, Loss: 0.45918188452720643\n",
      "Epoch 12, Batch 450, Loss: 0.40952047556638715\n",
      "Epoch 12, Batch 500, Loss: 0.38560096502304075\n",
      "Epoch 12, Batch 550, Loss: 0.3928692585229874\n",
      "Epoch 12, Batch 600, Loss: 0.38799875855445864\n",
      "Epoch 12, Batch 650, Loss: 0.3839153280854225\n",
      "Epoch 12, Batch 700, Loss: 0.42277260243892667\n",
      "Epoch 12, Batch 750, Loss: 0.42161490440368654\n",
      "Epoch 12, Batch 800, Loss: 0.40128649652004245\n",
      "Epoch 12, Batch 850, Loss: 0.3783100289106369\n",
      "Epoch 12, Batch 900, Loss: 0.41110296368598936\n",
      "Epoch 13, Batch 50, Loss: 0.425844315290451\n",
      "Epoch 13, Batch 100, Loss: 0.4247806233167648\n",
      "Epoch 13, Batch 150, Loss: 0.3757472443580627\n",
      "Epoch 13, Batch 200, Loss: 0.35564483821392057\n",
      "Epoch 13, Batch 250, Loss: 0.3639625036716461\n",
      "Epoch 13, Batch 300, Loss: 0.3777245059609413\n",
      "Epoch 13, Batch 350, Loss: 0.40664651900529863\n",
      "Epoch 13, Batch 400, Loss: 0.3753891158103943\n",
      "Epoch 13, Batch 450, Loss: 0.37653813123703\n",
      "Epoch 13, Batch 500, Loss: 0.4005410027503967\n",
      "Epoch 13, Batch 550, Loss: 0.371185419857502\n",
      "Epoch 13, Batch 600, Loss: 0.4015802812576294\n",
      "Epoch 13, Batch 650, Loss: 0.40175497651100156\n",
      "Epoch 13, Batch 700, Loss: 0.42304214000701906\n",
      "Epoch 13, Batch 750, Loss: 0.41454831659793856\n",
      "Epoch 13, Batch 800, Loss: 0.3810979199409485\n",
      "Epoch 13, Batch 850, Loss: 0.39603375613689423\n",
      "Epoch 13, Batch 900, Loss: 0.40278568744659426\n",
      "Epoch 14, Batch 50, Loss: 0.42687244832515714\n",
      "Epoch 14, Batch 100, Loss: 0.39277943402528764\n",
      "Epoch 14, Batch 150, Loss: 0.40927560716867445\n",
      "Epoch 14, Batch 200, Loss: 0.37804541379213336\n",
      "Epoch 14, Batch 250, Loss: 0.378114752471447\n",
      "Epoch 14, Batch 300, Loss: 0.3976506099104881\n",
      "Epoch 14, Batch 350, Loss: 0.3830751642584801\n",
      "Epoch 14, Batch 400, Loss: 0.3748141112923622\n",
      "Epoch 14, Batch 450, Loss: 0.37769844114780426\n",
      "Epoch 14, Batch 500, Loss: 0.3549992427229881\n",
      "Epoch 14, Batch 550, Loss: 0.37233778953552243\n",
      "Epoch 14, Batch 600, Loss: 0.36370686292648313\n",
      "Epoch 14, Batch 650, Loss: 0.3724891376495361\n",
      "Epoch 14, Batch 700, Loss: 0.3832584595680237\n",
      "Epoch 14, Batch 750, Loss: 0.3655263212323189\n",
      "Epoch 14, Batch 800, Loss: 0.4066874572634697\n",
      "Epoch 14, Batch 850, Loss: 0.3933499795198441\n",
      "Epoch 14, Batch 900, Loss: 0.3786086195707321\n",
      "Epoch 15, Batch 50, Loss: 0.4379172772169113\n",
      "Epoch 15, Batch 100, Loss: 0.469269323348999\n",
      "Epoch 15, Batch 150, Loss: 0.45756072878837584\n",
      "Epoch 15, Batch 200, Loss: 0.41151648879051206\n",
      "Epoch 15, Batch 250, Loss: 0.3976183360815048\n",
      "Epoch 15, Batch 300, Loss: 0.3901410520076752\n",
      "Epoch 15, Batch 350, Loss: 0.4110334411263466\n",
      "Epoch 15, Batch 400, Loss: 0.37399373441934586\n",
      "Epoch 15, Batch 450, Loss: 0.37936635315418243\n",
      "Epoch 15, Batch 500, Loss: 0.41113228887319564\n",
      "Epoch 15, Batch 550, Loss: 0.43430212140083313\n",
      "Epoch 15, Batch 600, Loss: 0.3450493758916855\n",
      "Epoch 15, Batch 650, Loss: 0.4030492040514946\n",
      "Epoch 15, Batch 700, Loss: 0.3865552815794945\n",
      "Epoch 15, Batch 750, Loss: 0.4136394166946411\n",
      "Epoch 15, Batch 800, Loss: 0.4052974939346313\n",
      "Epoch 15, Batch 850, Loss: 0.39003813564777373\n",
      "Epoch 15, Batch 900, Loss: 0.36411141902208327\n",
      "Epoch 16, Batch 50, Loss: 0.4076747676730156\n",
      "Epoch 16, Batch 100, Loss: 0.3695726543664932\n",
      "Epoch 16, Batch 150, Loss: 0.41235625982284546\n",
      "Epoch 16, Batch 200, Loss: 0.3955726373195648\n",
      "Epoch 16, Batch 250, Loss: 0.3808527231216431\n",
      "Epoch 16, Batch 300, Loss: 0.3864605975151062\n",
      "Epoch 16, Batch 350, Loss: 0.38758736968040464\n",
      "Epoch 16, Batch 400, Loss: 0.3873537814617157\n",
      "Epoch 16, Batch 450, Loss: 0.3897342815995216\n",
      "Epoch 16, Batch 500, Loss: 0.3872621369361877\n",
      "Epoch 16, Batch 550, Loss: 0.4659267604351044\n",
      "Epoch 16, Batch 600, Loss: 0.3590263563394547\n",
      "Epoch 16, Batch 650, Loss: 0.3375000765919685\n",
      "Epoch 16, Batch 700, Loss: 0.3857021114230156\n",
      "Epoch 16, Batch 750, Loss: 0.3726563474535942\n",
      "Epoch 16, Batch 800, Loss: 0.3759651678800583\n",
      "Epoch 16, Batch 850, Loss: 0.34880104541778567\n",
      "Epoch 16, Batch 900, Loss: 0.3654921931028366\n",
      "Epoch 17, Batch 50, Loss: 0.39735045611858366\n",
      "Epoch 17, Batch 100, Loss: 0.4041831433773041\n",
      "Epoch 17, Batch 150, Loss: 0.4122512936592102\n",
      "Epoch 17, Batch 200, Loss: 0.3840718615055084\n",
      "Epoch 17, Batch 250, Loss: 0.36144722759723663\n",
      "Epoch 17, Batch 300, Loss: 0.3749288409948349\n",
      "Epoch 17, Batch 350, Loss: 0.39211299002170563\n",
      "Epoch 17, Batch 400, Loss: 0.38026084780693054\n",
      "Epoch 17, Batch 450, Loss: 0.37395936995744705\n",
      "Epoch 17, Batch 500, Loss: 0.41182900458574295\n",
      "Epoch 17, Batch 550, Loss: 0.4115793365240097\n",
      "Epoch 17, Batch 600, Loss: 0.6923931014537811\n",
      "Epoch 17, Batch 650, Loss: 0.44620538532733917\n",
      "Epoch 17, Batch 700, Loss: 0.41059619903564454\n",
      "Epoch 17, Batch 750, Loss: 0.4017931205034256\n",
      "Epoch 17, Batch 800, Loss: 0.37373038738965986\n",
      "Epoch 17, Batch 850, Loss: 0.40926216274499894\n",
      "Epoch 17, Batch 900, Loss: 0.3732836058735847\n",
      "Epoch 18, Batch 50, Loss: 0.36218401670455935\n",
      "Epoch 18, Batch 100, Loss: 0.3719517105817795\n",
      "Epoch 18, Batch 150, Loss: 0.3782563382387161\n",
      "Epoch 18, Batch 200, Loss: 0.38355873674154284\n",
      "Epoch 18, Batch 250, Loss: 0.38537528455257414\n",
      "Epoch 18, Batch 300, Loss: 0.3748704555630684\n",
      "Epoch 18, Batch 350, Loss: 0.41228273838758467\n",
      "Epoch 18, Batch 400, Loss: 0.4359879657626152\n",
      "Epoch 18, Batch 450, Loss: 0.3784112891554832\n",
      "Epoch 18, Batch 500, Loss: 0.4037941706180572\n",
      "Epoch 18, Batch 550, Loss: 0.3939691513776779\n",
      "Epoch 18, Batch 600, Loss: 0.38294487416744233\n",
      "Epoch 18, Batch 650, Loss: 0.3772150409221649\n",
      "Epoch 18, Batch 700, Loss: 0.4092115592956543\n",
      "Epoch 18, Batch 750, Loss: 0.41344064235687256\n",
      "Epoch 18, Batch 800, Loss: 0.379005486369133\n",
      "Epoch 18, Batch 850, Loss: 0.4055123871564865\n",
      "Epoch 18, Batch 900, Loss: 0.3916945585608482\n",
      "Epoch 19, Batch 50, Loss: 0.38130625039339067\n",
      "Epoch 19, Batch 100, Loss: 0.36247604578733444\n",
      "Epoch 19, Batch 150, Loss: 0.39417581260204315\n",
      "Epoch 19, Batch 200, Loss: 0.3920227515697479\n",
      "Epoch 19, Batch 250, Loss: 0.3963396257162094\n",
      "Epoch 19, Batch 300, Loss: 0.4060979235172272\n",
      "Epoch 19, Batch 350, Loss: 0.5064866787195206\n",
      "Epoch 19, Batch 400, Loss: 0.47446822047233583\n",
      "Epoch 19, Batch 450, Loss: 0.44217043697834013\n",
      "Epoch 19, Batch 500, Loss: 0.3895843714475632\n",
      "Epoch 19, Batch 550, Loss: 0.3718110233545303\n",
      "Epoch 19, Batch 600, Loss: 0.3931790238618851\n",
      "Epoch 19, Batch 650, Loss: 0.38879834055900575\n",
      "Epoch 19, Batch 700, Loss: 0.41433036983013155\n",
      "Epoch 19, Batch 750, Loss: 0.4012899398803711\n",
      "Epoch 19, Batch 800, Loss: 0.38877025842666624\n",
      "Epoch 19, Batch 850, Loss: 0.3581683012843132\n",
      "Epoch 19, Batch 900, Loss: 0.3943029826879501\n",
      "Epoch 20, Batch 50, Loss: 0.3701478758454323\n",
      "Epoch 20, Batch 100, Loss: 0.35530837774276736\n",
      "Epoch 20, Batch 150, Loss: 0.371406991481781\n",
      "Epoch 20, Batch 200, Loss: 0.37865967601537703\n",
      "Epoch 20, Batch 250, Loss: 0.5347979116439819\n",
      "Epoch 20, Batch 300, Loss: 0.4263923454284668\n",
      "Epoch 20, Batch 350, Loss: 0.3834990954399109\n",
      "Epoch 20, Batch 400, Loss: 0.3653524973988533\n",
      "Epoch 20, Batch 450, Loss: 0.3888833248615265\n",
      "Epoch 20, Batch 500, Loss: 0.3555739882588387\n",
      "Epoch 20, Batch 550, Loss: 0.38900600105524064\n",
      "Epoch 20, Batch 600, Loss: 0.362706900537014\n",
      "Epoch 20, Batch 650, Loss: 0.39939513891935347\n",
      "Epoch 20, Batch 700, Loss: 0.370962053835392\n",
      "Epoch 20, Batch 750, Loss: 0.41078036308288574\n",
      "Epoch 20, Batch 800, Loss: 0.3906413531303406\n",
      "Epoch 20, Batch 850, Loss: 0.4014998197555542\n",
      "Epoch 20, Batch 900, Loss: 0.37145557582378386\n",
      "Epoch 21, Batch 50, Loss: 0.41955247163772585\n",
      "Epoch 21, Batch 100, Loss: 0.37684682190418245\n",
      "Epoch 21, Batch 150, Loss: 0.39386833816766736\n",
      "Epoch 21, Batch 200, Loss: 0.4133925211429596\n",
      "Epoch 21, Batch 250, Loss: 0.3517281225323677\n",
      "Epoch 21, Batch 300, Loss: 0.3372331395745277\n",
      "Epoch 21, Batch 350, Loss: 0.3969026133418083\n",
      "Epoch 21, Batch 400, Loss: 0.44648799538612366\n",
      "Epoch 21, Batch 450, Loss: 0.3953388738632202\n",
      "Epoch 21, Batch 500, Loss: 0.4112280964851379\n",
      "Epoch 21, Batch 550, Loss: 0.4011372393369675\n",
      "Epoch 21, Batch 600, Loss: 0.4134540927410126\n",
      "Epoch 21, Batch 650, Loss: 0.3771574360132217\n",
      "Epoch 21, Batch 700, Loss: 0.41587685227394106\n",
      "Epoch 21, Batch 750, Loss: 0.3707906293869019\n",
      "Epoch 21, Batch 800, Loss: 0.4082800340652466\n",
      "Epoch 21, Batch 850, Loss: 0.35771083652973173\n",
      "Epoch 21, Batch 900, Loss: 0.4055165773630142\n",
      "Epoch 22, Batch 50, Loss: 0.4449284881353378\n",
      "Epoch 22, Batch 100, Loss: 0.39413636714220046\n",
      "Epoch 22, Batch 150, Loss: 0.39225928246974945\n",
      "Epoch 22, Batch 200, Loss: 0.37368553012609484\n",
      "Epoch 22, Batch 250, Loss: 0.4209278482198715\n",
      "Epoch 22, Batch 300, Loss: 0.3561168286204338\n",
      "Epoch 22, Batch 350, Loss: 0.38286479860544204\n",
      "Epoch 22, Batch 400, Loss: 0.39497209280729295\n",
      "Epoch 22, Batch 450, Loss: 0.3799341741204262\n",
      "Epoch 22, Batch 500, Loss: 0.35311580687761307\n",
      "Epoch 22, Batch 550, Loss: 0.4199284029006958\n",
      "Epoch 22, Batch 600, Loss: 0.41259881913661955\n",
      "Epoch 22, Batch 650, Loss: 0.4010880094766617\n",
      "Epoch 22, Batch 700, Loss: 0.386895412504673\n",
      "Epoch 22, Batch 750, Loss: 0.41179206520318984\n",
      "Epoch 22, Batch 800, Loss: 0.3783087337017059\n",
      "Epoch 22, Batch 850, Loss: 0.4168292397260666\n",
      "Epoch 22, Batch 900, Loss: 0.37810429811477664\n",
      "Epoch 23, Batch 50, Loss: 0.39605247139930727\n",
      "Epoch 23, Batch 100, Loss: 0.39715795338153836\n",
      "Epoch 23, Batch 150, Loss: 0.3891058811545372\n",
      "Epoch 23, Batch 200, Loss: 0.3839957422018051\n",
      "Epoch 23, Batch 250, Loss: 0.4237787374854088\n",
      "Epoch 23, Batch 300, Loss: 0.38530898332595825\n",
      "Epoch 23, Batch 350, Loss: 0.4604844719171524\n",
      "Epoch 23, Batch 400, Loss: 0.40626611590385436\n",
      "Epoch 23, Batch 450, Loss: 0.43014896273612974\n",
      "Epoch 23, Batch 500, Loss: 0.433901187479496\n",
      "Epoch 23, Batch 550, Loss: 0.3814999195933342\n",
      "Epoch 23, Batch 600, Loss: 0.4165456321835518\n",
      "Epoch 23, Batch 650, Loss: 0.3928511217236519\n",
      "Epoch 23, Batch 700, Loss: 0.40778167188167574\n",
      "Epoch 23, Batch 750, Loss: 0.4560153949260712\n",
      "Epoch 23, Batch 800, Loss: 0.36587343841791153\n",
      "Epoch 23, Batch 850, Loss: 0.3798347970843315\n",
      "Epoch 23, Batch 900, Loss: 0.40120055168867114\n",
      "Epoch 24, Batch 50, Loss: 0.3606451290845871\n",
      "Epoch 24, Batch 100, Loss: 0.3598691001534462\n",
      "Epoch 24, Batch 150, Loss: 0.4059023097157478\n",
      "Epoch 24, Batch 200, Loss: 0.3962342092394829\n",
      "Epoch 24, Batch 250, Loss: 0.3825540894269943\n",
      "Epoch 24, Batch 300, Loss: 0.3591473463177681\n",
      "Epoch 24, Batch 350, Loss: 0.38531827569007876\n",
      "Epoch 24, Batch 400, Loss: 0.38278429746627807\n",
      "Epoch 24, Batch 450, Loss: 0.3995158910751343\n",
      "Epoch 24, Batch 500, Loss: 0.39828316956758497\n",
      "Epoch 24, Batch 550, Loss: 0.3774170213937759\n",
      "Epoch 24, Batch 600, Loss: 0.39203272461891175\n",
      "Epoch 24, Batch 650, Loss: 0.367719299197197\n",
      "Epoch 24, Batch 700, Loss: 0.44060894548892976\n",
      "Epoch 24, Batch 750, Loss: 0.40098704874515534\n",
      "Epoch 24, Batch 800, Loss: 0.3647850599884987\n",
      "Epoch 24, Batch 850, Loss: 0.42186343014240263\n",
      "Epoch 24, Batch 900, Loss: 0.3731005221605301\n",
      "Epoch 25, Batch 50, Loss: 0.35247458785772323\n",
      "Epoch 25, Batch 100, Loss: 0.3738700160384178\n",
      "Epoch 25, Batch 150, Loss: 0.38206990927457807\n",
      "Epoch 25, Batch 200, Loss: 0.3925070625543594\n",
      "Epoch 25, Batch 250, Loss: 0.3811349040269852\n",
      "Epoch 25, Batch 300, Loss: 0.366068931221962\n",
      "Epoch 25, Batch 350, Loss: 0.35563053637743\n",
      "Epoch 25, Batch 400, Loss: 0.41723711729049684\n",
      "Epoch 25, Batch 450, Loss: 0.3760361397266388\n",
      "Epoch 25, Batch 500, Loss: 0.38154302924871447\n",
      "Epoch 25, Batch 550, Loss: 0.37206553131341935\n",
      "Epoch 25, Batch 600, Loss: 0.3660953333973885\n",
      "Epoch 25, Batch 650, Loss: 0.3707756146788597\n",
      "Epoch 25, Batch 700, Loss: 0.3985038894414902\n",
      "Epoch 25, Batch 750, Loss: 0.4010296016931534\n",
      "Epoch 25, Batch 800, Loss: 0.3943892842531204\n",
      "Epoch 25, Batch 850, Loss: 0.37157437443733216\n",
      "Epoch 25, Batch 900, Loss: 0.38557079136371614\n",
      "Accuracy on test set: 0.685%\n",
      "Fitting for combination 13\n",
      "784\n",
      "1\n",
      "10\n",
      "[40, 10]\n",
      "True\n",
      "['tanh']\n",
      "SGD\n",
      "0.03\n",
      "0.03\n",
      "CrossEntropyLoss\n",
      "200\n",
      "25\n",
      "0\n",
      "Epoch 1, Batch 200, Loss: 2.1522229647636415\n",
      "Epoch 1, Batch 400, Loss: 1.4260713410377504\n",
      "Epoch 1, Batch 600, Loss: 1.3065128898620606\n",
      "Epoch 1, Batch 800, Loss: 1.2243532517552376\n",
      "Epoch 2, Batch 200, Loss: 1.1595434853434563\n",
      "Epoch 2, Batch 400, Loss: 1.1383841598033906\n",
      "Epoch 2, Batch 600, Loss: 1.133667093217373\n",
      "Epoch 2, Batch 800, Loss: 1.129949177503586\n",
      "Epoch 3, Batch 200, Loss: 1.0994625934958457\n",
      "Epoch 3, Batch 400, Loss: 1.1178975981473922\n",
      "Epoch 3, Batch 600, Loss: 1.1118240469694138\n",
      "Epoch 3, Batch 800, Loss: 1.111659831404686\n",
      "Epoch 4, Batch 200, Loss: 1.0815043872594834\n",
      "Epoch 4, Batch 400, Loss: 1.0950325560569762\n",
      "Epoch 4, Batch 600, Loss: 1.112872490286827\n",
      "Epoch 4, Batch 800, Loss: 1.1145518133044243\n",
      "Epoch 5, Batch 200, Loss: 1.1051133874058723\n",
      "Epoch 5, Batch 400, Loss: 1.0677935540676118\n",
      "Epoch 5, Batch 600, Loss: 1.0928582307696342\n",
      "Epoch 5, Batch 800, Loss: 1.090646686553955\n",
      "Epoch 6, Batch 200, Loss: 1.1220491391420364\n",
      "Epoch 6, Batch 400, Loss: 1.077772637307644\n",
      "Epoch 6, Batch 600, Loss: 1.0841569259762764\n",
      "Epoch 6, Batch 800, Loss: 1.0859232014417648\n",
      "Epoch 7, Batch 200, Loss: 1.0769111841917038\n",
      "Epoch 7, Batch 400, Loss: 1.0808051434159278\n",
      "Epoch 7, Batch 600, Loss: 1.0863627034425736\n",
      "Epoch 7, Batch 800, Loss: 1.1034053036570548\n",
      "Epoch 8, Batch 200, Loss: 1.0882162424921988\n",
      "Epoch 8, Batch 400, Loss: 1.0803431510925292\n",
      "Epoch 8, Batch 600, Loss: 1.0800586646795274\n",
      "Epoch 8, Batch 800, Loss: 1.102031855583191\n",
      "Epoch 9, Batch 200, Loss: 1.117498733997345\n",
      "Epoch 9, Batch 400, Loss: 1.0867144936323165\n",
      "Epoch 9, Batch 600, Loss: 1.092286551296711\n",
      "Epoch 9, Batch 800, Loss: 1.0789417073130607\n",
      "Epoch 10, Batch 200, Loss: 1.0654827901721\n",
      "Epoch 10, Batch 400, Loss: 1.0827635163068772\n",
      "Epoch 10, Batch 600, Loss: 1.1055445230007173\n",
      "Epoch 10, Batch 800, Loss: 1.0871989911794662\n",
      "Epoch 11, Batch 200, Loss: 1.0875080701708795\n",
      "Epoch 11, Batch 400, Loss: 1.0825883054733276\n",
      "Epoch 11, Batch 600, Loss: 1.0659407433867454\n",
      "Epoch 11, Batch 800, Loss: 1.1043459644913673\n",
      "Epoch 12, Batch 200, Loss: 1.0738885316252709\n",
      "Epoch 12, Batch 400, Loss: 1.0924847197532654\n",
      "Epoch 12, Batch 600, Loss: 1.0795048823952675\n",
      "Epoch 12, Batch 800, Loss: 1.0860298323631286\n",
      "Epoch 13, Batch 200, Loss: 1.072169335782528\n",
      "Epoch 13, Batch 400, Loss: 1.0905375450849533\n",
      "Epoch 13, Batch 600, Loss: 1.0813298097252846\n",
      "Epoch 13, Batch 800, Loss: 1.0885344195365905\n",
      "Epoch 14, Batch 200, Loss: 1.0726117607951164\n",
      "Epoch 14, Batch 400, Loss: 1.104463199675083\n",
      "Epoch 14, Batch 600, Loss: 1.0855586820840835\n",
      "Epoch 14, Batch 800, Loss: 1.0833992993831634\n",
      "Epoch 15, Batch 200, Loss: 1.076358935236931\n",
      "Epoch 15, Batch 400, Loss: 1.0755312168598175\n",
      "Epoch 15, Batch 600, Loss: 1.0895953664183617\n",
      "Epoch 15, Batch 800, Loss: 1.0776563721895218\n",
      "Epoch 16, Batch 200, Loss: 1.0957455986738205\n",
      "Epoch 16, Batch 400, Loss: 1.0653635382652282\n",
      "Epoch 16, Batch 600, Loss: 1.0765486457943916\n",
      "Epoch 16, Batch 800, Loss: 1.0928075531125068\n",
      "Epoch 17, Batch 200, Loss: 1.0642623937129974\n",
      "Epoch 17, Batch 400, Loss: 1.0788889610767365\n",
      "Epoch 17, Batch 600, Loss: 1.0733710566163064\n",
      "Epoch 17, Batch 800, Loss: 1.1061978006362916\n",
      "Epoch 18, Batch 200, Loss: 1.082881246805191\n",
      "Epoch 18, Batch 400, Loss: 1.0921331104636192\n",
      "Epoch 18, Batch 600, Loss: 1.081206994354725\n",
      "Epoch 18, Batch 800, Loss: 1.0732576659321784\n",
      "Epoch 19, Batch 200, Loss: 1.0942247149348259\n",
      "Epoch 19, Batch 400, Loss: 1.0671012210845947\n",
      "Epoch 19, Batch 600, Loss: 1.092332131564617\n",
      "Epoch 19, Batch 800, Loss: 1.086136812865734\n",
      "Epoch 20, Batch 200, Loss: 1.0846208941936493\n",
      "Epoch 20, Batch 400, Loss: 1.0676616045832634\n",
      "Epoch 20, Batch 600, Loss: 1.0944152188301086\n",
      "Epoch 20, Batch 800, Loss: 1.0783279839158058\n",
      "Epoch 21, Batch 200, Loss: 1.071927281320095\n",
      "Epoch 21, Batch 400, Loss: 1.0888871920108796\n",
      "Epoch 21, Batch 600, Loss: 1.0824975565075874\n",
      "Epoch 21, Batch 800, Loss: 1.080570602118969\n",
      "Epoch 22, Batch 200, Loss: 1.095118637084961\n",
      "Epoch 22, Batch 400, Loss: 1.0856411489844322\n",
      "Epoch 22, Batch 600, Loss: 1.0716500094532966\n",
      "Epoch 22, Batch 800, Loss: 1.0707061046361923\n",
      "Epoch 23, Batch 200, Loss: 1.086538597345352\n",
      "Epoch 23, Batch 400, Loss: 1.0735194849967957\n",
      "Epoch 23, Batch 600, Loss: 1.0895143336057662\n",
      "Epoch 23, Batch 800, Loss: 1.0723532104492188\n",
      "Epoch 24, Batch 200, Loss: 1.0704079845547676\n",
      "Epoch 24, Batch 400, Loss: 1.0581686851382255\n",
      "Epoch 24, Batch 600, Loss: 1.1123966303467752\n",
      "Epoch 24, Batch 800, Loss: 1.1044117322564124\n",
      "Epoch 25, Batch 200, Loss: 1.0608422738313674\n",
      "Epoch 25, Batch 400, Loss: 1.0805860099196434\n",
      "Epoch 25, Batch 600, Loss: 1.0859583735466003\n",
      "Epoch 25, Batch 800, Loss: 1.088928737640381\n",
      "Accuracy on test set: 0.8164%\n",
      "Fitting for combination 14\n",
      "784\n",
      "1\n",
      "10\n",
      "[40, 10]\n",
      "True\n",
      "['relu']\n",
      "Adam\n",
      "0.3\n",
      "0.01\n",
      "CrossEntropyLoss\n",
      "100\n",
      "25\n",
      "0\n",
      "Epoch 1, Batch 100, Loss: 12.30119959115982\n",
      "Epoch 1, Batch 200, Loss: 4.605099507570267\n",
      "Epoch 1, Batch 300, Loss: 7.2189945602417\n",
      "Epoch 1, Batch 400, Loss: 4.913888680934906\n",
      "Epoch 1, Batch 500, Loss: 7.345905610322952\n",
      "Epoch 1, Batch 600, Loss: 4.577956746816636\n",
      "Epoch 1, Batch 700, Loss: 4.889568388462067\n",
      "Epoch 1, Batch 800, Loss: 5.064829199314118\n",
      "Epoch 1, Batch 900, Loss: 5.533942866325378\n",
      "Epoch 2, Batch 100, Loss: 4.428665142059327\n",
      "Epoch 2, Batch 200, Loss: 3.8491287994384766\n",
      "Epoch 2, Batch 300, Loss: 4.862778446674347\n",
      "Epoch 2, Batch 400, Loss: 5.6830068516731265\n",
      "Epoch 2, Batch 500, Loss: 4.483915843963623\n",
      "Epoch 2, Batch 600, Loss: 4.4677178359031675\n",
      "Epoch 2, Batch 700, Loss: 5.320581502914429\n",
      "Epoch 2, Batch 800, Loss: 5.43236713886261\n",
      "Epoch 2, Batch 900, Loss: 7.583854078054428\n",
      "Epoch 3, Batch 100, Loss: 5.864244773387909\n",
      "Epoch 3, Batch 200, Loss: 5.141760330200196\n",
      "Epoch 3, Batch 300, Loss: 4.713862268924713\n",
      "Epoch 3, Batch 400, Loss: 4.26460853099823\n",
      "Epoch 3, Batch 500, Loss: 12.07598119020462\n",
      "Epoch 3, Batch 600, Loss: 4.331787523031235\n",
      "Epoch 3, Batch 700, Loss: 6.464990154504776\n",
      "Epoch 3, Batch 800, Loss: 7.975823575258255\n",
      "Epoch 3, Batch 900, Loss: 5.109059703350067\n",
      "Epoch 4, Batch 100, Loss: 5.238335976600647\n",
      "Epoch 4, Batch 200, Loss: 5.649692660570144\n",
      "Epoch 4, Batch 300, Loss: 5.6208649611473085\n",
      "Epoch 4, Batch 400, Loss: 4.7349899911880495\n",
      "Epoch 4, Batch 500, Loss: 4.6538363969326015\n",
      "Epoch 4, Batch 600, Loss: 4.358041245937347\n",
      "Epoch 4, Batch 700, Loss: 6.62364151597023\n",
      "Epoch 4, Batch 800, Loss: 6.204123258590698\n",
      "Epoch 4, Batch 900, Loss: 5.60073751449585\n",
      "Epoch 5, Batch 100, Loss: 4.572366071939468\n",
      "Epoch 5, Batch 200, Loss: 4.719549877643585\n",
      "Epoch 5, Batch 300, Loss: 5.793816437721253\n",
      "Epoch 5, Batch 400, Loss: 4.752944948673249\n",
      "Epoch 5, Batch 500, Loss: 3.7520876586437226\n",
      "Epoch 5, Batch 600, Loss: 5.409539612531662\n",
      "Epoch 5, Batch 700, Loss: 7.013384631872177\n",
      "Epoch 5, Batch 800, Loss: 6.1408562755584715\n",
      "Epoch 5, Batch 900, Loss: 5.324657880067825\n",
      "Epoch 6, Batch 100, Loss: 7.714896941184998\n",
      "Epoch 6, Batch 200, Loss: 5.504405090808868\n",
      "Epoch 6, Batch 300, Loss: 3.8508800685405733\n",
      "Epoch 6, Batch 400, Loss: 5.142610086202621\n",
      "Epoch 6, Batch 500, Loss: 5.210382103919983\n",
      "Epoch 6, Batch 600, Loss: 4.329153450727463\n",
      "Epoch 6, Batch 700, Loss: 8.5993587911129\n",
      "Epoch 6, Batch 800, Loss: 5.771918451786041\n",
      "Epoch 6, Batch 900, Loss: 4.714296207427979\n",
      "Epoch 7, Batch 100, Loss: 3.9528765726089476\n",
      "Epoch 7, Batch 200, Loss: 3.7864546465873716\n",
      "Epoch 7, Batch 300, Loss: 4.247268161773682\n",
      "Epoch 7, Batch 400, Loss: 4.853759070634842\n",
      "Epoch 7, Batch 500, Loss: 6.279315769672394\n",
      "Epoch 7, Batch 600, Loss: 5.225786885023117\n",
      "Epoch 7, Batch 700, Loss: 4.948426084518433\n",
      "Epoch 7, Batch 800, Loss: 3.936097477674484\n",
      "Epoch 7, Batch 900, Loss: 10.709565696716309\n",
      "Epoch 8, Batch 100, Loss: 5.426872228384018\n",
      "Epoch 8, Batch 200, Loss: 4.186588524580002\n",
      "Epoch 8, Batch 300, Loss: 6.396318018436432\n",
      "Epoch 8, Batch 400, Loss: 4.271461102962494\n",
      "Epoch 8, Batch 500, Loss: 7.126853141784668\n",
      "Epoch 8, Batch 600, Loss: 5.820365525484085\n",
      "Epoch 8, Batch 700, Loss: 6.002998380661011\n",
      "Epoch 8, Batch 800, Loss: 4.689082825183869\n",
      "Epoch 8, Batch 900, Loss: 3.5469915044307707\n",
      "Epoch 9, Batch 100, Loss: 6.995720812082291\n",
      "Epoch 9, Batch 200, Loss: 6.420080845355987\n",
      "Epoch 9, Batch 300, Loss: 5.656077644824982\n",
      "Epoch 9, Batch 400, Loss: 4.198885273933411\n",
      "Epoch 9, Batch 500, Loss: 3.227240697145462\n",
      "Epoch 9, Batch 600, Loss: 4.739050699472427\n",
      "Epoch 9, Batch 700, Loss: 4.0244319784641265\n",
      "Epoch 9, Batch 800, Loss: 4.823603510856628\n",
      "Epoch 9, Batch 900, Loss: 7.158375791311264\n",
      "Epoch 10, Batch 100, Loss: 5.502416350841522\n",
      "Epoch 10, Batch 200, Loss: 4.902553064823151\n",
      "Epoch 10, Batch 300, Loss: 13.708807051181793\n",
      "Epoch 10, Batch 400, Loss: 6.304207983016968\n",
      "Epoch 10, Batch 500, Loss: 7.148728864192963\n",
      "Epoch 10, Batch 600, Loss: 4.41080397605896\n",
      "Epoch 10, Batch 700, Loss: 3.3949815440177917\n",
      "Epoch 10, Batch 800, Loss: 7.112978123426437\n",
      "Epoch 10, Batch 900, Loss: 7.211811199188232\n",
      "Epoch 11, Batch 100, Loss: 5.744321063756943\n",
      "Epoch 11, Batch 200, Loss: 5.2758827614784245\n",
      "Epoch 11, Batch 300, Loss: 5.084099674224854\n",
      "Epoch 11, Batch 400, Loss: 5.420761847496033\n",
      "Epoch 11, Batch 500, Loss: 4.044105334281921\n",
      "Epoch 11, Batch 600, Loss: 7.054764831066132\n",
      "Epoch 11, Batch 700, Loss: 3.0439507603645324\n",
      "Epoch 11, Batch 800, Loss: 3.689915759563446\n",
      "Epoch 11, Batch 900, Loss: 3.4778590846061705\n",
      "Epoch 12, Batch 100, Loss: 4.392089638710022\n",
      "Epoch 12, Batch 200, Loss: 4.595377722978592\n",
      "Epoch 12, Batch 300, Loss: 6.238997101783752\n",
      "Epoch 12, Batch 400, Loss: 5.048030573129654\n",
      "Epoch 12, Batch 500, Loss: 6.043050810098648\n",
      "Epoch 12, Batch 600, Loss: 4.5861203837394715\n",
      "Epoch 12, Batch 700, Loss: 5.024931807518005\n",
      "Epoch 12, Batch 800, Loss: 4.8850351595878605\n",
      "Epoch 12, Batch 900, Loss: 5.568454515933991\n",
      "Epoch 13, Batch 100, Loss: 6.716930141448975\n",
      "Epoch 13, Batch 200, Loss: 5.076464221477509\n",
      "Epoch 13, Batch 300, Loss: 4.15906887292862\n",
      "Epoch 13, Batch 400, Loss: 3.329647911787033\n",
      "Epoch 13, Batch 500, Loss: 5.731641918420792\n",
      "Epoch 13, Batch 600, Loss: 7.630718319416046\n",
      "Epoch 13, Batch 700, Loss: 4.5844842004776005\n",
      "Epoch 13, Batch 800, Loss: 4.12216638803482\n",
      "Epoch 13, Batch 900, Loss: 4.029183937311172\n",
      "Epoch 14, Batch 100, Loss: 4.336058554649353\n",
      "Epoch 14, Batch 200, Loss: 5.811103715896606\n",
      "Epoch 14, Batch 300, Loss: 6.284270298480988\n",
      "Epoch 14, Batch 400, Loss: 6.207851836681366\n",
      "Epoch 14, Batch 500, Loss: 4.837673788070679\n",
      "Epoch 14, Batch 600, Loss: 4.918044716119766\n",
      "Epoch 14, Batch 700, Loss: 6.195954244136811\n",
      "Epoch 14, Batch 800, Loss: 3.7802908754348756\n",
      "Epoch 14, Batch 900, Loss: 6.338323693275452\n",
      "Epoch 15, Batch 100, Loss: 5.293229159116745\n",
      "Epoch 15, Batch 200, Loss: 5.880267344713211\n",
      "Epoch 15, Batch 300, Loss: 4.815438554286957\n",
      "Epoch 15, Batch 400, Loss: 10.593304827213288\n",
      "Epoch 15, Batch 500, Loss: 6.276733949184417\n",
      "Epoch 15, Batch 600, Loss: 3.2407855701446535\n",
      "Epoch 15, Batch 700, Loss: 4.165706732273102\n",
      "Epoch 15, Batch 800, Loss: 5.6795361816883085\n",
      "Epoch 15, Batch 900, Loss: 4.888907716274262\n",
      "Epoch 16, Batch 100, Loss: 9.685709509849548\n",
      "Epoch 16, Batch 200, Loss: 6.980602917671203\n",
      "Epoch 16, Batch 300, Loss: 5.656751246452331\n",
      "Epoch 16, Batch 400, Loss: 6.0160490667819975\n",
      "Epoch 16, Batch 500, Loss: 4.933009469509125\n",
      "Epoch 16, Batch 600, Loss: 6.781807638406754\n",
      "Epoch 16, Batch 700, Loss: 6.470548813343048\n",
      "Epoch 16, Batch 800, Loss: 3.5048918402194977\n",
      "Epoch 16, Batch 900, Loss: 5.345557540655136\n",
      "Epoch 17, Batch 100, Loss: 5.132997026443482\n",
      "Epoch 17, Batch 200, Loss: 4.363582807779312\n",
      "Epoch 17, Batch 300, Loss: 3.596327813863754\n",
      "Epoch 17, Batch 400, Loss: 4.974618122577668\n",
      "Epoch 17, Batch 500, Loss: 8.659574913978577\n",
      "Epoch 17, Batch 600, Loss: 5.286366716623307\n",
      "Epoch 17, Batch 700, Loss: 3.7155815148353577\n",
      "Epoch 17, Batch 800, Loss: 4.125779832601547\n",
      "Epoch 17, Batch 900, Loss: 5.085012996196747\n",
      "Epoch 18, Batch 100, Loss: 5.0202508902549745\n",
      "Epoch 18, Batch 200, Loss: 5.128885202407837\n",
      "Epoch 18, Batch 300, Loss: 4.0931363356113435\n",
      "Epoch 18, Batch 400, Loss: 6.069287227392197\n",
      "Epoch 18, Batch 500, Loss: 6.720676815509796\n",
      "Epoch 18, Batch 600, Loss: 5.350638449192047\n",
      "Epoch 18, Batch 700, Loss: 5.91024222612381\n",
      "Epoch 18, Batch 800, Loss: 4.212248528003693\n",
      "Epoch 18, Batch 900, Loss: 4.364808371067047\n",
      "Epoch 19, Batch 100, Loss: 4.4615675687789915\n",
      "Epoch 19, Batch 200, Loss: 7.274523952007294\n",
      "Epoch 19, Batch 300, Loss: 8.567254478931426\n",
      "Epoch 19, Batch 400, Loss: 10.175285990238189\n",
      "Epoch 19, Batch 500, Loss: 5.808221890926361\n",
      "Epoch 19, Batch 600, Loss: 5.259868197441101\n",
      "Epoch 19, Batch 700, Loss: 5.255434956550598\n",
      "Epoch 19, Batch 800, Loss: 5.525299255847931\n",
      "Epoch 19, Batch 900, Loss: 4.476488546133042\n",
      "Epoch 20, Batch 100, Loss: 4.84928094625473\n",
      "Epoch 20, Batch 200, Loss: 7.722639694213867\n",
      "Epoch 20, Batch 300, Loss: 5.73926815032959\n",
      "Epoch 20, Batch 400, Loss: 4.999757843017578\n",
      "Epoch 20, Batch 500, Loss: 5.715262007713318\n",
      "Epoch 20, Batch 600, Loss: 8.745790548324585\n",
      "Epoch 20, Batch 700, Loss: 4.112452141046524\n",
      "Epoch 20, Batch 800, Loss: 2.9316247498989103\n",
      "Epoch 20, Batch 900, Loss: 3.895040601491928\n",
      "Epoch 21, Batch 100, Loss: 8.935484132766724\n",
      "Epoch 21, Batch 200, Loss: 5.66889858007431\n",
      "Epoch 21, Batch 300, Loss: 4.992333979606628\n",
      "Epoch 21, Batch 400, Loss: 3.762332491874695\n",
      "Epoch 21, Batch 500, Loss: 4.224987654685974\n",
      "Epoch 21, Batch 600, Loss: 4.570349822044372\n",
      "Epoch 21, Batch 700, Loss: 5.987516993284226\n",
      "Epoch 21, Batch 800, Loss: 8.489216868877412\n",
      "Epoch 21, Batch 900, Loss: 5.776805834770203\n",
      "Epoch 22, Batch 100, Loss: 3.739170850515366\n",
      "Epoch 22, Batch 200, Loss: 4.186932108402252\n",
      "Epoch 22, Batch 300, Loss: 5.129306423664093\n",
      "Epoch 22, Batch 400, Loss: 6.041273312568665\n",
      "Epoch 22, Batch 500, Loss: 7.184531463384628\n",
      "Epoch 22, Batch 600, Loss: 5.218490028381348\n",
      "Epoch 22, Batch 700, Loss: 5.9130976462364195\n",
      "Epoch 22, Batch 800, Loss: 4.1711695098876955\n",
      "Epoch 22, Batch 900, Loss: 4.253575711250305\n",
      "Epoch 23, Batch 100, Loss: 5.607226178646088\n",
      "Epoch 23, Batch 200, Loss: 5.725341817140579\n",
      "Epoch 23, Batch 300, Loss: 6.289925414323807\n",
      "Epoch 23, Batch 400, Loss: 5.184538192749024\n",
      "Epoch 23, Batch 500, Loss: 4.760603158473969\n",
      "Epoch 23, Batch 600, Loss: 4.9121949577331545\n",
      "Epoch 23, Batch 700, Loss: 3.87561221241951\n",
      "Epoch 23, Batch 800, Loss: 5.378544949293136\n",
      "Epoch 23, Batch 900, Loss: 5.087520403861999\n",
      "Epoch 24, Batch 100, Loss: 5.639164160490036\n",
      "Epoch 24, Batch 200, Loss: 5.589332432746887\n",
      "Epoch 24, Batch 300, Loss: 4.973777625560761\n",
      "Epoch 24, Batch 400, Loss: 3.6971972155570985\n",
      "Epoch 24, Batch 500, Loss: 3.9805485081672667\n",
      "Epoch 24, Batch 600, Loss: 3.9276248061656953\n",
      "Epoch 24, Batch 700, Loss: 5.701885322332382\n",
      "Epoch 24, Batch 800, Loss: 5.296477957963943\n",
      "Epoch 24, Batch 900, Loss: 10.627363924980164\n",
      "Epoch 25, Batch 100, Loss: 7.095831882953644\n",
      "Epoch 25, Batch 200, Loss: 4.570285679101944\n",
      "Epoch 25, Batch 300, Loss: 5.786813567876816\n",
      "Epoch 25, Batch 400, Loss: 4.5196975028514865\n",
      "Epoch 25, Batch 500, Loss: 3.706663666963577\n",
      "Epoch 25, Batch 600, Loss: 4.102085926532745\n",
      "Epoch 25, Batch 700, Loss: 3.8839830017089843\n",
      "Epoch 25, Batch 800, Loss: 5.845512911081314\n",
      "Epoch 25, Batch 900, Loss: 4.509767410755157\n",
      "Accuracy on test set: 0.2804%\n",
      "Fitting for combination 15\n",
      "784\n",
      "1\n",
      "10\n",
      "[40, 10]\n",
      "True\n",
      "['relu']\n",
      "SGD\n",
      "0.03\n",
      "0\n",
      "CrossEntropyLoss\n",
      "100\n",
      "25\n",
      "0\n",
      "Epoch 1, Batch 100, Loss: 1.2477568727731705\n",
      "Epoch 1, Batch 200, Loss: 0.7096290493011475\n",
      "Epoch 1, Batch 300, Loss: 0.6453387016057968\n",
      "Epoch 1, Batch 400, Loss: 0.5783708286285401\n",
      "Epoch 1, Batch 500, Loss: 0.563743616938591\n",
      "Epoch 1, Batch 600, Loss: 0.5382095891237259\n",
      "Epoch 1, Batch 700, Loss: 0.4985040521621704\n",
      "Epoch 1, Batch 800, Loss: 0.48082533091306684\n",
      "Epoch 1, Batch 900, Loss: 0.5081545495986939\n",
      "Epoch 2, Batch 100, Loss: 0.4688196194171905\n",
      "Epoch 2, Batch 200, Loss: 0.48382079899311065\n",
      "Epoch 2, Batch 300, Loss: 0.4733696618676186\n",
      "Epoch 2, Batch 400, Loss: 0.4655536833405495\n",
      "Epoch 2, Batch 500, Loss: 0.4576123520731926\n",
      "Epoch 2, Batch 600, Loss: 0.43657647803425786\n",
      "Epoch 2, Batch 700, Loss: 0.4336512091755867\n",
      "Epoch 2, Batch 800, Loss: 0.4287546969950199\n",
      "Epoch 2, Batch 900, Loss: 0.44458185240626336\n",
      "Epoch 3, Batch 100, Loss: 0.42306166887283325\n",
      "Epoch 3, Batch 200, Loss: 0.41272551417350767\n",
      "Epoch 3, Batch 300, Loss: 0.43027101814746854\n",
      "Epoch 3, Batch 400, Loss: 0.4252408142387867\n",
      "Epoch 3, Batch 500, Loss: 0.40755499467253686\n",
      "Epoch 3, Batch 600, Loss: 0.40580220431089403\n",
      "Epoch 3, Batch 700, Loss: 0.414074814170599\n",
      "Epoch 3, Batch 800, Loss: 0.40903852730989454\n",
      "Epoch 3, Batch 900, Loss: 0.41564932331442833\n",
      "Epoch 4, Batch 100, Loss: 0.40470683634281157\n",
      "Epoch 4, Batch 200, Loss: 0.39514010116457937\n",
      "Epoch 4, Batch 300, Loss: 0.3827989257872105\n",
      "Epoch 4, Batch 400, Loss: 0.39539204686880114\n",
      "Epoch 4, Batch 500, Loss: 0.39665366291999815\n",
      "Epoch 4, Batch 600, Loss: 0.4112152560055256\n",
      "Epoch 4, Batch 700, Loss: 0.3874493625760078\n",
      "Epoch 4, Batch 800, Loss: 0.383393464833498\n",
      "Epoch 4, Batch 900, Loss: 0.38556524634361267\n",
      "Epoch 5, Batch 100, Loss: 0.3873500244319439\n",
      "Epoch 5, Batch 200, Loss: 0.39074749007821086\n",
      "Epoch 5, Batch 300, Loss: 0.36822597339749336\n",
      "Epoch 5, Batch 400, Loss: 0.3698038113117218\n",
      "Epoch 5, Batch 500, Loss: 0.3761288453638554\n",
      "Epoch 5, Batch 600, Loss: 0.39105128347873686\n",
      "Epoch 5, Batch 700, Loss: 0.3580543513596058\n",
      "Epoch 5, Batch 800, Loss: 0.3717935821413994\n",
      "Epoch 5, Batch 900, Loss: 0.3690649439394474\n",
      "Epoch 6, Batch 100, Loss: 0.3579433000087738\n",
      "Epoch 6, Batch 200, Loss: 0.3683895029127598\n",
      "Epoch 6, Batch 300, Loss: 0.35624594405293464\n",
      "Epoch 6, Batch 400, Loss: 0.37193150162696836\n",
      "Epoch 6, Batch 500, Loss: 0.3636270129680634\n",
      "Epoch 6, Batch 600, Loss: 0.3613286596536636\n",
      "Epoch 6, Batch 700, Loss: 0.3680731979012489\n",
      "Epoch 6, Batch 800, Loss: 0.36599267974495886\n",
      "Epoch 6, Batch 900, Loss: 0.35450389310717584\n",
      "Epoch 7, Batch 100, Loss: 0.36406048193573953\n",
      "Epoch 7, Batch 200, Loss: 0.35523866102099416\n",
      "Epoch 7, Batch 300, Loss: 0.33645806327462197\n",
      "Epoch 7, Batch 400, Loss: 0.34511520504951476\n",
      "Epoch 7, Batch 500, Loss: 0.34994853138923643\n",
      "Epoch 7, Batch 600, Loss: 0.34895075276494025\n",
      "Epoch 7, Batch 700, Loss: 0.3551008643209934\n",
      "Epoch 7, Batch 800, Loss: 0.34478612899780275\n",
      "Epoch 7, Batch 900, Loss: 0.3548727022111416\n",
      "Epoch 8, Batch 100, Loss: 0.3488764634728432\n",
      "Epoch 8, Batch 200, Loss: 0.35154214993119237\n",
      "Epoch 8, Batch 300, Loss: 0.33945922672748563\n",
      "Epoch 8, Batch 400, Loss: 0.3422185325622559\n",
      "Epoch 8, Batch 500, Loss: 0.33890254840254785\n",
      "Epoch 8, Batch 600, Loss: 0.3207992711663246\n",
      "Epoch 8, Batch 700, Loss: 0.3460656054317951\n",
      "Epoch 8, Batch 800, Loss: 0.3415945403277874\n",
      "Epoch 8, Batch 900, Loss: 0.3364426876604557\n",
      "Epoch 9, Batch 100, Loss: 0.3298116844892502\n",
      "Epoch 9, Batch 200, Loss: 0.32499228037893774\n",
      "Epoch 9, Batch 300, Loss: 0.34945531606674196\n",
      "Epoch 9, Batch 400, Loss: 0.32562856674194335\n",
      "Epoch 9, Batch 500, Loss: 0.3303728649020195\n",
      "Epoch 9, Batch 600, Loss: 0.34669941842556\n",
      "Epoch 9, Batch 700, Loss: 0.3238570183515549\n",
      "Epoch 9, Batch 800, Loss: 0.32645170629024506\n",
      "Epoch 9, Batch 900, Loss: 0.3201588723808527\n",
      "Epoch 10, Batch 100, Loss: 0.32097947627305984\n",
      "Epoch 10, Batch 200, Loss: 0.32507512003183364\n",
      "Epoch 10, Batch 300, Loss: 0.29764790177345274\n",
      "Epoch 10, Batch 400, Loss: 0.3393173475563526\n",
      "Epoch 10, Batch 500, Loss: 0.32393195331096647\n",
      "Epoch 10, Batch 600, Loss: 0.3326152440905571\n",
      "Epoch 10, Batch 700, Loss: 0.3192917089164257\n",
      "Epoch 10, Batch 800, Loss: 0.33495820991694925\n",
      "Epoch 10, Batch 900, Loss: 0.32489842355251314\n",
      "Epoch 11, Batch 100, Loss: 0.31291641905903816\n",
      "Epoch 11, Batch 200, Loss: 0.3139693570137024\n",
      "Epoch 11, Batch 300, Loss: 0.3079320354014635\n",
      "Epoch 11, Batch 400, Loss: 0.3110105989873409\n",
      "Epoch 11, Batch 500, Loss: 0.3291968148946762\n",
      "Epoch 11, Batch 600, Loss: 0.31268466979265214\n",
      "Epoch 11, Batch 700, Loss: 0.31767116248607635\n",
      "Epoch 11, Batch 800, Loss: 0.3190613116323948\n",
      "Epoch 11, Batch 900, Loss: 0.32601641163229944\n",
      "Epoch 12, Batch 100, Loss: 0.31869790755212307\n",
      "Epoch 12, Batch 200, Loss: 0.30035031273961066\n",
      "Epoch 12, Batch 300, Loss: 0.30021133363246916\n",
      "Epoch 12, Batch 400, Loss: 0.31157302752137184\n",
      "Epoch 12, Batch 500, Loss: 0.31766021370887754\n",
      "Epoch 12, Batch 600, Loss: 0.30294006660580636\n",
      "Epoch 12, Batch 700, Loss: 0.326936813890934\n",
      "Epoch 12, Batch 800, Loss: 0.3007852065563202\n",
      "Epoch 12, Batch 900, Loss: 0.3147445918619633\n",
      "Epoch 13, Batch 100, Loss: 0.30973717346787455\n",
      "Epoch 13, Batch 200, Loss: 0.3156515870988369\n",
      "Epoch 13, Batch 300, Loss: 0.2966006474196911\n",
      "Epoch 13, Batch 400, Loss: 0.29521703757345674\n",
      "Epoch 13, Batch 500, Loss: 0.30004249706864355\n",
      "Epoch 13, Batch 600, Loss: 0.3086092438548803\n",
      "Epoch 13, Batch 700, Loss: 0.3102462787926197\n",
      "Epoch 13, Batch 800, Loss: 0.31362263321876527\n",
      "Epoch 13, Batch 900, Loss: 0.2963421829044819\n",
      "Epoch 14, Batch 100, Loss: 0.29661551117897034\n",
      "Epoch 14, Batch 200, Loss: 0.29250217482447627\n",
      "Epoch 14, Batch 300, Loss: 0.30299927935004234\n",
      "Epoch 14, Batch 400, Loss: 0.301723345220089\n",
      "Epoch 14, Batch 500, Loss: 0.28454664327204227\n",
      "Epoch 14, Batch 600, Loss: 0.31392472580075265\n",
      "Epoch 14, Batch 700, Loss: 0.2972534031420946\n",
      "Epoch 14, Batch 800, Loss: 0.3017397604882717\n",
      "Epoch 14, Batch 900, Loss: 0.29612818829715254\n",
      "Epoch 15, Batch 100, Loss: 0.3047249435633421\n",
      "Epoch 15, Batch 200, Loss: 0.2861498096585274\n",
      "Epoch 15, Batch 300, Loss: 0.3009944627434015\n",
      "Epoch 15, Batch 400, Loss: 0.3055629596114159\n",
      "Epoch 15, Batch 500, Loss: 0.2740871535986662\n",
      "Epoch 15, Batch 600, Loss: 0.2829229447245598\n",
      "Epoch 15, Batch 700, Loss: 0.2849036826938391\n",
      "Epoch 15, Batch 800, Loss: 0.30845904678106306\n",
      "Epoch 15, Batch 900, Loss: 0.30762048214674\n",
      "Epoch 16, Batch 100, Loss: 0.28563410371541975\n",
      "Epoch 16, Batch 200, Loss: 0.29277281820774076\n",
      "Epoch 16, Batch 300, Loss: 0.2886930899322033\n",
      "Epoch 16, Batch 400, Loss: 0.30862244084477425\n",
      "Epoch 16, Batch 500, Loss: 0.28298794858157633\n",
      "Epoch 16, Batch 600, Loss: 0.2847148513048887\n",
      "Epoch 16, Batch 700, Loss: 0.2949575212597847\n",
      "Epoch 16, Batch 800, Loss: 0.2932024412602186\n",
      "Epoch 16, Batch 900, Loss: 0.2760487820208073\n",
      "Epoch 17, Batch 100, Loss: 0.281758985221386\n",
      "Epoch 17, Batch 200, Loss: 0.3035340374708176\n",
      "Epoch 17, Batch 300, Loss: 0.28398707903921605\n",
      "Epoch 17, Batch 400, Loss: 0.29099849924445154\n",
      "Epoch 17, Batch 500, Loss: 0.2796082856506109\n",
      "Epoch 17, Batch 600, Loss: 0.2855635318160057\n",
      "Epoch 17, Batch 700, Loss: 0.27698377184569833\n",
      "Epoch 17, Batch 800, Loss: 0.2795073522627354\n",
      "Epoch 17, Batch 900, Loss: 0.28114200204610823\n",
      "Epoch 18, Batch 100, Loss: 0.28288615338504314\n",
      "Epoch 18, Batch 200, Loss: 0.2760685093700886\n",
      "Epoch 18, Batch 300, Loss: 0.28808042787015437\n",
      "Epoch 18, Batch 400, Loss: 0.276226721405983\n",
      "Epoch 18, Batch 500, Loss: 0.2919485153257847\n",
      "Epoch 18, Batch 600, Loss: 0.2749433492869139\n",
      "Epoch 18, Batch 700, Loss: 0.28053681962192056\n",
      "Epoch 18, Batch 800, Loss: 0.28659698985517024\n",
      "Epoch 18, Batch 900, Loss: 0.27927554666996\n",
      "Epoch 19, Batch 100, Loss: 0.2822598320245743\n",
      "Epoch 19, Batch 200, Loss: 0.277098508477211\n",
      "Epoch 19, Batch 300, Loss: 0.2837308906763792\n",
      "Epoch 19, Batch 400, Loss: 0.2708266705274582\n",
      "Epoch 19, Batch 500, Loss: 0.2702275186032057\n",
      "Epoch 19, Batch 600, Loss: 0.2882897682487965\n",
      "Epoch 19, Batch 700, Loss: 0.294434824436903\n",
      "Epoch 19, Batch 800, Loss: 0.26427124232053756\n",
      "Epoch 19, Batch 900, Loss: 0.27212210431694983\n",
      "Epoch 20, Batch 100, Loss: 0.2704273892939091\n",
      "Epoch 20, Batch 200, Loss: 0.2627643171697855\n",
      "Epoch 20, Batch 300, Loss: 0.277590007558465\n",
      "Epoch 20, Batch 400, Loss: 0.2713015390187502\n",
      "Epoch 20, Batch 500, Loss: 0.2641735468804836\n",
      "Epoch 20, Batch 600, Loss: 0.2700164529681206\n",
      "Epoch 20, Batch 700, Loss: 0.2703528571873903\n",
      "Epoch 20, Batch 800, Loss: 0.2979211012274027\n",
      "Epoch 20, Batch 900, Loss: 0.27522427596151827\n",
      "Epoch 21, Batch 100, Loss: 0.2636748192459345\n",
      "Epoch 21, Batch 200, Loss: 0.25919832922518254\n",
      "Epoch 21, Batch 300, Loss: 0.2648104869574308\n",
      "Epoch 21, Batch 400, Loss: 0.26791968889534473\n",
      "Epoch 21, Batch 500, Loss: 0.28027510672807693\n",
      "Epoch 21, Batch 600, Loss: 0.2646563436090946\n",
      "Epoch 21, Batch 700, Loss: 0.26434144817292693\n",
      "Epoch 21, Batch 800, Loss: 0.27191907905042173\n",
      "Epoch 21, Batch 900, Loss: 0.2803614588081837\n",
      "Epoch 22, Batch 100, Loss: 0.27005526922643186\n",
      "Epoch 22, Batch 200, Loss: 0.26897234939038756\n",
      "Epoch 22, Batch 300, Loss: 0.2543236023187637\n",
      "Epoch 22, Batch 400, Loss: 0.26859071865677836\n",
      "Epoch 22, Batch 500, Loss: 0.2652570378780365\n",
      "Epoch 22, Batch 600, Loss: 0.2550004843622446\n",
      "Epoch 22, Batch 700, Loss: 0.270353562310338\n",
      "Epoch 22, Batch 800, Loss: 0.2825468287616968\n",
      "Epoch 22, Batch 900, Loss: 0.26408474579453467\n",
      "Epoch 23, Batch 100, Loss: 0.263634170293808\n",
      "Epoch 23, Batch 200, Loss: 0.2549514865875244\n",
      "Epoch 23, Batch 300, Loss: 0.2668393637984991\n",
      "Epoch 23, Batch 400, Loss: 0.2589489222317934\n",
      "Epoch 23, Batch 500, Loss: 0.26363026224076747\n",
      "Epoch 23, Batch 600, Loss: 0.2602452231943607\n",
      "Epoch 23, Batch 700, Loss: 0.27167345441877844\n",
      "Epoch 23, Batch 800, Loss: 0.2662869288772345\n",
      "Epoch 23, Batch 900, Loss: 0.25715120784938333\n",
      "Epoch 24, Batch 100, Loss: 0.24343809343874453\n",
      "Epoch 24, Batch 200, Loss: 0.25025923013687135\n",
      "Epoch 24, Batch 300, Loss: 0.2550228606164455\n",
      "Epoch 24, Batch 400, Loss: 0.26558610834181307\n",
      "Epoch 24, Batch 500, Loss: 0.2538449956476688\n",
      "Epoch 24, Batch 600, Loss: 0.2588695054501295\n",
      "Epoch 24, Batch 700, Loss: 0.2812724293768406\n",
      "Epoch 24, Batch 800, Loss: 0.2665178987383843\n",
      "Epoch 24, Batch 900, Loss: 0.27214995384216306\n",
      "Epoch 25, Batch 100, Loss: 0.24433126509189607\n",
      "Epoch 25, Batch 200, Loss: 0.23754253961145877\n",
      "Epoch 25, Batch 300, Loss: 0.2575444236397743\n",
      "Epoch 25, Batch 400, Loss: 0.2649190456420183\n",
      "Epoch 25, Batch 500, Loss: 0.2543134032562375\n",
      "Epoch 25, Batch 600, Loss: 0.2584939819574356\n",
      "Epoch 25, Batch 700, Loss: 0.2616482966393232\n",
      "Epoch 25, Batch 800, Loss: 0.2587369530647993\n",
      "Epoch 25, Batch 900, Loss: 0.27466493636369704\n",
      "Accuracy on test set: 0.8762%\n",
      "Fitting for combination 16\n",
      "784\n",
      "1\n",
      "10\n",
      "[50, 10]\n",
      "True\n",
      "['relu']\n",
      "Adam\n",
      "0.1\n",
      "0.03\n",
      "CrossEntropyLoss\n",
      "200\n",
      "25\n",
      "0\n",
      "Epoch 1, Batch 200, Loss: 5.903785960674286\n",
      "Epoch 1, Batch 400, Loss: 7.4567963933944705\n",
      "Epoch 1, Batch 600, Loss: 4.5850425457954405\n",
      "Epoch 1, Batch 800, Loss: 5.0004072868824005\n",
      "Epoch 2, Batch 200, Loss: 4.8631332337856295\n",
      "Epoch 2, Batch 400, Loss: 4.436575517654419\n",
      "Epoch 2, Batch 600, Loss: 4.868752211332321\n",
      "Epoch 2, Batch 800, Loss: 5.032574872374535\n",
      "Epoch 3, Batch 200, Loss: 6.862564462423324\n",
      "Epoch 3, Batch 400, Loss: 5.590887776613235\n",
      "Epoch 3, Batch 600, Loss: 7.01854332447052\n",
      "Epoch 3, Batch 800, Loss: 4.471952841877937\n",
      "Epoch 4, Batch 200, Loss: 6.112833869457245\n",
      "Epoch 4, Batch 400, Loss: 6.709963302612305\n",
      "Epoch 4, Batch 600, Loss: 5.425272167921066\n",
      "Epoch 4, Batch 800, Loss: 4.39356409907341\n",
      "Epoch 5, Batch 200, Loss: 5.441359865665436\n",
      "Epoch 5, Batch 400, Loss: 5.440023182630539\n",
      "Epoch 5, Batch 600, Loss: 4.945026803016662\n",
      "Epoch 5, Batch 800, Loss: 4.710599372386932\n",
      "Epoch 6, Batch 200, Loss: 5.77915403008461\n",
      "Epoch 6, Batch 400, Loss: 5.325330765247345\n",
      "Epoch 6, Batch 600, Loss: 4.37485671043396\n",
      "Epoch 6, Batch 800, Loss: 5.235044679641724\n",
      "Epoch 7, Batch 200, Loss: 4.968177365064621\n",
      "Epoch 7, Batch 400, Loss: 5.472881565093994\n",
      "Epoch 7, Batch 600, Loss: 6.32729932308197\n",
      "Epoch 7, Batch 800, Loss: 7.014458907842636\n",
      "Epoch 8, Batch 200, Loss: 5.011826883554459\n",
      "Epoch 8, Batch 400, Loss: 5.964237205982208\n",
      "Epoch 8, Batch 600, Loss: 5.935531455278396\n",
      "Epoch 8, Batch 800, Loss: 4.4352045267820355\n",
      "Epoch 9, Batch 200, Loss: 5.53864709854126\n",
      "Epoch 9, Batch 400, Loss: 5.542662944793701\n",
      "Epoch 9, Batch 600, Loss: 4.698026815652847\n",
      "Epoch 9, Batch 800, Loss: 6.439505523443222\n",
      "Epoch 10, Batch 200, Loss: 5.846539752483368\n",
      "Epoch 10, Batch 400, Loss: 4.655126934051514\n",
      "Epoch 10, Batch 600, Loss: 4.717521624565125\n",
      "Epoch 10, Batch 800, Loss: 5.971516432762146\n",
      "Epoch 11, Batch 200, Loss: 5.340880520343781\n",
      "Epoch 11, Batch 400, Loss: 5.219511353969574\n",
      "Epoch 11, Batch 600, Loss: 5.382366759777069\n",
      "Epoch 11, Batch 800, Loss: 4.484698497056961\n",
      "Epoch 12, Batch 200, Loss: 6.200738744735718\n",
      "Epoch 12, Batch 400, Loss: 5.465318276882171\n",
      "Epoch 12, Batch 600, Loss: 5.897954820394516\n",
      "Epoch 12, Batch 800, Loss: 5.909899431467056\n",
      "Epoch 13, Batch 200, Loss: 5.454850596189499\n",
      "Epoch 13, Batch 400, Loss: 7.803494281768799\n",
      "Epoch 13, Batch 600, Loss: 5.36005210518837\n",
      "Epoch 13, Batch 800, Loss: 5.411202765703202\n",
      "Epoch 14, Batch 200, Loss: 5.795605735778809\n",
      "Epoch 14, Batch 400, Loss: 5.914158326387406\n",
      "Epoch 14, Batch 600, Loss: 4.915616979598999\n",
      "Epoch 14, Batch 800, Loss: 3.895778373479843\n",
      "Epoch 15, Batch 200, Loss: 5.524954446554184\n",
      "Epoch 15, Batch 400, Loss: 6.439754874706268\n",
      "Epoch 15, Batch 600, Loss: 5.409839241504669\n",
      "Epoch 15, Batch 800, Loss: 5.8366324484348295\n",
      "Epoch 16, Batch 200, Loss: 7.7137344658374785\n",
      "Epoch 16, Batch 400, Loss: 8.02044298171997\n",
      "Epoch 16, Batch 600, Loss: 4.324137308597565\n",
      "Epoch 16, Batch 800, Loss: 4.595950165987015\n",
      "Epoch 17, Batch 200, Loss: 5.347433989048004\n",
      "Epoch 17, Batch 400, Loss: 5.098762985467911\n",
      "Epoch 17, Batch 600, Loss: 4.416475155353546\n",
      "Epoch 17, Batch 800, Loss: 5.659299429655075\n",
      "Epoch 18, Batch 200, Loss: 5.066298248767853\n",
      "Epoch 18, Batch 400, Loss: 3.966872143149376\n",
      "Epoch 18, Batch 600, Loss: 5.594309974908828\n",
      "Epoch 18, Batch 800, Loss: 5.835162850618363\n",
      "Epoch 19, Batch 200, Loss: 6.119950776100159\n",
      "Epoch 19, Batch 400, Loss: 6.2403802239894866\n",
      "Epoch 19, Batch 600, Loss: 6.164938282966614\n",
      "Epoch 19, Batch 800, Loss: 5.419870748519897\n",
      "Epoch 20, Batch 200, Loss: 5.659573668241501\n",
      "Epoch 20, Batch 400, Loss: 5.755125852823258\n",
      "Epoch 20, Batch 600, Loss: 5.260350008010864\n",
      "Epoch 20, Batch 800, Loss: 5.184856985807419\n",
      "Epoch 21, Batch 200, Loss: 5.367112214565277\n",
      "Epoch 21, Batch 400, Loss: 6.5720699489116665\n",
      "Epoch 21, Batch 600, Loss: 6.048371066451073\n",
      "Epoch 21, Batch 800, Loss: 5.698262984752655\n",
      "Epoch 22, Batch 200, Loss: 4.562223038673401\n",
      "Epoch 22, Batch 400, Loss: 5.159103789925576\n",
      "Epoch 22, Batch 600, Loss: 4.565266176462173\n",
      "Epoch 22, Batch 800, Loss: 9.58608159184456\n",
      "Epoch 23, Batch 200, Loss: 5.5940397775173185\n",
      "Epoch 23, Batch 400, Loss: 4.413972109556198\n",
      "Epoch 23, Batch 600, Loss: 7.013066130876541\n",
      "Epoch 23, Batch 800, Loss: 5.010702816843986\n",
      "Epoch 24, Batch 200, Loss: 5.016349917650222\n",
      "Epoch 24, Batch 400, Loss: 5.161600440740585\n",
      "Epoch 24, Batch 600, Loss: 8.06450308561325\n",
      "Epoch 24, Batch 800, Loss: 5.143563628792763\n",
      "Epoch 25, Batch 200, Loss: 5.212405021190643\n",
      "Epoch 25, Batch 400, Loss: 4.274963497519493\n",
      "Epoch 25, Batch 600, Loss: 5.259146327972412\n",
      "Epoch 25, Batch 800, Loss: 3.559222705960274\n",
      "Accuracy on test set: 0.3363%\n",
      "Fitting for combination 17\n",
      "784\n",
      "1\n",
      "10\n",
      "[50, 10]\n",
      "False\n",
      "['relu']\n",
      "SGD\n",
      "0.1\n",
      "0.01\n",
      "CrossEntropyLoss\n",
      "50\n",
      "25\n",
      "0\n",
      "Epoch 1, Batch 50, Loss: 0.5881837636232377\n",
      "Epoch 1, Batch 100, Loss: 0.3636411556601524\n",
      "Epoch 1, Batch 150, Loss: 0.32442081332206724\n",
      "Epoch 1, Batch 200, Loss: 0.3038864520192146\n",
      "Epoch 1, Batch 250, Loss: 0.2979665035009384\n",
      "Epoch 1, Batch 300, Loss: 0.2894038125872612\n",
      "Epoch 1, Batch 350, Loss: 0.27438630133867264\n",
      "Epoch 1, Batch 400, Loss: 0.2558853977918625\n",
      "Epoch 1, Batch 450, Loss: 0.2848860713839531\n",
      "Epoch 1, Batch 500, Loss: 0.2701523616909981\n",
      "Epoch 1, Batch 550, Loss: 0.26412335097789763\n",
      "Epoch 1, Batch 600, Loss: 0.2524838280677795\n",
      "Epoch 1, Batch 650, Loss: 0.2551153400540352\n",
      "Epoch 1, Batch 700, Loss: 0.25948457956314086\n",
      "Epoch 1, Batch 750, Loss: 0.26050050228834154\n",
      "Epoch 1, Batch 800, Loss: 0.26472205847501756\n",
      "Epoch 1, Batch 850, Loss: 0.25335059612989425\n",
      "Epoch 1, Batch 900, Loss: 0.24554552644491195\n",
      "Epoch 2, Batch 50, Loss: 0.24686793237924576\n",
      "Epoch 2, Batch 100, Loss: 0.240367331802845\n",
      "Epoch 2, Batch 150, Loss: 0.23719017535448075\n",
      "Epoch 2, Batch 200, Loss: 0.24441346675157546\n",
      "Epoch 2, Batch 250, Loss: 0.2478438764810562\n",
      "Epoch 2, Batch 300, Loss: 0.2509291408956051\n",
      "Epoch 2, Batch 350, Loss: 0.23677093908190727\n",
      "Epoch 2, Batch 400, Loss: 0.2393403869867325\n",
      "Epoch 2, Batch 450, Loss: 0.23982560485601426\n",
      "Epoch 2, Batch 500, Loss: 0.2327494803071022\n",
      "Epoch 2, Batch 550, Loss: 0.23275209963321686\n",
      "Epoch 2, Batch 600, Loss: 0.24684000879526138\n",
      "Epoch 2, Batch 650, Loss: 0.225928455889225\n",
      "Epoch 2, Batch 700, Loss: 0.2397717845439911\n",
      "Epoch 2, Batch 750, Loss: 0.24312202036380767\n",
      "Epoch 2, Batch 800, Loss: 0.25289263635873793\n",
      "Epoch 2, Batch 850, Loss: 0.24351321458816527\n",
      "Epoch 2, Batch 900, Loss: 0.22301961794495584\n",
      "Epoch 3, Batch 50, Loss: 0.22084664732217787\n",
      "Epoch 3, Batch 100, Loss: 0.246959647834301\n",
      "Epoch 3, Batch 150, Loss: 0.22813044458627701\n",
      "Epoch 3, Batch 200, Loss: 0.23180433690547944\n",
      "Epoch 3, Batch 250, Loss: 0.22369305282831192\n",
      "Epoch 3, Batch 300, Loss: 0.23251976639032365\n",
      "Epoch 3, Batch 350, Loss: 0.22960867822170258\n",
      "Epoch 3, Batch 400, Loss: 0.22625144049525261\n",
      "Epoch 3, Batch 450, Loss: 0.23493239730596543\n",
      "Epoch 3, Batch 500, Loss: 0.24132656753063203\n",
      "Epoch 3, Batch 550, Loss: 0.2385450792312622\n",
      "Epoch 3, Batch 600, Loss: 0.2213637512922287\n",
      "Epoch 3, Batch 650, Loss: 0.2314256078004837\n",
      "Epoch 3, Batch 700, Loss: 0.23901524424552917\n",
      "Epoch 3, Batch 750, Loss: 0.22430984124541284\n",
      "Epoch 3, Batch 800, Loss: 0.2437739360332489\n",
      "Epoch 3, Batch 850, Loss: 0.23330358564853668\n",
      "Epoch 3, Batch 900, Loss: 0.24593216717243194\n",
      "Epoch 4, Batch 50, Loss: 0.21595963567495347\n",
      "Epoch 4, Batch 100, Loss: 0.2294444450736046\n",
      "Epoch 4, Batch 150, Loss: 0.2273006945848465\n",
      "Epoch 4, Batch 200, Loss: 0.23074162676930426\n",
      "Epoch 4, Batch 250, Loss: 0.2435782441496849\n",
      "Epoch 4, Batch 300, Loss: 0.23049739480018616\n",
      "Epoch 4, Batch 350, Loss: 0.21618356466293334\n",
      "Epoch 4, Batch 400, Loss: 0.23643236130475997\n",
      "Epoch 4, Batch 450, Loss: 0.21419249832630158\n",
      "Epoch 4, Batch 500, Loss: 0.23281094580888748\n",
      "Epoch 4, Batch 550, Loss: 0.22771548062562944\n",
      "Epoch 4, Batch 600, Loss: 0.2301997196674347\n",
      "Epoch 4, Batch 650, Loss: 0.22664879024028778\n",
      "Epoch 4, Batch 700, Loss: 0.22796886712312697\n",
      "Epoch 4, Batch 750, Loss: 0.21815508231520653\n",
      "Epoch 4, Batch 800, Loss: 0.23891533344984053\n",
      "Epoch 4, Batch 850, Loss: 0.24190224319696427\n",
      "Epoch 4, Batch 900, Loss: 0.2349895256757736\n",
      "Epoch 5, Batch 50, Loss: 0.23772993862628936\n",
      "Epoch 5, Batch 100, Loss: 0.21768035620450973\n",
      "Epoch 5, Batch 150, Loss: 0.24302650332450867\n",
      "Epoch 5, Batch 200, Loss: 0.22400763034820556\n",
      "Epoch 5, Batch 250, Loss: 0.23341631650924682\n",
      "Epoch 5, Batch 300, Loss: 0.2231024208664894\n",
      "Epoch 5, Batch 350, Loss: 0.2263637900352478\n",
      "Epoch 5, Batch 400, Loss: 0.21563387751579285\n",
      "Epoch 5, Batch 450, Loss: 0.21997567906975746\n",
      "Epoch 5, Batch 500, Loss: 0.24400440722703934\n",
      "Epoch 5, Batch 550, Loss: 0.23623197853565217\n",
      "Epoch 5, Batch 600, Loss: 0.21909037381410598\n",
      "Epoch 5, Batch 650, Loss: 0.2249436604976654\n",
      "Epoch 5, Batch 700, Loss: 0.24162038654088974\n",
      "Epoch 5, Batch 750, Loss: 0.23792748540639877\n",
      "Epoch 5, Batch 800, Loss: 0.21186889111995696\n",
      "Epoch 5, Batch 850, Loss: 0.22635081872344018\n",
      "Epoch 5, Batch 900, Loss: 0.22322968453168868\n",
      "Epoch 6, Batch 50, Loss: 0.22219991356134414\n",
      "Epoch 6, Batch 100, Loss: 0.22519004374742507\n",
      "Epoch 6, Batch 150, Loss: 0.22168823212385178\n",
      "Epoch 6, Batch 200, Loss: 0.21973591089248656\n",
      "Epoch 6, Batch 250, Loss: 0.21310170531272887\n",
      "Epoch 6, Batch 300, Loss: 0.2146303677558899\n",
      "Epoch 6, Batch 350, Loss: 0.23392963916063308\n",
      "Epoch 6, Batch 400, Loss: 0.22535620570182802\n",
      "Epoch 6, Batch 450, Loss: 0.2255108541250229\n",
      "Epoch 6, Batch 500, Loss: 0.22829610228538513\n",
      "Epoch 6, Batch 550, Loss: 0.21985519543290138\n",
      "Epoch 6, Batch 600, Loss: 0.23437252879142761\n",
      "Epoch 6, Batch 650, Loss: 0.22835471838712693\n",
      "Epoch 6, Batch 700, Loss: 0.22105520948767662\n",
      "Epoch 6, Batch 750, Loss: 0.2342655524611473\n",
      "Epoch 6, Batch 800, Loss: 0.22721603274345398\n",
      "Epoch 6, Batch 850, Loss: 0.23788974463939666\n",
      "Epoch 6, Batch 900, Loss: 0.23636121690273285\n",
      "Epoch 7, Batch 50, Loss: 0.2195884467661381\n",
      "Epoch 7, Batch 100, Loss: 0.23139834195375442\n",
      "Epoch 7, Batch 150, Loss: 0.23142593264579772\n",
      "Epoch 7, Batch 200, Loss: 0.23007623225450516\n",
      "Epoch 7, Batch 250, Loss: 0.21910222515463829\n",
      "Epoch 7, Batch 300, Loss: 0.23083310157060624\n",
      "Epoch 7, Batch 350, Loss: 0.214467281550169\n",
      "Epoch 7, Batch 400, Loss: 0.21523867607116698\n",
      "Epoch 7, Batch 450, Loss: 0.22695476412773133\n",
      "Epoch 7, Batch 500, Loss: 0.22478121101856233\n",
      "Epoch 7, Batch 550, Loss: 0.21254339307546616\n",
      "Epoch 7, Batch 600, Loss: 0.22722683161497115\n",
      "Epoch 7, Batch 650, Loss: 0.2247013556957245\n",
      "Epoch 7, Batch 700, Loss: 0.21550586000084876\n",
      "Epoch 7, Batch 750, Loss: 0.2225065438449383\n",
      "Epoch 7, Batch 800, Loss: 0.2192025962471962\n",
      "Epoch 7, Batch 850, Loss: 0.21938098847866058\n",
      "Epoch 7, Batch 900, Loss: 0.24200741738080978\n",
      "Epoch 8, Batch 50, Loss: 0.22882289499044417\n",
      "Epoch 8, Batch 100, Loss: 0.22150001168251038\n",
      "Epoch 8, Batch 150, Loss: 0.23911695927381516\n",
      "Epoch 8, Batch 200, Loss: 0.23031962513923646\n",
      "Epoch 8, Batch 250, Loss: 0.21609878331422805\n",
      "Epoch 8, Batch 300, Loss: 0.22344843566417694\n",
      "Epoch 8, Batch 350, Loss: 0.21940080985426902\n",
      "Epoch 8, Batch 400, Loss: 0.22170322626829148\n",
      "Epoch 8, Batch 450, Loss: 0.22383869230747222\n",
      "Epoch 8, Batch 500, Loss: 0.22126293957233428\n",
      "Epoch 8, Batch 550, Loss: 0.23127888768911362\n",
      "Epoch 8, Batch 600, Loss: 0.22383823573589326\n",
      "Epoch 8, Batch 650, Loss: 0.22170949831604958\n",
      "Epoch 8, Batch 700, Loss: 0.226737402677536\n",
      "Epoch 8, Batch 750, Loss: 0.22689244374632836\n",
      "Epoch 8, Batch 800, Loss: 0.225939482152462\n",
      "Epoch 8, Batch 850, Loss: 0.21974268138408662\n",
      "Epoch 8, Batch 900, Loss: 0.21622816860675811\n",
      "Epoch 9, Batch 50, Loss: 0.21817407459020616\n",
      "Epoch 9, Batch 100, Loss: 0.21003208592534064\n",
      "Epoch 9, Batch 150, Loss: 0.23741389453411102\n",
      "Epoch 9, Batch 200, Loss: 0.22779468685388565\n",
      "Epoch 9, Batch 250, Loss: 0.23198263078927994\n",
      "Epoch 9, Batch 300, Loss: 0.22482759922742843\n",
      "Epoch 9, Batch 350, Loss: 0.22162135273218156\n",
      "Epoch 9, Batch 400, Loss: 0.22066700994968413\n",
      "Epoch 9, Batch 450, Loss: 0.23946260690689086\n",
      "Epoch 9, Batch 500, Loss: 0.2095460319519043\n",
      "Epoch 9, Batch 550, Loss: 0.22000578358769418\n",
      "Epoch 9, Batch 600, Loss: 0.23645983010530472\n",
      "Epoch 9, Batch 650, Loss: 0.22630297183990478\n",
      "Epoch 9, Batch 700, Loss: 0.22213760316371917\n",
      "Epoch 9, Batch 750, Loss: 0.22748637437820435\n",
      "Epoch 9, Batch 800, Loss: 0.23920647993683816\n",
      "Epoch 9, Batch 850, Loss: 0.21813635349273683\n",
      "Epoch 9, Batch 900, Loss: 0.22602253645658493\n",
      "Epoch 10, Batch 50, Loss: 0.22947800308465957\n",
      "Epoch 10, Batch 100, Loss: 0.23522011280059815\n",
      "Epoch 10, Batch 150, Loss: 0.2214327149093151\n",
      "Epoch 10, Batch 200, Loss: 0.22398174613714217\n",
      "Epoch 10, Batch 250, Loss: 0.22160665944218635\n",
      "Epoch 10, Batch 300, Loss: 0.2223949570953846\n",
      "Epoch 10, Batch 350, Loss: 0.23940917164087294\n",
      "Epoch 10, Batch 400, Loss: 0.21865062266588212\n",
      "Epoch 10, Batch 450, Loss: 0.2253330346941948\n",
      "Epoch 10, Batch 500, Loss: 0.22322950899600982\n",
      "Epoch 10, Batch 550, Loss: 0.22690126717090606\n",
      "Epoch 10, Batch 600, Loss: 0.2162740620970726\n",
      "Epoch 10, Batch 650, Loss: 0.21811349898576737\n",
      "Epoch 10, Batch 700, Loss: 0.22257325530052186\n",
      "Epoch 10, Batch 750, Loss: 0.23791748374700547\n",
      "Epoch 10, Batch 800, Loss: 0.20877105444669725\n",
      "Epoch 10, Batch 850, Loss: 0.22641631156206132\n",
      "Epoch 10, Batch 900, Loss: 0.21106148958206178\n",
      "Epoch 11, Batch 50, Loss: 0.20734997496008872\n",
      "Epoch 11, Batch 100, Loss: 0.2293777099251747\n",
      "Epoch 11, Batch 150, Loss: 0.21832820594310762\n",
      "Epoch 11, Batch 200, Loss: 0.22765647619962692\n",
      "Epoch 11, Batch 250, Loss: 0.22094108074903487\n",
      "Epoch 11, Batch 300, Loss: 0.21268792003393172\n",
      "Epoch 11, Batch 350, Loss: 0.22005366250872613\n",
      "Epoch 11, Batch 400, Loss: 0.23993703126907348\n",
      "Epoch 11, Batch 450, Loss: 0.2113994511961937\n",
      "Epoch 11, Batch 500, Loss: 0.2335647864639759\n",
      "Epoch 11, Batch 550, Loss: 0.21575380861759186\n",
      "Epoch 11, Batch 600, Loss: 0.2196546897292137\n",
      "Epoch 11, Batch 650, Loss: 0.23243800431489944\n",
      "Epoch 11, Batch 700, Loss: 0.2408927372097969\n",
      "Epoch 11, Batch 750, Loss: 0.2331486627459526\n",
      "Epoch 11, Batch 800, Loss: 0.2289553216099739\n",
      "Epoch 11, Batch 850, Loss: 0.20705747127532959\n",
      "Epoch 11, Batch 900, Loss: 0.22152728378772735\n",
      "Epoch 12, Batch 50, Loss: 0.2229346039891243\n",
      "Epoch 12, Batch 100, Loss: 0.20254890009760856\n",
      "Epoch 12, Batch 150, Loss: 0.21477507472038268\n",
      "Epoch 12, Batch 200, Loss: 0.22655583590269088\n",
      "Epoch 12, Batch 250, Loss: 0.21681833058595656\n",
      "Epoch 12, Batch 300, Loss: 0.23321166932582854\n",
      "Epoch 12, Batch 350, Loss: 0.22935270607471467\n",
      "Epoch 12, Batch 400, Loss: 0.21568813562393188\n",
      "Epoch 12, Batch 450, Loss: 0.23602884501218796\n",
      "Epoch 12, Batch 500, Loss: 0.2144455087184906\n",
      "Epoch 12, Batch 550, Loss: 0.218991761803627\n",
      "Epoch 12, Batch 600, Loss: 0.22709845021367073\n",
      "Epoch 12, Batch 650, Loss: 0.22147453129291533\n",
      "Epoch 12, Batch 700, Loss: 0.21974210023880006\n",
      "Epoch 12, Batch 750, Loss: 0.2240990513563156\n",
      "Epoch 12, Batch 800, Loss: 0.2209817311167717\n",
      "Epoch 12, Batch 850, Loss: 0.21828896880149842\n",
      "Epoch 12, Batch 900, Loss: 0.22270215600728988\n",
      "Epoch 13, Batch 50, Loss: 0.22087360173463821\n",
      "Epoch 13, Batch 100, Loss: 0.2101779019832611\n",
      "Epoch 13, Batch 150, Loss: 0.21917404383420944\n",
      "Epoch 13, Batch 200, Loss: 0.2227492141723633\n",
      "Epoch 13, Batch 250, Loss: 0.21286916390061378\n",
      "Epoch 13, Batch 300, Loss: 0.22711597636342049\n",
      "Epoch 13, Batch 350, Loss: 0.22986996114254\n",
      "Epoch 13, Batch 400, Loss: 0.2178438600897789\n",
      "Epoch 13, Batch 450, Loss: 0.21541158765554427\n",
      "Epoch 13, Batch 500, Loss: 0.22694163635373116\n",
      "Epoch 13, Batch 550, Loss: 0.21035695776343347\n",
      "Epoch 13, Batch 600, Loss: 0.22829761892557143\n",
      "Epoch 13, Batch 650, Loss: 0.21969438210129738\n",
      "Epoch 13, Batch 700, Loss: 0.22247580513358117\n",
      "Epoch 13, Batch 750, Loss: 0.2319552230834961\n",
      "Epoch 13, Batch 800, Loss: 0.22883779048919678\n",
      "Epoch 13, Batch 850, Loss: 0.23377514660358428\n",
      "Epoch 13, Batch 900, Loss: 0.21819267109036444\n",
      "Epoch 14, Batch 50, Loss: 0.2177043977379799\n",
      "Epoch 14, Batch 100, Loss: 0.22091830492019654\n",
      "Epoch 14, Batch 150, Loss: 0.21544050216674804\n",
      "Epoch 14, Batch 200, Loss: 0.22171490028500557\n",
      "Epoch 14, Batch 250, Loss: 0.21448728889226915\n",
      "Epoch 14, Batch 300, Loss: 0.22887028574943544\n",
      "Epoch 14, Batch 350, Loss: 0.22437674134969712\n",
      "Epoch 14, Batch 400, Loss: 0.2215779346227646\n",
      "Epoch 14, Batch 450, Loss: 0.225049210190773\n",
      "Epoch 14, Batch 500, Loss: 0.219365274310112\n",
      "Epoch 14, Batch 550, Loss: 0.2207548028230667\n",
      "Epoch 14, Batch 600, Loss: 0.21630377888679506\n",
      "Epoch 14, Batch 650, Loss: 0.21770066231489182\n",
      "Epoch 14, Batch 700, Loss: 0.2291189858317375\n",
      "Epoch 14, Batch 750, Loss: 0.21410882398486136\n",
      "Epoch 14, Batch 800, Loss: 0.2267533525824547\n",
      "Epoch 14, Batch 850, Loss: 0.21610531941056252\n",
      "Epoch 14, Batch 900, Loss: 0.22817409977316858\n",
      "Epoch 15, Batch 50, Loss: 0.2076231850683689\n",
      "Epoch 15, Batch 100, Loss: 0.210798859000206\n",
      "Epoch 15, Batch 150, Loss: 0.22601917669177055\n",
      "Epoch 15, Batch 200, Loss: 0.2147335071861744\n",
      "Epoch 15, Batch 250, Loss: 0.22061064809560776\n",
      "Epoch 15, Batch 300, Loss: 0.21303999274969102\n",
      "Epoch 15, Batch 350, Loss: 0.22690578013658524\n",
      "Epoch 15, Batch 400, Loss: 0.21557590782642364\n",
      "Epoch 15, Batch 450, Loss: 0.22123759970068932\n",
      "Epoch 15, Batch 500, Loss: 0.2202324962615967\n",
      "Epoch 15, Batch 550, Loss: 0.21823160767555236\n",
      "Epoch 15, Batch 600, Loss: 0.23526404023170472\n",
      "Epoch 15, Batch 650, Loss: 0.23032175302505492\n",
      "Epoch 15, Batch 700, Loss: 0.22492992654442787\n",
      "Epoch 15, Batch 750, Loss: 0.22097022205591202\n",
      "Epoch 15, Batch 800, Loss: 0.2253805735707283\n",
      "Epoch 15, Batch 850, Loss: 0.22217417314648627\n",
      "Epoch 15, Batch 900, Loss: 0.22040253907442092\n",
      "Epoch 16, Batch 50, Loss: 0.23190213471651078\n",
      "Epoch 16, Batch 100, Loss: 0.2163796356320381\n",
      "Epoch 16, Batch 150, Loss: 0.2237312251329422\n",
      "Epoch 16, Batch 200, Loss: 0.21085675656795502\n",
      "Epoch 16, Batch 250, Loss: 0.22643224030733108\n",
      "Epoch 16, Batch 300, Loss: 0.2098992210626602\n",
      "Epoch 16, Batch 350, Loss: 0.22061807915568352\n",
      "Epoch 16, Batch 400, Loss: 0.22268712788820266\n",
      "Epoch 16, Batch 450, Loss: 0.2272219768166542\n",
      "Epoch 16, Batch 500, Loss: 0.2270113831758499\n",
      "Epoch 16, Batch 550, Loss: 0.22375814840197564\n",
      "Epoch 16, Batch 600, Loss: 0.21234884083271027\n",
      "Epoch 16, Batch 650, Loss: 0.22632265478372574\n",
      "Epoch 16, Batch 700, Loss: 0.22110872879624366\n",
      "Epoch 16, Batch 750, Loss: 0.2344469967484474\n",
      "Epoch 16, Batch 800, Loss: 0.2152704754471779\n",
      "Epoch 16, Batch 850, Loss: 0.21619316011667253\n",
      "Epoch 16, Batch 900, Loss: 0.23693422645330428\n",
      "Epoch 17, Batch 50, Loss: 0.2163834722340107\n",
      "Epoch 17, Batch 100, Loss: 0.21302277103066444\n",
      "Epoch 17, Batch 150, Loss: 0.22960248664021493\n",
      "Epoch 17, Batch 200, Loss: 0.22048528164625167\n",
      "Epoch 17, Batch 250, Loss: 0.22617367014288903\n",
      "Epoch 17, Batch 300, Loss: 0.22708938807249068\n",
      "Epoch 17, Batch 350, Loss: 0.2232285913825035\n",
      "Epoch 17, Batch 400, Loss: 0.22622729510068892\n",
      "Epoch 17, Batch 450, Loss: 0.2209133267402649\n",
      "Epoch 17, Batch 500, Loss: 0.22419200122356414\n",
      "Epoch 17, Batch 550, Loss: 0.22728487864136696\n",
      "Epoch 17, Batch 600, Loss: 0.2122766813635826\n",
      "Epoch 17, Batch 650, Loss: 0.22696050018072128\n",
      "Epoch 17, Batch 700, Loss: 0.22440000414848327\n",
      "Epoch 17, Batch 750, Loss: 0.22499809056520462\n",
      "Epoch 17, Batch 800, Loss: 0.2233009585738182\n",
      "Epoch 17, Batch 850, Loss: 0.21715021774172782\n",
      "Epoch 17, Batch 900, Loss: 0.21660820335149766\n",
      "Epoch 18, Batch 50, Loss: 0.21521181881427764\n",
      "Epoch 18, Batch 100, Loss: 0.20177651882171632\n",
      "Epoch 18, Batch 150, Loss: 0.23590287387371064\n",
      "Epoch 18, Batch 200, Loss: 0.21317378148436547\n",
      "Epoch 18, Batch 250, Loss: 0.21571822762489318\n",
      "Epoch 18, Batch 300, Loss: 0.2320387229323387\n",
      "Epoch 18, Batch 350, Loss: 0.23371970981359483\n",
      "Epoch 18, Batch 400, Loss: 0.22984532579779626\n",
      "Epoch 18, Batch 450, Loss: 0.23165448397397995\n",
      "Epoch 18, Batch 500, Loss: 0.21437752291560172\n",
      "Epoch 18, Batch 550, Loss: 0.22306450515985488\n",
      "Epoch 18, Batch 600, Loss: 0.22063316851854325\n",
      "Epoch 18, Batch 650, Loss: 0.22013656795024872\n",
      "Epoch 18, Batch 700, Loss: 0.21347411394119262\n",
      "Epoch 18, Batch 750, Loss: 0.21889224871993065\n",
      "Epoch 18, Batch 800, Loss: 0.21867538690567018\n",
      "Epoch 18, Batch 850, Loss: 0.2219485406577587\n",
      "Epoch 18, Batch 900, Loss: 0.21986181408166885\n",
      "Epoch 19, Batch 50, Loss: 0.20489437237381936\n",
      "Epoch 19, Batch 100, Loss: 0.2268524095416069\n",
      "Epoch 19, Batch 150, Loss: 0.22391168802976608\n",
      "Epoch 19, Batch 200, Loss: 0.2181761710345745\n",
      "Epoch 19, Batch 250, Loss: 0.2236078417301178\n",
      "Epoch 19, Batch 300, Loss: 0.22159281820058824\n",
      "Epoch 19, Batch 350, Loss: 0.21928975790739058\n",
      "Epoch 19, Batch 400, Loss: 0.2234635293483734\n",
      "Epoch 19, Batch 450, Loss: 0.2231353262066841\n",
      "Epoch 19, Batch 500, Loss: 0.21703875064849854\n",
      "Epoch 19, Batch 550, Loss: 0.21777102455496788\n",
      "Epoch 19, Batch 600, Loss: 0.2197025817632675\n",
      "Epoch 19, Batch 650, Loss: 0.22026537775993346\n",
      "Epoch 19, Batch 700, Loss: 0.22114230528473855\n",
      "Epoch 19, Batch 750, Loss: 0.2263874138891697\n",
      "Epoch 19, Batch 800, Loss: 0.2272013668715954\n",
      "Epoch 19, Batch 850, Loss: 0.23039348542690277\n",
      "Epoch 19, Batch 900, Loss: 0.22571009695529937\n",
      "Epoch 20, Batch 50, Loss: 0.23040037453174592\n",
      "Epoch 20, Batch 100, Loss: 0.20727897316217422\n",
      "Epoch 20, Batch 150, Loss: 0.213288504332304\n",
      "Epoch 20, Batch 200, Loss: 0.21361662834882736\n",
      "Epoch 20, Batch 250, Loss: 0.22306006520986557\n",
      "Epoch 20, Batch 300, Loss: 0.21092693656682968\n",
      "Epoch 20, Batch 350, Loss: 0.21840866208076476\n",
      "Epoch 20, Batch 400, Loss: 0.22214446648955344\n",
      "Epoch 20, Batch 450, Loss: 0.22560857534408568\n",
      "Epoch 20, Batch 500, Loss: 0.23312133997678758\n",
      "Epoch 20, Batch 550, Loss: 0.214278444647789\n",
      "Epoch 20, Batch 600, Loss: 0.2302202633023262\n",
      "Epoch 20, Batch 650, Loss: 0.21712245255708695\n",
      "Epoch 20, Batch 700, Loss: 0.23428477138280868\n",
      "Epoch 20, Batch 750, Loss: 0.22507467553019522\n",
      "Epoch 20, Batch 800, Loss: 0.21152831643819808\n",
      "Epoch 20, Batch 850, Loss: 0.21233449891209602\n",
      "Epoch 20, Batch 900, Loss: 0.2317800633609295\n",
      "Epoch 21, Batch 50, Loss: 0.21583569586277007\n",
      "Epoch 21, Batch 100, Loss: 0.22005607306957245\n",
      "Epoch 21, Batch 150, Loss: 0.21050377935171127\n",
      "Epoch 21, Batch 200, Loss: 0.22489226788282393\n",
      "Epoch 21, Batch 250, Loss: 0.21719774156808852\n",
      "Epoch 21, Batch 300, Loss: 0.2225934800505638\n",
      "Epoch 21, Batch 350, Loss: 0.21945823535323142\n",
      "Epoch 21, Batch 400, Loss: 0.23007471323013307\n",
      "Epoch 21, Batch 450, Loss: 0.22353650897741317\n",
      "Epoch 21, Batch 500, Loss: 0.21781723946332932\n",
      "Epoch 21, Batch 550, Loss: 0.22161838978528978\n",
      "Epoch 21, Batch 600, Loss: 0.21497097611427307\n",
      "Epoch 21, Batch 650, Loss: 0.22370606482028962\n",
      "Epoch 21, Batch 700, Loss: 0.22511690467596054\n",
      "Epoch 21, Batch 750, Loss: 0.22556013241410255\n",
      "Epoch 21, Batch 800, Loss: 0.224945817142725\n",
      "Epoch 21, Batch 850, Loss: 0.2244250312447548\n",
      "Epoch 21, Batch 900, Loss: 0.22001166209578515\n",
      "Epoch 22, Batch 50, Loss: 0.21578742027282716\n",
      "Epoch 22, Batch 100, Loss: 0.22280483990907668\n",
      "Epoch 22, Batch 150, Loss: 0.2224675863981247\n",
      "Epoch 22, Batch 200, Loss: 0.2163773939013481\n",
      "Epoch 22, Batch 250, Loss: 0.21819056063890457\n",
      "Epoch 22, Batch 300, Loss: 0.22904857039451598\n",
      "Epoch 22, Batch 350, Loss: 0.2215418589115143\n",
      "Epoch 22, Batch 400, Loss: 0.21202895253896714\n",
      "Epoch 22, Batch 450, Loss: 0.22781062066555025\n",
      "Epoch 22, Batch 500, Loss: 0.21880075186491013\n",
      "Epoch 22, Batch 550, Loss: 0.22198862910270692\n",
      "Epoch 22, Batch 600, Loss: 0.22376535922288895\n",
      "Epoch 22, Batch 650, Loss: 0.22601226806640626\n",
      "Epoch 22, Batch 700, Loss: 0.2229074165225029\n",
      "Epoch 22, Batch 750, Loss: 0.22104436188936233\n",
      "Epoch 22, Batch 800, Loss: 0.2271731439232826\n",
      "Epoch 22, Batch 850, Loss: 0.22361505776643753\n",
      "Epoch 22, Batch 900, Loss: 0.2053913415968418\n",
      "Epoch 23, Batch 50, Loss: 0.22937711149454118\n",
      "Epoch 23, Batch 100, Loss: 0.22646046698093414\n",
      "Epoch 23, Batch 150, Loss: 0.20953578531742095\n",
      "Epoch 23, Batch 200, Loss: 0.2249597018957138\n",
      "Epoch 23, Batch 250, Loss: 0.2146658854186535\n",
      "Epoch 23, Batch 300, Loss: 0.22036808103322983\n",
      "Epoch 23, Batch 350, Loss: 0.22741006106138228\n",
      "Epoch 23, Batch 400, Loss: 0.22164918154478072\n",
      "Epoch 23, Batch 450, Loss: 0.22428745925426483\n",
      "Epoch 23, Batch 500, Loss: 0.21278946593403816\n",
      "Epoch 23, Batch 550, Loss: 0.20221836671233176\n",
      "Epoch 23, Batch 600, Loss: 0.23040427446365355\n",
      "Epoch 23, Batch 650, Loss: 0.22306786775588988\n",
      "Epoch 23, Batch 700, Loss: 0.2163321827352047\n",
      "Epoch 23, Batch 750, Loss: 0.21239951506257057\n",
      "Epoch 23, Batch 800, Loss: 0.226433707177639\n",
      "Epoch 23, Batch 850, Loss: 0.219130782186985\n",
      "Epoch 23, Batch 900, Loss: 0.2302851876616478\n",
      "Epoch 24, Batch 50, Loss: 0.2129582366347313\n",
      "Epoch 24, Batch 100, Loss: 0.21698183923959732\n",
      "Epoch 24, Batch 150, Loss: 0.21022336900234223\n",
      "Epoch 24, Batch 200, Loss: 0.21793719366192818\n",
      "Epoch 24, Batch 250, Loss: 0.22556042790412903\n",
      "Epoch 24, Batch 300, Loss: 0.21746438339352608\n",
      "Epoch 24, Batch 350, Loss: 0.2171942785382271\n",
      "Epoch 24, Batch 400, Loss: 0.22359009489417075\n",
      "Epoch 24, Batch 450, Loss: 0.22461338683962823\n",
      "Epoch 24, Batch 500, Loss: 0.21341904789209365\n",
      "Epoch 24, Batch 550, Loss: 0.2227981348335743\n",
      "Epoch 24, Batch 600, Loss: 0.22060119599103928\n",
      "Epoch 24, Batch 650, Loss: 0.2322095063328743\n",
      "Epoch 24, Batch 700, Loss: 0.2226218920946121\n",
      "Epoch 24, Batch 750, Loss: 0.2388928070664406\n",
      "Epoch 24, Batch 800, Loss: 0.22210755527019502\n",
      "Epoch 24, Batch 850, Loss: 0.2201746068894863\n",
      "Epoch 24, Batch 900, Loss: 0.21582939147949218\n",
      "Epoch 25, Batch 50, Loss: 0.2225356873869896\n",
      "Epoch 25, Batch 100, Loss: 0.22056015104055404\n",
      "Epoch 25, Batch 150, Loss: 0.21922308787703515\n",
      "Epoch 25, Batch 200, Loss: 0.22440416663885115\n",
      "Epoch 25, Batch 250, Loss: 0.22851681768894194\n",
      "Epoch 25, Batch 300, Loss: 0.20643327862024308\n",
      "Epoch 25, Batch 350, Loss: 0.2325959911942482\n",
      "Epoch 25, Batch 400, Loss: 0.22877735555171966\n",
      "Epoch 25, Batch 450, Loss: 0.2280817472934723\n",
      "Epoch 25, Batch 500, Loss: 0.22080110520124435\n",
      "Epoch 25, Batch 550, Loss: 0.20374465346336365\n",
      "Epoch 25, Batch 600, Loss: 0.21451966419816018\n",
      "Epoch 25, Batch 650, Loss: 0.23097749799489975\n",
      "Epoch 25, Batch 700, Loss: 0.2065600349009037\n",
      "Epoch 25, Batch 750, Loss: 0.21716442614793777\n",
      "Epoch 25, Batch 800, Loss: 0.22021627828478813\n",
      "Epoch 25, Batch 850, Loss: 0.21754568666219712\n",
      "Epoch 25, Batch 900, Loss: 0.2265658974647522\n",
      "Accuracy on test set: 0.8047%\n",
      "Fitting for combination 18\n",
      "784\n",
      "1\n",
      "10\n",
      "[50, 10]\n",
      "True\n",
      "['tanh']\n",
      "Adam\n",
      "0.1\n",
      "0.1\n",
      "CrossEntropyLoss\n",
      "100\n",
      "25\n",
      "0\n",
      "Epoch 1, Batch 100, Loss: 1.975063304901123\n",
      "Epoch 1, Batch 200, Loss: 1.9444118082523345\n",
      "Epoch 1, Batch 300, Loss: 1.9327378511428832\n",
      "Epoch 1, Batch 400, Loss: 1.9351517760753632\n",
      "Epoch 1, Batch 500, Loss: 1.8759047889709473\n",
      "Epoch 1, Batch 600, Loss: 1.9672534763813019\n",
      "Epoch 1, Batch 700, Loss: 1.9273616290092468\n",
      "Epoch 1, Batch 800, Loss: 1.9181465768814088\n",
      "Epoch 1, Batch 900, Loss: 1.9446297121047973\n",
      "Epoch 2, Batch 100, Loss: 2.05544802069664\n",
      "Epoch 2, Batch 200, Loss: 1.9573253858089448\n",
      "Epoch 2, Batch 300, Loss: 1.9122649228572846\n",
      "Epoch 2, Batch 400, Loss: 1.9765713691711426\n",
      "Epoch 2, Batch 500, Loss: 1.9578792929649353\n",
      "Epoch 2, Batch 600, Loss: 1.8990975797176362\n",
      "Epoch 2, Batch 700, Loss: 1.865111426115036\n",
      "Epoch 2, Batch 800, Loss: 1.888013435602188\n",
      "Epoch 2, Batch 900, Loss: 1.978279355764389\n",
      "Epoch 3, Batch 100, Loss: 1.890709615945816\n",
      "Epoch 3, Batch 200, Loss: 1.976199322938919\n",
      "Epoch 3, Batch 300, Loss: 1.966877943277359\n",
      "Epoch 3, Batch 400, Loss: 1.9209142589569093\n",
      "Epoch 3, Batch 500, Loss: 1.9361487364768981\n",
      "Epoch 3, Batch 600, Loss: 1.9165416216850282\n",
      "Epoch 3, Batch 700, Loss: 1.9113256514072419\n",
      "Epoch 3, Batch 800, Loss: 1.94481187582016\n",
      "Epoch 3, Batch 900, Loss: 1.9494223165512086\n",
      "Epoch 4, Batch 100, Loss: 1.9595584392547607\n",
      "Epoch 4, Batch 200, Loss: 1.9241278052330018\n",
      "Epoch 4, Batch 300, Loss: 1.9608050632476806\n",
      "Epoch 4, Batch 400, Loss: 1.9439142000675202\n",
      "Epoch 4, Batch 500, Loss: 1.946039558649063\n",
      "Epoch 4, Batch 600, Loss: 1.9471952247619628\n",
      "Epoch 4, Batch 700, Loss: 2.0059078681468963\n",
      "Epoch 4, Batch 800, Loss: 1.9676717436313629\n",
      "Epoch 4, Batch 900, Loss: 1.9672969055175782\n",
      "Epoch 5, Batch 100, Loss: 1.8851116156578065\n",
      "Epoch 5, Batch 200, Loss: 1.9462560892105103\n",
      "Epoch 5, Batch 300, Loss: 1.9884193789958955\n",
      "Epoch 5, Batch 400, Loss: 1.9249954283237458\n",
      "Epoch 5, Batch 500, Loss: 1.980618475675583\n",
      "Epoch 5, Batch 600, Loss: 1.998074117898941\n",
      "Epoch 5, Batch 700, Loss: 1.8782296431064607\n",
      "Epoch 5, Batch 800, Loss: 1.8755508136749268\n",
      "Epoch 5, Batch 900, Loss: 1.968836237192154\n",
      "Epoch 6, Batch 100, Loss: 1.9695243287086486\n",
      "Epoch 6, Batch 200, Loss: 1.979338939189911\n",
      "Epoch 6, Batch 300, Loss: 1.966804633140564\n",
      "Epoch 6, Batch 400, Loss: 1.9186257529258728\n",
      "Epoch 6, Batch 500, Loss: 1.9365267527103425\n",
      "Epoch 6, Batch 600, Loss: 2.056063311100006\n",
      "Epoch 6, Batch 700, Loss: 1.9471789038181304\n",
      "Epoch 6, Batch 800, Loss: 1.983349964618683\n",
      "Epoch 6, Batch 900, Loss: 1.8862324845790863\n",
      "Epoch 7, Batch 100, Loss: 1.90157777428627\n",
      "Epoch 7, Batch 200, Loss: 1.9759905600547791\n",
      "Epoch 7, Batch 300, Loss: 1.9196206653118133\n",
      "Epoch 7, Batch 400, Loss: 1.9353871643543243\n",
      "Epoch 7, Batch 500, Loss: 2.003717005252838\n",
      "Epoch 7, Batch 600, Loss: 1.9157246840000153\n",
      "Epoch 7, Batch 700, Loss: 1.8949841368198395\n",
      "Epoch 7, Batch 800, Loss: 1.960122014284134\n",
      "Epoch 7, Batch 900, Loss: 1.9106257009506225\n",
      "Epoch 8, Batch 100, Loss: 2.100959234237671\n",
      "Epoch 8, Batch 200, Loss: 1.928842487335205\n",
      "Epoch 8, Batch 300, Loss: 1.8765551853179931\n",
      "Epoch 8, Batch 400, Loss: 1.9408045244216918\n",
      "Epoch 8, Batch 500, Loss: 1.9500095915794373\n",
      "Epoch 8, Batch 600, Loss: 1.9825495159626008\n",
      "Epoch 8, Batch 700, Loss: 1.920061651468277\n",
      "Epoch 8, Batch 800, Loss: 1.9490698897838592\n",
      "Epoch 8, Batch 900, Loss: 1.9811655354499818\n",
      "Epoch 9, Batch 100, Loss: 1.982776243686676\n",
      "Epoch 9, Batch 200, Loss: 1.8861858546733856\n",
      "Epoch 9, Batch 300, Loss: 1.9119208943843842\n",
      "Epoch 9, Batch 400, Loss: 1.9569302487373352\n",
      "Epoch 9, Batch 500, Loss: 1.9811728239059447\n",
      "Epoch 9, Batch 600, Loss: 1.9978862380981446\n",
      "Epoch 9, Batch 700, Loss: 1.9365591168403626\n",
      "Epoch 9, Batch 800, Loss: 1.9218030202388763\n",
      "Epoch 9, Batch 900, Loss: 2.007451256513596\n",
      "Epoch 10, Batch 100, Loss: 1.9439491593837739\n",
      "Epoch 10, Batch 200, Loss: 1.9467713558673858\n",
      "Epoch 10, Batch 300, Loss: 1.963115358352661\n",
      "Epoch 10, Batch 400, Loss: 1.9445411550998688\n",
      "Epoch 10, Batch 500, Loss: 1.8918376922607423\n",
      "Epoch 10, Batch 600, Loss: 1.9406092619895936\n",
      "Epoch 10, Batch 700, Loss: 1.9736706948280334\n",
      "Epoch 10, Batch 800, Loss: 1.9283670425415038\n",
      "Epoch 10, Batch 900, Loss: 1.9493800842761992\n",
      "Epoch 11, Batch 100, Loss: 1.9658786487579345\n",
      "Epoch 11, Batch 200, Loss: 1.960838077068329\n",
      "Epoch 11, Batch 300, Loss: 1.928076717853546\n",
      "Epoch 11, Batch 400, Loss: 1.9121789801120759\n",
      "Epoch 11, Batch 500, Loss: 1.9163620495796203\n",
      "Epoch 11, Batch 600, Loss: 1.9449280667304993\n",
      "Epoch 11, Batch 700, Loss: 1.9081067967414855\n",
      "Epoch 11, Batch 800, Loss: 1.9224902760982514\n",
      "Epoch 11, Batch 900, Loss: 1.9045121002197265\n",
      "Epoch 12, Batch 100, Loss: 1.857399444580078\n",
      "Epoch 12, Batch 200, Loss: 2.0014737153053286\n",
      "Epoch 12, Batch 300, Loss: 1.9134280264377594\n",
      "Epoch 12, Batch 400, Loss: 1.9323914432525635\n",
      "Epoch 12, Batch 500, Loss: 1.9627801525592803\n",
      "Epoch 12, Batch 600, Loss: 1.877821122407913\n",
      "Epoch 12, Batch 700, Loss: 1.9814545929431915\n",
      "Epoch 12, Batch 800, Loss: 1.9725267469882966\n",
      "Epoch 12, Batch 900, Loss: 1.9208894217014312\n",
      "Epoch 13, Batch 100, Loss: 1.943179508447647\n",
      "Epoch 13, Batch 200, Loss: 1.9273282623291015\n",
      "Epoch 13, Batch 300, Loss: 1.9017454588413238\n",
      "Epoch 13, Batch 400, Loss: 1.9333805072307586\n",
      "Epoch 13, Batch 500, Loss: 1.906585932970047\n",
      "Epoch 13, Batch 600, Loss: 1.9597226989269256\n",
      "Epoch 13, Batch 700, Loss: 1.917172565460205\n",
      "Epoch 13, Batch 800, Loss: 1.942641078233719\n",
      "Epoch 13, Batch 900, Loss: 1.861537424325943\n",
      "Epoch 14, Batch 100, Loss: 1.9619083404541016\n",
      "Epoch 14, Batch 200, Loss: 1.9841841197013854\n",
      "Epoch 14, Batch 300, Loss: 1.9554010140895843\n",
      "Epoch 14, Batch 400, Loss: 1.9243998432159424\n",
      "Epoch 14, Batch 500, Loss: 1.90090686917305\n",
      "Epoch 14, Batch 600, Loss: 1.9457642590999604\n",
      "Epoch 14, Batch 700, Loss: 1.9436097490787505\n",
      "Epoch 14, Batch 800, Loss: 1.9026863944530488\n",
      "Epoch 14, Batch 900, Loss: 1.942029526233673\n",
      "Epoch 15, Batch 100, Loss: 1.941556646823883\n",
      "Epoch 15, Batch 200, Loss: 1.9648353850841522\n",
      "Epoch 15, Batch 300, Loss: 1.9488196933269502\n",
      "Epoch 15, Batch 400, Loss: 1.8828243482112885\n",
      "Epoch 15, Batch 500, Loss: 1.9441038739681245\n",
      "Epoch 15, Batch 600, Loss: 1.950018697977066\n",
      "Epoch 15, Batch 700, Loss: 1.927874482870102\n",
      "Epoch 15, Batch 800, Loss: 1.9307027637958527\n",
      "Epoch 15, Batch 900, Loss: 1.9612513506412506\n",
      "Epoch 16, Batch 100, Loss: 1.987506766319275\n",
      "Epoch 16, Batch 200, Loss: 1.9956297540664674\n",
      "Epoch 16, Batch 300, Loss: 1.932256566286087\n",
      "Epoch 16, Batch 400, Loss: 1.9380053794384002\n",
      "Epoch 16, Batch 500, Loss: 1.9402968037128447\n",
      "Epoch 16, Batch 600, Loss: 2.0100572085380555\n",
      "Epoch 16, Batch 700, Loss: 1.9492501139640808\n",
      "Epoch 16, Batch 800, Loss: 1.9531383907794952\n",
      "Epoch 16, Batch 900, Loss: 1.9492873644828796\n",
      "Epoch 17, Batch 100, Loss: 1.8719049751758576\n",
      "Epoch 17, Batch 200, Loss: 1.9167278695106507\n",
      "Epoch 17, Batch 300, Loss: 1.967547014951706\n",
      "Epoch 17, Batch 400, Loss: 1.9103788697719575\n",
      "Epoch 17, Batch 500, Loss: 1.9963597106933593\n",
      "Epoch 17, Batch 600, Loss: 1.9349361658096313\n",
      "Epoch 17, Batch 700, Loss: 1.9166518557071686\n",
      "Epoch 17, Batch 800, Loss: 1.9729553174972534\n",
      "Epoch 17, Batch 900, Loss: 1.9246890950202942\n",
      "Epoch 18, Batch 100, Loss: 1.9466232180595398\n",
      "Epoch 18, Batch 200, Loss: 1.9130762100219727\n",
      "Epoch 18, Batch 300, Loss: 1.914746779203415\n",
      "Epoch 18, Batch 400, Loss: 1.924709107875824\n",
      "Epoch 18, Batch 500, Loss: 1.9248142099380494\n",
      "Epoch 18, Batch 600, Loss: 1.9387790632247925\n",
      "Epoch 18, Batch 700, Loss: 1.9481753313541412\n",
      "Epoch 18, Batch 800, Loss: 1.9392973589897156\n",
      "Epoch 18, Batch 900, Loss: 1.9442891240119935\n",
      "Epoch 19, Batch 100, Loss: 1.940159978866577\n",
      "Epoch 19, Batch 200, Loss: 1.9790135538578033\n",
      "Epoch 19, Batch 300, Loss: 1.9603802561759949\n",
      "Epoch 19, Batch 400, Loss: 1.9475264215469361\n",
      "Epoch 19, Batch 500, Loss: 2.0152948665618897\n",
      "Epoch 19, Batch 600, Loss: 2.0368582606315613\n",
      "Epoch 19, Batch 700, Loss: 1.9511496722698212\n",
      "Epoch 19, Batch 800, Loss: 1.9109198021888734\n",
      "Epoch 19, Batch 900, Loss: 1.9256423544883727\n",
      "Epoch 20, Batch 100, Loss: 2.0033322417736055\n",
      "Epoch 20, Batch 200, Loss: 1.9791113913059235\n",
      "Epoch 20, Batch 300, Loss: 1.9998860573768615\n",
      "Epoch 20, Batch 400, Loss: 1.889237414598465\n",
      "Epoch 20, Batch 500, Loss: 1.967321881055832\n",
      "Epoch 20, Batch 600, Loss: 1.9410545921325684\n",
      "Epoch 20, Batch 700, Loss: 1.899697539806366\n",
      "Epoch 20, Batch 800, Loss: 1.9658062851428986\n",
      "Epoch 20, Batch 900, Loss: 1.901892911195755\n",
      "Epoch 21, Batch 100, Loss: 1.873084729909897\n",
      "Epoch 21, Batch 200, Loss: 1.970891191959381\n",
      "Epoch 21, Batch 300, Loss: 1.9302325081825256\n",
      "Epoch 21, Batch 400, Loss: 1.9737057280540466\n",
      "Epoch 21, Batch 500, Loss: 1.927891753911972\n",
      "Epoch 21, Batch 600, Loss: 1.9283160698413848\n",
      "Epoch 21, Batch 700, Loss: 1.894232519865036\n",
      "Epoch 21, Batch 800, Loss: 1.9069975185394288\n",
      "Epoch 21, Batch 900, Loss: 2.040833214521408\n",
      "Epoch 22, Batch 100, Loss: 1.9014724755287171\n",
      "Epoch 22, Batch 200, Loss: 1.9194793963432313\n",
      "Epoch 22, Batch 300, Loss: 1.9356782639026642\n",
      "Epoch 22, Batch 400, Loss: 1.9451027190685273\n",
      "Epoch 22, Batch 500, Loss: 1.9812985527515412\n",
      "Epoch 22, Batch 600, Loss: 1.9035167503356933\n",
      "Epoch 22, Batch 700, Loss: 1.9676800346374512\n",
      "Epoch 22, Batch 800, Loss: 1.93606369972229\n",
      "Epoch 22, Batch 900, Loss: 1.9774040138721467\n",
      "Epoch 23, Batch 100, Loss: 1.9767845153808594\n",
      "Epoch 23, Batch 200, Loss: 1.9211914932727814\n",
      "Epoch 23, Batch 300, Loss: 1.9452148747444153\n",
      "Epoch 23, Batch 400, Loss: 1.913027275800705\n",
      "Epoch 23, Batch 500, Loss: 1.9470265996456146\n",
      "Epoch 23, Batch 600, Loss: 1.9553340351581574\n",
      "Epoch 23, Batch 700, Loss: 1.9630318093299866\n",
      "Epoch 23, Batch 800, Loss: 1.9057393872737884\n",
      "Epoch 23, Batch 900, Loss: 1.918267775774002\n",
      "Epoch 24, Batch 100, Loss: 1.8534636723995208\n",
      "Epoch 24, Batch 200, Loss: 1.9757254278659822\n",
      "Epoch 24, Batch 300, Loss: 1.934995847940445\n",
      "Epoch 24, Batch 400, Loss: 1.9730534088611602\n",
      "Epoch 24, Batch 500, Loss: 1.931185220479965\n",
      "Epoch 24, Batch 600, Loss: 1.9382568871974946\n",
      "Epoch 24, Batch 700, Loss: 1.9298354375362397\n",
      "Epoch 24, Batch 800, Loss: 1.9371434271335601\n",
      "Epoch 24, Batch 900, Loss: 1.9064607524871826\n",
      "Epoch 25, Batch 100, Loss: 1.957383484840393\n",
      "Epoch 25, Batch 200, Loss: 2.0356213176250457\n",
      "Epoch 25, Batch 300, Loss: 1.9038347935676574\n",
      "Epoch 25, Batch 400, Loss: 1.9403841769695283\n",
      "Epoch 25, Batch 500, Loss: 1.9844266772270203\n",
      "Epoch 25, Batch 600, Loss: 1.9803134059906007\n",
      "Epoch 25, Batch 700, Loss: 1.9850214648246765\n",
      "Epoch 25, Batch 800, Loss: 1.9353522324562074\n",
      "Epoch 25, Batch 900, Loss: 1.9589747953414918\n",
      "Accuracy on test set: 0.2671%\n",
      "Fitting for combination 19\n",
      "784\n",
      "1\n",
      "10\n",
      "[50, 10]\n",
      "True\n",
      "['relu']\n",
      "SGD\n",
      "0.3\n",
      "0\n",
      "CrossEntropyLoss\n",
      "100\n",
      "25\n",
      "0\n",
      "Epoch 1, Batch 100, Loss: 1.1823943787813187\n",
      "Epoch 1, Batch 200, Loss: 0.7108990895748138\n",
      "Epoch 1, Batch 300, Loss: 0.6200701278448105\n",
      "Epoch 1, Batch 400, Loss: 0.6346173390746117\n",
      "Epoch 1, Batch 500, Loss: 0.5441871443390847\n",
      "Epoch 1, Batch 600, Loss: 0.5511507910490036\n",
      "Epoch 1, Batch 700, Loss: 0.5098217557370662\n",
      "Epoch 1, Batch 800, Loss: 0.4846866747736931\n",
      "Epoch 1, Batch 900, Loss: 0.4702225521206856\n",
      "Epoch 2, Batch 100, Loss: 0.4584159383177757\n",
      "Epoch 2, Batch 200, Loss: 0.45775712162256244\n",
      "Epoch 2, Batch 300, Loss: 0.4549404999613762\n",
      "Epoch 2, Batch 400, Loss: 0.4718582977354526\n",
      "Epoch 2, Batch 500, Loss: 0.4469537144899368\n",
      "Epoch 2, Batch 600, Loss: 0.46036104917526244\n",
      "Epoch 2, Batch 700, Loss: 0.41322070941329003\n",
      "Epoch 2, Batch 800, Loss: 0.4206561660766602\n",
      "Epoch 2, Batch 900, Loss: 0.4205231638252735\n",
      "Epoch 3, Batch 100, Loss: 0.4206548775732517\n",
      "Epoch 3, Batch 200, Loss: 0.42352831915020944\n",
      "Epoch 3, Batch 300, Loss: 0.4029178942739964\n",
      "Epoch 3, Batch 400, Loss: 0.4179033301770687\n",
      "Epoch 3, Batch 500, Loss: 0.3947051303088665\n",
      "Epoch 3, Batch 600, Loss: 0.4030512851476669\n",
      "Epoch 3, Batch 700, Loss: 0.3951986427605152\n",
      "Epoch 3, Batch 800, Loss: 0.4032712414860725\n",
      "Epoch 3, Batch 900, Loss: 0.39993021920323374\n",
      "Epoch 4, Batch 100, Loss: 0.3836141888797283\n",
      "Epoch 4, Batch 200, Loss: 0.38364594846963884\n",
      "Epoch 4, Batch 300, Loss: 0.3611954788863659\n",
      "Epoch 4, Batch 400, Loss: 0.3846121683716774\n",
      "Epoch 4, Batch 500, Loss: 0.3922944688051939\n",
      "Epoch 4, Batch 600, Loss: 0.3868093991279602\n",
      "Epoch 4, Batch 700, Loss: 0.38523503348231314\n",
      "Epoch 4, Batch 800, Loss: 0.3864487178623676\n",
      "Epoch 4, Batch 900, Loss: 0.3627582710981369\n",
      "Epoch 5, Batch 100, Loss: 0.3567595322430134\n",
      "Epoch 5, Batch 200, Loss: 0.36126276805996893\n",
      "Epoch 5, Batch 300, Loss: 0.38643492333590984\n",
      "Epoch 5, Batch 400, Loss: 0.36543077155947684\n",
      "Epoch 5, Batch 500, Loss: 0.3636063987016678\n",
      "Epoch 5, Batch 600, Loss: 0.3538720673322678\n",
      "Epoch 5, Batch 700, Loss: 0.34999105855822565\n",
      "Epoch 5, Batch 800, Loss: 0.3729741023480892\n",
      "Epoch 5, Batch 900, Loss: 0.3602971278131008\n",
      "Epoch 6, Batch 100, Loss: 0.3424097619950771\n",
      "Epoch 6, Batch 200, Loss: 0.359894382506609\n",
      "Epoch 6, Batch 300, Loss: 0.3353355964273214\n",
      "Epoch 6, Batch 400, Loss: 0.3497083207964897\n",
      "Epoch 6, Batch 500, Loss: 0.34955447986721994\n",
      "Epoch 6, Batch 600, Loss: 0.34972095757722854\n",
      "Epoch 6, Batch 700, Loss: 0.34709370076656343\n",
      "Epoch 6, Batch 800, Loss: 0.3668214947730303\n",
      "Epoch 6, Batch 900, Loss: 0.37562506780028343\n",
      "Epoch 7, Batch 100, Loss: 0.33671846479177475\n",
      "Epoch 7, Batch 200, Loss: 0.3322620913386345\n",
      "Epoch 7, Batch 300, Loss: 0.3261870360374451\n",
      "Epoch 7, Batch 400, Loss: 0.3248903653025627\n",
      "Epoch 7, Batch 500, Loss: 0.35345040962100027\n",
      "Epoch 7, Batch 600, Loss: 0.3491470958292484\n",
      "Epoch 7, Batch 700, Loss: 0.3548258349299431\n",
      "Epoch 7, Batch 800, Loss: 0.33249162703752516\n",
      "Epoch 7, Batch 900, Loss: 0.32542157217860224\n",
      "Epoch 8, Batch 100, Loss: 0.32906692221760747\n",
      "Epoch 8, Batch 200, Loss: 0.3105212327837944\n",
      "Epoch 8, Batch 300, Loss: 0.3310136766731739\n",
      "Epoch 8, Batch 400, Loss: 0.3444470153748989\n",
      "Epoch 8, Batch 500, Loss: 0.33323888555169107\n",
      "Epoch 8, Batch 600, Loss: 0.3296037143468857\n",
      "Epoch 8, Batch 700, Loss: 0.31494441598653794\n",
      "Epoch 8, Batch 800, Loss: 0.3315428977459669\n",
      "Epoch 8, Batch 900, Loss: 0.3462816295772791\n",
      "Epoch 9, Batch 100, Loss: 0.3481078553199768\n",
      "Epoch 9, Batch 200, Loss: 0.33372703567147255\n",
      "Epoch 9, Batch 300, Loss: 0.30786876179277894\n",
      "Epoch 9, Batch 400, Loss: 0.3201590216904879\n",
      "Epoch 9, Batch 500, Loss: 0.3233845775574446\n",
      "Epoch 9, Batch 600, Loss: 0.31710076302289963\n",
      "Epoch 9, Batch 700, Loss: 0.3125543025881052\n",
      "Epoch 9, Batch 800, Loss: 0.3366949202865362\n",
      "Epoch 9, Batch 900, Loss: 0.3370279134809971\n",
      "Epoch 10, Batch 100, Loss: 0.320085346698761\n",
      "Epoch 10, Batch 200, Loss: 0.3259115773439407\n",
      "Epoch 10, Batch 300, Loss: 0.31444187022745607\n",
      "Epoch 10, Batch 400, Loss: 0.3132588761299849\n",
      "Epoch 10, Batch 500, Loss: 0.3223638705164194\n",
      "Epoch 10, Batch 600, Loss: 0.33994205698370933\n",
      "Epoch 10, Batch 700, Loss: 0.3210206246376038\n",
      "Epoch 10, Batch 800, Loss: 0.30037537395954134\n",
      "Epoch 10, Batch 900, Loss: 0.3124409699440002\n",
      "Epoch 11, Batch 100, Loss: 0.32391422778367995\n",
      "Epoch 11, Batch 200, Loss: 0.3087379155308008\n",
      "Epoch 11, Batch 300, Loss: 0.30559909999370577\n",
      "Epoch 11, Batch 400, Loss: 0.3134326908737421\n",
      "Epoch 11, Batch 500, Loss: 0.29566565677523615\n",
      "Epoch 11, Batch 600, Loss: 0.3225473663210869\n",
      "Epoch 11, Batch 700, Loss: 0.3252869911491871\n",
      "Epoch 11, Batch 800, Loss: 0.32307762756943703\n",
      "Epoch 11, Batch 900, Loss: 0.30171263083815575\n",
      "Epoch 12, Batch 100, Loss: 0.3111838440597057\n",
      "Epoch 12, Batch 200, Loss: 0.2998142255097628\n",
      "Epoch 12, Batch 300, Loss: 0.32300998486578464\n",
      "Epoch 12, Batch 400, Loss: 0.2981988182663918\n",
      "Epoch 12, Batch 500, Loss: 0.29275130957365036\n",
      "Epoch 12, Batch 600, Loss: 0.30138226024806497\n",
      "Epoch 12, Batch 700, Loss: 0.3149076972901821\n",
      "Epoch 12, Batch 800, Loss: 0.32724492460489274\n",
      "Epoch 12, Batch 900, Loss: 0.3114767943322658\n",
      "Epoch 13, Batch 100, Loss: 0.2867751653492451\n",
      "Epoch 13, Batch 200, Loss: 0.28954618871212007\n",
      "Epoch 13, Batch 300, Loss: 0.3013973701000214\n",
      "Epoch 13, Batch 400, Loss: 0.31643431812524797\n",
      "Epoch 13, Batch 500, Loss: 0.3122149356454611\n",
      "Epoch 13, Batch 600, Loss: 0.29709770932793617\n",
      "Epoch 13, Batch 700, Loss: 0.2950690004229546\n",
      "Epoch 13, Batch 800, Loss: 0.32576204106211665\n",
      "Epoch 13, Batch 900, Loss: 0.3119773521274328\n",
      "Epoch 14, Batch 100, Loss: 0.2797990092635155\n",
      "Epoch 14, Batch 200, Loss: 0.29215090356767176\n",
      "Epoch 14, Batch 300, Loss: 0.30375725880265236\n",
      "Epoch 14, Batch 400, Loss: 0.31879463091492655\n",
      "Epoch 14, Batch 500, Loss: 0.30442682705819607\n",
      "Epoch 14, Batch 600, Loss: 0.3035367465019226\n",
      "Epoch 14, Batch 700, Loss: 0.2793559341132641\n",
      "Epoch 14, Batch 800, Loss: 0.2971425050497055\n",
      "Epoch 14, Batch 900, Loss: 0.2938567530363798\n",
      "Epoch 15, Batch 100, Loss: 0.2994450905919075\n",
      "Epoch 15, Batch 200, Loss: 0.28561059780418874\n",
      "Epoch 15, Batch 300, Loss: 0.3069864631444216\n",
      "Epoch 15, Batch 400, Loss: 0.2751356365531683\n",
      "Epoch 15, Batch 500, Loss: 0.2793045748770237\n",
      "Epoch 15, Batch 600, Loss: 0.3049002106487751\n",
      "Epoch 15, Batch 700, Loss: 0.2994641089439392\n",
      "Epoch 15, Batch 800, Loss: 0.29345005072653296\n",
      "Epoch 15, Batch 900, Loss: 0.3076473070681095\n",
      "Epoch 16, Batch 100, Loss: 0.2982621321082115\n",
      "Epoch 16, Batch 200, Loss: 0.2890824317932129\n",
      "Epoch 16, Batch 300, Loss: 0.2818029533326626\n",
      "Epoch 16, Batch 400, Loss: 0.28594924867153165\n",
      "Epoch 16, Batch 500, Loss: 0.297596500441432\n",
      "Epoch 16, Batch 600, Loss: 0.2776736434549093\n",
      "Epoch 16, Batch 700, Loss: 0.30014018043875695\n",
      "Epoch 16, Batch 800, Loss: 0.30242330305278303\n",
      "Epoch 16, Batch 900, Loss: 0.2798403738439083\n",
      "Epoch 17, Batch 100, Loss: 0.2847657138109207\n",
      "Epoch 17, Batch 200, Loss: 0.26310327485203744\n",
      "Epoch 17, Batch 300, Loss: 0.2778368853777647\n",
      "Epoch 17, Batch 400, Loss: 0.2872528334707022\n",
      "Epoch 17, Batch 500, Loss: 0.29901778072118756\n",
      "Epoch 17, Batch 600, Loss: 0.28732406098395585\n",
      "Epoch 17, Batch 700, Loss: 0.2934806987643242\n",
      "Epoch 17, Batch 800, Loss: 0.2964524804055691\n",
      "Epoch 17, Batch 900, Loss: 0.3051790721714497\n",
      "Epoch 18, Batch 100, Loss: 0.2789742228388786\n",
      "Epoch 18, Batch 200, Loss: 0.2934388507902622\n",
      "Epoch 18, Batch 300, Loss: 0.2780730428546667\n",
      "Epoch 18, Batch 400, Loss: 0.28394580125808716\n",
      "Epoch 18, Batch 500, Loss: 0.2924768163263798\n",
      "Epoch 18, Batch 600, Loss: 0.28106866762042043\n",
      "Epoch 18, Batch 700, Loss: 0.29018894255161287\n",
      "Epoch 18, Batch 800, Loss: 0.29125536769628524\n",
      "Epoch 18, Batch 900, Loss: 0.2774042545258999\n",
      "Epoch 19, Batch 100, Loss: 0.26735291086137297\n",
      "Epoch 19, Batch 200, Loss: 0.28978779815137384\n",
      "Epoch 19, Batch 300, Loss: 0.27248174406588077\n",
      "Epoch 19, Batch 400, Loss: 0.27433919586241245\n",
      "Epoch 19, Batch 500, Loss: 0.28502692729234697\n",
      "Epoch 19, Batch 600, Loss: 0.2794318375736475\n",
      "Epoch 19, Batch 700, Loss: 0.2723148395121098\n",
      "Epoch 19, Batch 800, Loss: 0.2946240599453449\n",
      "Epoch 19, Batch 900, Loss: 0.2851547472178936\n",
      "Epoch 20, Batch 100, Loss: 0.275342283770442\n",
      "Epoch 20, Batch 200, Loss: 0.2697110015153885\n",
      "Epoch 20, Batch 300, Loss: 0.2773570817708969\n",
      "Epoch 20, Batch 400, Loss: 0.2691433319449425\n",
      "Epoch 20, Batch 500, Loss: 0.28638079479336737\n",
      "Epoch 20, Batch 600, Loss: 0.27328911997377875\n",
      "Epoch 20, Batch 700, Loss: 0.2851076520979404\n",
      "Epoch 20, Batch 800, Loss: 0.2680782787501812\n",
      "Epoch 20, Batch 900, Loss: 0.27972781151533127\n",
      "Epoch 21, Batch 100, Loss: 0.2645508683472872\n",
      "Epoch 21, Batch 200, Loss: 0.2601162559911609\n",
      "Epoch 21, Batch 300, Loss: 0.2928017868846655\n",
      "Epoch 21, Batch 400, Loss: 0.2738235055655241\n",
      "Epoch 21, Batch 500, Loss: 0.28174917727708815\n",
      "Epoch 21, Batch 600, Loss: 0.2548987828940153\n",
      "Epoch 21, Batch 700, Loss: 0.2825454024970531\n",
      "Epoch 21, Batch 800, Loss: 0.27721078574657443\n",
      "Epoch 21, Batch 900, Loss: 0.2883608192950487\n",
      "Epoch 22, Batch 100, Loss: 0.2584873402863741\n",
      "Epoch 22, Batch 200, Loss: 0.28734021380543706\n",
      "Epoch 22, Batch 300, Loss: 0.26529886230826377\n",
      "Epoch 22, Batch 400, Loss: 0.2625391113758087\n",
      "Epoch 22, Batch 500, Loss: 0.2738472291827202\n",
      "Epoch 22, Batch 600, Loss: 0.286057883054018\n",
      "Epoch 22, Batch 700, Loss: 0.2746718700230122\n",
      "Epoch 22, Batch 800, Loss: 0.2692554017156363\n",
      "Epoch 22, Batch 900, Loss: 0.27104296796023847\n",
      "Epoch 23, Batch 100, Loss: 0.25510695174336434\n",
      "Epoch 23, Batch 200, Loss: 0.26537618674337865\n",
      "Epoch 23, Batch 300, Loss: 0.2737705462425947\n",
      "Epoch 23, Batch 400, Loss: 0.2723225738108158\n",
      "Epoch 23, Batch 500, Loss: 0.2690481528639793\n",
      "Epoch 23, Batch 600, Loss: 0.2655913027375936\n",
      "Epoch 23, Batch 700, Loss: 0.26651374243199827\n",
      "Epoch 23, Batch 800, Loss: 0.25852521978318693\n",
      "Epoch 23, Batch 900, Loss: 0.28356592886149884\n",
      "Epoch 24, Batch 100, Loss: 0.25724253937602043\n",
      "Epoch 24, Batch 200, Loss: 0.2510087090730667\n",
      "Epoch 24, Batch 300, Loss: 0.26350452832877636\n",
      "Epoch 24, Batch 400, Loss: 0.28981474101543425\n",
      "Epoch 24, Batch 500, Loss: 0.2611043769866228\n",
      "Epoch 24, Batch 600, Loss: 0.24916870720684528\n",
      "Epoch 24, Batch 700, Loss: 0.25621065504848956\n",
      "Epoch 24, Batch 800, Loss: 0.27182576194405556\n",
      "Epoch 24, Batch 900, Loss: 0.26339896269142626\n",
      "Epoch 25, Batch 100, Loss: 0.2594624266773462\n",
      "Epoch 25, Batch 200, Loss: 0.26274962574243543\n",
      "Epoch 25, Batch 300, Loss: 0.26058895580470565\n",
      "Epoch 25, Batch 400, Loss: 0.2682090014219284\n",
      "Epoch 25, Batch 500, Loss: 0.26428189866244794\n",
      "Epoch 25, Batch 600, Loss: 0.27466910690069196\n",
      "Epoch 25, Batch 700, Loss: 0.2540178506076336\n",
      "Epoch 25, Batch 800, Loss: 0.2579145532846451\n",
      "Epoch 25, Batch 900, Loss: 0.27074861489236357\n",
      "Accuracy on test set: 0.8477%\n",
      "Fitting for combination 20\n",
      "784\n",
      "2\n",
      "10\n",
      "[40, 10, 10]\n",
      "False\n",
      "['relu', 'sigmoid']\n",
      "Adam\n",
      "0.1\n",
      "0.3\n",
      "CrossEntropyLoss\n",
      "200\n",
      "25\n",
      "0\n",
      "Epoch 1, Batch 200, Loss: 4.64020414352417\n",
      "Epoch 1, Batch 400, Loss: 4.640442771911621\n",
      "Epoch 1, Batch 600, Loss: 4.655361030101776\n",
      "Epoch 1, Batch 800, Loss: 4.6500751948356625\n",
      "Epoch 2, Batch 200, Loss: 4.65286808013916\n",
      "Epoch 2, Batch 400, Loss: 4.651939206123352\n",
      "Epoch 2, Batch 600, Loss: 4.638114380836487\n",
      "Epoch 2, Batch 800, Loss: 4.647732698917389\n",
      "Epoch 3, Batch 200, Loss: 4.64256293296814\n",
      "Epoch 3, Batch 400, Loss: 4.649597582817077\n",
      "Epoch 3, Batch 600, Loss: 4.642167043685913\n",
      "Epoch 3, Batch 800, Loss: 4.648137485980987\n",
      "Epoch 4, Batch 200, Loss: 4.652710235118866\n",
      "Epoch 4, Batch 400, Loss: 4.648960223197937\n",
      "Epoch 4, Batch 600, Loss: 4.642690949440002\n",
      "Epoch 4, Batch 800, Loss: 4.640610582828522\n",
      "Epoch 5, Batch 200, Loss: 4.645005297660828\n",
      "Epoch 5, Batch 400, Loss: 4.646565952301025\n",
      "Epoch 5, Batch 600, Loss: 4.6441165685653685\n",
      "Epoch 5, Batch 800, Loss: 4.642656943798065\n",
      "Epoch 6, Batch 200, Loss: 4.645417845249176\n",
      "Epoch 6, Batch 400, Loss: 4.6466939473152165\n",
      "Epoch 6, Batch 600, Loss: 4.6469641256332395\n",
      "Epoch 6, Batch 800, Loss: 4.648078660964966\n",
      "Epoch 7, Batch 200, Loss: 4.645221765041351\n",
      "Epoch 7, Batch 400, Loss: 4.642405753135681\n",
      "Epoch 7, Batch 600, Loss: 4.6504270100593565\n",
      "Epoch 7, Batch 800, Loss: 4.6400412702560425\n",
      "Epoch 8, Batch 200, Loss: 4.644234852790833\n",
      "Epoch 8, Batch 400, Loss: 4.649192705154419\n",
      "Epoch 8, Batch 600, Loss: 4.652654058933258\n",
      "Epoch 8, Batch 800, Loss: 4.6436216616630555\n",
      "Epoch 9, Batch 200, Loss: 4.64030613899231\n",
      "Epoch 9, Batch 400, Loss: 4.638527612686158\n",
      "Epoch 9, Batch 600, Loss: 4.647963769435883\n",
      "Epoch 9, Batch 800, Loss: 4.649182484149933\n",
      "Epoch 10, Batch 200, Loss: 4.64538250207901\n",
      "Epoch 10, Batch 400, Loss: 4.643984138965607\n",
      "Epoch 10, Batch 600, Loss: 4.640583298206329\n",
      "Epoch 10, Batch 800, Loss: 4.638870635032654\n",
      "Epoch 11, Batch 200, Loss: 4.642664325237274\n",
      "Epoch 11, Batch 400, Loss: 4.6472812843322755\n",
      "Epoch 11, Batch 600, Loss: 4.647390735149384\n",
      "Epoch 11, Batch 800, Loss: 4.647323784828186\n",
      "Epoch 12, Batch 200, Loss: 4.65484049320221\n",
      "Epoch 12, Batch 400, Loss: 4.642451319694519\n",
      "Epoch 12, Batch 600, Loss: 4.645695111751556\n",
      "Epoch 12, Batch 800, Loss: 4.643428051471711\n",
      "Epoch 13, Batch 200, Loss: 4.646938421726227\n",
      "Epoch 13, Batch 400, Loss: 4.6458415508270265\n",
      "Epoch 13, Batch 600, Loss: 4.638955972194672\n",
      "Epoch 13, Batch 800, Loss: 4.64442688703537\n",
      "Epoch 14, Batch 200, Loss: 4.645569453239441\n",
      "Epoch 14, Batch 400, Loss: 4.643674376010895\n",
      "Epoch 14, Batch 600, Loss: 4.639795496463775\n",
      "Epoch 14, Batch 800, Loss: 4.646515071392059\n",
      "Epoch 15, Batch 200, Loss: 4.639757232666016\n",
      "Epoch 15, Batch 400, Loss: 4.643148460388184\n",
      "Epoch 15, Batch 600, Loss: 4.648873481750488\n",
      "Epoch 15, Batch 800, Loss: 4.655076417922974\n",
      "Epoch 16, Batch 200, Loss: 4.645791549682617\n",
      "Epoch 16, Batch 400, Loss: 4.644552726745605\n",
      "Epoch 16, Batch 600, Loss: 4.64626204252243\n",
      "Epoch 16, Batch 800, Loss: 4.649681134223938\n",
      "Epoch 17, Batch 200, Loss: 4.651144227981567\n",
      "Epoch 17, Batch 400, Loss: 4.643178014755249\n",
      "Epoch 17, Batch 600, Loss: 4.651505377292633\n",
      "Epoch 17, Batch 800, Loss: 4.64559406042099\n",
      "Epoch 18, Batch 200, Loss: 4.64028324842453\n",
      "Epoch 18, Batch 400, Loss: 4.643367185592651\n",
      "Epoch 18, Batch 600, Loss: 4.641474471092224\n",
      "Epoch 18, Batch 800, Loss: 4.642141830921173\n",
      "Epoch 19, Batch 200, Loss: 4.642315104007721\n",
      "Epoch 19, Batch 400, Loss: 4.637040011882782\n",
      "Epoch 19, Batch 600, Loss: 4.649196693897247\n",
      "Epoch 19, Batch 800, Loss: 4.653769183158874\n",
      "Epoch 20, Batch 200, Loss: 4.648490619659424\n",
      "Epoch 20, Batch 400, Loss: 4.649168846607208\n",
      "Epoch 20, Batch 600, Loss: 4.648773648738861\n",
      "Epoch 20, Batch 800, Loss: 4.644578738212585\n",
      "Epoch 21, Batch 200, Loss: 4.647997944355011\n",
      "Epoch 21, Batch 400, Loss: 4.640862088203431\n",
      "Epoch 21, Batch 600, Loss: 4.642344136238098\n",
      "Epoch 21, Batch 800, Loss: 4.648961491584778\n",
      "Epoch 22, Batch 200, Loss: 4.644673030376435\n",
      "Epoch 22, Batch 400, Loss: 4.655524568557739\n",
      "Epoch 22, Batch 600, Loss: 4.643170611858368\n",
      "Epoch 22, Batch 800, Loss: 4.641639013290405\n",
      "Epoch 23, Batch 200, Loss: 4.642517192363739\n",
      "Epoch 23, Batch 400, Loss: 4.642858834266662\n",
      "Epoch 23, Batch 600, Loss: 4.648536288738251\n",
      "Epoch 23, Batch 800, Loss: 4.646735312938691\n",
      "Epoch 24, Batch 200, Loss: 4.64374853849411\n",
      "Epoch 24, Batch 400, Loss: 4.64541307926178\n",
      "Epoch 24, Batch 600, Loss: 4.646780679225921\n",
      "Epoch 24, Batch 800, Loss: 4.645404069423676\n",
      "Epoch 25, Batch 200, Loss: 4.649119920730591\n",
      "Epoch 25, Batch 400, Loss: 4.642739157676697\n",
      "Epoch 25, Batch 600, Loss: 4.645535957813263\n",
      "Epoch 25, Batch 800, Loss: 4.645903651714325\n",
      "Accuracy on test set: 0.1%\n",
      "Fitting for combination 21\n",
      "784\n",
      "2\n",
      "10\n",
      "[40, 10, 10]\n",
      "True\n",
      "['relu', 'sigmoid']\n",
      "SGD\n",
      "0.1\n",
      "0.1\n",
      "CrossEntropyLoss\n",
      "100\n",
      "25\n",
      "0\n",
      "Epoch 1, Batch 100, Loss: 2.2559434580802917\n",
      "Epoch 1, Batch 200, Loss: 2.1410291266441344\n",
      "Epoch 1, Batch 300, Loss: 1.9902975451946259\n",
      "Epoch 1, Batch 400, Loss: 1.9274404907226563\n",
      "Epoch 1, Batch 500, Loss: 1.9064772045612335\n",
      "Epoch 1, Batch 600, Loss: 1.9015717029571533\n",
      "Epoch 1, Batch 700, Loss: 1.9047530269622803\n",
      "Epoch 1, Batch 800, Loss: 1.9031591832637786\n",
      "Epoch 1, Batch 900, Loss: 1.9042959022521972\n",
      "Epoch 2, Batch 100, Loss: 1.9065428566932678\n",
      "Epoch 2, Batch 200, Loss: 1.901347235441208\n",
      "Epoch 2, Batch 300, Loss: 1.904097000360489\n",
      "Epoch 2, Batch 400, Loss: 1.901840147972107\n",
      "Epoch 2, Batch 500, Loss: 1.9007216835021972\n",
      "Epoch 2, Batch 600, Loss: 1.9061941003799439\n",
      "Epoch 2, Batch 700, Loss: 1.907561273574829\n",
      "Epoch 2, Batch 800, Loss: 1.8980627632141114\n",
      "Epoch 2, Batch 900, Loss: 1.8991727709770203\n",
      "Epoch 3, Batch 100, Loss: 1.9027281177043915\n",
      "Epoch 3, Batch 200, Loss: 1.8980189883708953\n",
      "Epoch 3, Batch 300, Loss: 1.9051739704608917\n",
      "Epoch 3, Batch 400, Loss: 1.9011406111717224\n",
      "Epoch 3, Batch 500, Loss: 1.9000012731552125\n",
      "Epoch 3, Batch 600, Loss: 1.9044655740261078\n",
      "Epoch 3, Batch 700, Loss: 1.9015088987350464\n",
      "Epoch 3, Batch 800, Loss: 1.9015607857704162\n",
      "Epoch 3, Batch 900, Loss: 1.9072220313549042\n",
      "Epoch 4, Batch 100, Loss: 1.90324392080307\n",
      "Epoch 4, Batch 200, Loss: 1.8949862277507783\n",
      "Epoch 4, Batch 300, Loss: 1.9068622517585754\n",
      "Epoch 4, Batch 400, Loss: 1.9061742842197418\n",
      "Epoch 4, Batch 500, Loss: 1.8997743368148803\n",
      "Epoch 4, Batch 600, Loss: 1.8994435667991638\n",
      "Epoch 4, Batch 700, Loss: 1.899234391450882\n",
      "Epoch 4, Batch 800, Loss: 1.9039259052276611\n",
      "Epoch 4, Batch 900, Loss: 1.9093135964870454\n",
      "Epoch 5, Batch 100, Loss: 1.9029870426654816\n",
      "Epoch 5, Batch 200, Loss: 1.9009020328521729\n",
      "Epoch 5, Batch 300, Loss: 1.9029212856292725\n",
      "Epoch 5, Batch 400, Loss: 1.9049728262424468\n",
      "Epoch 5, Batch 500, Loss: 1.900559638738632\n",
      "Epoch 5, Batch 600, Loss: 1.9032802283763885\n",
      "Epoch 5, Batch 700, Loss: 1.9026221895217896\n",
      "Epoch 5, Batch 800, Loss: 1.9059529852867128\n",
      "Epoch 5, Batch 900, Loss: 1.9036501705646516\n",
      "Epoch 6, Batch 100, Loss: 1.901365578174591\n",
      "Epoch 6, Batch 200, Loss: 1.9035210382938386\n",
      "Epoch 6, Batch 300, Loss: 1.8999695539474488\n",
      "Epoch 6, Batch 400, Loss: 1.9054159605503083\n",
      "Epoch 6, Batch 500, Loss: 1.9033083820343017\n",
      "Epoch 6, Batch 600, Loss: 1.8930107426643372\n",
      "Epoch 6, Batch 700, Loss: 1.9065127849578858\n",
      "Epoch 6, Batch 800, Loss: 1.9034988188743591\n",
      "Epoch 6, Batch 900, Loss: 1.907734795808792\n",
      "Epoch 7, Batch 100, Loss: 1.9054552495479584\n",
      "Epoch 7, Batch 200, Loss: 1.9050291931629182\n",
      "Epoch 7, Batch 300, Loss: 1.9020373558998107\n",
      "Epoch 7, Batch 400, Loss: 1.9052554643154145\n",
      "Epoch 7, Batch 500, Loss: 1.9042943632602691\n",
      "Epoch 7, Batch 600, Loss: 1.9050509643554687\n",
      "Epoch 7, Batch 700, Loss: 1.9004942238330842\n",
      "Epoch 7, Batch 800, Loss: 1.898434089422226\n",
      "Epoch 7, Batch 900, Loss: 1.9001446056365967\n",
      "Epoch 8, Batch 100, Loss: 1.897524926662445\n",
      "Epoch 8, Batch 200, Loss: 1.9046633541584015\n",
      "Epoch 8, Batch 300, Loss: 1.899836198091507\n",
      "Epoch 8, Batch 400, Loss: 1.8994006145000457\n",
      "Epoch 8, Batch 500, Loss: 1.9045182180404663\n",
      "Epoch 8, Batch 600, Loss: 1.9096305418014525\n",
      "Epoch 8, Batch 700, Loss: 1.9066262698173524\n",
      "Epoch 8, Batch 800, Loss: 1.903586050271988\n",
      "Epoch 8, Batch 900, Loss: 1.8985029304027556\n",
      "Epoch 9, Batch 100, Loss: 1.9033707797527313\n",
      "Epoch 9, Batch 200, Loss: 1.9067252027988433\n",
      "Epoch 9, Batch 300, Loss: 1.9013858222961426\n",
      "Epoch 9, Batch 400, Loss: 1.9000292277336122\n",
      "Epoch 9, Batch 500, Loss: 1.902536189556122\n",
      "Epoch 9, Batch 600, Loss: 1.900523579120636\n",
      "Epoch 9, Batch 700, Loss: 1.9024677562713623\n",
      "Epoch 9, Batch 800, Loss: 1.9030290246009827\n",
      "Epoch 9, Batch 900, Loss: 1.9060744035243988\n",
      "Epoch 10, Batch 100, Loss: 1.9007897806167602\n",
      "Epoch 10, Batch 200, Loss: 1.9043880677223206\n",
      "Epoch 10, Batch 300, Loss: 1.9050841784477235\n",
      "Epoch 10, Batch 400, Loss: 1.9052223289012908\n",
      "Epoch 10, Batch 500, Loss: 1.9058389055728913\n",
      "Epoch 10, Batch 600, Loss: 1.901023087501526\n",
      "Epoch 10, Batch 700, Loss: 1.9055135726928711\n",
      "Epoch 10, Batch 800, Loss: 1.8990261483192443\n",
      "Epoch 10, Batch 900, Loss: 1.9032360196113587\n",
      "Epoch 11, Batch 100, Loss: 1.9075054013729096\n",
      "Epoch 11, Batch 200, Loss: 1.9046657943725587\n",
      "Epoch 11, Batch 300, Loss: 1.9033402502536774\n",
      "Epoch 11, Batch 400, Loss: 1.9016776871681214\n",
      "Epoch 11, Batch 500, Loss: 1.9014933228492736\n",
      "Epoch 11, Batch 600, Loss: 1.902327744960785\n",
      "Epoch 11, Batch 700, Loss: 1.9034587681293487\n",
      "Epoch 11, Batch 800, Loss: 1.9037565398216247\n",
      "Epoch 11, Batch 900, Loss: 1.9009161150455476\n",
      "Epoch 12, Batch 100, Loss: 1.9059332966804505\n",
      "Epoch 12, Batch 200, Loss: 1.903381165266037\n",
      "Epoch 12, Batch 300, Loss: 1.9020776546001434\n",
      "Epoch 12, Batch 400, Loss: 1.9003019630908966\n",
      "Epoch 12, Batch 500, Loss: 1.8998645699024201\n",
      "Epoch 12, Batch 600, Loss: 1.9039920914173125\n",
      "Epoch 12, Batch 700, Loss: 1.9039481806755065\n",
      "Epoch 12, Batch 800, Loss: 1.9050184977054596\n",
      "Epoch 12, Batch 900, Loss: 1.903980654478073\n",
      "Epoch 13, Batch 100, Loss: 1.8996325159072875\n",
      "Epoch 13, Batch 200, Loss: 1.9005702865123748\n",
      "Epoch 13, Batch 300, Loss: 1.904639377593994\n",
      "Epoch 13, Batch 400, Loss: 1.9041058540344238\n",
      "Epoch 13, Batch 500, Loss: 1.9082728481292726\n",
      "Epoch 13, Batch 600, Loss: 1.9049681067466735\n",
      "Epoch 13, Batch 700, Loss: 1.9025430798530578\n",
      "Epoch 13, Batch 800, Loss: 1.8940491020679473\n",
      "Epoch 13, Batch 900, Loss: 1.9028161180019378\n",
      "Epoch 14, Batch 100, Loss: 1.8989137184619904\n",
      "Epoch 14, Batch 200, Loss: 1.9044850063323975\n",
      "Epoch 14, Batch 300, Loss: 1.9051611161231994\n",
      "Epoch 14, Batch 400, Loss: 1.90055686712265\n",
      "Epoch 14, Batch 500, Loss: 1.906354706287384\n",
      "Epoch 14, Batch 600, Loss: 1.9059271109104157\n",
      "Epoch 14, Batch 700, Loss: 1.89961310505867\n",
      "Epoch 14, Batch 800, Loss: 1.9048647284507751\n",
      "Epoch 14, Batch 900, Loss: 1.901801964044571\n",
      "Epoch 15, Batch 100, Loss: 1.8995237958431244\n",
      "Epoch 15, Batch 200, Loss: 1.9052846622467041\n",
      "Epoch 15, Batch 300, Loss: 1.899888459444046\n",
      "Epoch 15, Batch 400, Loss: 1.9003355181217194\n",
      "Epoch 15, Batch 500, Loss: 1.9014979541301726\n",
      "Epoch 15, Batch 600, Loss: 1.904361070394516\n",
      "Epoch 15, Batch 700, Loss: 1.9037468111515046\n",
      "Epoch 15, Batch 800, Loss: 1.9057677507400512\n",
      "Epoch 15, Batch 900, Loss: 1.904332356452942\n",
      "Epoch 16, Batch 100, Loss: 1.9018967819213868\n",
      "Epoch 16, Batch 200, Loss: 1.9040286493301393\n",
      "Epoch 16, Batch 300, Loss: 1.9033063340187073\n",
      "Epoch 16, Batch 400, Loss: 1.9032564747333527\n",
      "Epoch 16, Batch 500, Loss: 1.9023169422149657\n",
      "Epoch 16, Batch 600, Loss: 1.8964097309112549\n",
      "Epoch 16, Batch 700, Loss: 1.907367787361145\n",
      "Epoch 16, Batch 800, Loss: 1.903265643119812\n",
      "Epoch 16, Batch 900, Loss: 1.906114456653595\n",
      "Epoch 17, Batch 100, Loss: 1.8973247444629668\n",
      "Epoch 17, Batch 200, Loss: 1.901148452758789\n",
      "Epoch 17, Batch 300, Loss: 1.908997530937195\n",
      "Epoch 17, Batch 400, Loss: 1.9017628574371337\n",
      "Epoch 17, Batch 500, Loss: 1.9015857660770417\n",
      "Epoch 17, Batch 600, Loss: 1.903308013677597\n",
      "Epoch 17, Batch 700, Loss: 1.9078537261486053\n",
      "Epoch 17, Batch 800, Loss: 1.9017769587039948\n",
      "Epoch 17, Batch 900, Loss: 1.9027192449569703\n",
      "Epoch 18, Batch 100, Loss: 1.9018786561489105\n",
      "Epoch 18, Batch 200, Loss: 1.9047820103168487\n",
      "Epoch 18, Batch 300, Loss: 1.9013896012306213\n",
      "Epoch 18, Batch 400, Loss: 1.9039545905590058\n",
      "Epoch 18, Batch 500, Loss: 1.8948492074012757\n",
      "Epoch 18, Batch 600, Loss: 1.9059000945091247\n",
      "Epoch 18, Batch 700, Loss: 1.9048325896263123\n",
      "Epoch 18, Batch 800, Loss: 1.9057986080646514\n",
      "Epoch 18, Batch 900, Loss: 1.9012132859230042\n",
      "Epoch 19, Batch 100, Loss: 1.9068902742862701\n",
      "Epoch 19, Batch 200, Loss: 1.902608368396759\n",
      "Epoch 19, Batch 300, Loss: 1.9048199927806855\n",
      "Epoch 19, Batch 400, Loss: 1.9061105287075042\n",
      "Epoch 19, Batch 500, Loss: 1.899559805393219\n",
      "Epoch 19, Batch 600, Loss: 1.9011067962646484\n",
      "Epoch 19, Batch 700, Loss: 1.9033645331859588\n",
      "Epoch 19, Batch 800, Loss: 1.901245700120926\n",
      "Epoch 19, Batch 900, Loss: 1.90124254822731\n",
      "Epoch 20, Batch 100, Loss: 1.9007577538490295\n",
      "Epoch 20, Batch 200, Loss: 1.9004378914833069\n",
      "Epoch 20, Batch 300, Loss: 1.9034139728546142\n",
      "Epoch 20, Batch 400, Loss: 1.9043771183490754\n",
      "Epoch 20, Batch 500, Loss: 1.898379555940628\n",
      "Epoch 20, Batch 600, Loss: 1.9043822407722473\n",
      "Epoch 20, Batch 700, Loss: 1.9051671183109284\n",
      "Epoch 20, Batch 800, Loss: 1.9070425152778625\n",
      "Epoch 20, Batch 900, Loss: 1.9044321846961976\n",
      "Epoch 21, Batch 100, Loss: 1.9012822020053863\n",
      "Epoch 21, Batch 200, Loss: 1.90836270570755\n",
      "Epoch 21, Batch 300, Loss: 1.8976713407039643\n",
      "Epoch 21, Batch 400, Loss: 1.900185890197754\n",
      "Epoch 21, Batch 500, Loss: 1.9005077874660492\n",
      "Epoch 21, Batch 600, Loss: 1.903881115913391\n",
      "Epoch 21, Batch 700, Loss: 1.9038188767433166\n",
      "Epoch 21, Batch 800, Loss: 1.904791942834854\n",
      "Epoch 21, Batch 900, Loss: 1.9060806560516357\n",
      "Epoch 22, Batch 100, Loss: 1.904466758966446\n",
      "Epoch 22, Batch 200, Loss: 1.9065622341632844\n",
      "Epoch 22, Batch 300, Loss: 1.9065762209892272\n",
      "Epoch 22, Batch 400, Loss: 1.9018800926208497\n",
      "Epoch 22, Batch 500, Loss: 1.8987233281135558\n",
      "Epoch 22, Batch 600, Loss: 1.8981129622459412\n",
      "Epoch 22, Batch 700, Loss: 1.9076733922958373\n",
      "Epoch 22, Batch 800, Loss: 1.9002046847343446\n",
      "Epoch 22, Batch 900, Loss: 1.9027671790122986\n",
      "Epoch 23, Batch 100, Loss: 1.9075465095043183\n",
      "Epoch 23, Batch 200, Loss: 1.9022969841957091\n",
      "Epoch 23, Batch 300, Loss: 1.9043894445896148\n",
      "Epoch 23, Batch 400, Loss: 1.9004858565330505\n",
      "Epoch 23, Batch 500, Loss: 1.9023030424118041\n",
      "Epoch 23, Batch 600, Loss: 1.898817969560623\n",
      "Epoch 23, Batch 700, Loss: 1.9017681348323823\n",
      "Epoch 23, Batch 800, Loss: 1.9050005745887757\n",
      "Epoch 23, Batch 900, Loss: 1.9002492153644561\n",
      "Epoch 24, Batch 100, Loss: 1.8978075063228608\n",
      "Epoch 24, Batch 200, Loss: 1.9010504543781281\n",
      "Epoch 24, Batch 300, Loss: 1.9016856908798219\n",
      "Epoch 24, Batch 400, Loss: 1.903023397922516\n",
      "Epoch 24, Batch 500, Loss: 1.9050915098190309\n",
      "Epoch 24, Batch 600, Loss: 1.9102885520458222\n",
      "Epoch 24, Batch 700, Loss: 1.900579687356949\n",
      "Epoch 24, Batch 800, Loss: 1.8994635343551636\n",
      "Epoch 24, Batch 900, Loss: 1.906395924091339\n",
      "Epoch 25, Batch 100, Loss: 1.9030692791938781\n",
      "Epoch 25, Batch 200, Loss: 1.90855917096138\n",
      "Epoch 25, Batch 300, Loss: 1.9044573283195496\n",
      "Epoch 25, Batch 400, Loss: 1.9009796917438506\n",
      "Epoch 25, Batch 500, Loss: 1.9028316748142242\n",
      "Epoch 25, Batch 600, Loss: 1.901197180747986\n",
      "Epoch 25, Batch 700, Loss: 1.9042335784435271\n",
      "Epoch 25, Batch 800, Loss: 1.9014614474773408\n",
      "Epoch 25, Batch 900, Loss: 1.9032713437080384\n",
      "Accuracy on test set: 0.2399%\n",
      "Fitting for combination 22\n",
      "784\n",
      "2\n",
      "10\n",
      "[40, 10, 10]\n",
      "False\n",
      "['relu', 'relu']\n",
      "Adam\n",
      "0.3\n",
      "0.1\n",
      "CrossEntropyLoss\n",
      "200\n",
      "25\n",
      "0\n",
      "Epoch 1, Batch 200, Loss: 5.899835393428803\n",
      "Epoch 1, Batch 400, Loss: 5.912384445667267\n",
      "Epoch 1, Batch 600, Loss: 7.0766316866874694\n",
      "Epoch 1, Batch 800, Loss: 4.994437239170074\n",
      "Epoch 2, Batch 200, Loss: 5.803936913013458\n",
      "Epoch 2, Batch 400, Loss: 5.168407083749771\n",
      "Epoch 2, Batch 600, Loss: 4.695246114730835\n",
      "Epoch 2, Batch 800, Loss: 4.6759293591976165\n",
      "Epoch 3, Batch 200, Loss: 6.2589154624938965\n",
      "Epoch 3, Batch 400, Loss: 5.653311588764191\n",
      "Epoch 3, Batch 600, Loss: 6.696792459487915\n",
      "Epoch 3, Batch 800, Loss: 8.851857190132142\n",
      "Epoch 4, Batch 200, Loss: 4.7890375399589535\n",
      "Epoch 4, Batch 400, Loss: 4.962646563053131\n",
      "Epoch 4, Batch 600, Loss: 5.861913006305695\n",
      "Epoch 4, Batch 800, Loss: 5.115968403816223\n",
      "Epoch 5, Batch 200, Loss: 12.162015503644943\n",
      "Epoch 5, Batch 400, Loss: 11.142400689125061\n",
      "Epoch 5, Batch 600, Loss: 4.824135829210281\n",
      "Epoch 5, Batch 800, Loss: 4.606621906757355\n",
      "Epoch 6, Batch 200, Loss: 4.679386322498321\n",
      "Epoch 6, Batch 400, Loss: 5.669914209842682\n",
      "Epoch 6, Batch 600, Loss: 8.442521320581436\n",
      "Epoch 6, Batch 800, Loss: 4.572794888019562\n",
      "Epoch 7, Batch 200, Loss: 4.9862931537628175\n",
      "Epoch 7, Batch 400, Loss: 8.186785190105438\n",
      "Epoch 7, Batch 600, Loss: 12.467244696617126\n",
      "Epoch 7, Batch 800, Loss: 4.910810894966126\n",
      "Epoch 8, Batch 200, Loss: 5.75423166513443\n",
      "Epoch 8, Batch 400, Loss: 4.571551034450531\n",
      "Epoch 8, Batch 600, Loss: 4.772099540233612\n",
      "Epoch 8, Batch 800, Loss: 4.732187757492065\n",
      "Epoch 9, Batch 200, Loss: 5.593001067638397\n",
      "Epoch 9, Batch 400, Loss: 5.459607828855514\n",
      "Epoch 9, Batch 600, Loss: 5.601724951267243\n",
      "Epoch 9, Batch 800, Loss: 4.783962252140046\n",
      "Epoch 10, Batch 200, Loss: 4.919164441823959\n",
      "Epoch 10, Batch 400, Loss: 7.788752951622009\n",
      "Epoch 10, Batch 600, Loss: 4.902761545181274\n",
      "Epoch 10, Batch 800, Loss: 5.093350605964661\n",
      "Epoch 11, Batch 200, Loss: 5.384008791446686\n",
      "Epoch 11, Batch 400, Loss: 5.0742747330665585\n",
      "Epoch 11, Batch 600, Loss: 15.051746006011962\n",
      "Epoch 11, Batch 800, Loss: 4.749632031917572\n",
      "Epoch 12, Batch 200, Loss: 5.909356136322021\n",
      "Epoch 12, Batch 400, Loss: 5.292742764949798\n",
      "Epoch 12, Batch 600, Loss: 5.400905241966248\n",
      "Epoch 12, Batch 800, Loss: 4.816225808858872\n",
      "Epoch 13, Batch 200, Loss: 6.375318953990936\n",
      "Epoch 13, Batch 400, Loss: 4.945827114582062\n",
      "Epoch 13, Batch 600, Loss: 4.937815903425217\n",
      "Epoch 13, Batch 800, Loss: 10.660138869285584\n",
      "Epoch 14, Batch 200, Loss: 6.192656235694885\n",
      "Epoch 14, Batch 400, Loss: 4.8999205136299135\n",
      "Epoch 14, Batch 600, Loss: 4.747896558046341\n",
      "Epoch 14, Batch 800, Loss: 4.556579592227936\n",
      "Epoch 15, Batch 200, Loss: 5.749166098833084\n",
      "Epoch 15, Batch 400, Loss: 4.722863883972168\n",
      "Epoch 15, Batch 600, Loss: 5.88319526553154\n",
      "Epoch 15, Batch 800, Loss: 4.7374862408638\n",
      "Epoch 16, Batch 200, Loss: 9.114468641281128\n",
      "Epoch 16, Batch 400, Loss: 4.789895374774932\n",
      "Epoch 16, Batch 600, Loss: 5.167044203281403\n",
      "Epoch 16, Batch 800, Loss: 4.985780733823776\n",
      "Epoch 17, Batch 200, Loss: 5.907469863891602\n",
      "Epoch 17, Batch 400, Loss: 6.4383012127876285\n",
      "Epoch 17, Batch 600, Loss: 4.93629164814949\n",
      "Epoch 17, Batch 800, Loss: 8.695133335590363\n",
      "Epoch 18, Batch 200, Loss: 4.659946502447128\n",
      "Epoch 18, Batch 400, Loss: 4.698805009126663\n",
      "Epoch 18, Batch 600, Loss: 5.505682023763657\n",
      "Epoch 18, Batch 800, Loss: 5.037758767604828\n",
      "Epoch 19, Batch 200, Loss: 6.4770423650741575\n",
      "Epoch 19, Batch 400, Loss: 5.756034426689148\n",
      "Epoch 19, Batch 600, Loss: 4.804524385929108\n",
      "Epoch 19, Batch 800, Loss: 16.144567713737487\n",
      "Epoch 20, Batch 200, Loss: 4.778305509090424\n",
      "Epoch 20, Batch 400, Loss: 8.418230609893799\n",
      "Epoch 20, Batch 600, Loss: 4.769619010686874\n",
      "Epoch 20, Batch 800, Loss: 4.650853145122528\n",
      "Epoch 21, Batch 200, Loss: 4.638268053531647\n",
      "Epoch 21, Batch 400, Loss: 5.513003942966461\n",
      "Epoch 21, Batch 600, Loss: 4.648168077468872\n",
      "Epoch 21, Batch 800, Loss: 5.550809121131897\n",
      "Epoch 22, Batch 200, Loss: 6.75036369562149\n",
      "Epoch 22, Batch 400, Loss: 6.79044868350029\n",
      "Epoch 22, Batch 600, Loss: 5.2079897665977475\n",
      "Epoch 22, Batch 800, Loss: 4.661090242862701\n",
      "Epoch 23, Batch 200, Loss: 5.194510872364044\n",
      "Epoch 23, Batch 400, Loss: 10.128563541173936\n",
      "Epoch 23, Batch 600, Loss: 4.87542448759079\n",
      "Epoch 23, Batch 800, Loss: 7.108806705474853\n",
      "Epoch 24, Batch 200, Loss: 5.399642565250397\n",
      "Epoch 24, Batch 400, Loss: 4.671732405424118\n",
      "Epoch 24, Batch 600, Loss: 8.716256380081177\n",
      "Epoch 24, Batch 800, Loss: 17.69300843477249\n",
      "Epoch 25, Batch 200, Loss: 7.3585113537311555\n",
      "Epoch 25, Batch 400, Loss: 4.499175554513931\n",
      "Epoch 25, Batch 600, Loss: 6.732534832954407\n",
      "Epoch 25, Batch 800, Loss: 4.610196429491043\n",
      "Accuracy on test set: 0.0992%\n",
      "Fitting for combination 23\n",
      "784\n",
      "2\n",
      "10\n",
      "[40, 10, 10]\n",
      "True\n",
      "['relu', 'sigmoid']\n",
      "SGD\n",
      "0.01\n",
      "0.03\n",
      "CrossEntropyLoss\n",
      "50\n",
      "25\n",
      "0\n",
      "Epoch 1, Batch 50, Loss: 1.1599955868721008\n",
      "Epoch 1, Batch 100, Loss: 1.1535516738891602\n",
      "Epoch 1, Batch 150, Loss: 1.1475989198684693\n",
      "Epoch 1, Batch 200, Loss: 1.1381655311584473\n",
      "Epoch 1, Batch 250, Loss: 1.130938618183136\n",
      "Epoch 1, Batch 300, Loss: 1.1258139157295226\n",
      "Epoch 1, Batch 350, Loss: 1.1184313368797303\n",
      "Epoch 1, Batch 400, Loss: 1.1112795329093934\n",
      "Epoch 1, Batch 450, Loss: 1.1033404517173766\n",
      "Epoch 1, Batch 500, Loss: 1.094435875415802\n",
      "Epoch 1, Batch 550, Loss: 1.0834784483909607\n",
      "Epoch 1, Batch 600, Loss: 1.074262146949768\n",
      "Epoch 1, Batch 650, Loss: 1.0641270399093627\n",
      "Epoch 1, Batch 700, Loss: 1.057432324886322\n",
      "Epoch 1, Batch 750, Loss: 1.0408203983306885\n",
      "Epoch 1, Batch 800, Loss: 1.029533576965332\n",
      "Epoch 1, Batch 850, Loss: 1.020377584695816\n",
      "Epoch 1, Batch 900, Loss: 1.011023803949356\n",
      "Epoch 2, Batch 50, Loss: 0.9932373082637787\n",
      "Epoch 2, Batch 100, Loss: 0.9782723414897919\n",
      "Epoch 2, Batch 150, Loss: 0.9705581772327423\n",
      "Epoch 2, Batch 200, Loss: 0.9597531914710998\n",
      "Epoch 2, Batch 250, Loss: 0.9464406061172486\n",
      "Epoch 2, Batch 300, Loss: 0.9396387529373169\n",
      "Epoch 2, Batch 350, Loss: 0.9314531421661377\n",
      "Epoch 2, Batch 400, Loss: 0.9269090402126312\n",
      "Epoch 2, Batch 450, Loss: 0.909154360294342\n",
      "Epoch 2, Batch 500, Loss: 0.9071246349811554\n",
      "Epoch 2, Batch 550, Loss: 0.898535224199295\n",
      "Epoch 2, Batch 600, Loss: 0.8946513032913208\n",
      "Epoch 2, Batch 650, Loss: 0.8868437826633453\n",
      "Epoch 2, Batch 700, Loss: 0.8766164267063141\n",
      "Epoch 2, Batch 750, Loss: 0.8671896743774414\n",
      "Epoch 2, Batch 800, Loss: 0.8648611080646514\n",
      "Epoch 2, Batch 850, Loss: 0.856571592092514\n",
      "Epoch 2, Batch 900, Loss: 0.8529367542266846\n",
      "Epoch 3, Batch 50, Loss: 0.8462009596824646\n",
      "Epoch 3, Batch 100, Loss: 0.8356382536888123\n",
      "Epoch 3, Batch 150, Loss: 0.8341373491287232\n",
      "Epoch 3, Batch 200, Loss: 0.8272491538524628\n",
      "Epoch 3, Batch 250, Loss: 0.825891273021698\n",
      "Epoch 3, Batch 300, Loss: 0.8228507399559021\n",
      "Epoch 3, Batch 350, Loss: 0.8127910685539246\n",
      "Epoch 3, Batch 400, Loss: 0.8087632071971893\n",
      "Epoch 3, Batch 450, Loss: 0.8092755961418152\n",
      "Epoch 3, Batch 500, Loss: 0.8012663996219636\n",
      "Epoch 3, Batch 550, Loss: 0.8036419129371644\n",
      "Epoch 3, Batch 600, Loss: 0.7981843745708466\n",
      "Epoch 3, Batch 650, Loss: 0.7963854253292084\n",
      "Epoch 3, Batch 700, Loss: 0.788139408826828\n",
      "Epoch 3, Batch 750, Loss: 0.7856277060508728\n",
      "Epoch 3, Batch 800, Loss: 0.7823746192455292\n",
      "Epoch 3, Batch 850, Loss: 0.785432425737381\n",
      "Epoch 3, Batch 900, Loss: 0.777113893032074\n",
      "Epoch 4, Batch 50, Loss: 0.7752516567707062\n",
      "Epoch 4, Batch 100, Loss: 0.7657641077041626\n",
      "Epoch 4, Batch 150, Loss: 0.766828396320343\n",
      "Epoch 4, Batch 200, Loss: 0.7620678901672363\n",
      "Epoch 4, Batch 250, Loss: 0.7666546773910522\n",
      "Epoch 4, Batch 300, Loss: 0.7588587594032288\n",
      "Epoch 4, Batch 350, Loss: 0.7571246325969696\n",
      "Epoch 4, Batch 400, Loss: 0.7588751316070557\n",
      "Epoch 4, Batch 450, Loss: 0.7507786333560944\n",
      "Epoch 4, Batch 500, Loss: 0.7531104362010956\n",
      "Epoch 4, Batch 550, Loss: 0.7494107973575592\n",
      "Epoch 4, Batch 600, Loss: 0.7517724668979645\n",
      "Epoch 4, Batch 650, Loss: 0.7411795091629029\n",
      "Epoch 4, Batch 700, Loss: 0.7440976464748382\n",
      "Epoch 4, Batch 750, Loss: 0.7453562998771668\n",
      "Epoch 4, Batch 800, Loss: 0.7443101704120636\n",
      "Epoch 4, Batch 850, Loss: 0.7404353499412537\n",
      "Epoch 4, Batch 900, Loss: 0.7332886350154877\n",
      "Epoch 5, Batch 50, Loss: 0.7347517013549805\n",
      "Epoch 5, Batch 100, Loss: 0.7377916049957275\n",
      "Epoch 5, Batch 150, Loss: 0.7268392026424408\n",
      "Epoch 5, Batch 200, Loss: 0.7244392693042755\n",
      "Epoch 5, Batch 250, Loss: 0.7238355600833892\n",
      "Epoch 5, Batch 300, Loss: 0.7281607592105865\n",
      "Epoch 5, Batch 350, Loss: 0.7259485876560211\n",
      "Epoch 5, Batch 400, Loss: 0.7285074245929718\n",
      "Epoch 5, Batch 450, Loss: 0.7264382553100586\n",
      "Epoch 5, Batch 500, Loss: 0.7207582461833953\n",
      "Epoch 5, Batch 550, Loss: 0.7191910398006439\n",
      "Epoch 5, Batch 600, Loss: 0.7192435657978058\n",
      "Epoch 5, Batch 650, Loss: 0.7120322096347809\n",
      "Epoch 5, Batch 700, Loss: 0.7180385100841522\n",
      "Epoch 5, Batch 750, Loss: 0.715758467912674\n",
      "Epoch 5, Batch 800, Loss: 0.7168402981758117\n",
      "Epoch 5, Batch 850, Loss: 0.7080610609054565\n",
      "Epoch 5, Batch 900, Loss: 0.7062377393245697\n",
      "Epoch 6, Batch 50, Loss: 0.7130751657485962\n",
      "Epoch 6, Batch 100, Loss: 0.7056175470352173\n",
      "Epoch 6, Batch 150, Loss: 0.7065695703029633\n",
      "Epoch 6, Batch 200, Loss: 0.7045878398418427\n",
      "Epoch 6, Batch 250, Loss: 0.7049179875850677\n",
      "Epoch 6, Batch 300, Loss: 0.7025375509262085\n",
      "Epoch 6, Batch 350, Loss: 0.7040010225772858\n",
      "Epoch 6, Batch 400, Loss: 0.7025294661521911\n",
      "Epoch 6, Batch 450, Loss: 0.696009396314621\n",
      "Epoch 6, Batch 500, Loss: 0.7052479219436646\n",
      "Epoch 6, Batch 550, Loss: 0.6988201797008514\n",
      "Epoch 6, Batch 600, Loss: 0.6987589645385742\n",
      "Epoch 6, Batch 650, Loss: 0.6980909574031829\n",
      "Epoch 6, Batch 700, Loss: 0.6976759517192841\n",
      "Epoch 6, Batch 750, Loss: 0.6989930951595307\n",
      "Epoch 6, Batch 800, Loss: 0.6940091335773468\n",
      "Epoch 6, Batch 850, Loss: 0.6964737153053284\n",
      "Epoch 6, Batch 900, Loss: 0.6934079778194427\n",
      "Epoch 7, Batch 50, Loss: 0.6873625314235687\n",
      "Epoch 7, Batch 100, Loss: 0.6901909554004669\n",
      "Epoch 7, Batch 150, Loss: 0.695685064792633\n",
      "Epoch 7, Batch 200, Loss: 0.6910648632049561\n",
      "Epoch 7, Batch 250, Loss: 0.6953734076023101\n",
      "Epoch 7, Batch 300, Loss: 0.6933671355247497\n",
      "Epoch 7, Batch 350, Loss: 0.6859241211414338\n",
      "Epoch 7, Batch 400, Loss: 0.6842192268371582\n",
      "Epoch 7, Batch 450, Loss: 0.6816252589225769\n",
      "Epoch 7, Batch 500, Loss: 0.6832748425006866\n",
      "Epoch 7, Batch 550, Loss: 0.6871674466133117\n",
      "Epoch 7, Batch 600, Loss: 0.6835885608196258\n",
      "Epoch 7, Batch 650, Loss: 0.6843212747573852\n",
      "Epoch 7, Batch 700, Loss: 0.6844759452342987\n",
      "Epoch 7, Batch 750, Loss: 0.683335930109024\n",
      "Epoch 7, Batch 800, Loss: 0.6887942850589752\n",
      "Epoch 7, Batch 850, Loss: 0.6837564861774444\n",
      "Epoch 7, Batch 900, Loss: 0.68004678606987\n",
      "Epoch 8, Batch 50, Loss: 0.684917287826538\n",
      "Epoch 8, Batch 100, Loss: 0.678264957666397\n",
      "Epoch 8, Batch 150, Loss: 0.6768129861354828\n",
      "Epoch 8, Batch 200, Loss: 0.6790713346004487\n",
      "Epoch 8, Batch 250, Loss: 0.6789261436462403\n",
      "Epoch 8, Batch 300, Loss: 0.6744476664066315\n",
      "Epoch 8, Batch 350, Loss: 0.6731954765319824\n",
      "Epoch 8, Batch 400, Loss: 0.6764530992507934\n",
      "Epoch 8, Batch 450, Loss: 0.6762741124629974\n",
      "Epoch 8, Batch 500, Loss: 0.6744285023212433\n",
      "Epoch 8, Batch 550, Loss: 0.6717599546909332\n",
      "Epoch 8, Batch 600, Loss: 0.6748868143558502\n",
      "Epoch 8, Batch 650, Loss: 0.6747958779335022\n",
      "Epoch 8, Batch 700, Loss: 0.6753058624267578\n",
      "Epoch 8, Batch 750, Loss: 0.6723540663719177\n",
      "Epoch 8, Batch 800, Loss: 0.6680039751529694\n",
      "Epoch 8, Batch 850, Loss: 0.6803670060634613\n",
      "Epoch 8, Batch 900, Loss: 0.6764779567718506\n",
      "Epoch 9, Batch 50, Loss: 0.6690184640884399\n",
      "Epoch 9, Batch 100, Loss: 0.6723074066638947\n",
      "Epoch 9, Batch 150, Loss: 0.668304899930954\n",
      "Epoch 9, Batch 200, Loss: 0.6680855298042297\n",
      "Epoch 9, Batch 250, Loss: 0.6667509186267853\n",
      "Epoch 9, Batch 300, Loss: 0.6730837786197662\n",
      "Epoch 9, Batch 350, Loss: 0.6659407150745392\n",
      "Epoch 9, Batch 400, Loss: 0.6633993947505951\n",
      "Epoch 9, Batch 450, Loss: 0.66565105676651\n",
      "Epoch 9, Batch 500, Loss: 0.6725777447223663\n",
      "Epoch 9, Batch 550, Loss: 0.6679660975933075\n",
      "Epoch 9, Batch 600, Loss: 0.6692726063728333\n",
      "Epoch 9, Batch 650, Loss: 0.6648509621620178\n",
      "Epoch 9, Batch 700, Loss: 0.6676874780654907\n",
      "Epoch 9, Batch 750, Loss: 0.6660424113273621\n",
      "Epoch 9, Batch 800, Loss: 0.6649125754833222\n",
      "Epoch 9, Batch 850, Loss: 0.6672674822807312\n",
      "Epoch 9, Batch 900, Loss: 0.6686375606060028\n",
      "Epoch 10, Batch 50, Loss: 0.666004694700241\n",
      "Epoch 10, Batch 100, Loss: 0.6640922069549561\n",
      "Epoch 10, Batch 150, Loss: 0.6604698693752289\n",
      "Epoch 10, Batch 200, Loss: 0.6650793600082398\n",
      "Epoch 10, Batch 250, Loss: 0.6656788837909698\n",
      "Epoch 10, Batch 300, Loss: 0.6599235653877258\n",
      "Epoch 10, Batch 350, Loss: 0.6676543319225311\n",
      "Epoch 10, Batch 400, Loss: 0.6646840345859527\n",
      "Epoch 10, Batch 450, Loss: 0.6609194874763489\n",
      "Epoch 10, Batch 500, Loss: 0.6627307176589966\n",
      "Epoch 10, Batch 550, Loss: 0.6591321349143981\n",
      "Epoch 10, Batch 600, Loss: 0.6663472557067871\n",
      "Epoch 10, Batch 650, Loss: 0.6590498757362365\n",
      "Epoch 10, Batch 700, Loss: 0.6665387392044068\n",
      "Epoch 10, Batch 750, Loss: 0.6663712513446808\n",
      "Epoch 10, Batch 800, Loss: 0.6608349466323853\n",
      "Epoch 10, Batch 850, Loss: 0.6544648861885071\n",
      "Epoch 10, Batch 900, Loss: 0.654653730392456\n",
      "Epoch 11, Batch 50, Loss: 0.6579719984531402\n",
      "Epoch 11, Batch 100, Loss: 0.6633806467056275\n",
      "Epoch 11, Batch 150, Loss: 0.6628326261043549\n",
      "Epoch 11, Batch 200, Loss: 0.6608715963363647\n",
      "Epoch 11, Batch 250, Loss: 0.6583593547344208\n",
      "Epoch 11, Batch 300, Loss: 0.6530988585948944\n",
      "Epoch 11, Batch 350, Loss: 0.657297033071518\n",
      "Epoch 11, Batch 400, Loss: 0.6576049458980561\n",
      "Epoch 11, Batch 450, Loss: 0.6577160465717315\n",
      "Epoch 11, Batch 500, Loss: 0.6555992639064789\n",
      "Epoch 11, Batch 550, Loss: 0.6557437741756439\n",
      "Epoch 11, Batch 600, Loss: 0.6575221943855286\n",
      "Epoch 11, Batch 650, Loss: 0.6571429240703582\n",
      "Epoch 11, Batch 700, Loss: 0.6589444696903228\n",
      "Epoch 11, Batch 750, Loss: 0.6520782732963561\n",
      "Epoch 11, Batch 800, Loss: 0.6564131426811218\n",
      "Epoch 11, Batch 850, Loss: 0.658914144039154\n",
      "Epoch 11, Batch 900, Loss: 0.6502545511722565\n",
      "Epoch 12, Batch 50, Loss: 0.6509062087535858\n",
      "Epoch 12, Batch 100, Loss: 0.655373637676239\n",
      "Epoch 12, Batch 150, Loss: 0.6590098905563354\n",
      "Epoch 12, Batch 200, Loss: 0.6550751066207886\n",
      "Epoch 12, Batch 250, Loss: 0.6544414234161376\n",
      "Epoch 12, Batch 300, Loss: 0.6543521952629089\n",
      "Epoch 12, Batch 350, Loss: 0.6593019080162048\n",
      "Epoch 12, Batch 400, Loss: 0.6520363569259644\n",
      "Epoch 12, Batch 450, Loss: 0.6561686360836029\n",
      "Epoch 12, Batch 500, Loss: 0.6553709185123444\n",
      "Epoch 12, Batch 550, Loss: 0.6534364426136017\n",
      "Epoch 12, Batch 600, Loss: 0.656287442445755\n",
      "Epoch 12, Batch 650, Loss: 0.6636025393009186\n",
      "Epoch 12, Batch 700, Loss: 0.6531874430179596\n",
      "Epoch 12, Batch 750, Loss: 0.6541198515892028\n",
      "Epoch 12, Batch 800, Loss: 0.6455803298950196\n",
      "Epoch 12, Batch 850, Loss: 0.6518365001678467\n",
      "Epoch 12, Batch 900, Loss: 0.6511653769016266\n",
      "Epoch 13, Batch 50, Loss: 0.6507496786117554\n",
      "Epoch 13, Batch 100, Loss: 0.6511539530754089\n",
      "Epoch 13, Batch 150, Loss: 0.6511033129692078\n",
      "Epoch 13, Batch 200, Loss: 0.6518883538246155\n",
      "Epoch 13, Batch 250, Loss: 0.6472154772281646\n",
      "Epoch 13, Batch 300, Loss: 0.6561514437198639\n",
      "Epoch 13, Batch 350, Loss: 0.6503538572788239\n",
      "Epoch 13, Batch 400, Loss: 0.6599723362922668\n",
      "Epoch 13, Batch 450, Loss: 0.6523437142372132\n",
      "Epoch 13, Batch 500, Loss: 0.6565173530578613\n",
      "Epoch 13, Batch 550, Loss: 0.653249591588974\n",
      "Epoch 13, Batch 600, Loss: 0.6488058078289032\n",
      "Epoch 13, Batch 650, Loss: 0.6488518118858337\n",
      "Epoch 13, Batch 700, Loss: 0.6548568737506867\n",
      "Epoch 13, Batch 750, Loss: 0.6507799053192138\n",
      "Epoch 13, Batch 800, Loss: 0.6506384253501892\n",
      "Epoch 13, Batch 850, Loss: 0.6504391157627105\n",
      "Epoch 13, Batch 900, Loss: 0.6508522117137909\n",
      "Epoch 14, Batch 50, Loss: 0.6538926124572754\n",
      "Epoch 14, Batch 100, Loss: 0.652108496427536\n",
      "Epoch 14, Batch 150, Loss: 0.6543583834171295\n",
      "Epoch 14, Batch 200, Loss: 0.6476067805290222\n",
      "Epoch 14, Batch 250, Loss: 0.648980211019516\n",
      "Epoch 14, Batch 300, Loss: 0.6522601926326752\n",
      "Epoch 14, Batch 350, Loss: 0.6435429203510284\n",
      "Epoch 14, Batch 400, Loss: 0.6541298735141754\n",
      "Epoch 14, Batch 450, Loss: 0.6481121718883515\n",
      "Epoch 14, Batch 500, Loss: 0.647944781780243\n",
      "Epoch 14, Batch 550, Loss: 0.6473264193534851\n",
      "Epoch 14, Batch 600, Loss: 0.6535757923126221\n",
      "Epoch 14, Batch 650, Loss: 0.6437321031093597\n",
      "Epoch 14, Batch 700, Loss: 0.6483704245090485\n",
      "Epoch 14, Batch 750, Loss: 0.6517931282520294\n",
      "Epoch 14, Batch 800, Loss: 0.6450074172019958\n",
      "Epoch 14, Batch 850, Loss: 0.6502832281589508\n",
      "Epoch 14, Batch 900, Loss: 0.6594821405410767\n",
      "Epoch 15, Batch 50, Loss: 0.6474883687496186\n",
      "Epoch 15, Batch 100, Loss: 0.6445663189888\n",
      "Epoch 15, Batch 150, Loss: 0.6538801050186157\n",
      "Epoch 15, Batch 200, Loss: 0.6437125658988953\n",
      "Epoch 15, Batch 250, Loss: 0.647800680398941\n",
      "Epoch 15, Batch 300, Loss: 0.6514293837547303\n",
      "Epoch 15, Batch 350, Loss: 0.6454680275917053\n",
      "Epoch 15, Batch 400, Loss: 0.6528130757808686\n",
      "Epoch 15, Batch 450, Loss: 0.6504747140407562\n",
      "Epoch 15, Batch 500, Loss: 0.6438730418682098\n",
      "Epoch 15, Batch 550, Loss: 0.6465811157226562\n",
      "Epoch 15, Batch 600, Loss: 0.6527413439750671\n",
      "Epoch 15, Batch 650, Loss: 0.651280437707901\n",
      "Epoch 15, Batch 700, Loss: 0.6537544476985931\n",
      "Epoch 15, Batch 750, Loss: 0.6448489379882812\n",
      "Epoch 15, Batch 800, Loss: 0.6490200066566467\n",
      "Epoch 15, Batch 850, Loss: 0.6458751702308655\n",
      "Epoch 15, Batch 900, Loss: 0.6425791895389557\n",
      "Epoch 16, Batch 50, Loss: 0.6432261598110199\n",
      "Epoch 16, Batch 100, Loss: 0.6467651033401489\n",
      "Epoch 16, Batch 150, Loss: 0.6521602141857147\n",
      "Epoch 16, Batch 200, Loss: 0.6492647516727448\n",
      "Epoch 16, Batch 250, Loss: 0.6445078015327453\n",
      "Epoch 16, Batch 300, Loss: 0.6504052901268005\n",
      "Epoch 16, Batch 350, Loss: 0.6490065944194794\n",
      "Epoch 16, Batch 400, Loss: 0.6504505443572998\n",
      "Epoch 16, Batch 450, Loss: 0.6458202385902405\n",
      "Epoch 16, Batch 500, Loss: 0.647395555973053\n",
      "Epoch 16, Batch 550, Loss: 0.6501278984546661\n",
      "Epoch 16, Batch 600, Loss: 0.6477574574947357\n",
      "Epoch 16, Batch 650, Loss: 0.6442919194698333\n",
      "Epoch 16, Batch 700, Loss: 0.6446843612194061\n",
      "Epoch 16, Batch 750, Loss: 0.6434783732891083\n",
      "Epoch 16, Batch 800, Loss: 0.6453037989139557\n",
      "Epoch 16, Batch 850, Loss: 0.648617308139801\n",
      "Epoch 16, Batch 900, Loss: 0.64574005484581\n",
      "Epoch 17, Batch 50, Loss: 0.6465504264831543\n",
      "Epoch 17, Batch 100, Loss: 0.6469596183300018\n",
      "Epoch 17, Batch 150, Loss: 0.6390796267986297\n",
      "Epoch 17, Batch 200, Loss: 0.6463435864448548\n",
      "Epoch 17, Batch 250, Loss: 0.6446155655384064\n",
      "Epoch 17, Batch 300, Loss: 0.6480010318756103\n",
      "Epoch 17, Batch 350, Loss: 0.6484363520145416\n",
      "Epoch 17, Batch 400, Loss: 0.6444999635219574\n",
      "Epoch 17, Batch 450, Loss: 0.6458881056308746\n",
      "Epoch 17, Batch 500, Loss: 0.6427967858314514\n",
      "Epoch 17, Batch 550, Loss: 0.6452069520950318\n",
      "Epoch 17, Batch 600, Loss: 0.6481858932971954\n",
      "Epoch 17, Batch 650, Loss: 0.6526846575737\n",
      "Epoch 17, Batch 700, Loss: 0.643551777601242\n",
      "Epoch 17, Batch 750, Loss: 0.642600131034851\n",
      "Epoch 17, Batch 800, Loss: 0.6515420353412629\n",
      "Epoch 17, Batch 850, Loss: 0.6411971771717071\n",
      "Epoch 17, Batch 900, Loss: 0.649958975315094\n",
      "Epoch 18, Batch 50, Loss: 0.6462592244148254\n",
      "Epoch 18, Batch 100, Loss: 0.6465671157836914\n",
      "Epoch 18, Batch 150, Loss: 0.6483089816570282\n",
      "Epoch 18, Batch 200, Loss: 0.6477271831035614\n",
      "Epoch 18, Batch 250, Loss: 0.6415999412536622\n",
      "Epoch 18, Batch 300, Loss: 0.6420745515823364\n",
      "Epoch 18, Batch 350, Loss: 0.6510359978675843\n",
      "Epoch 18, Batch 400, Loss: 0.645787183046341\n",
      "Epoch 18, Batch 450, Loss: 0.6420855832099914\n",
      "Epoch 18, Batch 500, Loss: 0.651501168012619\n",
      "Epoch 18, Batch 550, Loss: 0.6450376451015473\n",
      "Epoch 18, Batch 600, Loss: 0.6443583166599274\n",
      "Epoch 18, Batch 650, Loss: 0.6436239159107209\n",
      "Epoch 18, Batch 700, Loss: 0.6433661437034607\n",
      "Epoch 18, Batch 750, Loss: 0.6458357393741607\n",
      "Epoch 18, Batch 800, Loss: 0.643772646188736\n",
      "Epoch 18, Batch 850, Loss: 0.6439834403991699\n",
      "Epoch 18, Batch 900, Loss: 0.6417829358577728\n",
      "Epoch 19, Batch 50, Loss: 0.6445533955097198\n",
      "Epoch 19, Batch 100, Loss: 0.6433575689792633\n",
      "Epoch 19, Batch 150, Loss: 0.6431755161285401\n",
      "Epoch 19, Batch 200, Loss: 0.6463508343696595\n",
      "Epoch 19, Batch 250, Loss: 0.6406380319595337\n",
      "Epoch 19, Batch 300, Loss: 0.6499875223636628\n",
      "Epoch 19, Batch 350, Loss: 0.6448997116088867\n",
      "Epoch 19, Batch 400, Loss: 0.6436119794845581\n",
      "Epoch 19, Batch 450, Loss: 0.6439116096496582\n",
      "Epoch 19, Batch 500, Loss: 0.6445175409317017\n",
      "Epoch 19, Batch 550, Loss: 0.6482887136936187\n",
      "Epoch 19, Batch 600, Loss: 0.6498989295959473\n",
      "Epoch 19, Batch 650, Loss: 0.6461570155620575\n",
      "Epoch 19, Batch 700, Loss: 0.6391840612888336\n",
      "Epoch 19, Batch 750, Loss: 0.6432752418518066\n",
      "Epoch 19, Batch 800, Loss: 0.6450718200206756\n",
      "Epoch 19, Batch 850, Loss: 0.6448759257793426\n",
      "Epoch 19, Batch 900, Loss: 0.6441616308689118\n",
      "Epoch 20, Batch 50, Loss: 0.6383592391014099\n",
      "Epoch 20, Batch 100, Loss: 0.6428606355190277\n",
      "Epoch 20, Batch 150, Loss: 0.6465486180782318\n",
      "Epoch 20, Batch 200, Loss: 0.6502983427047729\n",
      "Epoch 20, Batch 250, Loss: 0.647852531671524\n",
      "Epoch 20, Batch 300, Loss: 0.6458194923400878\n",
      "Epoch 20, Batch 350, Loss: 0.6459710502624512\n",
      "Epoch 20, Batch 400, Loss: 0.6483241939544677\n",
      "Epoch 20, Batch 450, Loss: 0.6424499821662902\n",
      "Epoch 20, Batch 500, Loss: 0.6450927150249481\n",
      "Epoch 20, Batch 550, Loss: 0.6414135313034057\n",
      "Epoch 20, Batch 600, Loss: 0.6426356518268586\n",
      "Epoch 20, Batch 650, Loss: 0.6395742726325989\n",
      "Epoch 20, Batch 700, Loss: 0.6391251337528229\n",
      "Epoch 20, Batch 750, Loss: 0.6403442931175232\n",
      "Epoch 20, Batch 800, Loss: 0.6455044770240783\n",
      "Epoch 20, Batch 850, Loss: 0.6512139976024628\n",
      "Epoch 20, Batch 900, Loss: 0.637719396352768\n",
      "Epoch 21, Batch 50, Loss: 0.6428516638278962\n",
      "Epoch 21, Batch 100, Loss: 0.646731241941452\n",
      "Epoch 21, Batch 150, Loss: 0.6465962946414947\n",
      "Epoch 21, Batch 200, Loss: 0.6469576573371887\n",
      "Epoch 21, Batch 250, Loss: 0.6396111059188843\n",
      "Epoch 21, Batch 300, Loss: 0.6392170810699462\n",
      "Epoch 21, Batch 350, Loss: 0.6418528151512146\n",
      "Epoch 21, Batch 400, Loss: 0.6400654590129853\n",
      "Epoch 21, Batch 450, Loss: 0.6446271812915803\n",
      "Epoch 21, Batch 500, Loss: 0.6436905014514923\n",
      "Epoch 21, Batch 550, Loss: 0.650090001821518\n",
      "Epoch 21, Batch 600, Loss: 0.6417206835746765\n",
      "Epoch 21, Batch 650, Loss: 0.6387373054027558\n",
      "Epoch 21, Batch 700, Loss: 0.6500142800807953\n",
      "Epoch 21, Batch 750, Loss: 0.6391320967674256\n",
      "Epoch 21, Batch 800, Loss: 0.6455170571804046\n",
      "Epoch 21, Batch 850, Loss: 0.6419767785072327\n",
      "Epoch 21, Batch 900, Loss: 0.6400017702579498\n",
      "Epoch 22, Batch 50, Loss: 0.6383501088619232\n",
      "Epoch 22, Batch 100, Loss: 0.6410052525997162\n",
      "Epoch 22, Batch 150, Loss: 0.6477231371402741\n",
      "Epoch 22, Batch 200, Loss: 0.641248905658722\n",
      "Epoch 22, Batch 250, Loss: 0.6365112566947937\n",
      "Epoch 22, Batch 300, Loss: 0.649234721660614\n",
      "Epoch 22, Batch 350, Loss: 0.6407400333881378\n",
      "Epoch 22, Batch 400, Loss: 0.6420034563541412\n",
      "Epoch 22, Batch 450, Loss: 0.6385268938541412\n",
      "Epoch 22, Batch 500, Loss: 0.639739363193512\n",
      "Epoch 22, Batch 550, Loss: 0.6429402446746826\n",
      "Epoch 22, Batch 600, Loss: 0.6423621666431427\n",
      "Epoch 22, Batch 650, Loss: 0.6460955977439881\n",
      "Epoch 22, Batch 700, Loss: 0.6441445112228393\n",
      "Epoch 22, Batch 750, Loss: 0.6423191344738006\n",
      "Epoch 22, Batch 800, Loss: 0.646154271364212\n",
      "Epoch 22, Batch 850, Loss: 0.6442217063903809\n",
      "Epoch 22, Batch 900, Loss: 0.6457720363140106\n",
      "Epoch 23, Batch 50, Loss: 0.6444428324699402\n",
      "Epoch 23, Batch 100, Loss: 0.6356656169891357\n",
      "Epoch 23, Batch 150, Loss: 0.6425797748565674\n",
      "Epoch 23, Batch 200, Loss: 0.6449528133869171\n",
      "Epoch 23, Batch 250, Loss: 0.6433789944648742\n",
      "Epoch 23, Batch 300, Loss: 0.6379212939739227\n",
      "Epoch 23, Batch 350, Loss: 0.6452138614654541\n",
      "Epoch 23, Batch 400, Loss: 0.637834290266037\n",
      "Epoch 23, Batch 450, Loss: 0.6369737422466278\n",
      "Epoch 23, Batch 500, Loss: 0.640995225906372\n",
      "Epoch 23, Batch 550, Loss: 0.6346105074882508\n",
      "Epoch 23, Batch 600, Loss: 0.6424195063114166\n",
      "Epoch 23, Batch 650, Loss: 0.6419157648086548\n",
      "Epoch 23, Batch 700, Loss: 0.6415805649757386\n",
      "Epoch 23, Batch 750, Loss: 0.6408507084846496\n",
      "Epoch 23, Batch 800, Loss: 0.6458881211280822\n",
      "Epoch 23, Batch 850, Loss: 0.6444030213356018\n",
      "Epoch 23, Batch 900, Loss: 0.6424039125442504\n",
      "Epoch 24, Batch 50, Loss: 0.6379510390758515\n",
      "Epoch 24, Batch 100, Loss: 0.6489238619804383\n",
      "Epoch 24, Batch 150, Loss: 0.6414692032337189\n",
      "Epoch 24, Batch 200, Loss: 0.6348810470104218\n",
      "Epoch 24, Batch 250, Loss: 0.6367506170272828\n",
      "Epoch 24, Batch 300, Loss: 0.6427520024776459\n",
      "Epoch 24, Batch 350, Loss: 0.6409765982627869\n",
      "Epoch 24, Batch 400, Loss: 0.6449659752845764\n",
      "Epoch 24, Batch 450, Loss: 0.6462782680988312\n",
      "Epoch 24, Batch 500, Loss: 0.636947820186615\n",
      "Epoch 24, Batch 550, Loss: 0.6365156817436218\n",
      "Epoch 24, Batch 600, Loss: 0.6404405951499939\n",
      "Epoch 24, Batch 650, Loss: 0.6377574265003204\n",
      "Epoch 24, Batch 700, Loss: 0.6327449321746826\n",
      "Epoch 24, Batch 750, Loss: 0.6387498557567597\n",
      "Epoch 24, Batch 800, Loss: 0.6313968861103058\n",
      "Epoch 24, Batch 850, Loss: 0.6351110887527466\n",
      "Epoch 24, Batch 900, Loss: 0.6430367875099182\n",
      "Epoch 25, Batch 50, Loss: 0.639450169801712\n",
      "Epoch 25, Batch 100, Loss: 0.6346958911418915\n",
      "Epoch 25, Batch 150, Loss: 0.6380963671207428\n",
      "Epoch 25, Batch 200, Loss: 0.6373636102676392\n",
      "Epoch 25, Batch 250, Loss: 0.6331379628181457\n",
      "Epoch 25, Batch 300, Loss: 0.6352527606487274\n",
      "Epoch 25, Batch 350, Loss: 0.6352111923694611\n",
      "Epoch 25, Batch 400, Loss: 0.6332111251354218\n",
      "Epoch 25, Batch 450, Loss: 0.6279630208015442\n",
      "Epoch 25, Batch 500, Loss: 0.6304276144504547\n",
      "Epoch 25, Batch 550, Loss: 0.6370153284072876\n",
      "Epoch 25, Batch 600, Loss: 0.6287327432632446\n",
      "Epoch 25, Batch 650, Loss: 0.6376925003528595\n",
      "Epoch 25, Batch 700, Loss: 0.636958259344101\n",
      "Epoch 25, Batch 750, Loss: 0.636214839220047\n",
      "Epoch 25, Batch 800, Loss: 0.6267928314208985\n",
      "Epoch 25, Batch 850, Loss: 0.6398526179790497\n",
      "Epoch 25, Batch 900, Loss: 0.6326724648475647\n",
      "Accuracy on test set: 0.58%\n",
      "Fitting for combination 24\n",
      "784\n",
      "2\n",
      "10\n",
      "[40, 20, 10]\n",
      "True\n",
      "['relu', 'tanh']\n",
      "Adam\n",
      "0.3\n",
      "0.1\n",
      "CrossEntropyLoss\n",
      "200\n",
      "25\n",
      "0\n",
      "Epoch 1, Batch 200, Loss: 5.126485834121704\n",
      "Epoch 1, Batch 400, Loss: 5.015749974250793\n",
      "Epoch 1, Batch 600, Loss: 5.134737932682038\n",
      "Epoch 1, Batch 800, Loss: 5.17448269367218\n",
      "Epoch 2, Batch 200, Loss: 5.215262787342072\n",
      "Epoch 2, Batch 400, Loss: 5.131928770542145\n",
      "Epoch 2, Batch 600, Loss: 5.218823118209839\n",
      "Epoch 2, Batch 800, Loss: 5.111101324558258\n",
      "Epoch 3, Batch 200, Loss: 4.947997913360596\n",
      "Epoch 3, Batch 400, Loss: 5.137720854282379\n",
      "Epoch 3, Batch 600, Loss: 5.139086520671844\n",
      "Epoch 3, Batch 800, Loss: 5.0270182585716245\n",
      "Epoch 4, Batch 200, Loss: 5.199078612327575\n",
      "Epoch 4, Batch 400, Loss: 5.166434044837952\n",
      "Epoch 4, Batch 600, Loss: 5.32407616853714\n",
      "Epoch 4, Batch 800, Loss: 5.1570385825634\n",
      "Epoch 5, Batch 200, Loss: 5.033776202201843\n",
      "Epoch 5, Batch 400, Loss: 5.198616397380829\n",
      "Epoch 5, Batch 600, Loss: 5.080804841518402\n",
      "Epoch 5, Batch 800, Loss: 5.104598791599273\n",
      "Epoch 6, Batch 200, Loss: 5.042145054340363\n",
      "Epoch 6, Batch 400, Loss: 5.0841044378280635\n",
      "Epoch 6, Batch 600, Loss: 5.2972408652305605\n",
      "Epoch 6, Batch 800, Loss: 5.1967858004570004\n",
      "Epoch 7, Batch 200, Loss: 5.121680943965912\n",
      "Epoch 7, Batch 400, Loss: 5.109882576465607\n",
      "Epoch 7, Batch 600, Loss: 5.174229934215545\n",
      "Epoch 7, Batch 800, Loss: 5.183212275505066\n",
      "Epoch 8, Batch 200, Loss: 5.034852223396301\n",
      "Epoch 8, Batch 400, Loss: 5.145123262405395\n",
      "Epoch 8, Batch 600, Loss: 5.0891170835495\n",
      "Epoch 8, Batch 800, Loss: 5.12153076171875\n",
      "Epoch 9, Batch 200, Loss: 5.209412789344787\n",
      "Epoch 9, Batch 400, Loss: 5.2143292498588565\n",
      "Epoch 9, Batch 600, Loss: 5.312666001319886\n",
      "Epoch 9, Batch 800, Loss: 5.170382332801819\n",
      "Epoch 10, Batch 200, Loss: 5.187130951881409\n",
      "Epoch 10, Batch 400, Loss: 5.084205343723297\n",
      "Epoch 10, Batch 600, Loss: 5.176675825119019\n",
      "Epoch 10, Batch 800, Loss: 5.133212656974792\n",
      "Epoch 11, Batch 200, Loss: 5.22479576587677\n",
      "Epoch 11, Batch 400, Loss: 5.166561539173126\n",
      "Epoch 11, Batch 600, Loss: 5.112459042072296\n",
      "Epoch 11, Batch 800, Loss: 5.152765874862671\n",
      "Epoch 12, Batch 200, Loss: 5.178584153652191\n",
      "Epoch 12, Batch 400, Loss: 5.125059611797333\n",
      "Epoch 12, Batch 600, Loss: 5.141351137161255\n",
      "Epoch 12, Batch 800, Loss: 5.093963027000427\n",
      "Epoch 13, Batch 200, Loss: 5.205008654594422\n",
      "Epoch 13, Batch 400, Loss: 5.11207263469696\n",
      "Epoch 13, Batch 600, Loss: 5.236255385875702\n",
      "Epoch 13, Batch 800, Loss: 5.197617263793945\n",
      "Epoch 14, Batch 200, Loss: 5.290636987686157\n",
      "Epoch 14, Batch 400, Loss: 5.082816333770752\n",
      "Epoch 14, Batch 600, Loss: 5.1453643321990965\n",
      "Epoch 14, Batch 800, Loss: 5.239417500495911\n",
      "Epoch 15, Batch 200, Loss: 5.351560726165771\n",
      "Epoch 15, Batch 400, Loss: 5.042431116104126\n",
      "Epoch 15, Batch 600, Loss: 5.305859773159027\n",
      "Epoch 15, Batch 800, Loss: 5.123108644485473\n",
      "Epoch 16, Batch 200, Loss: 5.021868510246277\n",
      "Epoch 16, Batch 400, Loss: 4.97427404999733\n",
      "Epoch 16, Batch 600, Loss: 5.177833313941956\n",
      "Epoch 16, Batch 800, Loss: 5.0176053452491765\n",
      "Epoch 17, Batch 200, Loss: 5.059219250679016\n",
      "Epoch 17, Batch 400, Loss: 5.19287034034729\n",
      "Epoch 17, Batch 600, Loss: 5.116851704120636\n",
      "Epoch 17, Batch 800, Loss: 5.09940593957901\n",
      "Epoch 18, Batch 200, Loss: 5.103127074241638\n",
      "Epoch 18, Batch 400, Loss: 5.206779057979584\n",
      "Epoch 18, Batch 600, Loss: 5.158250818252563\n",
      "Epoch 18, Batch 800, Loss: 5.204363453388214\n",
      "Epoch 19, Batch 200, Loss: 5.125715010166168\n",
      "Epoch 19, Batch 400, Loss: 5.099136209487915\n",
      "Epoch 19, Batch 600, Loss: 5.081917605400085\n",
      "Epoch 19, Batch 800, Loss: 5.175576620101928\n",
      "Epoch 20, Batch 200, Loss: 5.039133079051972\n",
      "Epoch 20, Batch 400, Loss: 5.23036075592041\n",
      "Epoch 20, Batch 600, Loss: 5.121270198822021\n",
      "Epoch 20, Batch 800, Loss: 5.234654340744019\n",
      "Epoch 21, Batch 200, Loss: 5.025801572799683\n",
      "Epoch 21, Batch 400, Loss: 4.966245727539063\n",
      "Epoch 21, Batch 600, Loss: 5.105216403007507\n",
      "Epoch 21, Batch 800, Loss: 5.231439180374146\n",
      "Epoch 22, Batch 200, Loss: 5.101186318397522\n",
      "Epoch 22, Batch 400, Loss: 5.209649126529694\n",
      "Epoch 22, Batch 600, Loss: 5.265154776573181\n",
      "Epoch 22, Batch 800, Loss: 5.165191752910614\n",
      "Epoch 23, Batch 200, Loss: 5.186366443634033\n",
      "Epoch 23, Batch 400, Loss: 5.002825751304626\n",
      "Epoch 23, Batch 600, Loss: 5.185217220783233\n",
      "Epoch 23, Batch 800, Loss: 5.228966941833496\n",
      "Epoch 24, Batch 200, Loss: 5.377045359611511\n",
      "Epoch 24, Batch 400, Loss: 5.01764734506607\n",
      "Epoch 24, Batch 600, Loss: 5.2382700061798095\n",
      "Epoch 24, Batch 800, Loss: 5.052398672103882\n",
      "Epoch 25, Batch 200, Loss: 5.23053893327713\n",
      "Epoch 25, Batch 400, Loss: 5.045414226055145\n",
      "Epoch 25, Batch 600, Loss: 5.194064965248108\n",
      "Epoch 25, Batch 800, Loss: 5.193455204963684\n",
      "Accuracy on test set: 0.1%\n",
      "Fitting for combination 25\n",
      "784\n",
      "2\n",
      "10\n",
      "[40, 20, 10]\n",
      "False\n",
      "['relu', 'sigmoid']\n",
      "SGD\n",
      "0.01\n",
      "0.03\n",
      "CrossEntropyLoss\n",
      "50\n",
      "25\n",
      "0\n",
      "Epoch 1, Batch 50, Loss: 1.1615736365318299\n",
      "Epoch 1, Batch 100, Loss: 1.145974178314209\n",
      "Epoch 1, Batch 150, Loss: 1.1314292740821839\n",
      "Epoch 1, Batch 200, Loss: 1.1195291829109193\n",
      "Epoch 1, Batch 250, Loss: 1.1069477677345276\n",
      "Epoch 1, Batch 300, Loss: 1.090418963432312\n",
      "Epoch 1, Batch 350, Loss: 1.0776570868492126\n",
      "Epoch 1, Batch 400, Loss: 1.0617747759819032\n",
      "Epoch 1, Batch 450, Loss: 1.045565927028656\n",
      "Epoch 1, Batch 500, Loss: 1.025665786266327\n",
      "Epoch 1, Batch 550, Loss: 1.0041282320022582\n",
      "Epoch 1, Batch 600, Loss: 0.9864499127864838\n",
      "Epoch 1, Batch 650, Loss: 0.9664533257484436\n",
      "Epoch 1, Batch 700, Loss: 0.9462688434123993\n",
      "Epoch 1, Batch 750, Loss: 0.9284791457653045\n",
      "Epoch 1, Batch 800, Loss: 0.9105893695354461\n",
      "Epoch 1, Batch 850, Loss: 0.9007416009902954\n",
      "Epoch 1, Batch 900, Loss: 0.8821317028999328\n",
      "Epoch 2, Batch 50, Loss: 0.8543421065807343\n",
      "Epoch 2, Batch 100, Loss: 0.8418956530094147\n",
      "Epoch 2, Batch 150, Loss: 0.8310160458087921\n",
      "Epoch 2, Batch 200, Loss: 0.8133164036273957\n",
      "Epoch 2, Batch 250, Loss: 0.807199524641037\n",
      "Epoch 2, Batch 300, Loss: 0.7995859777927399\n",
      "Epoch 2, Batch 350, Loss: 0.7852393126487732\n",
      "Epoch 2, Batch 400, Loss: 0.7733002769947052\n",
      "Epoch 2, Batch 450, Loss: 0.7608081209659576\n",
      "Epoch 2, Batch 500, Loss: 0.7583751559257508\n",
      "Epoch 2, Batch 550, Loss: 0.7502939987182617\n",
      "Epoch 2, Batch 600, Loss: 0.7426386094093322\n",
      "Epoch 2, Batch 650, Loss: 0.7333633601665497\n",
      "Epoch 2, Batch 700, Loss: 0.7248627734184265\n",
      "Epoch 2, Batch 750, Loss: 0.7246124792098999\n",
      "Epoch 2, Batch 800, Loss: 0.7144583117961884\n",
      "Epoch 2, Batch 850, Loss: 0.7133109402656556\n",
      "Epoch 2, Batch 900, Loss: 0.6998914897441864\n",
      "Epoch 3, Batch 50, Loss: 0.6958691000938415\n",
      "Epoch 3, Batch 100, Loss: 0.6889966416358948\n",
      "Epoch 3, Batch 150, Loss: 0.6817457103729248\n",
      "Epoch 3, Batch 200, Loss: 0.6780735456943512\n",
      "Epoch 3, Batch 250, Loss: 0.6702933692932129\n",
      "Epoch 3, Batch 300, Loss: 0.6660526943206787\n",
      "Epoch 3, Batch 350, Loss: 0.6643641948699951\n",
      "Epoch 3, Batch 400, Loss: 0.6600041913986207\n",
      "Epoch 3, Batch 450, Loss: 0.6558569121360779\n",
      "Epoch 3, Batch 500, Loss: 0.6545346474647522\n",
      "Epoch 3, Batch 550, Loss: 0.6530524718761445\n",
      "Epoch 3, Batch 600, Loss: 0.6438284206390381\n",
      "Epoch 3, Batch 650, Loss: 0.6391785526275635\n",
      "Epoch 3, Batch 700, Loss: 0.6442862963676452\n",
      "Epoch 3, Batch 750, Loss: 0.6306523907184601\n",
      "Epoch 3, Batch 800, Loss: 0.6266609632968902\n",
      "Epoch 3, Batch 850, Loss: 0.6283712983131409\n",
      "Epoch 3, Batch 900, Loss: 0.6277500522136689\n",
      "Epoch 4, Batch 50, Loss: 0.6253155207633972\n",
      "Epoch 4, Batch 100, Loss: 0.621674839258194\n",
      "Epoch 4, Batch 150, Loss: 0.6109052085876465\n",
      "Epoch 4, Batch 200, Loss: 0.6090786123275757\n",
      "Epoch 4, Batch 250, Loss: 0.6041804802417755\n",
      "Epoch 4, Batch 300, Loss: 0.6121197938919067\n",
      "Epoch 4, Batch 350, Loss: 0.6091598558425904\n",
      "Epoch 4, Batch 400, Loss: 0.605511816740036\n",
      "Epoch 4, Batch 450, Loss: 0.609111853837967\n",
      "Epoch 4, Batch 500, Loss: 0.6099843215942383\n",
      "Epoch 4, Batch 550, Loss: 0.6082574701309205\n",
      "Epoch 4, Batch 600, Loss: 0.6028590357303619\n",
      "Epoch 4, Batch 650, Loss: 0.5848677504062653\n",
      "Epoch 4, Batch 700, Loss: 0.600151275396347\n",
      "Epoch 4, Batch 750, Loss: 0.5911271011829377\n",
      "Epoch 4, Batch 800, Loss: 0.5920881843566894\n",
      "Epoch 4, Batch 850, Loss: 0.5919406759738922\n",
      "Epoch 4, Batch 900, Loss: 0.5911439406871796\n",
      "Epoch 5, Batch 50, Loss: 0.5920888185501099\n",
      "Epoch 5, Batch 100, Loss: 0.5822236669063569\n",
      "Epoch 5, Batch 150, Loss: 0.5874886929988861\n",
      "Epoch 5, Batch 200, Loss: 0.5837051129341125\n",
      "Epoch 5, Batch 250, Loss: 0.574623566865921\n",
      "Epoch 5, Batch 300, Loss: 0.5878236675262452\n",
      "Epoch 5, Batch 350, Loss: 0.5784811365604401\n",
      "Epoch 5, Batch 400, Loss: 0.5842570459842682\n",
      "Epoch 5, Batch 450, Loss: 0.5787549990415574\n",
      "Epoch 5, Batch 500, Loss: 0.5751326966285706\n",
      "Epoch 5, Batch 550, Loss: 0.5707683050632477\n",
      "Epoch 5, Batch 600, Loss: 0.5735460352897644\n",
      "Epoch 5, Batch 650, Loss: 0.5705590891838074\n",
      "Epoch 5, Batch 700, Loss: 0.5691313540935516\n",
      "Epoch 5, Batch 750, Loss: 0.5709051978588104\n",
      "Epoch 5, Batch 800, Loss: 0.5682439208030701\n",
      "Epoch 5, Batch 850, Loss: 0.5743390011787415\n",
      "Epoch 5, Batch 900, Loss: 0.5637946426868439\n",
      "Epoch 6, Batch 50, Loss: 0.5685571378469467\n",
      "Epoch 6, Batch 100, Loss: 0.5745858490467072\n",
      "Epoch 6, Batch 150, Loss: 0.5565548324584961\n",
      "Epoch 6, Batch 200, Loss: 0.5599857795238495\n",
      "Epoch 6, Batch 250, Loss: 0.5570841300487518\n",
      "Epoch 6, Batch 300, Loss: 0.5612629747390747\n",
      "Epoch 6, Batch 350, Loss: 0.5661255598068238\n",
      "Epoch 6, Batch 400, Loss: 0.5567528462409973\n",
      "Epoch 6, Batch 450, Loss: 0.5566177433729171\n",
      "Epoch 6, Batch 500, Loss: 0.56257284283638\n",
      "Epoch 6, Batch 550, Loss: 0.5685768502950669\n",
      "Epoch 6, Batch 600, Loss: 0.558495237827301\n",
      "Epoch 6, Batch 650, Loss: 0.5583111476898194\n",
      "Epoch 6, Batch 700, Loss: 0.5566240566968917\n",
      "Epoch 6, Batch 750, Loss: 0.5474403953552246\n",
      "Epoch 6, Batch 800, Loss: 0.5542248672246933\n",
      "Epoch 6, Batch 850, Loss: 0.5514024943113327\n",
      "Epoch 6, Batch 900, Loss: 0.55919562458992\n",
      "Epoch 7, Batch 50, Loss: 0.5546129697561264\n",
      "Epoch 7, Batch 100, Loss: 0.5464961886405945\n",
      "Epoch 7, Batch 150, Loss: 0.5577404296398163\n",
      "Epoch 7, Batch 200, Loss: 0.5549573332071305\n",
      "Epoch 7, Batch 250, Loss: 0.5429907894134521\n",
      "Epoch 7, Batch 300, Loss: 0.5529736328125\n",
      "Epoch 7, Batch 350, Loss: 0.5502997833490372\n",
      "Epoch 7, Batch 400, Loss: 0.5525509506464005\n",
      "Epoch 7, Batch 450, Loss: 0.5493573623895646\n",
      "Epoch 7, Batch 500, Loss: 0.5518415343761444\n",
      "Epoch 7, Batch 550, Loss: 0.5413874697685241\n",
      "Epoch 7, Batch 600, Loss: 0.5534609723091125\n",
      "Epoch 7, Batch 650, Loss: 0.5514615494012832\n",
      "Epoch 7, Batch 700, Loss: 0.5380807346105576\n",
      "Epoch 7, Batch 750, Loss: 0.5412253648042679\n",
      "Epoch 7, Batch 800, Loss: 0.5445547008514404\n",
      "Epoch 7, Batch 850, Loss: 0.5515003842115402\n",
      "Epoch 7, Batch 900, Loss: 0.5455976474285126\n",
      "Epoch 8, Batch 50, Loss: 0.5402860409021377\n",
      "Epoch 8, Batch 100, Loss: 0.5440633082389832\n",
      "Epoch 8, Batch 150, Loss: 0.5363830119371414\n",
      "Epoch 8, Batch 200, Loss: 0.5325123822689056\n",
      "Epoch 8, Batch 250, Loss: 0.5358601832389831\n",
      "Epoch 8, Batch 300, Loss: 0.540510908961296\n",
      "Epoch 8, Batch 350, Loss: 0.5491128718852997\n",
      "Epoch 8, Batch 400, Loss: 0.5420251595973968\n",
      "Epoch 8, Batch 450, Loss: 0.54903728723526\n",
      "Epoch 8, Batch 500, Loss: 0.5480188965797425\n",
      "Epoch 8, Batch 550, Loss: 0.5510994702577591\n",
      "Epoch 8, Batch 600, Loss: 0.5443419063091278\n",
      "Epoch 8, Batch 650, Loss: 0.5381378191709518\n",
      "Epoch 8, Batch 700, Loss: 0.5395397663116455\n",
      "Epoch 8, Batch 750, Loss: 0.5366533076763154\n",
      "Epoch 8, Batch 800, Loss: 0.53954176902771\n",
      "Epoch 8, Batch 850, Loss: 0.525071462392807\n",
      "Epoch 8, Batch 900, Loss: 0.5348628401756287\n",
      "Epoch 9, Batch 50, Loss: 0.5346114879846573\n",
      "Epoch 9, Batch 100, Loss: 0.5380027097463608\n",
      "Epoch 9, Batch 150, Loss: 0.5397441905736923\n",
      "Epoch 9, Batch 200, Loss: 0.5343312376737595\n",
      "Epoch 9, Batch 250, Loss: 0.5383996671438217\n",
      "Epoch 9, Batch 300, Loss: 0.5349646103382111\n",
      "Epoch 9, Batch 350, Loss: 0.5302829492092133\n",
      "Epoch 9, Batch 400, Loss: 0.5320629143714904\n",
      "Epoch 9, Batch 450, Loss: 0.5327248024940491\n",
      "Epoch 9, Batch 500, Loss: 0.54057950258255\n",
      "Epoch 9, Batch 550, Loss: 0.5377949035167694\n",
      "Epoch 9, Batch 600, Loss: 0.5261554044485092\n",
      "Epoch 9, Batch 650, Loss: 0.5337904095649719\n",
      "Epoch 9, Batch 700, Loss: 0.5303460437059403\n",
      "Epoch 9, Batch 750, Loss: 0.5326313686370849\n",
      "Epoch 9, Batch 800, Loss: 0.5317860281467438\n",
      "Epoch 9, Batch 850, Loss: 0.530109132528305\n",
      "Epoch 9, Batch 900, Loss: 0.526758462190628\n",
      "Epoch 10, Batch 50, Loss: 0.5316621845960617\n",
      "Epoch 10, Batch 100, Loss: 0.5376750975847244\n",
      "Epoch 10, Batch 150, Loss: 0.5278535056114196\n",
      "Epoch 10, Batch 200, Loss: 0.5314213126897812\n",
      "Epoch 10, Batch 250, Loss: 0.5183792567253113\n",
      "Epoch 10, Batch 300, Loss: 0.530256364941597\n",
      "Epoch 10, Batch 350, Loss: 0.5237735641002655\n",
      "Epoch 10, Batch 400, Loss: 0.5259661543369293\n",
      "Epoch 10, Batch 450, Loss: 0.5285881561040878\n",
      "Epoch 10, Batch 500, Loss: 0.5263670021295548\n",
      "Epoch 10, Batch 550, Loss: 0.5188133853673935\n",
      "Epoch 10, Batch 600, Loss: 0.5325612372159958\n",
      "Epoch 10, Batch 650, Loss: 0.5318266344070435\n",
      "Epoch 10, Batch 700, Loss: 0.5247678250074387\n",
      "Epoch 10, Batch 750, Loss: 0.5239366656541824\n",
      "Epoch 10, Batch 800, Loss: 0.5296096020936966\n",
      "Epoch 10, Batch 850, Loss: 0.5296033436059951\n",
      "Epoch 10, Batch 900, Loss: 0.5230472767353058\n",
      "Epoch 11, Batch 50, Loss: 0.5275124418735504\n",
      "Epoch 11, Batch 100, Loss: 0.5216008287668228\n",
      "Epoch 11, Batch 150, Loss: 0.5289551728963852\n",
      "Epoch 11, Batch 200, Loss: 0.523271707892418\n",
      "Epoch 11, Batch 250, Loss: 0.5202635216712952\n",
      "Epoch 11, Batch 300, Loss: 0.5230860882997512\n",
      "Epoch 11, Batch 350, Loss: 0.51885318338871\n",
      "Epoch 11, Batch 400, Loss: 0.5206429880857467\n",
      "Epoch 11, Batch 450, Loss: 0.5228086298704148\n",
      "Epoch 11, Batch 500, Loss: 0.5200842970609665\n",
      "Epoch 11, Batch 550, Loss: 0.5289373940229416\n",
      "Epoch 11, Batch 600, Loss: 0.5251445758342743\n",
      "Epoch 11, Batch 650, Loss: 0.5207353901863098\n",
      "Epoch 11, Batch 700, Loss: 0.5194373118877411\n",
      "Epoch 11, Batch 750, Loss: 0.527161283493042\n",
      "Epoch 11, Batch 800, Loss: 0.5190533775091172\n",
      "Epoch 11, Batch 850, Loss: 0.5173800224065781\n",
      "Epoch 11, Batch 900, Loss: 0.5187913733720779\n",
      "Epoch 12, Batch 50, Loss: 0.5260710906982422\n",
      "Epoch 12, Batch 100, Loss: 0.51829008102417\n",
      "Epoch 12, Batch 150, Loss: 0.5213593244552612\n",
      "Epoch 12, Batch 200, Loss: 0.5153881520032882\n",
      "Epoch 12, Batch 250, Loss: 0.5160131901502609\n",
      "Epoch 12, Batch 300, Loss: 0.5225094401836395\n",
      "Epoch 12, Batch 350, Loss: 0.5183581149578095\n",
      "Epoch 12, Batch 400, Loss: 0.5200529056787491\n",
      "Epoch 12, Batch 450, Loss: 0.5231847208738327\n",
      "Epoch 12, Batch 500, Loss: 0.5239569026231766\n",
      "Epoch 12, Batch 550, Loss: 0.5223602622747421\n",
      "Epoch 12, Batch 600, Loss: 0.5138426965475082\n",
      "Epoch 12, Batch 650, Loss: 0.521373661160469\n",
      "Epoch 12, Batch 700, Loss: 0.5099432593584061\n",
      "Epoch 12, Batch 750, Loss: 0.5159102737903595\n",
      "Epoch 12, Batch 800, Loss: 0.520997496843338\n",
      "Epoch 12, Batch 850, Loss: 0.5146899992227554\n",
      "Epoch 12, Batch 900, Loss: 0.5113384479284286\n",
      "Epoch 13, Batch 50, Loss: 0.518601730465889\n",
      "Epoch 13, Batch 100, Loss: 0.5225047045946121\n",
      "Epoch 13, Batch 150, Loss: 0.5216880303621292\n",
      "Epoch 13, Batch 200, Loss: 0.5155388313531876\n",
      "Epoch 13, Batch 250, Loss: 0.5131448948383331\n",
      "Epoch 13, Batch 300, Loss: 0.5229110711812973\n",
      "Epoch 13, Batch 350, Loss: 0.5145811218023301\n",
      "Epoch 13, Batch 400, Loss: 0.5155880582332611\n",
      "Epoch 13, Batch 450, Loss: 0.5151970285177231\n",
      "Epoch 13, Batch 500, Loss: 0.513900939822197\n",
      "Epoch 13, Batch 550, Loss: 0.5071602022647858\n",
      "Epoch 13, Batch 600, Loss: 0.4989169979095459\n",
      "Epoch 13, Batch 650, Loss: 0.5186129862070084\n",
      "Epoch 13, Batch 700, Loss: 0.5148498922586441\n",
      "Epoch 13, Batch 750, Loss: 0.5109230417013169\n",
      "Epoch 13, Batch 800, Loss: 0.5185649693012238\n",
      "Epoch 13, Batch 850, Loss: 0.5173530662059784\n",
      "Epoch 13, Batch 900, Loss: 0.5026484620571137\n",
      "Epoch 14, Batch 50, Loss: 0.5121001642942429\n",
      "Epoch 14, Batch 100, Loss: 0.514568230509758\n",
      "Epoch 14, Batch 150, Loss: 0.5131603175401688\n",
      "Epoch 14, Batch 200, Loss: 0.5123029863834381\n",
      "Epoch 14, Batch 250, Loss: 0.5140632706880569\n",
      "Epoch 14, Batch 300, Loss: 0.5141120010614395\n",
      "Epoch 14, Batch 350, Loss: 0.5069796246290207\n",
      "Epoch 14, Batch 400, Loss: 0.516783242225647\n",
      "Epoch 14, Batch 450, Loss: 0.5075381767749786\n",
      "Epoch 14, Batch 500, Loss: 0.5102040016651154\n",
      "Epoch 14, Batch 550, Loss: 0.5066123813390732\n",
      "Epoch 14, Batch 600, Loss: 0.5081620872020721\n",
      "Epoch 14, Batch 650, Loss: 0.511293249130249\n",
      "Epoch 14, Batch 700, Loss: 0.5102791774272919\n",
      "Epoch 14, Batch 750, Loss: 0.5093447387218475\n",
      "Epoch 14, Batch 800, Loss: 0.5131009668111801\n",
      "Epoch 14, Batch 850, Loss: 0.5018996065855026\n",
      "Epoch 14, Batch 900, Loss: 0.5132830554246902\n",
      "Epoch 15, Batch 50, Loss: 0.5118809378147126\n",
      "Epoch 15, Batch 100, Loss: 0.5017906254529954\n",
      "Epoch 15, Batch 150, Loss: 0.5079981422424317\n",
      "Epoch 15, Batch 200, Loss: 0.5056497091054917\n",
      "Epoch 15, Batch 250, Loss: 0.5138955682516098\n",
      "Epoch 15, Batch 300, Loss: 0.5043888705968856\n",
      "Epoch 15, Batch 350, Loss: 0.5110445004701615\n",
      "Epoch 15, Batch 400, Loss: 0.5105122166872025\n",
      "Epoch 15, Batch 450, Loss: 0.5086252057552337\n",
      "Epoch 15, Batch 500, Loss: 0.5157713067531585\n",
      "Epoch 15, Batch 550, Loss: 0.5015845310688019\n",
      "Epoch 15, Batch 600, Loss: 0.5064479982852936\n",
      "Epoch 15, Batch 650, Loss: 0.5066644823551179\n",
      "Epoch 15, Batch 700, Loss: 0.5078462183475494\n",
      "Epoch 15, Batch 750, Loss: 0.5067733091115951\n",
      "Epoch 15, Batch 800, Loss: 0.5079154539108276\n",
      "Epoch 15, Batch 850, Loss: 0.49914279460906985\n",
      "Epoch 15, Batch 900, Loss: 0.5098796963691712\n",
      "Epoch 16, Batch 50, Loss: 0.5072909468412399\n",
      "Epoch 16, Batch 100, Loss: 0.4986649227142334\n",
      "Epoch 16, Batch 150, Loss: 0.5040877425670623\n",
      "Epoch 16, Batch 200, Loss: 0.5097270447015763\n",
      "Epoch 16, Batch 250, Loss: 0.509030492901802\n",
      "Epoch 16, Batch 300, Loss: 0.5043436431884766\n",
      "Epoch 16, Batch 350, Loss: 0.5086330503225327\n",
      "Epoch 16, Batch 400, Loss: 0.508560088276863\n",
      "Epoch 16, Batch 450, Loss: 0.5067404878139495\n",
      "Epoch 16, Batch 500, Loss: 0.5072995853424073\n",
      "Epoch 16, Batch 550, Loss: 0.5083442723751068\n",
      "Epoch 16, Batch 600, Loss: 0.4993625229597092\n",
      "Epoch 16, Batch 650, Loss: 0.49613689720630644\n",
      "Epoch 16, Batch 700, Loss: 0.5072833436727524\n",
      "Epoch 16, Batch 750, Loss: 0.497256333231926\n",
      "Epoch 16, Batch 800, Loss: 0.505139302611351\n",
      "Epoch 16, Batch 850, Loss: 0.5050438648462295\n",
      "Epoch 16, Batch 900, Loss: 0.5067516750097275\n",
      "Epoch 17, Batch 50, Loss: 0.494751513004303\n",
      "Epoch 17, Batch 100, Loss: 0.4997189563512802\n",
      "Epoch 17, Batch 150, Loss: 0.5022648167610169\n",
      "Epoch 17, Batch 200, Loss: 0.5004340773820877\n",
      "Epoch 17, Batch 250, Loss: 0.5004000699520111\n",
      "Epoch 17, Batch 300, Loss: 0.5056530624628067\n",
      "Epoch 17, Batch 350, Loss: 0.4993486970663071\n",
      "Epoch 17, Batch 400, Loss: 0.504573973417282\n",
      "Epoch 17, Batch 450, Loss: 0.506723603606224\n",
      "Epoch 17, Batch 500, Loss: 0.4999529564380646\n",
      "Epoch 17, Batch 550, Loss: 0.5156402546167373\n",
      "Epoch 17, Batch 600, Loss: 0.5030327481031418\n",
      "Epoch 17, Batch 650, Loss: 0.4972309994697571\n",
      "Epoch 17, Batch 700, Loss: 0.5008330881595612\n",
      "Epoch 17, Batch 750, Loss: 0.4985093903541565\n",
      "Epoch 17, Batch 800, Loss: 0.4986724555492401\n",
      "Epoch 17, Batch 850, Loss: 0.5026911491155625\n",
      "Epoch 17, Batch 900, Loss: 0.5121698886156082\n",
      "Epoch 18, Batch 50, Loss: 0.4968677192926407\n",
      "Epoch 18, Batch 100, Loss: 0.5066897296905517\n",
      "Epoch 18, Batch 150, Loss: 0.5028334468603134\n",
      "Epoch 18, Batch 200, Loss: 0.4996553307771683\n",
      "Epoch 18, Batch 250, Loss: 0.49829110980033875\n",
      "Epoch 18, Batch 300, Loss: 0.4972370320558548\n",
      "Epoch 18, Batch 350, Loss: 0.4999890649318695\n",
      "Epoch 18, Batch 400, Loss: 0.5015220749378204\n",
      "Epoch 18, Batch 450, Loss: 0.4946270799636841\n",
      "Epoch 18, Batch 500, Loss: 0.5026246464252472\n",
      "Epoch 18, Batch 550, Loss: 0.5023408848047256\n",
      "Epoch 18, Batch 600, Loss: 0.507284733057022\n",
      "Epoch 18, Batch 650, Loss: 0.5061833637952805\n",
      "Epoch 18, Batch 700, Loss: 0.4958586150407791\n",
      "Epoch 18, Batch 750, Loss: 0.5035374706983566\n",
      "Epoch 18, Batch 800, Loss: 0.5002919965982437\n",
      "Epoch 18, Batch 850, Loss: 0.4877895998954773\n",
      "Epoch 18, Batch 900, Loss: 0.4936581248044968\n",
      "Epoch 19, Batch 50, Loss: 0.4936953592300415\n",
      "Epoch 19, Batch 100, Loss: 0.4966200911998749\n",
      "Epoch 19, Batch 150, Loss: 0.4962325316667557\n",
      "Epoch 19, Batch 200, Loss: 0.49899727165699004\n",
      "Epoch 19, Batch 250, Loss: 0.5042341762781143\n",
      "Epoch 19, Batch 300, Loss: 0.5041016674041748\n",
      "Epoch 19, Batch 350, Loss: 0.4964863580465317\n",
      "Epoch 19, Batch 400, Loss: 0.49313964068889615\n",
      "Epoch 19, Batch 450, Loss: 0.5053277480602264\n",
      "Epoch 19, Batch 500, Loss: 0.49790824234485626\n",
      "Epoch 19, Batch 550, Loss: 0.5010690975189209\n",
      "Epoch 19, Batch 600, Loss: 0.488703219294548\n",
      "Epoch 19, Batch 650, Loss: 0.48408747494220733\n",
      "Epoch 19, Batch 700, Loss: 0.4952867406606674\n",
      "Epoch 19, Batch 750, Loss: 0.4933018833398819\n",
      "Epoch 19, Batch 800, Loss: 0.49912399411201475\n",
      "Epoch 19, Batch 850, Loss: 0.5000730866193771\n",
      "Epoch 19, Batch 900, Loss: 0.50350292801857\n",
      "Epoch 20, Batch 50, Loss: 0.502871065735817\n",
      "Epoch 20, Batch 100, Loss: 0.4954393655061722\n",
      "Epoch 20, Batch 150, Loss: 0.4948625987768173\n",
      "Epoch 20, Batch 200, Loss: 0.4983532428741455\n",
      "Epoch 20, Batch 250, Loss: 0.4973713618516922\n",
      "Epoch 20, Batch 300, Loss: 0.49237945675849915\n",
      "Epoch 20, Batch 350, Loss: 0.49551174402236936\n",
      "Epoch 20, Batch 400, Loss: 0.4954740583896637\n",
      "Epoch 20, Batch 450, Loss: 0.49515332698822023\n",
      "Epoch 20, Batch 500, Loss: 0.48925411760807036\n",
      "Epoch 20, Batch 550, Loss: 0.4950938212871552\n",
      "Epoch 20, Batch 600, Loss: 0.498270206451416\n",
      "Epoch 20, Batch 650, Loss: 0.4914621090888977\n",
      "Epoch 20, Batch 700, Loss: 0.5011969918012619\n",
      "Epoch 20, Batch 750, Loss: 0.4966281288862228\n",
      "Epoch 20, Batch 800, Loss: 0.49572415471076964\n",
      "Epoch 20, Batch 850, Loss: 0.4911532527208328\n",
      "Epoch 20, Batch 900, Loss: 0.4958483189344406\n",
      "Epoch 21, Batch 50, Loss: 0.48867115676403045\n",
      "Epoch 21, Batch 100, Loss: 0.4972831118106842\n",
      "Epoch 21, Batch 150, Loss: 0.49524322271347043\n",
      "Epoch 21, Batch 200, Loss: 0.4976815778017044\n",
      "Epoch 21, Batch 250, Loss: 0.49443356454372406\n",
      "Epoch 21, Batch 300, Loss: 0.49021877586841583\n",
      "Epoch 21, Batch 350, Loss: 0.4919864112138748\n",
      "Epoch 21, Batch 400, Loss: 0.48746011674404144\n",
      "Epoch 21, Batch 450, Loss: 0.4945708775520325\n",
      "Epoch 21, Batch 500, Loss: 0.49305848360061644\n",
      "Epoch 21, Batch 550, Loss: 0.49763494670391084\n",
      "Epoch 21, Batch 600, Loss: 0.48836765587329867\n",
      "Epoch 21, Batch 650, Loss: 0.49517036497592926\n",
      "Epoch 21, Batch 700, Loss: 0.49690024375915526\n",
      "Epoch 21, Batch 750, Loss: 0.4897522413730621\n",
      "Epoch 21, Batch 800, Loss: 0.4954749858379364\n",
      "Epoch 21, Batch 850, Loss: 0.5032997459173203\n",
      "Epoch 21, Batch 900, Loss: 0.49141933023929596\n",
      "Epoch 22, Batch 50, Loss: 0.49145206809043884\n",
      "Epoch 22, Batch 100, Loss: 0.4848947548866272\n",
      "Epoch 22, Batch 150, Loss: 0.4951169419288635\n",
      "Epoch 22, Batch 200, Loss: 0.498934166431427\n",
      "Epoch 22, Batch 250, Loss: 0.4855010062456131\n",
      "Epoch 22, Batch 300, Loss: 0.5043418657779694\n",
      "Epoch 22, Batch 350, Loss: 0.48598730862140654\n",
      "Epoch 22, Batch 400, Loss: 0.49343917369842527\n",
      "Epoch 22, Batch 450, Loss: 0.49688064157962797\n",
      "Epoch 22, Batch 500, Loss: 0.5012971520423889\n",
      "Epoch 22, Batch 550, Loss: 0.49408293426036837\n",
      "Epoch 22, Batch 600, Loss: 0.49056333661079404\n",
      "Epoch 22, Batch 650, Loss: 0.48657535552978515\n",
      "Epoch 22, Batch 700, Loss: 0.486890025138855\n",
      "Epoch 22, Batch 750, Loss: 0.4926197636127472\n",
      "Epoch 22, Batch 800, Loss: 0.4955305045843124\n",
      "Epoch 22, Batch 850, Loss: 0.4923505848646164\n",
      "Epoch 22, Batch 900, Loss: 0.487455113530159\n",
      "Epoch 23, Batch 50, Loss: 0.4931005346775055\n",
      "Epoch 23, Batch 100, Loss: 0.4928402233123779\n",
      "Epoch 23, Batch 150, Loss: 0.48491819739341735\n",
      "Epoch 23, Batch 200, Loss: 0.49130712628364565\n",
      "Epoch 23, Batch 250, Loss: 0.4896097004413605\n",
      "Epoch 23, Batch 300, Loss: 0.490472377538681\n",
      "Epoch 23, Batch 350, Loss: 0.4868510067462921\n",
      "Epoch 23, Batch 400, Loss: 0.49168613612651824\n",
      "Epoch 23, Batch 450, Loss: 0.48968153655529023\n",
      "Epoch 23, Batch 500, Loss: 0.495917996764183\n",
      "Epoch 23, Batch 550, Loss: 0.49081397473812105\n",
      "Epoch 23, Batch 600, Loss: 0.4957949835062027\n",
      "Epoch 23, Batch 650, Loss: 0.49363847851753234\n",
      "Epoch 23, Batch 700, Loss: 0.48851991176605225\n",
      "Epoch 23, Batch 750, Loss: 0.49869785487651824\n",
      "Epoch 23, Batch 800, Loss: 0.4849694776535034\n",
      "Epoch 23, Batch 850, Loss: 0.4880507391691208\n",
      "Epoch 23, Batch 900, Loss: 0.48822104394435883\n",
      "Epoch 24, Batch 50, Loss: 0.4999236077070236\n",
      "Epoch 24, Batch 100, Loss: 0.4931405907869339\n",
      "Epoch 24, Batch 150, Loss: 0.49791942358016966\n",
      "Epoch 24, Batch 200, Loss: 0.49118132770061496\n",
      "Epoch 24, Batch 250, Loss: 0.490771661400795\n",
      "Epoch 24, Batch 300, Loss: 0.48215043902397153\n",
      "Epoch 24, Batch 350, Loss: 0.4851749932765961\n",
      "Epoch 24, Batch 400, Loss: 0.4825929379463196\n",
      "Epoch 24, Batch 450, Loss: 0.4853088599443436\n",
      "Epoch 24, Batch 500, Loss: 0.4888644117116928\n",
      "Epoch 24, Batch 550, Loss: 0.49343463897705075\n",
      "Epoch 24, Batch 600, Loss: 0.48316593825817106\n",
      "Epoch 24, Batch 650, Loss: 0.4899061143398285\n",
      "Epoch 24, Batch 700, Loss: 0.49511133968830107\n",
      "Epoch 24, Batch 750, Loss: 0.4788186252117157\n",
      "Epoch 24, Batch 800, Loss: 0.4945471143722534\n",
      "Epoch 24, Batch 850, Loss: 0.496658735871315\n",
      "Epoch 24, Batch 900, Loss: 0.4914170974493027\n",
      "Epoch 25, Batch 50, Loss: 0.4821155518293381\n",
      "Epoch 25, Batch 100, Loss: 0.484702507853508\n",
      "Epoch 25, Batch 150, Loss: 0.4887587314844131\n",
      "Epoch 25, Batch 200, Loss: 0.49378682613372804\n",
      "Epoch 25, Batch 250, Loss: 0.49069787204265597\n",
      "Epoch 25, Batch 300, Loss: 0.4899799245595932\n",
      "Epoch 25, Batch 350, Loss: 0.47936109244823455\n",
      "Epoch 25, Batch 400, Loss: 0.48526963412761687\n",
      "Epoch 25, Batch 450, Loss: 0.488525413274765\n",
      "Epoch 25, Batch 500, Loss: 0.48611244082450866\n",
      "Epoch 25, Batch 550, Loss: 0.4887575447559357\n",
      "Epoch 25, Batch 600, Loss: 0.485513750910759\n",
      "Epoch 25, Batch 650, Loss: 0.4914864480495453\n",
      "Epoch 25, Batch 700, Loss: 0.49383967876434326\n",
      "Epoch 25, Batch 750, Loss: 0.48789955794811246\n",
      "Epoch 25, Batch 800, Loss: 0.4929314768314362\n",
      "Epoch 25, Batch 850, Loss: 0.4904037368297577\n",
      "Epoch 25, Batch 900, Loss: 0.4910732823610306\n",
      "Accuracy on test set: 0.7258%\n",
      "Fitting for combination 26\n",
      "784\n",
      "2\n",
      "10\n",
      "[40, 20, 10]\n",
      "True\n",
      "['relu', 'relu']\n",
      "Adam\n",
      "0.03\n",
      "0.01\n",
      "CrossEntropyLoss\n",
      "50\n",
      "25\n",
      "0\n",
      "Epoch 1, Batch 50, Loss: 0.6937862396240234\n",
      "Epoch 1, Batch 100, Loss: 0.4488062447309494\n",
      "Epoch 1, Batch 150, Loss: 0.43338274359703066\n",
      "Epoch 1, Batch 200, Loss: 0.432976256608963\n",
      "Epoch 1, Batch 250, Loss: 0.4186412400007248\n",
      "Epoch 1, Batch 300, Loss: 0.4242026036977768\n",
      "Epoch 1, Batch 350, Loss: 0.4014288926124573\n",
      "Epoch 1, Batch 400, Loss: 0.3972351044416428\n",
      "Epoch 1, Batch 450, Loss: 0.3914610916376114\n",
      "Epoch 1, Batch 500, Loss: 0.39501829326152804\n",
      "Epoch 1, Batch 550, Loss: 0.39589658975601194\n",
      "Epoch 1, Batch 600, Loss: 0.4334395337104797\n",
      "Epoch 1, Batch 650, Loss: 0.3837898576259613\n",
      "Epoch 1, Batch 700, Loss: 0.38016059070825575\n",
      "Epoch 1, Batch 750, Loss: 0.4170654100179672\n",
      "Epoch 1, Batch 800, Loss: 0.40974352300167083\n",
      "Epoch 1, Batch 850, Loss: 0.40562405169010163\n",
      "Epoch 1, Batch 900, Loss: 0.40314636647701263\n",
      "Epoch 2, Batch 50, Loss: 0.3827794274687767\n",
      "Epoch 2, Batch 100, Loss: 0.3717398625612259\n",
      "Epoch 2, Batch 150, Loss: 0.3740054306387901\n",
      "Epoch 2, Batch 200, Loss: 0.387357120513916\n",
      "Epoch 2, Batch 250, Loss: 0.3811050197482109\n",
      "Epoch 2, Batch 300, Loss: 0.40773714780807496\n",
      "Epoch 2, Batch 350, Loss: 0.4252246618270874\n",
      "Epoch 2, Batch 400, Loss: 0.45735813558101657\n",
      "Epoch 2, Batch 450, Loss: 0.41688642084598543\n",
      "Epoch 2, Batch 500, Loss: 0.3761435908079147\n",
      "Epoch 2, Batch 550, Loss: 0.3788367560505867\n",
      "Epoch 2, Batch 600, Loss: 0.3747306534647942\n",
      "Epoch 2, Batch 650, Loss: 0.38852172046899797\n",
      "Epoch 2, Batch 700, Loss: 0.4058996284008026\n",
      "Epoch 2, Batch 750, Loss: 0.39249758690595626\n",
      "Epoch 2, Batch 800, Loss: 0.39723102390766146\n",
      "Epoch 2, Batch 850, Loss: 0.3864557200670242\n",
      "Epoch 2, Batch 900, Loss: 0.3779333534836769\n",
      "Epoch 3, Batch 50, Loss: 0.3862024858593941\n",
      "Epoch 3, Batch 100, Loss: 0.4026667672395706\n",
      "Epoch 3, Batch 150, Loss: 0.39877672255039215\n",
      "Epoch 3, Batch 200, Loss: 0.3865915834903717\n",
      "Epoch 3, Batch 250, Loss: 0.37418133109807966\n",
      "Epoch 3, Batch 300, Loss: 0.4274016880989075\n",
      "Epoch 3, Batch 350, Loss: 0.39804074823856356\n",
      "Epoch 3, Batch 400, Loss: 0.4350619286298752\n",
      "Epoch 3, Batch 450, Loss: 0.4414079761505127\n",
      "Epoch 3, Batch 500, Loss: 0.41522471606731415\n",
      "Epoch 3, Batch 550, Loss: 0.38501098394393923\n",
      "Epoch 3, Batch 600, Loss: 0.40072160482406616\n",
      "Epoch 3, Batch 650, Loss: 0.4063678902387619\n",
      "Epoch 3, Batch 700, Loss: 0.41321753680706025\n",
      "Epoch 3, Batch 750, Loss: 0.4048173648118973\n",
      "Epoch 3, Batch 800, Loss: 0.4092654240131378\n",
      "Epoch 3, Batch 850, Loss: 0.38387484192848204\n",
      "Epoch 3, Batch 900, Loss: 0.39596947222948076\n",
      "Epoch 4, Batch 50, Loss: 0.4100981652736664\n",
      "Epoch 4, Batch 100, Loss: 0.393761944770813\n",
      "Epoch 4, Batch 150, Loss: 0.3580123630166054\n",
      "Epoch 4, Batch 200, Loss: 0.45741498231887817\n",
      "Epoch 4, Batch 250, Loss: 0.42613498091697694\n",
      "Epoch 4, Batch 300, Loss: 0.3932093560695648\n",
      "Epoch 4, Batch 350, Loss: 0.4301830905675888\n",
      "Epoch 4, Batch 400, Loss: 0.39267298132181166\n",
      "Epoch 4, Batch 450, Loss: 0.4129455471038818\n",
      "Epoch 4, Batch 500, Loss: 0.370309080183506\n",
      "Epoch 4, Batch 550, Loss: 0.39769780933856963\n",
      "Epoch 4, Batch 600, Loss: 0.390935352742672\n",
      "Epoch 4, Batch 650, Loss: 0.4249961793422699\n",
      "Epoch 4, Batch 700, Loss: 0.4063815271854401\n",
      "Epoch 4, Batch 750, Loss: 0.40880309879779814\n",
      "Epoch 4, Batch 800, Loss: 0.40246809720993043\n",
      "Epoch 4, Batch 850, Loss: 0.393247594833374\n",
      "Epoch 4, Batch 900, Loss: 0.43264919191598894\n",
      "Epoch 5, Batch 50, Loss: 0.40271483778953554\n",
      "Epoch 5, Batch 100, Loss: 0.4017810535430908\n",
      "Epoch 5, Batch 150, Loss: 0.4047838068008423\n",
      "Epoch 5, Batch 200, Loss: 0.3939035975933075\n",
      "Epoch 5, Batch 250, Loss: 0.3970827126502991\n",
      "Epoch 5, Batch 300, Loss: 0.34691480934619906\n",
      "Epoch 5, Batch 350, Loss: 0.35930100798606873\n",
      "Epoch 5, Batch 400, Loss: 0.38519165515899656\n",
      "Epoch 5, Batch 450, Loss: 0.38024944096803664\n",
      "Epoch 5, Batch 500, Loss: 0.3952518454194069\n",
      "Epoch 5, Batch 550, Loss: 0.4164454647898674\n",
      "Epoch 5, Batch 600, Loss: 0.4008303272724152\n",
      "Epoch 5, Batch 650, Loss: 0.40274231672286986\n",
      "Epoch 5, Batch 700, Loss: 0.3954817897081375\n",
      "Epoch 5, Batch 750, Loss: 0.38598966002464297\n",
      "Epoch 5, Batch 800, Loss: 0.5224903154373169\n",
      "Epoch 5, Batch 850, Loss: 0.46174342453479766\n",
      "Epoch 5, Batch 900, Loss: 0.39578156352043153\n",
      "Epoch 6, Batch 50, Loss: 0.3880191069841385\n",
      "Epoch 6, Batch 100, Loss: 0.3748316240310669\n",
      "Epoch 6, Batch 150, Loss: 0.369198160469532\n",
      "Epoch 6, Batch 200, Loss: 0.4021892553567886\n",
      "Epoch 6, Batch 250, Loss: 0.39380147486925127\n",
      "Epoch 6, Batch 300, Loss: 0.3941578581929207\n",
      "Epoch 6, Batch 350, Loss: 0.40042474001646045\n",
      "Epoch 6, Batch 400, Loss: 0.4135500764846802\n",
      "Epoch 6, Batch 450, Loss: 0.3970347654819488\n",
      "Epoch 6, Batch 500, Loss: 0.42636068910360336\n",
      "Epoch 6, Batch 550, Loss: 0.39509392380714414\n",
      "Epoch 6, Batch 600, Loss: 0.3630310347676277\n",
      "Epoch 6, Batch 650, Loss: 0.3794764012098312\n",
      "Epoch 6, Batch 700, Loss: 0.4115534162521362\n",
      "Epoch 6, Batch 750, Loss: 0.40544253021478655\n",
      "Epoch 6, Batch 800, Loss: 0.38907755792140963\n",
      "Epoch 6, Batch 850, Loss: 0.35632685244083406\n",
      "Epoch 6, Batch 900, Loss: 0.40241905868053435\n",
      "Epoch 7, Batch 50, Loss: 0.4260877430438995\n",
      "Epoch 7, Batch 100, Loss: 0.3955940741300583\n",
      "Epoch 7, Batch 150, Loss: 0.42771868288517\n",
      "Epoch 7, Batch 200, Loss: 0.47071658819913864\n",
      "Epoch 7, Batch 250, Loss: 0.4355179435014725\n",
      "Epoch 7, Batch 300, Loss: 0.41676964342594147\n",
      "Epoch 7, Batch 350, Loss: 0.42523289024829863\n",
      "Epoch 7, Batch 400, Loss: 0.39845327615737913\n",
      "Epoch 7, Batch 450, Loss: 0.3778214204311371\n",
      "Epoch 7, Batch 500, Loss: 0.3979254174232483\n",
      "Epoch 7, Batch 550, Loss: 0.3791402658820152\n",
      "Epoch 7, Batch 600, Loss: 0.3851511445641518\n",
      "Epoch 7, Batch 650, Loss: 0.398424571454525\n",
      "Epoch 7, Batch 700, Loss: 0.39626959562301634\n",
      "Epoch 7, Batch 750, Loss: 0.4024829226732254\n",
      "Epoch 7, Batch 800, Loss: 0.3814353781938553\n",
      "Epoch 7, Batch 850, Loss: 0.4056092056632042\n",
      "Epoch 7, Batch 900, Loss: 0.4518111383914947\n",
      "Epoch 8, Batch 50, Loss: 0.4038490676879883\n",
      "Epoch 8, Batch 100, Loss: 0.4109204989671707\n",
      "Epoch 8, Batch 150, Loss: 0.37890889585018156\n",
      "Epoch 8, Batch 200, Loss: 0.4680066275596619\n",
      "Epoch 8, Batch 250, Loss: 0.4510014939308167\n",
      "Epoch 8, Batch 300, Loss: 0.39014327853918074\n",
      "Epoch 8, Batch 350, Loss: 0.4142670261859894\n",
      "Epoch 8, Batch 400, Loss: 0.39889724910259244\n",
      "Epoch 8, Batch 450, Loss: 0.4105485111474991\n",
      "Epoch 8, Batch 500, Loss: 0.3752877661585808\n",
      "Epoch 8, Batch 550, Loss: 0.3833853632211685\n",
      "Epoch 8, Batch 600, Loss: 0.3645792230963707\n",
      "Epoch 8, Batch 650, Loss: 0.3755487164855003\n",
      "Epoch 8, Batch 700, Loss: 0.4543368422985077\n",
      "Epoch 8, Batch 750, Loss: 0.42424335777759553\n",
      "Epoch 8, Batch 800, Loss: 0.40312077283859254\n",
      "Epoch 8, Batch 850, Loss: 0.38315669894218446\n",
      "Epoch 8, Batch 900, Loss: 0.4062080174684525\n",
      "Epoch 9, Batch 50, Loss: 0.38247104078531263\n",
      "Epoch 9, Batch 100, Loss: 0.36731008797883985\n",
      "Epoch 9, Batch 150, Loss: 0.4015271678566933\n",
      "Epoch 9, Batch 200, Loss: 0.41172618329524996\n",
      "Epoch 9, Batch 250, Loss: 0.40420963495969775\n",
      "Epoch 9, Batch 300, Loss: 0.4115801456570625\n",
      "Epoch 9, Batch 350, Loss: 0.3882991999387741\n",
      "Epoch 9, Batch 400, Loss: 0.4042328354716301\n",
      "Epoch 9, Batch 450, Loss: 0.3966373723745346\n",
      "Epoch 9, Batch 500, Loss: 0.38145047456026077\n",
      "Epoch 9, Batch 550, Loss: 0.39184282630681994\n",
      "Epoch 9, Batch 600, Loss: 0.4012486273050308\n",
      "Epoch 9, Batch 650, Loss: 0.35947619587183\n",
      "Epoch 9, Batch 700, Loss: 0.3828615480661392\n",
      "Epoch 9, Batch 750, Loss: 0.3768038034439087\n",
      "Epoch 9, Batch 800, Loss: 0.4022031080722809\n",
      "Epoch 9, Batch 850, Loss: 0.4011545044183731\n",
      "Epoch 9, Batch 900, Loss: 0.42888486206531523\n",
      "Epoch 10, Batch 50, Loss: 0.3844519627094269\n",
      "Epoch 10, Batch 100, Loss: 0.43722590804100037\n",
      "Epoch 10, Batch 150, Loss: 0.4053378474712372\n",
      "Epoch 10, Batch 200, Loss: 0.39319857716560364\n",
      "Epoch 10, Batch 250, Loss: 0.4380532336235046\n",
      "Epoch 10, Batch 300, Loss: 0.411397288441658\n",
      "Epoch 10, Batch 350, Loss: 0.37768483221530913\n",
      "Epoch 10, Batch 400, Loss: 0.40213595926761625\n",
      "Epoch 10, Batch 450, Loss: 0.37422109156847\n",
      "Epoch 10, Batch 500, Loss: 0.3776421368122101\n",
      "Epoch 10, Batch 550, Loss: 0.40094545125961306\n",
      "Epoch 10, Batch 600, Loss: 0.38290371626615527\n",
      "Epoch 10, Batch 650, Loss: 0.42282390177249907\n",
      "Epoch 10, Batch 700, Loss: 0.40839147090911865\n",
      "Epoch 10, Batch 750, Loss: 0.4204263758659363\n",
      "Epoch 10, Batch 800, Loss: 0.4283577519655228\n",
      "Epoch 10, Batch 850, Loss: 0.41599147737026215\n",
      "Epoch 10, Batch 900, Loss: 0.4093177753686905\n",
      "Epoch 11, Batch 50, Loss: 0.42572155356407165\n",
      "Epoch 11, Batch 100, Loss: 0.37686603724956513\n",
      "Epoch 11, Batch 150, Loss: 0.43043369472026827\n",
      "Epoch 11, Batch 200, Loss: 0.4024271804094315\n",
      "Epoch 11, Batch 250, Loss: 0.4077368053793907\n",
      "Epoch 11, Batch 300, Loss: 0.4023208004236221\n",
      "Epoch 11, Batch 350, Loss: 0.3893875068426132\n",
      "Epoch 11, Batch 400, Loss: 0.3821604335308075\n",
      "Epoch 11, Batch 450, Loss: 0.41743437588214877\n",
      "Epoch 11, Batch 500, Loss: 0.37146952986717224\n",
      "Epoch 11, Batch 550, Loss: 0.3714272803068161\n",
      "Epoch 11, Batch 600, Loss: 0.3945813846588135\n",
      "Epoch 11, Batch 650, Loss: 0.3974155676364899\n",
      "Epoch 11, Batch 700, Loss: 0.40747941881418226\n",
      "Epoch 11, Batch 750, Loss: 0.3841512688994408\n",
      "Epoch 11, Batch 800, Loss: 0.3558818045258522\n",
      "Epoch 11, Batch 850, Loss: 0.3919536286592484\n",
      "Epoch 11, Batch 900, Loss: 0.3910395091772079\n",
      "Epoch 12, Batch 50, Loss: 0.394891517162323\n",
      "Epoch 12, Batch 100, Loss: 0.36242107808589935\n",
      "Epoch 12, Batch 150, Loss: 0.41401428282260894\n",
      "Epoch 12, Batch 200, Loss: 0.39876523822546006\n",
      "Epoch 12, Batch 250, Loss: 0.37924111396074295\n",
      "Epoch 12, Batch 300, Loss: 0.4222205138206482\n",
      "Epoch 12, Batch 350, Loss: 0.40797188878059387\n",
      "Epoch 12, Batch 400, Loss: 0.39088760673999784\n",
      "Epoch 12, Batch 450, Loss: 0.3961014869809151\n",
      "Epoch 12, Batch 500, Loss: 0.38393665075302125\n",
      "Epoch 12, Batch 550, Loss: 0.3912057003378868\n",
      "Epoch 12, Batch 600, Loss: 0.4214174973964691\n",
      "Epoch 12, Batch 650, Loss: 0.3987450897693634\n",
      "Epoch 12, Batch 700, Loss: 0.41835876166820524\n",
      "Epoch 12, Batch 750, Loss: 0.40450161397457124\n",
      "Epoch 12, Batch 800, Loss: 0.3829217028617859\n",
      "Epoch 12, Batch 850, Loss: 0.3696556967496872\n",
      "Epoch 12, Batch 900, Loss: 0.37016322404146196\n",
      "Epoch 13, Batch 50, Loss: 0.38812034606933593\n",
      "Epoch 13, Batch 100, Loss: 0.397221248447895\n",
      "Epoch 13, Batch 150, Loss: 0.41362730383872987\n",
      "Epoch 13, Batch 200, Loss: 0.3876097273826599\n",
      "Epoch 13, Batch 250, Loss: 0.4158261716365814\n",
      "Epoch 13, Batch 300, Loss: 0.4255708113312721\n",
      "Epoch 13, Batch 350, Loss: 0.3962609574198723\n",
      "Epoch 13, Batch 400, Loss: 0.3840623730421066\n",
      "Epoch 13, Batch 450, Loss: 0.39216644048690796\n",
      "Epoch 13, Batch 500, Loss: 0.3997948557138443\n",
      "Epoch 13, Batch 550, Loss: 0.4040889674425125\n",
      "Epoch 13, Batch 600, Loss: 0.4347037094831467\n",
      "Epoch 13, Batch 650, Loss: 0.38722658455371856\n",
      "Epoch 13, Batch 700, Loss: 0.40268358945846555\n",
      "Epoch 13, Batch 750, Loss: 0.40107019007205963\n",
      "Epoch 13, Batch 800, Loss: 0.36174401015043256\n",
      "Epoch 13, Batch 850, Loss: 0.4319940161705017\n",
      "Epoch 13, Batch 900, Loss: 0.3637387180328369\n",
      "Epoch 14, Batch 50, Loss: 0.37833759367465974\n",
      "Epoch 14, Batch 100, Loss: 0.35762031346559525\n",
      "Epoch 14, Batch 150, Loss: 0.38290424704551695\n",
      "Epoch 14, Batch 200, Loss: 0.3968410933017731\n",
      "Epoch 14, Batch 250, Loss: 0.4088096934556961\n",
      "Epoch 14, Batch 300, Loss: 0.3905634951591492\n",
      "Epoch 14, Batch 350, Loss: 0.3718503224849701\n",
      "Epoch 14, Batch 400, Loss: 0.4360235530138016\n",
      "Epoch 14, Batch 450, Loss: 0.4206578031182289\n",
      "Epoch 14, Batch 500, Loss: 0.43549929797649384\n",
      "Epoch 14, Batch 550, Loss: 0.40266085147857666\n",
      "Epoch 14, Batch 600, Loss: 0.38275155663490296\n",
      "Epoch 14, Batch 650, Loss: 0.38728298783302306\n",
      "Epoch 14, Batch 700, Loss: 0.3793064558506012\n",
      "Epoch 14, Batch 750, Loss: 0.3827369001507759\n",
      "Epoch 14, Batch 800, Loss: 0.38124253004789355\n",
      "Epoch 14, Batch 850, Loss: 0.37936390936374664\n",
      "Epoch 14, Batch 900, Loss: 0.3790603131055832\n",
      "Epoch 15, Batch 50, Loss: 0.4091908210515976\n",
      "Epoch 15, Batch 100, Loss: 0.42192883908748624\n",
      "Epoch 15, Batch 150, Loss: 0.39848288387060166\n",
      "Epoch 15, Batch 200, Loss: 0.3951633867621422\n",
      "Epoch 15, Batch 250, Loss: 0.38876197427511217\n",
      "Epoch 15, Batch 300, Loss: 0.40573864221572875\n",
      "Epoch 15, Batch 350, Loss: 0.3956497412919998\n",
      "Epoch 15, Batch 400, Loss: 0.36297612071037294\n",
      "Epoch 15, Batch 450, Loss: 0.39345976144075395\n",
      "Epoch 15, Batch 500, Loss: 0.37449416756629944\n",
      "Epoch 15, Batch 550, Loss: 0.3934978204965591\n",
      "Epoch 15, Batch 600, Loss: 0.3846788376569748\n",
      "Epoch 15, Batch 650, Loss: 0.382089062333107\n",
      "Epoch 15, Batch 700, Loss: 0.38734937489032745\n",
      "Epoch 15, Batch 750, Loss: 0.3910176008939743\n",
      "Epoch 15, Batch 800, Loss: 0.45225634455680847\n",
      "Epoch 15, Batch 850, Loss: 0.386412553191185\n",
      "Epoch 15, Batch 900, Loss: 0.4297477787733078\n",
      "Epoch 16, Batch 50, Loss: 0.39275729954242705\n",
      "Epoch 16, Batch 100, Loss: 0.3830377185344696\n",
      "Epoch 16, Batch 150, Loss: 0.378226038813591\n",
      "Epoch 16, Batch 200, Loss: 0.41905765891075136\n",
      "Epoch 16, Batch 250, Loss: 0.3837831202149391\n",
      "Epoch 16, Batch 300, Loss: 0.36799472659826277\n",
      "Epoch 16, Batch 350, Loss: 0.3791733726859093\n",
      "Epoch 16, Batch 400, Loss: 0.40693304300308225\n",
      "Epoch 16, Batch 450, Loss: 0.37892698794603347\n",
      "Epoch 16, Batch 500, Loss: 0.4376268929243088\n",
      "Epoch 16, Batch 550, Loss: 0.38450897604227063\n",
      "Epoch 16, Batch 600, Loss: 0.37911740750074385\n",
      "Epoch 16, Batch 650, Loss: 0.4203263384103775\n",
      "Epoch 16, Batch 700, Loss: 0.4133008408546448\n",
      "Epoch 16, Batch 750, Loss: 0.36460215657949446\n",
      "Epoch 16, Batch 800, Loss: 0.3780786091089249\n",
      "Epoch 16, Batch 850, Loss: 0.4098805177211762\n",
      "Epoch 16, Batch 900, Loss: 0.37481857001781466\n",
      "Epoch 17, Batch 50, Loss: 0.4098953351378441\n",
      "Epoch 17, Batch 100, Loss: 0.38039616465568543\n",
      "Epoch 17, Batch 150, Loss: 0.40421819806098935\n",
      "Epoch 17, Batch 200, Loss: 0.42688786149024965\n",
      "Epoch 17, Batch 250, Loss: 0.3668260616064072\n",
      "Epoch 17, Batch 300, Loss: 0.38412074506282806\n",
      "Epoch 17, Batch 350, Loss: 0.39263635098934174\n",
      "Epoch 17, Batch 400, Loss: 0.37738984495401384\n",
      "Epoch 17, Batch 450, Loss: 0.3979597574472427\n",
      "Epoch 17, Batch 500, Loss: 0.4037086349725723\n",
      "Epoch 17, Batch 550, Loss: 0.4081756502389908\n",
      "Epoch 17, Batch 600, Loss: 0.3835108208656311\n",
      "Epoch 17, Batch 650, Loss: 0.4240723389387131\n",
      "Epoch 17, Batch 700, Loss: 0.38940471291542056\n",
      "Epoch 17, Batch 750, Loss: 0.37727262526750566\n",
      "Epoch 17, Batch 800, Loss: 0.4127655291557312\n",
      "Epoch 17, Batch 850, Loss: 0.40488249242305757\n",
      "Epoch 17, Batch 900, Loss: 0.3977870428562164\n",
      "Epoch 18, Batch 50, Loss: 0.3890839576721191\n",
      "Epoch 18, Batch 100, Loss: 0.37581797659397126\n",
      "Epoch 18, Batch 150, Loss: 0.4084485584497452\n",
      "Epoch 18, Batch 200, Loss: 0.38744695246219635\n",
      "Epoch 18, Batch 250, Loss: 0.40210235089063645\n",
      "Epoch 18, Batch 300, Loss: 0.4071249285340309\n",
      "Epoch 18, Batch 350, Loss: 0.40841870844364164\n",
      "Epoch 18, Batch 400, Loss: 0.38283525466918944\n",
      "Epoch 18, Batch 450, Loss: 0.38271173417568205\n",
      "Epoch 18, Batch 500, Loss: 0.3665089276432991\n",
      "Epoch 18, Batch 550, Loss: 0.42360159397125247\n",
      "Epoch 18, Batch 600, Loss: 0.3714394387602806\n",
      "Epoch 18, Batch 650, Loss: 0.38492807567119597\n",
      "Epoch 18, Batch 700, Loss: 0.4096233105659485\n",
      "Epoch 18, Batch 750, Loss: 0.38324175000190736\n",
      "Epoch 18, Batch 800, Loss: 0.40250982224941256\n",
      "Epoch 18, Batch 850, Loss: 0.4098041695356369\n",
      "Epoch 18, Batch 900, Loss: 0.36299406200647355\n",
      "Epoch 19, Batch 50, Loss: 0.5201024186611175\n",
      "Epoch 19, Batch 100, Loss: 0.4500015479326248\n",
      "Epoch 19, Batch 150, Loss: 0.44030154645442965\n",
      "Epoch 19, Batch 200, Loss: 0.4645107990503311\n",
      "Epoch 19, Batch 250, Loss: 0.40637645304203035\n",
      "Epoch 19, Batch 300, Loss: 0.41020254224538805\n",
      "Epoch 19, Batch 350, Loss: 0.42019560158252717\n",
      "Epoch 19, Batch 400, Loss: 0.4037121880054474\n",
      "Epoch 19, Batch 450, Loss: 0.4406202435493469\n",
      "Epoch 19, Batch 500, Loss: 0.38559045404195785\n",
      "Epoch 19, Batch 550, Loss: 0.39925344675779345\n",
      "Epoch 19, Batch 600, Loss: 0.38351319104433057\n",
      "Epoch 19, Batch 650, Loss: 0.3536640253663063\n",
      "Epoch 19, Batch 700, Loss: 0.3898939204216003\n",
      "Epoch 19, Batch 750, Loss: 0.4004263937473297\n",
      "Epoch 19, Batch 800, Loss: 0.42150326937437055\n",
      "Epoch 19, Batch 850, Loss: 0.41629610031843184\n",
      "Epoch 19, Batch 900, Loss: 0.4190998476743698\n",
      "Epoch 20, Batch 50, Loss: 0.39060123950243\n",
      "Epoch 20, Batch 100, Loss: 0.38811651051044466\n",
      "Epoch 20, Batch 150, Loss: 0.3845228844881058\n",
      "Epoch 20, Batch 200, Loss: 0.3971190541982651\n",
      "Epoch 20, Batch 250, Loss: 0.3967192763090134\n",
      "Epoch 20, Batch 300, Loss: 0.37968576908111573\n",
      "Epoch 20, Batch 350, Loss: 0.4281582689285278\n",
      "Epoch 20, Batch 400, Loss: 0.4333011454343796\n",
      "Epoch 20, Batch 450, Loss: 0.3904372727870941\n",
      "Epoch 20, Batch 500, Loss: 0.3787423127889633\n",
      "Epoch 20, Batch 550, Loss: 0.36890161752700806\n",
      "Epoch 20, Batch 600, Loss: 0.4045917177200317\n",
      "Epoch 20, Batch 650, Loss: 0.4158349123597145\n",
      "Epoch 20, Batch 700, Loss: 0.3880724009871483\n",
      "Epoch 20, Batch 750, Loss: 0.3980250933766365\n",
      "Epoch 20, Batch 800, Loss: 0.3769111502170563\n",
      "Epoch 20, Batch 850, Loss: 0.36157103031873705\n",
      "Epoch 20, Batch 900, Loss: 0.3838652759790421\n",
      "Epoch 21, Batch 50, Loss: 0.38153443396091463\n",
      "Epoch 21, Batch 100, Loss: 0.3718389394879341\n",
      "Epoch 21, Batch 150, Loss: 0.42082769334316256\n",
      "Epoch 21, Batch 200, Loss: 0.40575035810470583\n",
      "Epoch 21, Batch 250, Loss: 0.3786561751365662\n",
      "Epoch 21, Batch 300, Loss: 0.3723034808039665\n",
      "Epoch 21, Batch 350, Loss: 0.40404289424419404\n",
      "Epoch 21, Batch 400, Loss: 0.3785318937897682\n",
      "Epoch 21, Batch 450, Loss: 0.3528763699531555\n",
      "Epoch 21, Batch 500, Loss: 0.37330397456884384\n",
      "Epoch 21, Batch 550, Loss: 0.38337048918008804\n",
      "Epoch 21, Batch 600, Loss: 0.4082407915592194\n",
      "Epoch 21, Batch 650, Loss: 0.40410279273986816\n",
      "Epoch 21, Batch 700, Loss: 0.3890389612317085\n",
      "Epoch 21, Batch 750, Loss: 0.37926636189222335\n",
      "Epoch 21, Batch 800, Loss: 0.3898183912038803\n",
      "Epoch 21, Batch 850, Loss: 0.397253497838974\n",
      "Epoch 21, Batch 900, Loss: 0.38096722662448884\n",
      "Epoch 22, Batch 50, Loss: 0.3962043291330338\n",
      "Epoch 22, Batch 100, Loss: 0.3891712588071823\n",
      "Epoch 22, Batch 150, Loss: 0.4015221470594406\n",
      "Epoch 22, Batch 200, Loss: 0.38932319641113283\n",
      "Epoch 22, Batch 250, Loss: 0.4605924451351166\n",
      "Epoch 22, Batch 300, Loss: 0.4585151183605194\n",
      "Epoch 22, Batch 350, Loss: 0.3794463902711868\n",
      "Epoch 22, Batch 400, Loss: 0.40028888881206515\n",
      "Epoch 22, Batch 450, Loss: 0.4064572423696518\n",
      "Epoch 22, Batch 500, Loss: 0.39832112461328506\n",
      "Epoch 22, Batch 550, Loss: 0.3863156545162201\n",
      "Epoch 22, Batch 600, Loss: 0.37631144672632216\n",
      "Epoch 22, Batch 650, Loss: 0.40176387012004855\n",
      "Epoch 22, Batch 700, Loss: 0.40937221586704253\n",
      "Epoch 22, Batch 750, Loss: 0.37906604915857317\n",
      "Epoch 22, Batch 800, Loss: 0.4069687366485596\n",
      "Epoch 22, Batch 850, Loss: 0.40027651488780974\n",
      "Epoch 22, Batch 900, Loss: 0.3544914650917053\n",
      "Epoch 23, Batch 50, Loss: 0.41913813710212705\n",
      "Epoch 23, Batch 100, Loss: 0.38030365645885467\n",
      "Epoch 23, Batch 150, Loss: 0.37119971692562104\n",
      "Epoch 23, Batch 200, Loss: 0.41614550471305845\n",
      "Epoch 23, Batch 250, Loss: 0.373320926129818\n",
      "Epoch 23, Batch 300, Loss: 0.38800735235214234\n",
      "Epoch 23, Batch 350, Loss: 0.40315047562122347\n",
      "Epoch 23, Batch 400, Loss: 0.3697688347101212\n",
      "Epoch 23, Batch 450, Loss: 0.4455139857530594\n",
      "Epoch 23, Batch 500, Loss: 0.39425742149353027\n",
      "Epoch 23, Batch 550, Loss: 0.391181743144989\n",
      "Epoch 23, Batch 600, Loss: 0.397872565984726\n",
      "Epoch 23, Batch 650, Loss: 0.39512389004230497\n",
      "Epoch 23, Batch 700, Loss: 0.35895501136779784\n",
      "Epoch 23, Batch 750, Loss: 0.3937122762203217\n",
      "Epoch 23, Batch 800, Loss: 0.4096551966667175\n",
      "Epoch 23, Batch 850, Loss: 0.3723417192697525\n",
      "Epoch 23, Batch 900, Loss: 0.38562207043170926\n",
      "Epoch 24, Batch 50, Loss: 0.37403806000947953\n",
      "Epoch 24, Batch 100, Loss: 0.39304849445819856\n",
      "Epoch 24, Batch 150, Loss: 0.3837832963466644\n",
      "Epoch 24, Batch 200, Loss: 0.37378220140933993\n",
      "Epoch 24, Batch 250, Loss: 0.4097632449865341\n",
      "Epoch 24, Batch 300, Loss: 0.3624057999253273\n",
      "Epoch 24, Batch 350, Loss: 0.37346395999193194\n",
      "Epoch 24, Batch 400, Loss: 0.3876087838411331\n",
      "Epoch 24, Batch 450, Loss: 0.4057648730278015\n",
      "Epoch 24, Batch 500, Loss: 0.3633349335193634\n",
      "Epoch 24, Batch 550, Loss: 0.40660777390003205\n",
      "Epoch 24, Batch 600, Loss: 0.43844338953495027\n",
      "Epoch 24, Batch 650, Loss: 0.3938295215368271\n",
      "Epoch 24, Batch 700, Loss: 0.4098466926813126\n",
      "Epoch 24, Batch 750, Loss: 0.37679669231176377\n",
      "Epoch 24, Batch 800, Loss: 0.39946460366249087\n",
      "Epoch 24, Batch 850, Loss: 0.38352995574474336\n",
      "Epoch 24, Batch 900, Loss: 0.39702720642089845\n",
      "Epoch 25, Batch 50, Loss: 0.3854436594247818\n",
      "Epoch 25, Batch 100, Loss: 0.4081320223212242\n",
      "Epoch 25, Batch 150, Loss: 0.4101240408420563\n",
      "Epoch 25, Batch 200, Loss: 0.3853318491578102\n",
      "Epoch 25, Batch 250, Loss: 0.38971614122390746\n",
      "Epoch 25, Batch 300, Loss: 0.4213848623633385\n",
      "Epoch 25, Batch 350, Loss: 0.4105279639363289\n",
      "Epoch 25, Batch 400, Loss: 0.39734633088111876\n",
      "Epoch 25, Batch 450, Loss: 0.3821136376261711\n",
      "Epoch 25, Batch 500, Loss: 0.393521890938282\n",
      "Epoch 25, Batch 550, Loss: 0.39440890073776247\n",
      "Epoch 25, Batch 600, Loss: 0.40298370361328123\n",
      "Epoch 25, Batch 650, Loss: 0.3720988243818283\n",
      "Epoch 25, Batch 700, Loss: 0.3967318961024284\n",
      "Epoch 25, Batch 750, Loss: 0.39761200696229937\n",
      "Epoch 25, Batch 800, Loss: 0.38535246431827547\n",
      "Epoch 25, Batch 850, Loss: 0.4194447535276413\n",
      "Epoch 25, Batch 900, Loss: 0.3932585850358009\n",
      "Accuracy on test set: 0.719%\n",
      "Fitting for combination 27\n",
      "784\n",
      "2\n",
      "10\n",
      "[40, 20, 10]\n",
      "True\n",
      "['relu', 'sigmoid']\n",
      "SGD\n",
      "0.01\n",
      "0.1\n",
      "CrossEntropyLoss\n",
      "200\n",
      "25\n",
      "0\n",
      "Epoch 1, Batch 200, Loss: 4.6093905782699585\n",
      "Epoch 1, Batch 400, Loss: 4.539248795509338\n",
      "Epoch 1, Batch 600, Loss: 4.5046940565109255\n",
      "Epoch 1, Batch 800, Loss: 4.468139572143555\n",
      "Epoch 2, Batch 200, Loss: 4.346212794780731\n",
      "Epoch 2, Batch 400, Loss: 4.218851194381714\n",
      "Epoch 2, Batch 600, Loss: 4.072041283845902\n",
      "Epoch 2, Batch 800, Loss: 3.946016558408737\n",
      "Epoch 3, Batch 200, Loss: 3.808511972427368\n",
      "Epoch 3, Batch 400, Loss: 3.7618075370788575\n",
      "Epoch 3, Batch 600, Loss: 3.7326811504364015\n",
      "Epoch 3, Batch 800, Loss: 3.7057637822628022\n",
      "Epoch 4, Batch 200, Loss: 3.682435084581375\n",
      "Epoch 4, Batch 400, Loss: 3.684164490699768\n",
      "Epoch 4, Batch 600, Loss: 3.6789940750598906\n",
      "Epoch 4, Batch 800, Loss: 3.668833099603653\n",
      "Epoch 5, Batch 200, Loss: 3.6743488907814026\n",
      "Epoch 5, Batch 400, Loss: 3.6644053900241853\n",
      "Epoch 5, Batch 600, Loss: 3.6639026379585267\n",
      "Epoch 5, Batch 800, Loss: 3.674998379945755\n",
      "Epoch 6, Batch 200, Loss: 3.6718349158763885\n",
      "Epoch 6, Batch 400, Loss: 3.667156678438187\n",
      "Epoch 6, Batch 600, Loss: 3.6762611436843873\n",
      "Epoch 6, Batch 800, Loss: 3.6713276255130767\n",
      "Epoch 7, Batch 200, Loss: 3.677019863128662\n",
      "Epoch 7, Batch 400, Loss: 3.6756884622573853\n",
      "Epoch 7, Batch 600, Loss: 3.680903146266937\n",
      "Epoch 7, Batch 800, Loss: 3.6751252126693728\n",
      "Epoch 8, Batch 200, Loss: 3.6783218538761138\n",
      "Epoch 8, Batch 400, Loss: 3.682995915412903\n",
      "Epoch 8, Batch 600, Loss: 3.684275871515274\n",
      "Epoch 8, Batch 800, Loss: 3.6793095099925996\n",
      "Epoch 9, Batch 200, Loss: 3.679962681531906\n",
      "Epoch 9, Batch 400, Loss: 3.680124388933182\n",
      "Epoch 9, Batch 600, Loss: 3.6793916428089144\n",
      "Epoch 9, Batch 800, Loss: 3.686231874227524\n",
      "Epoch 10, Batch 200, Loss: 3.683979631662369\n",
      "Epoch 10, Batch 400, Loss: 3.679509493112564\n",
      "Epoch 10, Batch 600, Loss: 3.676169980764389\n",
      "Epoch 10, Batch 800, Loss: 3.679522408246994\n",
      "Epoch 11, Batch 200, Loss: 3.6765900647640226\n",
      "Epoch 11, Batch 400, Loss: 3.6767872524261476\n",
      "Epoch 11, Batch 600, Loss: 3.677301642894745\n",
      "Epoch 11, Batch 800, Loss: 3.6822068107128145\n",
      "Epoch 12, Batch 200, Loss: 3.6733664369583128\n",
      "Epoch 12, Batch 400, Loss: 3.669484119415283\n",
      "Epoch 12, Batch 600, Loss: 3.677664235830307\n",
      "Epoch 12, Batch 800, Loss: 3.6816545474529265\n",
      "Epoch 13, Batch 200, Loss: 3.676458158493042\n",
      "Epoch 13, Batch 400, Loss: 3.678647834062576\n",
      "Epoch 13, Batch 600, Loss: 3.678934192657471\n",
      "Epoch 13, Batch 800, Loss: 3.673016803264618\n",
      "Epoch 14, Batch 200, Loss: 3.6843851709365847\n",
      "Epoch 14, Batch 400, Loss: 3.674962160587311\n",
      "Epoch 14, Batch 600, Loss: 3.674112728834152\n",
      "Epoch 14, Batch 800, Loss: 3.675495208501816\n",
      "Epoch 15, Batch 200, Loss: 3.673313270807266\n",
      "Epoch 15, Batch 400, Loss: 3.6804996716976164\n",
      "Epoch 15, Batch 600, Loss: 3.6724161100387573\n",
      "Epoch 15, Batch 800, Loss: 3.681651269197464\n",
      "Epoch 16, Batch 200, Loss: 3.674938416481018\n",
      "Epoch 16, Batch 400, Loss: 3.6823245537281037\n",
      "Epoch 16, Batch 600, Loss: 3.670759356021881\n",
      "Epoch 16, Batch 800, Loss: 3.67643221616745\n",
      "Epoch 17, Batch 200, Loss: 3.6773489499092102\n",
      "Epoch 17, Batch 400, Loss: 3.6848128056526184\n",
      "Epoch 17, Batch 600, Loss: 3.6768665885925294\n",
      "Epoch 17, Batch 800, Loss: 3.6702371859550476\n",
      "Epoch 18, Batch 200, Loss: 3.6790312302112578\n",
      "Epoch 18, Batch 400, Loss: 3.6731476354599\n",
      "Epoch 18, Batch 600, Loss: 3.683851352930069\n",
      "Epoch 18, Batch 800, Loss: 3.678525671958923\n",
      "Epoch 19, Batch 200, Loss: 3.683850053548813\n",
      "Epoch 19, Batch 400, Loss: 3.672175589799881\n",
      "Epoch 19, Batch 600, Loss: 3.6721832084655763\n",
      "Epoch 19, Batch 800, Loss: 3.6766725754737855\n",
      "Epoch 20, Batch 200, Loss: 3.6771212112903595\n",
      "Epoch 20, Batch 400, Loss: 3.6716378831863405\n",
      "Epoch 20, Batch 600, Loss: 3.678310662508011\n",
      "Epoch 20, Batch 800, Loss: 3.67777756690979\n",
      "Epoch 21, Batch 200, Loss: 3.677065873146057\n",
      "Epoch 21, Batch 400, Loss: 3.6802392315864565\n",
      "Epoch 21, Batch 600, Loss: 3.676523698568344\n",
      "Epoch 21, Batch 800, Loss: 3.676320670843124\n",
      "Epoch 22, Batch 200, Loss: 3.6778217935562134\n",
      "Epoch 22, Batch 400, Loss: 3.6770095729827883\n",
      "Epoch 22, Batch 600, Loss: 3.6839807486534117\n",
      "Epoch 22, Batch 800, Loss: 3.6676780343055726\n",
      "Epoch 23, Batch 200, Loss: 3.682385814189911\n",
      "Epoch 23, Batch 400, Loss: 3.6775303184986115\n",
      "Epoch 23, Batch 600, Loss: 3.673370233774185\n",
      "Epoch 23, Batch 800, Loss: 3.6688808119297027\n",
      "Epoch 24, Batch 200, Loss: 3.6793474733829497\n",
      "Epoch 24, Batch 400, Loss: 3.675255173444748\n",
      "Epoch 24, Batch 600, Loss: 3.677414371967316\n",
      "Epoch 24, Batch 800, Loss: 3.6759915590286254\n",
      "Epoch 25, Batch 200, Loss: 3.6784679925441743\n",
      "Epoch 25, Batch 400, Loss: 3.675598884820938\n",
      "Epoch 25, Batch 600, Loss: 3.679103100299835\n",
      "Epoch 25, Batch 800, Loss: 3.673099774122238\n",
      "Accuracy on test set: 0.2434%\n",
      "Fitting for combination 28\n",
      "784\n",
      "2\n",
      "10\n",
      "[40, 30, 10]\n",
      "True\n",
      "['relu', 'tanh']\n",
      "Adam\n",
      "0.3\n",
      "0.01\n",
      "CrossEntropyLoss\n",
      "200\n",
      "25\n",
      "0\n",
      "Epoch 1, Batch 200, Loss: 6.481494691371918\n",
      "Epoch 1, Batch 400, Loss: 6.047985405921936\n",
      "Epoch 1, Batch 600, Loss: 5.884886271953583\n",
      "Epoch 1, Batch 800, Loss: 6.190352790355682\n",
      "Epoch 2, Batch 200, Loss: 6.123599259853363\n",
      "Epoch 2, Batch 400, Loss: 6.308322718143463\n",
      "Epoch 2, Batch 600, Loss: 5.808686299324036\n",
      "Epoch 2, Batch 800, Loss: 6.385411605834961\n",
      "Epoch 3, Batch 200, Loss: 5.690359311103821\n",
      "Epoch 3, Batch 400, Loss: 6.3742199444770815\n",
      "Epoch 3, Batch 600, Loss: 6.037139160633087\n",
      "Epoch 3, Batch 800, Loss: 6.293621859550476\n",
      "Epoch 4, Batch 200, Loss: 5.8603355717659\n",
      "Epoch 4, Batch 400, Loss: 6.699003176689148\n",
      "Epoch 4, Batch 600, Loss: 5.650732455253601\n",
      "Epoch 4, Batch 800, Loss: 6.108822135925293\n",
      "Epoch 5, Batch 200, Loss: 6.628483784198761\n",
      "Epoch 5, Batch 400, Loss: 6.242588307857513\n",
      "Epoch 5, Batch 600, Loss: 6.039754960536957\n",
      "Epoch 5, Batch 800, Loss: 6.930202031135559\n",
      "Epoch 6, Batch 200, Loss: 5.877644693851471\n",
      "Epoch 6, Batch 400, Loss: 6.337762734889984\n",
      "Epoch 6, Batch 600, Loss: 5.492801377773285\n",
      "Epoch 6, Batch 800, Loss: 6.655521831512451\n",
      "Epoch 7, Batch 200, Loss: 5.941089534759522\n",
      "Epoch 7, Batch 400, Loss: 6.620708228349685\n",
      "Epoch 7, Batch 600, Loss: 6.184827795028687\n",
      "Epoch 7, Batch 800, Loss: 6.1244851899147035\n",
      "Epoch 8, Batch 200, Loss: 6.040640823841095\n",
      "Epoch 8, Batch 400, Loss: 6.019799859523773\n",
      "Epoch 8, Batch 600, Loss: 6.591570460796357\n",
      "Epoch 8, Batch 800, Loss: 6.244972772598267\n",
      "Epoch 9, Batch 200, Loss: 5.911654222011566\n",
      "Epoch 9, Batch 400, Loss: 6.412189042568206\n",
      "Epoch 9, Batch 600, Loss: 6.538202569484711\n",
      "Epoch 9, Batch 800, Loss: 6.585902307033539\n",
      "Epoch 10, Batch 200, Loss: 6.825254373550415\n",
      "Epoch 10, Batch 400, Loss: 6.05556875705719\n",
      "Epoch 10, Batch 600, Loss: 5.776630563735962\n",
      "Epoch 10, Batch 800, Loss: 5.743848435878753\n",
      "Epoch 11, Batch 200, Loss: 6.443495585918426\n",
      "Epoch 11, Batch 400, Loss: 6.039369924068451\n",
      "Epoch 11, Batch 600, Loss: 5.716079483032226\n",
      "Epoch 11, Batch 800, Loss: 5.71109699010849\n",
      "Epoch 12, Batch 200, Loss: 6.289015011787415\n",
      "Epoch 12, Batch 400, Loss: 6.149615995883941\n",
      "Epoch 12, Batch 600, Loss: 6.995485167503357\n",
      "Epoch 12, Batch 800, Loss: 6.263756380081177\n",
      "Epoch 13, Batch 200, Loss: 5.705241103172302\n",
      "Epoch 13, Batch 400, Loss: 5.621290831565857\n",
      "Epoch 13, Batch 600, Loss: 6.178954756259918\n",
      "Epoch 13, Batch 800, Loss: 6.018664536476135\n",
      "Epoch 14, Batch 200, Loss: 6.041378697156906\n",
      "Epoch 14, Batch 400, Loss: 6.611406733989716\n",
      "Epoch 14, Batch 600, Loss: 6.294619073867798\n",
      "Epoch 14, Batch 800, Loss: 5.769989953041077\n",
      "Epoch 15, Batch 200, Loss: 5.802320864200592\n",
      "Epoch 15, Batch 400, Loss: 6.288806781768799\n",
      "Epoch 15, Batch 600, Loss: 5.814591810703278\n",
      "Epoch 15, Batch 800, Loss: 6.211688005924225\n",
      "Epoch 16, Batch 200, Loss: 6.02310239315033\n",
      "Epoch 16, Batch 400, Loss: 6.117738239765167\n",
      "Epoch 16, Batch 600, Loss: 6.179177701473236\n",
      "Epoch 16, Batch 800, Loss: 6.092121036052704\n",
      "Epoch 17, Batch 200, Loss: 6.374410300254822\n",
      "Epoch 17, Batch 400, Loss: 5.631971189975738\n",
      "Epoch 17, Batch 600, Loss: 6.436536977291107\n",
      "Epoch 17, Batch 800, Loss: 6.5968008685112\n",
      "Epoch 18, Batch 200, Loss: 6.054114074707031\n",
      "Epoch 18, Batch 400, Loss: 5.794482494592667\n",
      "Epoch 18, Batch 600, Loss: 6.54534859418869\n",
      "Epoch 18, Batch 800, Loss: 6.105129803419113\n",
      "Epoch 19, Batch 200, Loss: 6.11799572467804\n",
      "Epoch 19, Batch 400, Loss: 6.01492084145546\n",
      "Epoch 19, Batch 600, Loss: 5.903089787960052\n",
      "Epoch 19, Batch 800, Loss: 5.624963011741638\n",
      "Epoch 20, Batch 200, Loss: 5.965214769840241\n",
      "Epoch 20, Batch 400, Loss: 6.39518669128418\n",
      "Epoch 20, Batch 600, Loss: 5.628068206310272\n",
      "Epoch 20, Batch 800, Loss: 5.323033149242401\n",
      "Epoch 21, Batch 200, Loss: 5.835126945972442\n",
      "Epoch 21, Batch 400, Loss: 5.848694849014282\n",
      "Epoch 21, Batch 600, Loss: 6.486809594631195\n",
      "Epoch 21, Batch 800, Loss: 6.327320947647094\n",
      "Epoch 22, Batch 200, Loss: 5.816411876678467\n",
      "Epoch 22, Batch 400, Loss: 6.1005879235267635\n",
      "Epoch 22, Batch 600, Loss: 6.489148862361908\n",
      "Epoch 22, Batch 800, Loss: 6.053081727027893\n",
      "Epoch 23, Batch 200, Loss: 5.8266296744346615\n",
      "Epoch 23, Batch 400, Loss: 5.952003557682037\n",
      "Epoch 23, Batch 600, Loss: 6.054439582824707\n",
      "Epoch 23, Batch 800, Loss: 5.723406858444214\n",
      "Epoch 24, Batch 200, Loss: 6.386150059700012\n",
      "Epoch 24, Batch 400, Loss: 5.840605537891388\n",
      "Epoch 24, Batch 600, Loss: 5.941099169254303\n",
      "Epoch 24, Batch 800, Loss: 5.9330308985710145\n",
      "Epoch 25, Batch 200, Loss: 5.930300259590149\n",
      "Epoch 25, Batch 400, Loss: 5.913851866722107\n",
      "Epoch 25, Batch 600, Loss: 5.711447775363922\n",
      "Epoch 25, Batch 800, Loss: 6.26363951921463\n",
      "Accuracy on test set: 0.0902%\n",
      "Fitting for combination 29\n",
      "784\n",
      "2\n",
      "10\n",
      "[40, 30, 10]\n",
      "False\n",
      "['relu', 'tanh']\n",
      "SGD\n",
      "0.03\n",
      "0.01\n",
      "CrossEntropyLoss\n",
      "50\n",
      "25\n",
      "0\n",
      "Epoch 1, Batch 50, Loss: 0.9789744973182678\n",
      "Epoch 1, Batch 100, Loss: 0.6876639115810395\n",
      "Epoch 1, Batch 150, Loss: 0.5494753682613372\n",
      "Epoch 1, Batch 200, Loss: 0.47945268869400026\n",
      "Epoch 1, Batch 250, Loss: 0.41998273849487305\n",
      "Epoch 1, Batch 300, Loss: 0.4022506296634674\n",
      "Epoch 1, Batch 350, Loss: 0.3867921894788742\n",
      "Epoch 1, Batch 400, Loss: 0.3713999772071838\n",
      "Epoch 1, Batch 450, Loss: 0.3388930779695511\n",
      "Epoch 1, Batch 500, Loss: 0.34325323909521105\n",
      "Epoch 1, Batch 550, Loss: 0.3344377827644348\n",
      "Epoch 1, Batch 600, Loss: 0.31937734365463255\n",
      "Epoch 1, Batch 650, Loss: 0.3186745375394821\n",
      "Epoch 1, Batch 700, Loss: 0.30975075632333754\n",
      "Epoch 1, Batch 750, Loss: 0.29403680622577666\n",
      "Epoch 1, Batch 800, Loss: 0.2934317657351494\n",
      "Epoch 1, Batch 850, Loss: 0.29728043973445895\n",
      "Epoch 1, Batch 900, Loss: 0.2948429614305496\n",
      "Epoch 2, Batch 50, Loss: 0.2834689053893089\n",
      "Epoch 2, Batch 100, Loss: 0.2722590497136116\n",
      "Epoch 2, Batch 150, Loss: 0.2631179404258728\n",
      "Epoch 2, Batch 200, Loss: 0.26480840355157853\n",
      "Epoch 2, Batch 250, Loss: 0.27141508281230925\n",
      "Epoch 2, Batch 300, Loss: 0.25814092516899106\n",
      "Epoch 2, Batch 350, Loss: 0.2565710833668709\n",
      "Epoch 2, Batch 400, Loss: 0.2609058910608292\n",
      "Epoch 2, Batch 450, Loss: 0.26517270982265473\n",
      "Epoch 2, Batch 500, Loss: 0.24476782649755477\n",
      "Epoch 2, Batch 550, Loss: 0.2588710376620293\n",
      "Epoch 2, Batch 600, Loss: 0.2595040971040726\n",
      "Epoch 2, Batch 650, Loss: 0.25759449511766436\n",
      "Epoch 2, Batch 700, Loss: 0.2641640478372574\n",
      "Epoch 2, Batch 750, Loss: 0.2426404121518135\n",
      "Epoch 2, Batch 800, Loss: 0.25476808339357376\n",
      "Epoch 2, Batch 850, Loss: 0.25498256891965865\n",
      "Epoch 2, Batch 900, Loss: 0.25656866788864136\n",
      "Epoch 3, Batch 50, Loss: 0.24655427604913713\n",
      "Epoch 3, Batch 100, Loss: 0.2419567337632179\n",
      "Epoch 3, Batch 150, Loss: 0.23920391738414765\n",
      "Epoch 3, Batch 200, Loss: 0.2499306958913803\n",
      "Epoch 3, Batch 250, Loss: 0.25023342549800875\n",
      "Epoch 3, Batch 300, Loss: 0.23418817222118377\n",
      "Epoch 3, Batch 350, Loss: 0.23398618549108505\n",
      "Epoch 3, Batch 400, Loss: 0.24713145941495895\n",
      "Epoch 3, Batch 450, Loss: 0.24672028228640555\n",
      "Epoch 3, Batch 500, Loss: 0.23488272577524186\n",
      "Epoch 3, Batch 550, Loss: 0.23751268148422242\n",
      "Epoch 3, Batch 600, Loss: 0.23072721630334855\n",
      "Epoch 3, Batch 650, Loss: 0.2435476464033127\n",
      "Epoch 3, Batch 700, Loss: 0.24762411296367645\n",
      "Epoch 3, Batch 750, Loss: 0.23625544518232344\n",
      "Epoch 3, Batch 800, Loss: 0.23517938256263732\n",
      "Epoch 3, Batch 850, Loss: 0.23946623861789704\n",
      "Epoch 3, Batch 900, Loss: 0.23726919770240784\n",
      "Epoch 4, Batch 50, Loss: 0.2323551768064499\n",
      "Epoch 4, Batch 100, Loss: 0.22842141717672348\n",
      "Epoch 4, Batch 150, Loss: 0.24506157636642456\n",
      "Epoch 4, Batch 200, Loss: 0.23224028408527375\n",
      "Epoch 4, Batch 250, Loss: 0.23676435440778731\n",
      "Epoch 4, Batch 300, Loss: 0.22961458653211594\n",
      "Epoch 4, Batch 350, Loss: 0.23611972749233245\n",
      "Epoch 4, Batch 400, Loss: 0.2164289991557598\n",
      "Epoch 4, Batch 450, Loss: 0.22330515772104265\n",
      "Epoch 4, Batch 500, Loss: 0.25645753622055056\n",
      "Epoch 4, Batch 550, Loss: 0.23442911267280578\n",
      "Epoch 4, Batch 600, Loss: 0.23138094514608384\n",
      "Epoch 4, Batch 650, Loss: 0.22481574684381486\n",
      "Epoch 4, Batch 700, Loss: 0.2305720740556717\n",
      "Epoch 4, Batch 750, Loss: 0.22910888642072677\n",
      "Epoch 4, Batch 800, Loss: 0.22956319987773896\n",
      "Epoch 4, Batch 850, Loss: 0.23401762366294862\n",
      "Epoch 4, Batch 900, Loss: 0.21708011567592622\n",
      "Epoch 5, Batch 50, Loss: 0.22226004928350448\n",
      "Epoch 5, Batch 100, Loss: 0.22681252866983415\n",
      "Epoch 5, Batch 150, Loss: 0.22316792160272597\n",
      "Epoch 5, Batch 200, Loss: 0.2288551351428032\n",
      "Epoch 5, Batch 250, Loss: 0.2256501665711403\n",
      "Epoch 5, Batch 300, Loss: 0.2329958289861679\n",
      "Epoch 5, Batch 350, Loss: 0.22441481232643126\n",
      "Epoch 5, Batch 400, Loss: 0.23505145341157913\n",
      "Epoch 5, Batch 450, Loss: 0.21565792173147202\n",
      "Epoch 5, Batch 500, Loss: 0.2228532138466835\n",
      "Epoch 5, Batch 550, Loss: 0.22133689790964126\n",
      "Epoch 5, Batch 600, Loss: 0.22478168040513993\n",
      "Epoch 5, Batch 650, Loss: 0.23900481075048446\n",
      "Epoch 5, Batch 700, Loss: 0.2221328003704548\n",
      "Epoch 5, Batch 750, Loss: 0.2209104883670807\n",
      "Epoch 5, Batch 800, Loss: 0.21931132793426514\n",
      "Epoch 5, Batch 850, Loss: 0.22845897749066352\n",
      "Epoch 5, Batch 900, Loss: 0.22813575357198715\n",
      "Epoch 6, Batch 50, Loss: 0.22121112287044525\n",
      "Epoch 6, Batch 100, Loss: 0.22511855453252794\n",
      "Epoch 6, Batch 150, Loss: 0.23534542113542556\n",
      "Epoch 6, Batch 200, Loss: 0.2235201492905617\n",
      "Epoch 6, Batch 250, Loss: 0.2233293503522873\n",
      "Epoch 6, Batch 300, Loss: 0.21779983550310134\n",
      "Epoch 6, Batch 350, Loss: 0.21829599142074585\n",
      "Epoch 6, Batch 400, Loss: 0.21885237574577332\n",
      "Epoch 6, Batch 450, Loss: 0.22090703651309013\n",
      "Epoch 6, Batch 500, Loss: 0.22561554685235025\n",
      "Epoch 6, Batch 550, Loss: 0.22361116319894792\n",
      "Epoch 6, Batch 600, Loss: 0.2208482640981674\n",
      "Epoch 6, Batch 650, Loss: 0.21411109194159508\n",
      "Epoch 6, Batch 700, Loss: 0.224390452504158\n",
      "Epoch 6, Batch 750, Loss: 0.22804980456829071\n",
      "Epoch 6, Batch 800, Loss: 0.22655171290040016\n",
      "Epoch 6, Batch 850, Loss: 0.22744388550519942\n",
      "Epoch 6, Batch 900, Loss: 0.21970704108476638\n",
      "Epoch 7, Batch 50, Loss: 0.2241014873981476\n",
      "Epoch 7, Batch 100, Loss: 0.2229243138432503\n",
      "Epoch 7, Batch 150, Loss: 0.2210904137790203\n",
      "Epoch 7, Batch 200, Loss: 0.21592185497283936\n",
      "Epoch 7, Batch 250, Loss: 0.22835539922118187\n",
      "Epoch 7, Batch 300, Loss: 0.22360203057527542\n",
      "Epoch 7, Batch 350, Loss: 0.22725798040628434\n",
      "Epoch 7, Batch 400, Loss: 0.2138583767414093\n",
      "Epoch 7, Batch 450, Loss: 0.22638746052980424\n",
      "Epoch 7, Batch 500, Loss: 0.22082662776112558\n",
      "Epoch 7, Batch 550, Loss: 0.21668003410100936\n",
      "Epoch 7, Batch 600, Loss: 0.2119809556007385\n",
      "Epoch 7, Batch 650, Loss: 0.21250559031963348\n",
      "Epoch 7, Batch 700, Loss: 0.22650310039520263\n",
      "Epoch 7, Batch 750, Loss: 0.23394738405942916\n",
      "Epoch 7, Batch 800, Loss: 0.212170450091362\n",
      "Epoch 7, Batch 850, Loss: 0.2144536307454109\n",
      "Epoch 7, Batch 900, Loss: 0.2084984415769577\n",
      "Epoch 8, Batch 50, Loss: 0.22151923298835755\n",
      "Epoch 8, Batch 100, Loss: 0.21082429811358452\n",
      "Epoch 8, Batch 150, Loss: 0.21547653630375863\n",
      "Epoch 8, Batch 200, Loss: 0.22193627744913103\n",
      "Epoch 8, Batch 250, Loss: 0.21235672771930694\n",
      "Epoch 8, Batch 300, Loss: 0.2168668043613434\n",
      "Epoch 8, Batch 350, Loss: 0.22171110153198242\n",
      "Epoch 8, Batch 400, Loss: 0.20982876151800156\n",
      "Epoch 8, Batch 450, Loss: 0.2121025124192238\n",
      "Epoch 8, Batch 500, Loss: 0.2120799082517624\n",
      "Epoch 8, Batch 550, Loss: 0.2153353613615036\n",
      "Epoch 8, Batch 600, Loss: 0.22082524627447128\n",
      "Epoch 8, Batch 650, Loss: 0.23017755508422852\n",
      "Epoch 8, Batch 700, Loss: 0.21972598314285277\n",
      "Epoch 8, Batch 750, Loss: 0.21899555772542953\n",
      "Epoch 8, Batch 800, Loss: 0.23243729770183563\n",
      "Epoch 8, Batch 850, Loss: 0.22023300111293792\n",
      "Epoch 8, Batch 900, Loss: 0.22179398745298384\n",
      "Epoch 9, Batch 50, Loss: 0.21599388390779495\n",
      "Epoch 9, Batch 100, Loss: 0.21373962581157685\n",
      "Epoch 9, Batch 150, Loss: 0.20265907242894174\n",
      "Epoch 9, Batch 200, Loss: 0.21273262143135072\n",
      "Epoch 9, Batch 250, Loss: 0.20810891151428224\n",
      "Epoch 9, Batch 300, Loss: 0.2139669770002365\n",
      "Epoch 9, Batch 350, Loss: 0.2246712201833725\n",
      "Epoch 9, Batch 400, Loss: 0.225314781665802\n",
      "Epoch 9, Batch 450, Loss: 0.21002180323004724\n",
      "Epoch 9, Batch 500, Loss: 0.2187910160422325\n",
      "Epoch 9, Batch 550, Loss: 0.21817911356687547\n",
      "Epoch 9, Batch 600, Loss: 0.21876246601343155\n",
      "Epoch 9, Batch 650, Loss: 0.2225698474049568\n",
      "Epoch 9, Batch 700, Loss: 0.21963610261678695\n",
      "Epoch 9, Batch 750, Loss: 0.22083786457777024\n",
      "Epoch 9, Batch 800, Loss: 0.2125381922721863\n",
      "Epoch 9, Batch 850, Loss: 0.2207746323943138\n",
      "Epoch 9, Batch 900, Loss: 0.21830775290727616\n",
      "Epoch 10, Batch 50, Loss: 0.2054783733189106\n",
      "Epoch 10, Batch 100, Loss: 0.20875035002827644\n",
      "Epoch 10, Batch 150, Loss: 0.21237876445055007\n",
      "Epoch 10, Batch 200, Loss: 0.21720372676849364\n",
      "Epoch 10, Batch 250, Loss: 0.2087904505431652\n",
      "Epoch 10, Batch 300, Loss: 0.21519048571586608\n",
      "Epoch 10, Batch 350, Loss: 0.21589088529348374\n",
      "Epoch 10, Batch 400, Loss: 0.2285681062936783\n",
      "Epoch 10, Batch 450, Loss: 0.2213797752559185\n",
      "Epoch 10, Batch 500, Loss: 0.2057857969403267\n",
      "Epoch 10, Batch 550, Loss: 0.22331569731235504\n",
      "Epoch 10, Batch 600, Loss: 0.20593050748109817\n",
      "Epoch 10, Batch 650, Loss: 0.21920702666044234\n",
      "Epoch 10, Batch 700, Loss: 0.20290931612253188\n",
      "Epoch 10, Batch 750, Loss: 0.2180348414182663\n",
      "Epoch 10, Batch 800, Loss: 0.21947046786546706\n",
      "Epoch 10, Batch 850, Loss: 0.22249847054481506\n",
      "Epoch 10, Batch 900, Loss: 0.2189870484173298\n",
      "Epoch 11, Batch 50, Loss: 0.21848194509744645\n",
      "Epoch 11, Batch 100, Loss: 0.21292895317077637\n",
      "Epoch 11, Batch 150, Loss: 0.22299022167921068\n",
      "Epoch 11, Batch 200, Loss: 0.21271806970238685\n",
      "Epoch 11, Batch 250, Loss: 0.2205941790342331\n",
      "Epoch 11, Batch 300, Loss: 0.20417446807026862\n",
      "Epoch 11, Batch 350, Loss: 0.2135040520131588\n",
      "Epoch 11, Batch 400, Loss: 0.21865046560764312\n",
      "Epoch 11, Batch 450, Loss: 0.21370377629995346\n",
      "Epoch 11, Batch 500, Loss: 0.21312608271837236\n",
      "Epoch 11, Batch 550, Loss: 0.21818565011024474\n",
      "Epoch 11, Batch 600, Loss: 0.21684472441673278\n",
      "Epoch 11, Batch 650, Loss: 0.20479253083467483\n",
      "Epoch 11, Batch 700, Loss: 0.2228921017050743\n",
      "Epoch 11, Batch 750, Loss: 0.21076465725898744\n",
      "Epoch 11, Batch 800, Loss: 0.2115644869208336\n",
      "Epoch 11, Batch 850, Loss: 0.2060297031700611\n",
      "Epoch 11, Batch 900, Loss: 0.2211841332912445\n",
      "Epoch 12, Batch 50, Loss: 0.20507559537887574\n",
      "Epoch 12, Batch 100, Loss: 0.2147686657309532\n",
      "Epoch 12, Batch 150, Loss: 0.217273341268301\n",
      "Epoch 12, Batch 200, Loss: 0.20451421037316322\n",
      "Epoch 12, Batch 250, Loss: 0.21816347062587738\n",
      "Epoch 12, Batch 300, Loss: 0.2125379991531372\n",
      "Epoch 12, Batch 350, Loss: 0.19674200296401978\n",
      "Epoch 12, Batch 400, Loss: 0.21655304446816445\n",
      "Epoch 12, Batch 450, Loss: 0.21961716264486314\n",
      "Epoch 12, Batch 500, Loss: 0.22201019436120986\n",
      "Epoch 12, Batch 550, Loss: 0.20733299732208252\n",
      "Epoch 12, Batch 600, Loss: 0.21950271785259245\n",
      "Epoch 12, Batch 650, Loss: 0.21313681244850158\n",
      "Epoch 12, Batch 700, Loss: 0.2137863552570343\n",
      "Epoch 12, Batch 750, Loss: 0.2162622520327568\n",
      "Epoch 12, Batch 800, Loss: 0.20622634440660476\n",
      "Epoch 12, Batch 850, Loss: 0.22390851587057115\n",
      "Epoch 12, Batch 900, Loss: 0.2108348774909973\n",
      "Epoch 13, Batch 50, Loss: 0.20834921702742576\n",
      "Epoch 13, Batch 100, Loss: 0.21128478169441223\n",
      "Epoch 13, Batch 150, Loss: 0.20622415974736213\n",
      "Epoch 13, Batch 200, Loss: 0.21047212362289427\n",
      "Epoch 13, Batch 250, Loss: 0.20760838955640792\n",
      "Epoch 13, Batch 300, Loss: 0.2068854144215584\n",
      "Epoch 13, Batch 350, Loss: 0.2141842105984688\n",
      "Epoch 13, Batch 400, Loss: 0.213332559466362\n",
      "Epoch 13, Batch 450, Loss: 0.20159370750188826\n",
      "Epoch 13, Batch 500, Loss: 0.22114830195903779\n",
      "Epoch 13, Batch 550, Loss: 0.20649852022528647\n",
      "Epoch 13, Batch 600, Loss: 0.22065921127796173\n",
      "Epoch 13, Batch 650, Loss: 0.21970993608236314\n",
      "Epoch 13, Batch 700, Loss: 0.22395524621009827\n",
      "Epoch 13, Batch 750, Loss: 0.21806682586669923\n",
      "Epoch 13, Batch 800, Loss: 0.21416400134563446\n",
      "Epoch 13, Batch 850, Loss: 0.21384086906909944\n",
      "Epoch 13, Batch 900, Loss: 0.21823610663414\n",
      "Epoch 14, Batch 50, Loss: 0.21248404651880265\n",
      "Epoch 14, Batch 100, Loss: 0.2051074832677841\n",
      "Epoch 14, Batch 150, Loss: 0.19598670765757562\n",
      "Epoch 14, Batch 200, Loss: 0.20263577818870546\n",
      "Epoch 14, Batch 250, Loss: 0.21920552596449852\n",
      "Epoch 14, Batch 300, Loss: 0.19941381961107255\n",
      "Epoch 14, Batch 350, Loss: 0.20927969008684158\n",
      "Epoch 14, Batch 400, Loss: 0.21983683601021767\n",
      "Epoch 14, Batch 450, Loss: 0.20883541136980058\n",
      "Epoch 14, Batch 500, Loss: 0.21209184870123862\n",
      "Epoch 14, Batch 550, Loss: 0.21331576019525528\n",
      "Epoch 14, Batch 600, Loss: 0.2222122320532799\n",
      "Epoch 14, Batch 650, Loss: 0.21470221310853957\n",
      "Epoch 14, Batch 700, Loss: 0.22531401723623276\n",
      "Epoch 14, Batch 750, Loss: 0.2141767492890358\n",
      "Epoch 14, Batch 800, Loss: 0.21326234251260756\n",
      "Epoch 14, Batch 850, Loss: 0.2122991980612278\n",
      "Epoch 14, Batch 900, Loss: 0.2086686286330223\n",
      "Epoch 15, Batch 50, Loss: 0.20178375601768495\n",
      "Epoch 15, Batch 100, Loss: 0.21245545849204064\n",
      "Epoch 15, Batch 150, Loss: 0.2155137750506401\n",
      "Epoch 15, Batch 200, Loss: 0.2085838483273983\n",
      "Epoch 15, Batch 250, Loss: 0.20610372602939606\n",
      "Epoch 15, Batch 300, Loss: 0.1991892424225807\n",
      "Epoch 15, Batch 350, Loss: 0.21396457821130752\n",
      "Epoch 15, Batch 400, Loss: 0.21364514097571374\n",
      "Epoch 15, Batch 450, Loss: 0.22393307566642762\n",
      "Epoch 15, Batch 500, Loss: 0.20761537969112395\n",
      "Epoch 15, Batch 550, Loss: 0.2127284026145935\n",
      "Epoch 15, Batch 600, Loss: 0.20933266907930373\n",
      "Epoch 15, Batch 650, Loss: 0.21352437600493432\n",
      "Epoch 15, Batch 700, Loss: 0.21244304805994033\n",
      "Epoch 15, Batch 750, Loss: 0.20893687322735788\n",
      "Epoch 15, Batch 800, Loss: 0.20536245167255401\n",
      "Epoch 15, Batch 850, Loss: 0.21142719343304633\n",
      "Epoch 15, Batch 900, Loss: 0.21580963626503943\n",
      "Epoch 16, Batch 50, Loss: 0.20987848699092865\n",
      "Epoch 16, Batch 100, Loss: 0.20707974046468736\n",
      "Epoch 16, Batch 150, Loss: 0.21162320971488952\n",
      "Epoch 16, Batch 200, Loss: 0.2218233984708786\n",
      "Epoch 16, Batch 250, Loss: 0.19596360564231874\n",
      "Epoch 16, Batch 300, Loss: 0.21003473103046416\n",
      "Epoch 16, Batch 350, Loss: 0.20854257375001908\n",
      "Epoch 16, Batch 400, Loss: 0.21289102166891097\n",
      "Epoch 16, Batch 450, Loss: 0.20473576009273528\n",
      "Epoch 16, Batch 500, Loss: 0.21250938028097152\n",
      "Epoch 16, Batch 550, Loss: 0.21136005342006683\n",
      "Epoch 16, Batch 600, Loss: 0.2028265577554703\n",
      "Epoch 16, Batch 650, Loss: 0.21522517442703248\n",
      "Epoch 16, Batch 700, Loss: 0.21173967897892\n",
      "Epoch 16, Batch 750, Loss: 0.21076088413596153\n",
      "Epoch 16, Batch 800, Loss: 0.20912435710430144\n",
      "Epoch 16, Batch 850, Loss: 0.20758925169706344\n",
      "Epoch 16, Batch 900, Loss: 0.21864450633525848\n",
      "Epoch 17, Batch 50, Loss: 0.2171911281347275\n",
      "Epoch 17, Batch 100, Loss: 0.21654797196388245\n",
      "Epoch 17, Batch 150, Loss: 0.21491774469614028\n",
      "Epoch 17, Batch 200, Loss: 0.2137932536005974\n",
      "Epoch 17, Batch 250, Loss: 0.2076195579767227\n",
      "Epoch 17, Batch 300, Loss: 0.21096683412790299\n",
      "Epoch 17, Batch 350, Loss: 0.21113500297069548\n",
      "Epoch 17, Batch 400, Loss: 0.21519314855337143\n",
      "Epoch 17, Batch 450, Loss: 0.213000131547451\n",
      "Epoch 17, Batch 500, Loss: 0.1973213028907776\n",
      "Epoch 17, Batch 550, Loss: 0.21790661424398422\n",
      "Epoch 17, Batch 600, Loss: 0.21727123662829398\n",
      "Epoch 17, Batch 650, Loss: 0.20712756916880606\n",
      "Epoch 17, Batch 700, Loss: 0.21224664255976677\n",
      "Epoch 17, Batch 750, Loss: 0.20189502269029616\n",
      "Epoch 17, Batch 800, Loss: 0.20851511716842652\n",
      "Epoch 17, Batch 850, Loss: 0.21267499089241026\n",
      "Epoch 17, Batch 900, Loss: 0.19533739298582076\n",
      "Epoch 18, Batch 50, Loss: 0.21336393281817437\n",
      "Epoch 18, Batch 100, Loss: 0.21073972105979918\n",
      "Epoch 18, Batch 150, Loss: 0.20193311542272568\n",
      "Epoch 18, Batch 200, Loss: 0.21293844610452653\n",
      "Epoch 18, Batch 250, Loss: 0.2026759724318981\n",
      "Epoch 18, Batch 300, Loss: 0.20742975294589996\n",
      "Epoch 18, Batch 350, Loss: 0.20895735934376716\n",
      "Epoch 18, Batch 400, Loss: 0.20346557438373566\n",
      "Epoch 18, Batch 450, Loss: 0.21233282282948493\n",
      "Epoch 18, Batch 500, Loss: 0.20883658051490783\n",
      "Epoch 18, Batch 550, Loss: 0.2027844136953354\n",
      "Epoch 18, Batch 600, Loss: 0.20954839646816253\n",
      "Epoch 18, Batch 650, Loss: 0.21616982027888298\n",
      "Epoch 18, Batch 700, Loss: 0.21497294068336487\n",
      "Epoch 18, Batch 750, Loss: 0.21406823843717576\n",
      "Epoch 18, Batch 800, Loss: 0.2153744149208069\n",
      "Epoch 18, Batch 850, Loss: 0.20383826196193694\n",
      "Epoch 18, Batch 900, Loss: 0.19827541872859\n",
      "Epoch 19, Batch 50, Loss: 0.21583883970975876\n",
      "Epoch 19, Batch 100, Loss: 0.21021073043346405\n",
      "Epoch 19, Batch 150, Loss: 0.2046720091998577\n",
      "Epoch 19, Batch 200, Loss: 0.21119777262210845\n",
      "Epoch 19, Batch 250, Loss: 0.203820578455925\n",
      "Epoch 19, Batch 300, Loss: 0.20508466124534608\n",
      "Epoch 19, Batch 350, Loss: 0.20983708173036575\n",
      "Epoch 19, Batch 400, Loss: 0.2110155177116394\n",
      "Epoch 19, Batch 450, Loss: 0.21378291994333268\n",
      "Epoch 19, Batch 500, Loss: 0.2000789825618267\n",
      "Epoch 19, Batch 550, Loss: 0.2106923282146454\n",
      "Epoch 19, Batch 600, Loss: 0.21523573040962218\n",
      "Epoch 19, Batch 650, Loss: 0.20409824803471566\n",
      "Epoch 19, Batch 700, Loss: 0.20594191014766694\n",
      "Epoch 19, Batch 750, Loss: 0.20688440710306166\n",
      "Epoch 19, Batch 800, Loss: 0.20992268696427346\n",
      "Epoch 19, Batch 850, Loss: 0.219314346909523\n",
      "Epoch 19, Batch 900, Loss: 0.2092365550994873\n",
      "Epoch 20, Batch 50, Loss: 0.19744407773017883\n",
      "Epoch 20, Batch 100, Loss: 0.19471190974116326\n",
      "Epoch 20, Batch 150, Loss: 0.20296010434627532\n",
      "Epoch 20, Batch 200, Loss: 0.2166139754652977\n",
      "Epoch 20, Batch 250, Loss: 0.21470585346221924\n",
      "Epoch 20, Batch 300, Loss: 0.2086891809105873\n",
      "Epoch 20, Batch 350, Loss: 0.2103407821059227\n",
      "Epoch 20, Batch 400, Loss: 0.20966905519366263\n",
      "Epoch 20, Batch 450, Loss: 0.21429941505193711\n",
      "Epoch 20, Batch 500, Loss: 0.19182838901877403\n",
      "Epoch 20, Batch 550, Loss: 0.21397417470812796\n",
      "Epoch 20, Batch 600, Loss: 0.21367365807294847\n",
      "Epoch 20, Batch 650, Loss: 0.19940609082579613\n",
      "Epoch 20, Batch 700, Loss: 0.21392970502376557\n",
      "Epoch 20, Batch 750, Loss: 0.21119628578424454\n",
      "Epoch 20, Batch 800, Loss: 0.21699171960353852\n",
      "Epoch 20, Batch 850, Loss: 0.2031079526245594\n",
      "Epoch 20, Batch 900, Loss: 0.19690594658255578\n",
      "Epoch 21, Batch 50, Loss: 0.21046288192272186\n",
      "Epoch 21, Batch 100, Loss: 0.1966518783569336\n",
      "Epoch 21, Batch 150, Loss: 0.20120665818452835\n",
      "Epoch 21, Batch 200, Loss: 0.2106517106294632\n",
      "Epoch 21, Batch 250, Loss: 0.20333714216947554\n",
      "Epoch 21, Batch 300, Loss: 0.217985759973526\n",
      "Epoch 21, Batch 350, Loss: 0.2014539785683155\n",
      "Epoch 21, Batch 400, Loss: 0.21324936151504517\n",
      "Epoch 21, Batch 450, Loss: 0.21667799413204192\n",
      "Epoch 21, Batch 500, Loss: 0.19955097794532775\n",
      "Epoch 21, Batch 550, Loss: 0.2050291931629181\n",
      "Epoch 21, Batch 600, Loss: 0.20932434558868407\n",
      "Epoch 21, Batch 650, Loss: 0.21317728489637375\n",
      "Epoch 21, Batch 700, Loss: 0.20600953280925752\n",
      "Epoch 21, Batch 750, Loss: 0.20771663665771484\n",
      "Epoch 21, Batch 800, Loss: 0.2141625951230526\n",
      "Epoch 21, Batch 850, Loss: 0.20346066027879714\n",
      "Epoch 21, Batch 900, Loss: 0.1934552149474621\n",
      "Epoch 22, Batch 50, Loss: 0.2117063073813915\n",
      "Epoch 22, Batch 100, Loss: 0.20822421461343765\n",
      "Epoch 22, Batch 150, Loss: 0.21107231602072715\n",
      "Epoch 22, Batch 200, Loss: 0.20698050439357757\n",
      "Epoch 22, Batch 250, Loss: 0.2226367624104023\n",
      "Epoch 22, Batch 300, Loss: 0.20071357131004333\n",
      "Epoch 22, Batch 350, Loss: 0.20301839768886565\n",
      "Epoch 22, Batch 400, Loss: 0.2119896002113819\n",
      "Epoch 22, Batch 450, Loss: 0.2076827169954777\n",
      "Epoch 22, Batch 500, Loss: 0.19137989833950997\n",
      "Epoch 22, Batch 550, Loss: 0.1976643031835556\n",
      "Epoch 22, Batch 600, Loss: 0.20113840103149414\n",
      "Epoch 22, Batch 650, Loss: 0.2082254856824875\n",
      "Epoch 22, Batch 700, Loss: 0.2017207035422325\n",
      "Epoch 22, Batch 750, Loss: 0.21026648104190826\n",
      "Epoch 22, Batch 800, Loss: 0.1983721150457859\n",
      "Epoch 22, Batch 850, Loss: 0.2016272436082363\n",
      "Epoch 22, Batch 900, Loss: 0.217302588224411\n",
      "Epoch 23, Batch 50, Loss: 0.2114148786664009\n",
      "Epoch 23, Batch 100, Loss: 0.2006975694000721\n",
      "Epoch 23, Batch 150, Loss: 0.20691621720790862\n",
      "Epoch 23, Batch 200, Loss: 0.19648464515805245\n",
      "Epoch 23, Batch 250, Loss: 0.21028739362955093\n",
      "Epoch 23, Batch 300, Loss: 0.20474935427308083\n",
      "Epoch 23, Batch 350, Loss: 0.20076530069112777\n",
      "Epoch 23, Batch 400, Loss: 0.21149892359972\n",
      "Epoch 23, Batch 450, Loss: 0.21750645697116852\n",
      "Epoch 23, Batch 500, Loss: 0.20359254002571106\n",
      "Epoch 23, Batch 550, Loss: 0.1983407874405384\n",
      "Epoch 23, Batch 600, Loss: 0.2032269549369812\n",
      "Epoch 23, Batch 650, Loss: 0.20397999852895737\n",
      "Epoch 23, Batch 700, Loss: 0.2061944469809532\n",
      "Epoch 23, Batch 750, Loss: 0.21203050762414932\n",
      "Epoch 23, Batch 800, Loss: 0.20293582275509833\n",
      "Epoch 23, Batch 850, Loss: 0.2000075763463974\n",
      "Epoch 23, Batch 900, Loss: 0.21217300057411193\n",
      "Epoch 24, Batch 50, Loss: 0.19154878914356233\n",
      "Epoch 24, Batch 100, Loss: 0.19879342928528787\n",
      "Epoch 24, Batch 150, Loss: 0.20777873665094376\n",
      "Epoch 24, Batch 200, Loss: 0.20377692461013794\n",
      "Epoch 24, Batch 250, Loss: 0.20430108845233919\n",
      "Epoch 24, Batch 300, Loss: 0.2010991083085537\n",
      "Epoch 24, Batch 350, Loss: 0.20328361436724662\n",
      "Epoch 24, Batch 400, Loss: 0.2018916943669319\n",
      "Epoch 24, Batch 450, Loss: 0.20438794955611228\n",
      "Epoch 24, Batch 500, Loss: 0.21186479389667512\n",
      "Epoch 24, Batch 550, Loss: 0.21620424568653107\n",
      "Epoch 24, Batch 600, Loss: 0.21025554120540618\n",
      "Epoch 24, Batch 650, Loss: 0.19989557787775994\n",
      "Epoch 24, Batch 700, Loss: 0.21165942132472992\n",
      "Epoch 24, Batch 750, Loss: 0.20930598706007003\n",
      "Epoch 24, Batch 800, Loss: 0.20150543600320817\n",
      "Epoch 24, Batch 850, Loss: 0.2027195417881012\n",
      "Epoch 24, Batch 900, Loss: 0.22202269211411477\n",
      "Epoch 25, Batch 50, Loss: 0.19542474672198296\n",
      "Epoch 25, Batch 100, Loss: 0.21220822095870973\n",
      "Epoch 25, Batch 150, Loss: 0.21552059605717658\n",
      "Epoch 25, Batch 200, Loss: 0.21377277731895447\n",
      "Epoch 25, Batch 250, Loss: 0.20583184972405433\n",
      "Epoch 25, Batch 300, Loss: 0.19854723021388054\n",
      "Epoch 25, Batch 350, Loss: 0.21281385004520417\n",
      "Epoch 25, Batch 400, Loss: 0.2149377140402794\n",
      "Epoch 25, Batch 450, Loss: 0.2027817052602768\n",
      "Epoch 25, Batch 500, Loss: 0.2049374035000801\n",
      "Epoch 25, Batch 550, Loss: 0.19271999448537827\n",
      "Epoch 25, Batch 600, Loss: 0.21318447574973107\n",
      "Epoch 25, Batch 650, Loss: 0.20753992676734925\n",
      "Epoch 25, Batch 700, Loss: 0.2093683287501335\n",
      "Epoch 25, Batch 750, Loss: 0.20544745177030563\n",
      "Epoch 25, Batch 800, Loss: 0.201057510972023\n",
      "Epoch 25, Batch 850, Loss: 0.20175840988755225\n",
      "Epoch 25, Batch 900, Loss: 0.1947292736172676\n",
      "Accuracy on test set: 0.8458%\n",
      "Fitting for combination 30\n",
      "784\n",
      "2\n",
      "10\n",
      "[40, 30, 10]\n",
      "False\n",
      "['relu', 'tanh']\n",
      "Adam\n",
      "0.03\n",
      "0.1\n",
      "CrossEntropyLoss\n",
      "100\n",
      "25\n",
      "0\n",
      "Epoch 1, Batch 100, Loss: 1.3298699694871903\n",
      "Epoch 1, Batch 200, Loss: 1.2225784188508988\n",
      "Epoch 1, Batch 300, Loss: 1.1728974175453186\n",
      "Epoch 1, Batch 400, Loss: 1.176350553035736\n",
      "Epoch 1, Batch 500, Loss: 1.238726853132248\n",
      "Epoch 1, Batch 600, Loss: 1.2270022642612457\n",
      "Epoch 1, Batch 700, Loss: 1.216375368833542\n",
      "Epoch 1, Batch 800, Loss: 1.135329338312149\n",
      "Epoch 1, Batch 900, Loss: 1.2077695494890213\n",
      "Epoch 2, Batch 100, Loss: 1.2442807316780091\n",
      "Epoch 2, Batch 200, Loss: 1.1936619585752488\n",
      "Epoch 2, Batch 300, Loss: 1.1674573957920074\n",
      "Epoch 2, Batch 400, Loss: 1.1682248920202256\n",
      "Epoch 2, Batch 500, Loss: 1.1562391632795335\n",
      "Epoch 2, Batch 600, Loss: 1.138264643549919\n",
      "Epoch 2, Batch 700, Loss: 1.1714102894067764\n",
      "Epoch 2, Batch 800, Loss: 1.1430034810304641\n",
      "Epoch 2, Batch 900, Loss: 1.2234626877307893\n",
      "Epoch 3, Batch 100, Loss: 1.2315928995609284\n",
      "Epoch 3, Batch 200, Loss: 1.2217419028282166\n",
      "Epoch 3, Batch 300, Loss: 1.3108687508106232\n",
      "Epoch 3, Batch 400, Loss: 1.2227738440036773\n",
      "Epoch 3, Batch 500, Loss: 1.1867510503530503\n",
      "Epoch 3, Batch 600, Loss: 1.36613551735878\n",
      "Epoch 3, Batch 700, Loss: 1.2943187439441681\n",
      "Epoch 3, Batch 800, Loss: 1.2429837012290954\n",
      "Epoch 3, Batch 900, Loss: 1.2872639632225036\n",
      "Epoch 4, Batch 100, Loss: 1.1788253164291382\n",
      "Epoch 4, Batch 200, Loss: 1.227040969133377\n",
      "Epoch 4, Batch 300, Loss: 1.2832059288024902\n",
      "Epoch 4, Batch 400, Loss: 1.2937523698806763\n",
      "Epoch 4, Batch 500, Loss: 1.2521019721031188\n",
      "Epoch 4, Batch 600, Loss: 1.2056592857837678\n",
      "Epoch 4, Batch 700, Loss: 1.240762470960617\n",
      "Epoch 4, Batch 800, Loss: 1.20024839758873\n",
      "Epoch 4, Batch 900, Loss: 1.235304503440857\n",
      "Epoch 5, Batch 100, Loss: 1.1621675884723663\n",
      "Epoch 5, Batch 200, Loss: 1.2072166115045548\n",
      "Epoch 5, Batch 300, Loss: 1.1633174377679825\n",
      "Epoch 5, Batch 400, Loss: 1.174625362753868\n",
      "Epoch 5, Batch 500, Loss: 1.342985165119171\n",
      "Epoch 5, Batch 600, Loss: 1.2403744745254517\n",
      "Epoch 5, Batch 700, Loss: 1.2436872023344039\n",
      "Epoch 5, Batch 800, Loss: 1.3933566772937775\n",
      "Epoch 5, Batch 900, Loss: 1.2840416300296784\n",
      "Epoch 6, Batch 100, Loss: 1.2822429263591766\n",
      "Epoch 6, Batch 200, Loss: 1.2662211155891419\n",
      "Epoch 6, Batch 300, Loss: 1.261687800884247\n",
      "Epoch 6, Batch 400, Loss: 1.2891212511062622\n",
      "Epoch 6, Batch 500, Loss: 1.2544544106721878\n",
      "Epoch 6, Batch 600, Loss: 1.282092354297638\n",
      "Epoch 6, Batch 700, Loss: 1.2497103166580201\n",
      "Epoch 6, Batch 800, Loss: 1.2947430038452148\n",
      "Epoch 6, Batch 900, Loss: 1.2829547643661499\n",
      "Epoch 7, Batch 100, Loss: 1.1895423460006713\n",
      "Epoch 7, Batch 200, Loss: 1.3004579901695252\n",
      "Epoch 7, Batch 300, Loss: 1.26555823802948\n",
      "Epoch 7, Batch 400, Loss: 1.2213054925203324\n",
      "Epoch 7, Batch 500, Loss: 1.4325258684158326\n",
      "Epoch 7, Batch 600, Loss: 1.2567592191696166\n",
      "Epoch 7, Batch 700, Loss: 1.2878692972660064\n",
      "Epoch 7, Batch 800, Loss: 1.2524194157123565\n",
      "Epoch 7, Batch 900, Loss: 1.2947031772136688\n",
      "Epoch 8, Batch 100, Loss: 1.2046446651220322\n",
      "Epoch 8, Batch 200, Loss: 1.242867841720581\n",
      "Epoch 8, Batch 300, Loss: 1.2450128698349\n",
      "Epoch 8, Batch 400, Loss: 1.2347595071792603\n",
      "Epoch 8, Batch 500, Loss: 1.2940029144287108\n",
      "Epoch 8, Batch 600, Loss: 1.2737835019826889\n",
      "Epoch 8, Batch 700, Loss: 1.2814378589391708\n",
      "Epoch 8, Batch 800, Loss: 1.2767533159255982\n",
      "Epoch 8, Batch 900, Loss: 1.3375836598873139\n",
      "Epoch 9, Batch 100, Loss: 1.3069760954380036\n",
      "Epoch 9, Batch 200, Loss: 1.3098034179210662\n",
      "Epoch 9, Batch 300, Loss: 1.3117967891693114\n",
      "Epoch 9, Batch 400, Loss: 1.229343742132187\n",
      "Epoch 9, Batch 500, Loss: 1.2636459827423097\n",
      "Epoch 9, Batch 600, Loss: 1.2141353297233581\n",
      "Epoch 9, Batch 700, Loss: 1.2411053967475891\n",
      "Epoch 9, Batch 800, Loss: 1.2835306596755982\n",
      "Epoch 9, Batch 900, Loss: 1.1795397943258286\n",
      "Epoch 10, Batch 100, Loss: 1.298314859867096\n",
      "Epoch 10, Batch 200, Loss: 1.3038920080661773\n",
      "Epoch 10, Batch 300, Loss: 1.2788632798194886\n",
      "Epoch 10, Batch 400, Loss: 1.2343915951251985\n",
      "Epoch 10, Batch 500, Loss: 1.2834503412246705\n",
      "Epoch 10, Batch 600, Loss: 1.3754128038883209\n",
      "Epoch 10, Batch 700, Loss: 1.2565839350223542\n",
      "Epoch 10, Batch 800, Loss: 1.2775356602668761\n",
      "Epoch 10, Batch 900, Loss: 1.2049884301424028\n",
      "Epoch 11, Batch 100, Loss: 1.279062607884407\n",
      "Epoch 11, Batch 200, Loss: 1.2432820010185242\n",
      "Epoch 11, Batch 300, Loss: 1.2451684379577637\n",
      "Epoch 11, Batch 400, Loss: 1.338335601091385\n",
      "Epoch 11, Batch 500, Loss: 1.3451773750782012\n",
      "Epoch 11, Batch 600, Loss: 1.2484626424312593\n",
      "Epoch 11, Batch 700, Loss: 1.3263637769222258\n",
      "Epoch 11, Batch 800, Loss: 1.3776104795932769\n",
      "Epoch 11, Batch 900, Loss: 1.2261567121744157\n",
      "Epoch 12, Batch 100, Loss: 1.23305637717247\n",
      "Epoch 12, Batch 200, Loss: 1.3376772391796112\n",
      "Epoch 12, Batch 300, Loss: 1.2551320815086364\n",
      "Epoch 12, Batch 400, Loss: 1.2542431628704072\n",
      "Epoch 12, Batch 500, Loss: 1.222313139438629\n",
      "Epoch 12, Batch 600, Loss: 1.2376694416999816\n",
      "Epoch 12, Batch 700, Loss: 1.267353538274765\n",
      "Epoch 12, Batch 800, Loss: 1.2508073341846466\n",
      "Epoch 12, Batch 900, Loss: 1.3290448892116546\n",
      "Epoch 13, Batch 100, Loss: 1.3217836272716523\n",
      "Epoch 13, Batch 200, Loss: 1.236317720413208\n",
      "Epoch 13, Batch 300, Loss: 1.2535088312625886\n",
      "Epoch 13, Batch 400, Loss: 1.327207200527191\n",
      "Epoch 13, Batch 500, Loss: 1.248737394809723\n",
      "Epoch 13, Batch 600, Loss: 1.2781280159950257\n",
      "Epoch 13, Batch 700, Loss: 1.2134930527210235\n",
      "Epoch 13, Batch 800, Loss: 1.2769345080852508\n",
      "Epoch 13, Batch 900, Loss: 1.2461015009880065\n",
      "Epoch 14, Batch 100, Loss: 1.2724178874492644\n",
      "Epoch 14, Batch 200, Loss: 1.3640165781974793\n",
      "Epoch 14, Batch 300, Loss: 1.304609763622284\n",
      "Epoch 14, Batch 400, Loss: 1.386523770093918\n",
      "Epoch 14, Batch 500, Loss: 1.2378369498252868\n",
      "Epoch 14, Batch 600, Loss: 1.2738542246818543\n",
      "Epoch 14, Batch 700, Loss: 1.282838819026947\n",
      "Epoch 14, Batch 800, Loss: 1.2685177218914032\n",
      "Epoch 14, Batch 900, Loss: 1.2446082293987275\n",
      "Epoch 15, Batch 100, Loss: 1.2412398338317872\n",
      "Epoch 15, Batch 200, Loss: 1.3636567044258117\n",
      "Epoch 15, Batch 300, Loss: 1.3319124150276185\n",
      "Epoch 15, Batch 400, Loss: 1.229989973306656\n",
      "Epoch 15, Batch 500, Loss: 1.2782762491703032\n",
      "Epoch 15, Batch 600, Loss: 1.2706301879882813\n",
      "Epoch 15, Batch 700, Loss: 1.2368375074863434\n",
      "Epoch 15, Batch 800, Loss: 1.2668133854866028\n",
      "Epoch 15, Batch 900, Loss: 1.3204917347431182\n",
      "Epoch 16, Batch 100, Loss: 1.296008253097534\n",
      "Epoch 16, Batch 200, Loss: 1.2341824650764466\n",
      "Epoch 16, Batch 300, Loss: 1.2983719968795777\n",
      "Epoch 16, Batch 400, Loss: 1.291397830247879\n",
      "Epoch 16, Batch 500, Loss: 1.2786332631111146\n",
      "Epoch 16, Batch 600, Loss: 1.244353107213974\n",
      "Epoch 16, Batch 700, Loss: 1.2456281006336212\n",
      "Epoch 16, Batch 800, Loss: 1.2949077212810516\n",
      "Epoch 16, Batch 900, Loss: 1.434721121788025\n",
      "Epoch 17, Batch 100, Loss: 1.3557740569114685\n",
      "Epoch 17, Batch 200, Loss: 1.3435418021678924\n",
      "Epoch 17, Batch 300, Loss: 1.232003219127655\n",
      "Epoch 17, Batch 400, Loss: 1.308389674425125\n",
      "Epoch 17, Batch 500, Loss: 1.364610732793808\n",
      "Epoch 17, Batch 600, Loss: 1.3462366795539855\n",
      "Epoch 17, Batch 700, Loss: 1.2605436027050019\n",
      "Epoch 17, Batch 800, Loss: 1.3947607946395875\n",
      "Epoch 17, Batch 900, Loss: 1.2742438876628877\n",
      "Epoch 18, Batch 100, Loss: 1.3067221093177794\n",
      "Epoch 18, Batch 200, Loss: 1.2732573187351226\n",
      "Epoch 18, Batch 300, Loss: 1.3076448637247085\n",
      "Epoch 18, Batch 400, Loss: 1.230499086380005\n",
      "Epoch 18, Batch 500, Loss: 1.2466676723957062\n",
      "Epoch 18, Batch 600, Loss: 1.239485999941826\n",
      "Epoch 18, Batch 700, Loss: 1.2368553686141968\n",
      "Epoch 18, Batch 800, Loss: 1.287212415933609\n",
      "Epoch 18, Batch 900, Loss: 1.30435275554657\n",
      "Epoch 19, Batch 100, Loss: 1.2248053848743439\n",
      "Epoch 19, Batch 200, Loss: 1.2313164675235748\n",
      "Epoch 19, Batch 300, Loss: 1.2223338723182677\n",
      "Epoch 19, Batch 400, Loss: 1.2138535284996033\n",
      "Epoch 19, Batch 500, Loss: 1.4367252695560455\n",
      "Epoch 19, Batch 600, Loss: 1.2310395526885987\n",
      "Epoch 19, Batch 700, Loss: 1.2003219574689865\n",
      "Epoch 19, Batch 800, Loss: 1.3029863774776458\n",
      "Epoch 19, Batch 900, Loss: 1.2810631060600282\n",
      "Epoch 20, Batch 100, Loss: 1.3308193230628966\n",
      "Epoch 20, Batch 200, Loss: 1.2406812596321106\n",
      "Epoch 20, Batch 300, Loss: 1.2793132758140564\n",
      "Epoch 20, Batch 400, Loss: 1.2235420191287993\n",
      "Epoch 20, Batch 500, Loss: 1.2317931795120238\n",
      "Epoch 20, Batch 600, Loss: 1.2198800718784333\n",
      "Epoch 20, Batch 700, Loss: 1.1939453673362732\n",
      "Epoch 20, Batch 800, Loss: 1.2779870688915254\n",
      "Epoch 20, Batch 900, Loss: 1.3688290417194366\n",
      "Epoch 21, Batch 100, Loss: 1.216171817779541\n",
      "Epoch 21, Batch 200, Loss: 1.2703149402141571\n",
      "Epoch 21, Batch 300, Loss: 1.2158368015289307\n",
      "Epoch 21, Batch 400, Loss: 1.3312595963478089\n",
      "Epoch 21, Batch 500, Loss: 1.2641625213623047\n",
      "Epoch 21, Batch 600, Loss: 1.2354834616184234\n",
      "Epoch 21, Batch 700, Loss: 1.2277645426988602\n",
      "Epoch 21, Batch 800, Loss: 1.2127619868516921\n",
      "Epoch 21, Batch 900, Loss: 1.3073318827152252\n",
      "Epoch 22, Batch 100, Loss: 1.25093998670578\n",
      "Epoch 22, Batch 200, Loss: 1.2555780398845673\n",
      "Epoch 22, Batch 300, Loss: 1.2664000082015991\n",
      "Epoch 22, Batch 400, Loss: 1.317051500082016\n",
      "Epoch 22, Batch 500, Loss: 1.2655734491348267\n",
      "Epoch 22, Batch 600, Loss: 1.2014635390043258\n",
      "Epoch 22, Batch 700, Loss: 1.358505027294159\n",
      "Epoch 22, Batch 800, Loss: 1.2628757309913636\n",
      "Epoch 22, Batch 900, Loss: 1.2113894975185395\n",
      "Epoch 23, Batch 100, Loss: 1.2893742901086807\n",
      "Epoch 23, Batch 200, Loss: 1.323659934401512\n",
      "Epoch 23, Batch 300, Loss: 1.2087168103456498\n",
      "Epoch 23, Batch 400, Loss: 1.2806568765640258\n",
      "Epoch 23, Batch 500, Loss: 1.3109628295898437\n",
      "Epoch 23, Batch 600, Loss: 1.2518654918670655\n",
      "Epoch 23, Batch 700, Loss: 1.2777689623832702\n",
      "Epoch 23, Batch 800, Loss: 1.315633715391159\n",
      "Epoch 23, Batch 900, Loss: 1.2650406777858734\n",
      "Epoch 24, Batch 100, Loss: 1.2269928026199342\n",
      "Epoch 24, Batch 200, Loss: 1.2839855074882507\n",
      "Epoch 24, Batch 300, Loss: 1.3451167261600494\n",
      "Epoch 24, Batch 400, Loss: 1.2619576227664948\n",
      "Epoch 24, Batch 500, Loss: 1.2274357235431672\n",
      "Epoch 24, Batch 600, Loss: 1.2413467597961425\n",
      "Epoch 24, Batch 700, Loss: 1.3244222104549408\n",
      "Epoch 24, Batch 800, Loss: 1.295052682161331\n",
      "Epoch 24, Batch 900, Loss: 1.261791888475418\n",
      "Epoch 25, Batch 100, Loss: 1.233400456905365\n",
      "Epoch 25, Batch 200, Loss: 1.2167722654342652\n",
      "Epoch 25, Batch 300, Loss: 1.2270847523212434\n",
      "Epoch 25, Batch 400, Loss: 1.3163823568820954\n",
      "Epoch 25, Batch 500, Loss: 1.4049945628643037\n",
      "Epoch 25, Batch 600, Loss: 1.2444242608547211\n",
      "Epoch 25, Batch 700, Loss: 1.2337453162670136\n",
      "Epoch 25, Batch 800, Loss: 1.2129399764537812\n",
      "Epoch 25, Batch 900, Loss: 1.3227222907543181\n",
      "Accuracy on test set: 0.2135%\n",
      "Fitting for combination 31\n",
      "784\n",
      "2\n",
      "10\n",
      "[40, 30, 10]\n",
      "True\n",
      "['relu', 'relu']\n",
      "SGD\n",
      "0.3\n",
      "0.1\n",
      "CrossEntropyLoss\n",
      "50\n",
      "25\n",
      "0\n",
      "Epoch 1, Batch 50, Loss: 0.8272808194160461\n",
      "Epoch 1, Batch 100, Loss: 0.7016888856887817\n",
      "Epoch 1, Batch 150, Loss: 0.6680067330598831\n",
      "Epoch 1, Batch 200, Loss: 0.6504687148332596\n",
      "Epoch 1, Batch 250, Loss: 0.6443329191207886\n",
      "Epoch 1, Batch 300, Loss: 0.6447239923477173\n",
      "Epoch 1, Batch 350, Loss: 0.6061951124668121\n",
      "Epoch 1, Batch 400, Loss: 0.633748141527176\n",
      "Epoch 1, Batch 450, Loss: 0.6247843426465988\n",
      "Epoch 1, Batch 500, Loss: 0.6118345701694489\n",
      "Epoch 1, Batch 550, Loss: 0.6599955815076828\n",
      "Epoch 1, Batch 600, Loss: 0.6845625978708267\n",
      "Epoch 1, Batch 650, Loss: 0.6697607779502869\n",
      "Epoch 1, Batch 700, Loss: 0.644087843298912\n",
      "Epoch 1, Batch 750, Loss: 0.6232810097932816\n",
      "Epoch 1, Batch 800, Loss: 0.6504984360933304\n",
      "Epoch 1, Batch 850, Loss: 0.6360532236099243\n",
      "Epoch 1, Batch 900, Loss: 0.631020700931549\n",
      "Epoch 2, Batch 50, Loss: 0.6591212791204453\n",
      "Epoch 2, Batch 100, Loss: 0.6275337541103363\n",
      "Epoch 2, Batch 150, Loss: 0.6417017149925232\n",
      "Epoch 2, Batch 200, Loss: 0.6197474020719528\n",
      "Epoch 2, Batch 250, Loss: 0.6539544367790222\n",
      "Epoch 2, Batch 300, Loss: 0.6774951159954071\n",
      "Epoch 2, Batch 350, Loss: 0.6644400012493134\n",
      "Epoch 2, Batch 400, Loss: 0.6373794662952423\n",
      "Epoch 2, Batch 450, Loss: 0.6576136726140976\n",
      "Epoch 2, Batch 500, Loss: 0.6219536280632019\n",
      "Epoch 2, Batch 550, Loss: 0.6088853538036346\n",
      "Epoch 2, Batch 600, Loss: 0.5927780205011368\n",
      "Epoch 2, Batch 650, Loss: 0.6158029729127884\n",
      "Epoch 2, Batch 700, Loss: 0.5963208842277526\n",
      "Epoch 2, Batch 750, Loss: 0.6286051440238952\n",
      "Epoch 2, Batch 800, Loss: 0.6241380131244659\n",
      "Epoch 2, Batch 850, Loss: 0.6387748557329178\n",
      "Epoch 2, Batch 900, Loss: 0.6618128395080567\n",
      "Epoch 3, Batch 50, Loss: 0.6340901184082032\n",
      "Epoch 3, Batch 100, Loss: 0.6140708959102631\n",
      "Epoch 3, Batch 150, Loss: 0.611894418001175\n",
      "Epoch 3, Batch 200, Loss: 0.6180062305927276\n",
      "Epoch 3, Batch 250, Loss: 0.6128947257995605\n",
      "Epoch 3, Batch 300, Loss: 0.6266649299860001\n",
      "Epoch 3, Batch 350, Loss: 0.6651494330167771\n",
      "Epoch 3, Batch 400, Loss: 0.6193666517734527\n",
      "Epoch 3, Batch 450, Loss: 0.6188344520330429\n",
      "Epoch 3, Batch 500, Loss: 0.6109587389230728\n",
      "Epoch 3, Batch 550, Loss: 0.6400657403469086\n",
      "Epoch 3, Batch 600, Loss: 0.6652069008350372\n",
      "Epoch 3, Batch 650, Loss: 0.6516864842176437\n",
      "Epoch 3, Batch 700, Loss: 0.7536984169483185\n",
      "Epoch 3, Batch 750, Loss: 0.7123602294921875\n",
      "Epoch 3, Batch 800, Loss: 0.7054564034938813\n",
      "Epoch 3, Batch 850, Loss: 0.6963575714826584\n",
      "Epoch 3, Batch 900, Loss: 0.6701856637001038\n",
      "Epoch 4, Batch 50, Loss: 0.688660923242569\n",
      "Epoch 4, Batch 100, Loss: 0.7672580659389496\n",
      "Epoch 4, Batch 150, Loss: 0.7050147473812103\n",
      "Epoch 4, Batch 200, Loss: 0.6667780023813248\n",
      "Epoch 4, Batch 250, Loss: 0.681619883775711\n",
      "Epoch 4, Batch 300, Loss: 0.6666253817081451\n",
      "Epoch 4, Batch 350, Loss: 0.6630805617570877\n",
      "Epoch 4, Batch 400, Loss: 0.6466453158855439\n",
      "Epoch 4, Batch 450, Loss: 0.7538388359546662\n",
      "Epoch 4, Batch 500, Loss: 0.8656460916996003\n",
      "Epoch 4, Batch 550, Loss: 0.8348946785926818\n",
      "Epoch 4, Batch 600, Loss: 0.8601655578613281\n",
      "Epoch 4, Batch 650, Loss: 0.8623786079883575\n",
      "Epoch 4, Batch 700, Loss: 0.8237984776496887\n",
      "Epoch 4, Batch 750, Loss: 0.8608889639377594\n",
      "Epoch 4, Batch 800, Loss: 0.8381782793998718\n",
      "Epoch 4, Batch 850, Loss: 0.8156257152557373\n",
      "Epoch 4, Batch 900, Loss: 0.9482775259017945\n",
      "Epoch 5, Batch 50, Loss: 0.7565001881122589\n",
      "Epoch 5, Batch 100, Loss: 0.8051188933849335\n",
      "Epoch 5, Batch 150, Loss: 0.8999760401248932\n",
      "Epoch 5, Batch 200, Loss: 1.0284360241889954\n",
      "Epoch 5, Batch 250, Loss: 1.0114283180236816\n",
      "Epoch 5, Batch 300, Loss: 1.011390633583069\n",
      "Epoch 5, Batch 350, Loss: 0.9980802297592163\n",
      "Epoch 5, Batch 400, Loss: 0.9976975095272064\n",
      "Epoch 5, Batch 450, Loss: 1.0045742785930634\n",
      "Epoch 5, Batch 500, Loss: 1.0080302143096924\n",
      "Epoch 5, Batch 550, Loss: 1.005764832496643\n",
      "Epoch 5, Batch 600, Loss: 1.0020070636272431\n",
      "Epoch 5, Batch 650, Loss: 0.9990299284458161\n",
      "Epoch 5, Batch 700, Loss: 1.0112788486480713\n",
      "Epoch 5, Batch 750, Loss: 1.0019759881496428\n",
      "Epoch 5, Batch 800, Loss: 1.003343461751938\n",
      "Epoch 5, Batch 850, Loss: 1.0059757781028749\n",
      "Epoch 5, Batch 900, Loss: 1.0084463512897492\n",
      "Epoch 6, Batch 50, Loss: 0.9964313578605651\n",
      "Epoch 6, Batch 100, Loss: 0.9805158972740173\n",
      "Epoch 6, Batch 150, Loss: 0.9835114240646362\n",
      "Epoch 6, Batch 200, Loss: 0.9912936651706695\n",
      "Epoch 6, Batch 250, Loss: 1.003923295736313\n",
      "Epoch 6, Batch 300, Loss: 0.9970998811721802\n",
      "Epoch 6, Batch 350, Loss: 0.9856023573875428\n",
      "Epoch 6, Batch 400, Loss: 0.9849243259429932\n",
      "Epoch 6, Batch 450, Loss: 0.9913791561126709\n",
      "Epoch 6, Batch 500, Loss: 0.9814441895484924\n",
      "Epoch 6, Batch 550, Loss: 0.9930898261070251\n",
      "Epoch 6, Batch 600, Loss: 0.9977165567874908\n",
      "Epoch 6, Batch 650, Loss: 0.9792986607551575\n",
      "Epoch 6, Batch 700, Loss: 0.9847087836265564\n",
      "Epoch 6, Batch 750, Loss: 0.9785815525054932\n",
      "Epoch 6, Batch 800, Loss: 0.9850285303592682\n",
      "Epoch 6, Batch 850, Loss: 0.9902919912338257\n",
      "Epoch 6, Batch 900, Loss: 0.9902077996730805\n",
      "Epoch 7, Batch 50, Loss: 0.982441987991333\n",
      "Epoch 7, Batch 100, Loss: 0.9850952255725861\n",
      "Epoch 7, Batch 150, Loss: 0.9789723074436187\n",
      "Epoch 7, Batch 200, Loss: 0.9837638473510742\n",
      "Epoch 7, Batch 250, Loss: 0.9920603740215301\n",
      "Epoch 7, Batch 300, Loss: 1.0059938418865204\n",
      "Epoch 7, Batch 350, Loss: 0.9910477447509766\n",
      "Epoch 7, Batch 400, Loss: 0.981354296207428\n",
      "Epoch 7, Batch 450, Loss: 0.9829888534545899\n",
      "Epoch 7, Batch 500, Loss: 0.9835917174816131\n",
      "Epoch 7, Batch 550, Loss: 1.0104185700416566\n",
      "Epoch 7, Batch 600, Loss: 0.9950447642803192\n",
      "Epoch 7, Batch 650, Loss: 0.9912291646003724\n",
      "Epoch 7, Batch 700, Loss: 0.9787836241722107\n",
      "Epoch 7, Batch 750, Loss: 0.996965434551239\n",
      "Epoch 7, Batch 800, Loss: 0.9867074072360993\n",
      "Epoch 7, Batch 850, Loss: 1.003918048143387\n",
      "Epoch 7, Batch 900, Loss: 0.9836345517635345\n",
      "Epoch 8, Batch 50, Loss: 0.9914523184299469\n",
      "Epoch 8, Batch 100, Loss: 0.9899837350845337\n",
      "Epoch 8, Batch 150, Loss: 0.9851644086837769\n",
      "Epoch 8, Batch 200, Loss: 0.9931717586517333\n",
      "Epoch 8, Batch 250, Loss: 1.0007070863246919\n",
      "Epoch 8, Batch 300, Loss: 0.9793934321403504\n",
      "Epoch 8, Batch 350, Loss: 0.9811918437480927\n",
      "Epoch 8, Batch 400, Loss: 0.9839611768722534\n",
      "Epoch 8, Batch 450, Loss: 0.9785924100875855\n",
      "Epoch 8, Batch 500, Loss: 0.9827483606338501\n",
      "Epoch 8, Batch 550, Loss: 0.990322642326355\n",
      "Epoch 8, Batch 600, Loss: 0.990418199300766\n",
      "Epoch 8, Batch 650, Loss: 0.9917496645450592\n",
      "Epoch 8, Batch 700, Loss: 0.9868441879749298\n",
      "Epoch 8, Batch 750, Loss: 0.9920624303817749\n",
      "Epoch 8, Batch 800, Loss: 0.9852015411853791\n",
      "Epoch 8, Batch 850, Loss: 0.9912396824359894\n",
      "Epoch 8, Batch 900, Loss: 0.9789781177043915\n",
      "Epoch 9, Batch 50, Loss: 0.9866321337223053\n",
      "Epoch 9, Batch 100, Loss: 0.9886458253860474\n",
      "Epoch 9, Batch 150, Loss: 0.9774986863136291\n",
      "Epoch 9, Batch 200, Loss: 0.9802705073356628\n",
      "Epoch 9, Batch 250, Loss: 0.9868478655815125\n",
      "Epoch 9, Batch 300, Loss: 0.9931040155887604\n",
      "Epoch 9, Batch 350, Loss: 0.9870132493972779\n",
      "Epoch 9, Batch 400, Loss: 0.981177054643631\n",
      "Epoch 9, Batch 450, Loss: 0.980570204257965\n",
      "Epoch 9, Batch 500, Loss: 0.9809792006015777\n",
      "Epoch 9, Batch 550, Loss: 0.9987720167636871\n",
      "Epoch 9, Batch 600, Loss: 0.9857063114643096\n",
      "Epoch 9, Batch 650, Loss: 0.9829839611053467\n",
      "Epoch 9, Batch 700, Loss: 0.9877507972717285\n",
      "Epoch 9, Batch 750, Loss: 0.9902225768566132\n",
      "Epoch 9, Batch 800, Loss: 0.9826617324352265\n",
      "Epoch 9, Batch 850, Loss: 0.9891287767887116\n",
      "Epoch 9, Batch 900, Loss: 0.9918334209918975\n",
      "Epoch 10, Batch 50, Loss: 0.9945852601528168\n",
      "Epoch 10, Batch 100, Loss: 0.9814346837997436\n",
      "Epoch 10, Batch 150, Loss: 0.985896577835083\n",
      "Epoch 10, Batch 200, Loss: 0.9972045147418975\n",
      "Epoch 10, Batch 250, Loss: 0.9826973569393158\n",
      "Epoch 10, Batch 300, Loss: 0.9884806823730469\n",
      "Epoch 10, Batch 350, Loss: 0.9883930587768555\n",
      "Epoch 10, Batch 400, Loss: 0.9910620403289795\n",
      "Epoch 10, Batch 450, Loss: 1.0003826165199279\n",
      "Epoch 10, Batch 500, Loss: 0.9905098378658295\n",
      "Epoch 10, Batch 550, Loss: 0.9756063759326935\n",
      "Epoch 10, Batch 600, Loss: 0.9771425318717957\n",
      "Epoch 10, Batch 650, Loss: 0.9854170572757721\n",
      "Epoch 10, Batch 700, Loss: 1.0125301909446716\n",
      "Epoch 10, Batch 750, Loss: 0.9950804448127747\n",
      "Epoch 10, Batch 800, Loss: 0.9840899765491485\n",
      "Epoch 10, Batch 850, Loss: 0.9962638342380523\n",
      "Epoch 10, Batch 900, Loss: 0.9866906797885895\n",
      "Epoch 11, Batch 50, Loss: 0.9934850609302521\n",
      "Epoch 11, Batch 100, Loss: 0.9814672338962555\n",
      "Epoch 11, Batch 150, Loss: 0.9853229832649231\n",
      "Epoch 11, Batch 200, Loss: 0.97613645195961\n",
      "Epoch 11, Batch 250, Loss: 0.9911064052581787\n",
      "Epoch 11, Batch 300, Loss: 0.9790360021591187\n",
      "Epoch 11, Batch 350, Loss: 0.9889809823036194\n",
      "Epoch 11, Batch 400, Loss: 1.0000559329986571\n",
      "Epoch 11, Batch 450, Loss: 0.9941764664649964\n",
      "Epoch 11, Batch 500, Loss: 1.0058235907554627\n",
      "Epoch 11, Batch 550, Loss: 0.9911458146572113\n",
      "Epoch 11, Batch 600, Loss: 0.9842878341674804\n",
      "Epoch 11, Batch 650, Loss: 0.9854495024681091\n",
      "Epoch 11, Batch 700, Loss: 0.9956083548069\n",
      "Epoch 11, Batch 750, Loss: 0.9886137497425079\n",
      "Epoch 11, Batch 800, Loss: 0.9986204624176025\n",
      "Epoch 11, Batch 850, Loss: 0.9866967642307282\n",
      "Epoch 11, Batch 900, Loss: 0.9798385298252106\n",
      "Epoch 12, Batch 50, Loss: 0.9884020221233368\n",
      "Epoch 12, Batch 100, Loss: 0.9866701030731201\n",
      "Epoch 12, Batch 150, Loss: 0.977022932767868\n",
      "Epoch 12, Batch 200, Loss: 0.9861360192298889\n",
      "Epoch 12, Batch 250, Loss: 0.9912752461433411\n",
      "Epoch 12, Batch 300, Loss: 0.9853850424289703\n",
      "Epoch 12, Batch 350, Loss: 0.9819528937339783\n",
      "Epoch 12, Batch 400, Loss: 0.9906443953514099\n",
      "Epoch 12, Batch 450, Loss: 0.991127746105194\n",
      "Epoch 12, Batch 500, Loss: 0.9832171940803528\n",
      "Epoch 12, Batch 550, Loss: 0.9845738816261291\n",
      "Epoch 12, Batch 600, Loss: 0.9922218000888825\n",
      "Epoch 12, Batch 650, Loss: 0.9844760620594024\n",
      "Epoch 12, Batch 700, Loss: 0.9905124032497405\n",
      "Epoch 12, Batch 750, Loss: 0.986838207244873\n",
      "Epoch 12, Batch 800, Loss: 0.980367511510849\n",
      "Epoch 12, Batch 850, Loss: 0.9904448688030243\n",
      "Epoch 12, Batch 900, Loss: 0.9826330578327179\n",
      "Epoch 13, Batch 50, Loss: 0.9972248256206513\n",
      "Epoch 13, Batch 100, Loss: 0.9985697782039642\n",
      "Epoch 13, Batch 150, Loss: 0.991853758096695\n",
      "Epoch 13, Batch 200, Loss: 0.9903555107116699\n",
      "Epoch 13, Batch 250, Loss: 0.9897617661952972\n",
      "Epoch 13, Batch 300, Loss: 0.9927958250045776\n",
      "Epoch 13, Batch 350, Loss: 0.9769636142253876\n",
      "Epoch 13, Batch 400, Loss: 0.9874976634979248\n",
      "Epoch 13, Batch 450, Loss: 0.9918353092670441\n",
      "Epoch 13, Batch 500, Loss: 0.9876276266574859\n",
      "Epoch 13, Batch 550, Loss: 0.9822649919986725\n",
      "Epoch 13, Batch 600, Loss: 0.993963862657547\n",
      "Epoch 13, Batch 650, Loss: 0.9761422491073608\n",
      "Epoch 13, Batch 700, Loss: 0.99406409740448\n",
      "Epoch 13, Batch 750, Loss: 0.9916119706630707\n",
      "Epoch 13, Batch 800, Loss: 0.9809056389331817\n",
      "Epoch 13, Batch 850, Loss: 0.974954183101654\n",
      "Epoch 13, Batch 900, Loss: 0.9904363238811493\n",
      "Epoch 14, Batch 50, Loss: 0.9935845351219177\n",
      "Epoch 14, Batch 100, Loss: 0.988971266746521\n",
      "Epoch 14, Batch 150, Loss: 0.9880636870861054\n",
      "Epoch 14, Batch 200, Loss: 0.9793842589855194\n",
      "Epoch 14, Batch 250, Loss: 0.990744080543518\n",
      "Epoch 14, Batch 300, Loss: 0.9877390587329864\n",
      "Epoch 14, Batch 350, Loss: 0.9870168387889862\n",
      "Epoch 14, Batch 400, Loss: 0.9849262857437133\n",
      "Epoch 14, Batch 450, Loss: 1.0118418502807618\n",
      "Epoch 14, Batch 500, Loss: 0.9949683570861816\n",
      "Epoch 14, Batch 550, Loss: 0.9861348450183869\n",
      "Epoch 14, Batch 600, Loss: 0.9888571763038635\n",
      "Epoch 14, Batch 650, Loss: 0.9832643020153046\n",
      "Epoch 14, Batch 700, Loss: 0.9835206580162048\n",
      "Epoch 14, Batch 750, Loss: 0.9913710415363312\n",
      "Epoch 14, Batch 800, Loss: 0.9873695862293244\n",
      "Epoch 14, Batch 850, Loss: 0.9809679508209228\n",
      "Epoch 14, Batch 900, Loss: 0.9849800252914429\n",
      "Epoch 15, Batch 50, Loss: 0.9960085654258728\n",
      "Epoch 15, Batch 100, Loss: 0.9942433333396912\n",
      "Epoch 15, Batch 150, Loss: 0.9869662117958069\n",
      "Epoch 15, Batch 200, Loss: 1.1196059012413024\n",
      "Epoch 15, Batch 250, Loss: 1.1512538504600525\n",
      "Epoch 15, Batch 300, Loss: 1.1515534734725952\n",
      "Epoch 15, Batch 350, Loss: 1.1518952083587646\n",
      "Epoch 15, Batch 400, Loss: 1.1517084908485413\n",
      "Epoch 15, Batch 450, Loss: 1.1515475916862488\n",
      "Epoch 15, Batch 500, Loss: 1.1517463374137877\n",
      "Epoch 15, Batch 550, Loss: 1.15118239402771\n",
      "Epoch 15, Batch 600, Loss: 1.1515080451965332\n",
      "Epoch 15, Batch 650, Loss: 1.152013213634491\n",
      "Epoch 15, Batch 700, Loss: 1.1515831494331359\n",
      "Epoch 15, Batch 750, Loss: 1.1517750787734986\n",
      "Epoch 15, Batch 800, Loss: 1.1515878796577455\n",
      "Epoch 15, Batch 850, Loss: 1.151778881549835\n",
      "Epoch 15, Batch 900, Loss: 1.1511491870880126\n",
      "Epoch 16, Batch 50, Loss: 1.1515019989013673\n",
      "Epoch 16, Batch 100, Loss: 1.1520607018470763\n",
      "Epoch 16, Batch 150, Loss: 1.151551241874695\n",
      "Epoch 16, Batch 200, Loss: 1.1518053770065309\n",
      "Epoch 16, Batch 250, Loss: 1.1517386031150818\n",
      "Epoch 16, Batch 300, Loss: 1.1516372752189636\n",
      "Epoch 16, Batch 350, Loss: 1.1516084599494933\n",
      "Epoch 16, Batch 400, Loss: 1.151478090286255\n",
      "Epoch 16, Batch 450, Loss: 1.1518136382102966\n",
      "Epoch 16, Batch 500, Loss: 1.151920268535614\n",
      "Epoch 16, Batch 550, Loss: 1.1514887189865113\n",
      "Epoch 16, Batch 600, Loss: 1.1514579510688783\n",
      "Epoch 16, Batch 650, Loss: 1.1517948961257936\n",
      "Epoch 16, Batch 700, Loss: 1.1519301080703734\n",
      "Epoch 16, Batch 750, Loss: 1.1514827871322633\n",
      "Epoch 16, Batch 800, Loss: 1.151873288154602\n",
      "Epoch 16, Batch 850, Loss: 1.1518330597877502\n",
      "Epoch 16, Batch 900, Loss: 1.1514484286308289\n",
      "Epoch 17, Batch 50, Loss: 1.151456971168518\n",
      "Epoch 17, Batch 100, Loss: 1.1517058634757995\n",
      "Epoch 17, Batch 150, Loss: 1.1515186142921447\n",
      "Epoch 17, Batch 200, Loss: 1.151869626045227\n",
      "Epoch 17, Batch 250, Loss: 1.1512177729606627\n",
      "Epoch 17, Batch 300, Loss: 1.151596529483795\n",
      "Epoch 17, Batch 350, Loss: 1.1519200229644775\n",
      "Epoch 17, Batch 400, Loss: 1.1517157101631164\n",
      "Epoch 17, Batch 450, Loss: 1.1517582368850707\n",
      "Epoch 17, Batch 500, Loss: 1.151476879119873\n",
      "Epoch 17, Batch 550, Loss: 1.151253321170807\n",
      "Epoch 17, Batch 600, Loss: 1.1520680141448976\n",
      "Epoch 17, Batch 650, Loss: 1.151604106426239\n",
      "Epoch 17, Batch 700, Loss: 1.151778268814087\n",
      "Epoch 17, Batch 750, Loss: 1.151293752193451\n",
      "Epoch 17, Batch 800, Loss: 1.15135694026947\n",
      "Epoch 17, Batch 850, Loss: 1.15193213224411\n",
      "Epoch 17, Batch 900, Loss: 1.1517453241348266\n",
      "Epoch 18, Batch 50, Loss: 1.151521348953247\n",
      "Epoch 18, Batch 100, Loss: 1.1517249083518981\n",
      "Epoch 18, Batch 150, Loss: 1.151838195323944\n",
      "Epoch 18, Batch 200, Loss: 1.1513494729995728\n",
      "Epoch 18, Batch 250, Loss: 1.1513227534294128\n",
      "Epoch 18, Batch 300, Loss: 1.1515552926063537\n",
      "Epoch 18, Batch 350, Loss: 1.1515265536308288\n",
      "Epoch 18, Batch 400, Loss: 1.1517716598510743\n",
      "Epoch 18, Batch 450, Loss: 1.1518161249160768\n",
      "Epoch 18, Batch 500, Loss: 1.1512858939170838\n",
      "Epoch 18, Batch 550, Loss: 1.1514821362495422\n",
      "Epoch 18, Batch 600, Loss: 1.1516906142234802\n",
      "Epoch 18, Batch 650, Loss: 1.1516197991371155\n",
      "Epoch 18, Batch 700, Loss: 1.1518723106384277\n",
      "Epoch 18, Batch 750, Loss: 1.151502981185913\n",
      "Epoch 18, Batch 800, Loss: 1.1516106009483338\n",
      "Epoch 18, Batch 850, Loss: 1.152081298828125\n",
      "Epoch 18, Batch 900, Loss: 1.1514912700653077\n",
      "Epoch 19, Batch 50, Loss: 1.1516725945472717\n",
      "Epoch 19, Batch 100, Loss: 1.1517860579490662\n",
      "Epoch 19, Batch 150, Loss: 1.1516393876075746\n",
      "Epoch 19, Batch 200, Loss: 1.1513396096229553\n",
      "Epoch 19, Batch 250, Loss: 1.1517766165733336\n",
      "Epoch 19, Batch 300, Loss: 1.1516675519943238\n",
      "Epoch 19, Batch 350, Loss: 1.1514321541786194\n",
      "Epoch 19, Batch 400, Loss: 1.1515774512290955\n",
      "Epoch 19, Batch 450, Loss: 1.1517555332183838\n",
      "Epoch 19, Batch 500, Loss: 1.1514627575874328\n",
      "Epoch 19, Batch 550, Loss: 1.1515645027160644\n",
      "Epoch 19, Batch 600, Loss: 1.1517653489112853\n",
      "Epoch 19, Batch 650, Loss: 1.1514457988739013\n",
      "Epoch 19, Batch 700, Loss: 1.1516941690444946\n",
      "Epoch 19, Batch 750, Loss: 1.151518485546112\n",
      "Epoch 19, Batch 800, Loss: 1.1515696167945861\n",
      "Epoch 19, Batch 850, Loss: 1.151636896133423\n",
      "Epoch 19, Batch 900, Loss: 1.151825523376465\n",
      "Epoch 20, Batch 50, Loss: 1.1513841700553895\n",
      "Epoch 20, Batch 100, Loss: 1.151336793899536\n",
      "Epoch 20, Batch 150, Loss: 1.1513900995254516\n",
      "Epoch 20, Batch 200, Loss: 1.1514767241477966\n",
      "Epoch 20, Batch 250, Loss: 1.1515881180763246\n",
      "Epoch 20, Batch 300, Loss: 1.1517203807830811\n",
      "Epoch 20, Batch 350, Loss: 1.1517645597457886\n",
      "Epoch 20, Batch 400, Loss: 1.1514457154273987\n",
      "Epoch 20, Batch 450, Loss: 1.1512300205230712\n",
      "Epoch 20, Batch 500, Loss: 1.1514003801345825\n",
      "Epoch 20, Batch 550, Loss: 1.1517870426177979\n",
      "Epoch 20, Batch 600, Loss: 1.1517631196975708\n",
      "Epoch 20, Batch 650, Loss: 1.1516242599487305\n",
      "Epoch 20, Batch 700, Loss: 1.1516565370559693\n",
      "Epoch 20, Batch 750, Loss: 1.1517263674736022\n",
      "Epoch 20, Batch 800, Loss: 1.151597547531128\n",
      "Epoch 20, Batch 850, Loss: 1.1511887621879577\n",
      "Epoch 20, Batch 900, Loss: 1.1517298555374145\n",
      "Epoch 21, Batch 50, Loss: 1.151361358165741\n",
      "Epoch 21, Batch 100, Loss: 1.1518303561210632\n",
      "Epoch 21, Batch 150, Loss: 1.151266598701477\n",
      "Epoch 21, Batch 200, Loss: 1.1511270117759704\n",
      "Epoch 21, Batch 250, Loss: 1.1514708614349365\n",
      "Epoch 21, Batch 300, Loss: 1.1516745734214782\n",
      "Epoch 21, Batch 350, Loss: 1.1515917372703552\n",
      "Epoch 21, Batch 400, Loss: 1.151765296459198\n",
      "Epoch 21, Batch 450, Loss: 1.1518408226966859\n",
      "Epoch 21, Batch 500, Loss: 1.1519380021095276\n",
      "Epoch 21, Batch 550, Loss: 1.151732497215271\n",
      "Epoch 21, Batch 600, Loss: 1.1515437960624695\n",
      "Epoch 21, Batch 650, Loss: 1.151739044189453\n",
      "Epoch 21, Batch 700, Loss: 1.1516736674308776\n",
      "Epoch 21, Batch 750, Loss: 1.1515979695320129\n",
      "Epoch 21, Batch 800, Loss: 1.1513638043403625\n",
      "Epoch 21, Batch 850, Loss: 1.1516679096221925\n",
      "Epoch 21, Batch 900, Loss: 1.1518470430374146\n",
      "Epoch 22, Batch 50, Loss: 1.152103672027588\n",
      "Epoch 22, Batch 100, Loss: 1.1516261029243469\n",
      "Epoch 22, Batch 150, Loss: 1.1511940598487853\n",
      "Epoch 22, Batch 200, Loss: 1.1517600297927857\n",
      "Epoch 22, Batch 250, Loss: 1.1514056491851807\n",
      "Epoch 22, Batch 300, Loss: 1.1511691117286682\n",
      "Epoch 22, Batch 350, Loss: 1.1517996549606324\n",
      "Epoch 22, Batch 400, Loss: 1.1513577342033385\n",
      "Epoch 22, Batch 450, Loss: 1.151404047012329\n",
      "Epoch 22, Batch 500, Loss: 1.1519391274452209\n",
      "Epoch 22, Batch 550, Loss: 1.1517116928100586\n",
      "Epoch 22, Batch 600, Loss: 1.1516057205200196\n",
      "Epoch 22, Batch 650, Loss: 1.151394443511963\n",
      "Epoch 22, Batch 700, Loss: 1.1517568802833558\n",
      "Epoch 22, Batch 750, Loss: 1.1519083571434021\n",
      "Epoch 22, Batch 800, Loss: 1.1517563247680664\n",
      "Epoch 22, Batch 850, Loss: 1.1516555523872376\n",
      "Epoch 22, Batch 900, Loss: 1.1514928555488586\n",
      "Epoch 23, Batch 50, Loss: 1.1516411232948303\n",
      "Epoch 23, Batch 100, Loss: 1.1513402128219605\n",
      "Epoch 23, Batch 150, Loss: 1.15151273727417\n",
      "Epoch 23, Batch 200, Loss: 1.1512761163711547\n",
      "Epoch 23, Batch 250, Loss: 1.1515323805809021\n",
      "Epoch 23, Batch 300, Loss: 1.1519244742393493\n",
      "Epoch 23, Batch 350, Loss: 1.151465950012207\n",
      "Epoch 23, Batch 400, Loss: 1.1511960959434508\n",
      "Epoch 23, Batch 450, Loss: 1.151819577217102\n",
      "Epoch 23, Batch 500, Loss: 1.1515298891067505\n",
      "Epoch 23, Batch 550, Loss: 1.1518641662597657\n",
      "Epoch 23, Batch 600, Loss: 1.1513421297073365\n",
      "Epoch 23, Batch 650, Loss: 1.1515029096603393\n",
      "Epoch 23, Batch 700, Loss: 1.1513061428070068\n",
      "Epoch 23, Batch 750, Loss: 1.1515531516075135\n",
      "Epoch 23, Batch 800, Loss: 1.1516076183319093\n",
      "Epoch 23, Batch 850, Loss: 1.1518423676490783\n",
      "Epoch 23, Batch 900, Loss: 1.151618890762329\n",
      "Epoch 24, Batch 50, Loss: 1.1515631866455078\n",
      "Epoch 24, Batch 100, Loss: 1.1519645023345948\n",
      "Epoch 24, Batch 150, Loss: 1.1514933013916016\n",
      "Epoch 24, Batch 200, Loss: 1.1515511417388915\n",
      "Epoch 24, Batch 250, Loss: 1.1515359449386597\n",
      "Epoch 24, Batch 300, Loss: 1.1517089819908142\n",
      "Epoch 24, Batch 350, Loss: 1.151736581325531\n",
      "Epoch 24, Batch 400, Loss: 1.1515634727478028\n",
      "Epoch 24, Batch 450, Loss: 1.1513437175750731\n",
      "Epoch 24, Batch 500, Loss: 1.1516035604476929\n",
      "Epoch 24, Batch 550, Loss: 1.1515640997886658\n",
      "Epoch 24, Batch 600, Loss: 1.151772105693817\n",
      "Epoch 24, Batch 650, Loss: 1.1514048719406127\n",
      "Epoch 24, Batch 700, Loss: 1.1511617612838745\n",
      "Epoch 24, Batch 750, Loss: 1.1517894673347473\n",
      "Epoch 24, Batch 800, Loss: 1.1519282102584838\n",
      "Epoch 24, Batch 850, Loss: 1.1513990187644958\n",
      "Epoch 24, Batch 900, Loss: 1.1519113683700561\n",
      "Epoch 25, Batch 50, Loss: 1.1517137956619263\n",
      "Epoch 25, Batch 100, Loss: 1.151428542137146\n",
      "Epoch 25, Batch 150, Loss: 1.1515985369682311\n",
      "Epoch 25, Batch 200, Loss: 1.1516285610198975\n",
      "Epoch 25, Batch 250, Loss: 1.1520489001274108\n",
      "Epoch 25, Batch 300, Loss: 1.1517507457733154\n",
      "Epoch 25, Batch 350, Loss: 1.1517382264137268\n",
      "Epoch 25, Batch 400, Loss: 1.151574969291687\n",
      "Epoch 25, Batch 450, Loss: 1.1512448120117187\n",
      "Epoch 25, Batch 500, Loss: 1.1516945719718934\n",
      "Epoch 25, Batch 550, Loss: 1.1519432139396668\n",
      "Epoch 25, Batch 600, Loss: 1.1516469955444335\n",
      "Epoch 25, Batch 650, Loss: 1.1516891980171204\n",
      "Epoch 25, Batch 700, Loss: 1.151330099105835\n",
      "Epoch 25, Batch 750, Loss: 1.1517544436454772\n",
      "Epoch 25, Batch 800, Loss: 1.1514304494857788\n",
      "Epoch 25, Batch 850, Loss: 1.150932059288025\n",
      "Epoch 25, Batch 900, Loss: 1.1518899393081665\n",
      "Accuracy on test set: 0.1%\n",
      "Fitting for combination 32\n",
      "784\n",
      "2\n",
      "10\n",
      "[40, 40, 10]\n",
      "True\n",
      "['relu', 'sigmoid']\n",
      "Adam\n",
      "0.03\n",
      "0.03\n",
      "CrossEntropyLoss\n",
      "50\n",
      "25\n",
      "0\n",
      "Epoch 1, Batch 50, Loss: 0.710342088341713\n",
      "Epoch 1, Batch 100, Loss: 0.5532188695669175\n",
      "Epoch 1, Batch 150, Loss: 0.5192642271518707\n",
      "Epoch 1, Batch 200, Loss: 0.5209268355369567\n",
      "Epoch 1, Batch 250, Loss: 0.518799420595169\n",
      "Epoch 1, Batch 300, Loss: 0.5144812721014023\n",
      "Epoch 1, Batch 350, Loss: 0.5103117573261261\n",
      "Epoch 1, Batch 400, Loss: 0.500470694899559\n",
      "Epoch 1, Batch 450, Loss: 0.5022439336776734\n",
      "Epoch 1, Batch 500, Loss: 0.491184623837471\n",
      "Epoch 1, Batch 550, Loss: 0.5193235498666763\n",
      "Epoch 1, Batch 600, Loss: 0.5007874923944473\n",
      "Epoch 1, Batch 650, Loss: 0.5202443683147431\n",
      "Epoch 1, Batch 700, Loss: 0.5125403344631195\n",
      "Epoch 1, Batch 750, Loss: 0.5233669435977936\n",
      "Epoch 1, Batch 800, Loss: 0.5161967319250107\n",
      "Epoch 1, Batch 850, Loss: 0.52753937125206\n",
      "Epoch 1, Batch 900, Loss: 0.5104466235637665\n",
      "Epoch 2, Batch 50, Loss: 0.520376997590065\n",
      "Epoch 2, Batch 100, Loss: 0.5105471044778824\n",
      "Epoch 2, Batch 150, Loss: 0.4991422432661057\n",
      "Epoch 2, Batch 200, Loss: 0.4850667518377304\n",
      "Epoch 2, Batch 250, Loss: 0.5148494213819503\n",
      "Epoch 2, Batch 300, Loss: 0.5049376887083054\n",
      "Epoch 2, Batch 350, Loss: 0.5007043582201004\n",
      "Epoch 2, Batch 400, Loss: 0.49838724851608274\n",
      "Epoch 2, Batch 450, Loss: 0.5027163898944855\n",
      "Epoch 2, Batch 500, Loss: 0.4837550932168961\n",
      "Epoch 2, Batch 550, Loss: 0.482337184548378\n",
      "Epoch 2, Batch 600, Loss: 0.4849691379070282\n",
      "Epoch 2, Batch 650, Loss: 0.489762207865715\n",
      "Epoch 2, Batch 700, Loss: 0.5062594819068909\n",
      "Epoch 2, Batch 750, Loss: 0.4877933317422867\n",
      "Epoch 2, Batch 800, Loss: 0.48476432502269745\n",
      "Epoch 2, Batch 850, Loss: 0.500035001039505\n",
      "Epoch 2, Batch 900, Loss: 0.49623496651649474\n",
      "Epoch 3, Batch 50, Loss: 0.49958795189857486\n",
      "Epoch 3, Batch 100, Loss: 0.5043541806936264\n",
      "Epoch 3, Batch 150, Loss: 0.5169918578863144\n",
      "Epoch 3, Batch 200, Loss: 0.5237088537216187\n",
      "Epoch 3, Batch 250, Loss: 0.49852150738239287\n",
      "Epoch 3, Batch 300, Loss: 0.5067556381225586\n",
      "Epoch 3, Batch 350, Loss: 0.49740224599838256\n",
      "Epoch 3, Batch 400, Loss: 0.5044195091724396\n",
      "Epoch 3, Batch 450, Loss: 0.492179149389267\n",
      "Epoch 3, Batch 500, Loss: 0.48650111377239225\n",
      "Epoch 3, Batch 550, Loss: 0.4987698197364807\n",
      "Epoch 3, Batch 600, Loss: 0.4944682192802429\n",
      "Epoch 3, Batch 650, Loss: 0.49058779060840607\n",
      "Epoch 3, Batch 700, Loss: 0.4856123161315918\n",
      "Epoch 3, Batch 750, Loss: 0.5029181623458863\n",
      "Epoch 3, Batch 800, Loss: 0.5065394002199173\n",
      "Epoch 3, Batch 850, Loss: 0.49212638676166537\n",
      "Epoch 3, Batch 900, Loss: 0.48977638602256773\n",
      "Epoch 4, Batch 50, Loss: 0.5135182338953018\n",
      "Epoch 4, Batch 100, Loss: 0.5143414276838303\n",
      "Epoch 4, Batch 150, Loss: 0.5082329618930816\n",
      "Epoch 4, Batch 200, Loss: 0.5051219898462296\n",
      "Epoch 4, Batch 250, Loss: 0.4763840091228485\n",
      "Epoch 4, Batch 300, Loss: 0.4946890103816986\n",
      "Epoch 4, Batch 350, Loss: 0.504049248099327\n",
      "Epoch 4, Batch 400, Loss: 0.4806748151779175\n",
      "Epoch 4, Batch 450, Loss: 0.5188882982730866\n",
      "Epoch 4, Batch 500, Loss: 0.4977645152807236\n",
      "Epoch 4, Batch 550, Loss: 0.47362321615219116\n",
      "Epoch 4, Batch 600, Loss: 0.49114502310752867\n",
      "Epoch 4, Batch 650, Loss: 0.5143819814920425\n",
      "Epoch 4, Batch 700, Loss: 0.48513131380081176\n",
      "Epoch 4, Batch 750, Loss: 0.4895723348855972\n",
      "Epoch 4, Batch 800, Loss: 0.4977628916501999\n",
      "Epoch 4, Batch 850, Loss: 0.4796530878543854\n",
      "Epoch 4, Batch 900, Loss: 0.5220135736465454\n",
      "Epoch 5, Batch 50, Loss: 0.49649380683898925\n",
      "Epoch 5, Batch 100, Loss: 0.5008412319421768\n",
      "Epoch 5, Batch 150, Loss: 0.5112761735916138\n",
      "Epoch 5, Batch 200, Loss: 0.5270773983001709\n",
      "Epoch 5, Batch 250, Loss: 0.5053839206695556\n",
      "Epoch 5, Batch 300, Loss: 0.5071572500467301\n",
      "Epoch 5, Batch 350, Loss: 0.5185431224107743\n",
      "Epoch 5, Batch 400, Loss: 0.4994000464677811\n",
      "Epoch 5, Batch 450, Loss: 0.49834284603595735\n",
      "Epoch 5, Batch 500, Loss: 0.49241382598876954\n",
      "Epoch 5, Batch 550, Loss: 0.49342344343662264\n",
      "Epoch 5, Batch 600, Loss: 0.49532132923603056\n",
      "Epoch 5, Batch 650, Loss: 0.4911466157436371\n",
      "Epoch 5, Batch 700, Loss: 0.490732713341713\n",
      "Epoch 5, Batch 750, Loss: 0.4981035977602005\n",
      "Epoch 5, Batch 800, Loss: 0.49533761978149415\n",
      "Epoch 5, Batch 850, Loss: 0.4801624143123627\n",
      "Epoch 5, Batch 900, Loss: 0.5063622051477432\n",
      "Epoch 6, Batch 50, Loss: 0.49457270264625547\n",
      "Epoch 6, Batch 100, Loss: 0.4966931462287903\n",
      "Epoch 6, Batch 150, Loss: 0.5220137280225754\n",
      "Epoch 6, Batch 200, Loss: 0.4913896256685257\n",
      "Epoch 6, Batch 250, Loss: 0.49332471907138825\n",
      "Epoch 6, Batch 300, Loss: 0.489407389163971\n",
      "Epoch 6, Batch 350, Loss: 0.4962228584289551\n",
      "Epoch 6, Batch 400, Loss: 0.49308353424072265\n",
      "Epoch 6, Batch 450, Loss: 0.4914067202806473\n",
      "Epoch 6, Batch 500, Loss: 0.49004498541355135\n",
      "Epoch 6, Batch 550, Loss: 0.5036299532651901\n",
      "Epoch 6, Batch 600, Loss: 0.49431010127067565\n",
      "Epoch 6, Batch 650, Loss: 0.4895668250322342\n",
      "Epoch 6, Batch 700, Loss: 0.5183376199007035\n",
      "Epoch 6, Batch 750, Loss: 0.47595342218875886\n",
      "Epoch 6, Batch 800, Loss: 0.48832073450088503\n",
      "Epoch 6, Batch 850, Loss: 0.5232543355226517\n",
      "Epoch 6, Batch 900, Loss: 0.5100973737239838\n",
      "Epoch 7, Batch 50, Loss: 0.5009804558753967\n",
      "Epoch 7, Batch 100, Loss: 0.49413926124572755\n",
      "Epoch 7, Batch 150, Loss: 0.5055678874254227\n",
      "Epoch 7, Batch 200, Loss: 0.5147272723913193\n",
      "Epoch 7, Batch 250, Loss: 0.48761071741580964\n",
      "Epoch 7, Batch 300, Loss: 0.49720135033130647\n",
      "Epoch 7, Batch 350, Loss: 0.49752538800239565\n",
      "Epoch 7, Batch 400, Loss: 0.4990130788087845\n",
      "Epoch 7, Batch 450, Loss: 0.49908780574798584\n",
      "Epoch 7, Batch 500, Loss: 0.4950321614742279\n",
      "Epoch 7, Batch 550, Loss: 0.5009733462333679\n",
      "Epoch 7, Batch 600, Loss: 0.48712914407253266\n",
      "Epoch 7, Batch 650, Loss: 0.4940891754627228\n",
      "Epoch 7, Batch 700, Loss: 0.528225743174553\n",
      "Epoch 7, Batch 750, Loss: 0.5189910978078842\n",
      "Epoch 7, Batch 800, Loss: 0.4927837520837784\n",
      "Epoch 7, Batch 850, Loss: 0.49896650671958925\n",
      "Epoch 7, Batch 900, Loss: 0.49728927850723265\n",
      "Epoch 8, Batch 50, Loss: 0.48694644153118133\n",
      "Epoch 8, Batch 100, Loss: 0.5004095953702926\n",
      "Epoch 8, Batch 150, Loss: 0.5177997243404389\n",
      "Epoch 8, Batch 200, Loss: 0.4892622923851013\n",
      "Epoch 8, Batch 250, Loss: 0.5197753840684891\n",
      "Epoch 8, Batch 300, Loss: 0.4849951481819153\n",
      "Epoch 8, Batch 350, Loss: 0.4832124638557434\n",
      "Epoch 8, Batch 400, Loss: 0.49635054111480714\n",
      "Epoch 8, Batch 450, Loss: 0.49003326773643496\n",
      "Epoch 8, Batch 500, Loss: 0.5137213122844696\n",
      "Epoch 8, Batch 550, Loss: 0.49789115726947786\n",
      "Epoch 8, Batch 600, Loss: 0.5127523529529572\n",
      "Epoch 8, Batch 650, Loss: 0.516308051943779\n",
      "Epoch 8, Batch 700, Loss: 0.5623372888565064\n",
      "Epoch 8, Batch 750, Loss: 0.49267003536224363\n",
      "Epoch 8, Batch 800, Loss: 0.5157645177841187\n",
      "Epoch 8, Batch 850, Loss: 0.5062836027145385\n",
      "Epoch 8, Batch 900, Loss: 0.4827337646484375\n",
      "Epoch 9, Batch 50, Loss: 0.49849554359912873\n",
      "Epoch 9, Batch 100, Loss: 0.48960341572761534\n",
      "Epoch 9, Batch 150, Loss: 0.49436271488666533\n",
      "Epoch 9, Batch 200, Loss: 0.4909822869300842\n",
      "Epoch 9, Batch 250, Loss: 0.518085070848465\n",
      "Epoch 9, Batch 300, Loss: 0.5006256294250488\n",
      "Epoch 9, Batch 350, Loss: 0.48986612796783446\n",
      "Epoch 9, Batch 400, Loss: 0.499374737739563\n",
      "Epoch 9, Batch 450, Loss: 0.49226482331752774\n",
      "Epoch 9, Batch 500, Loss: 0.5008855330944061\n",
      "Epoch 9, Batch 550, Loss: 0.5045514851808548\n",
      "Epoch 9, Batch 600, Loss: 0.4874604058265686\n",
      "Epoch 9, Batch 650, Loss: 0.48910163283348085\n",
      "Epoch 9, Batch 700, Loss: 0.4969322627782822\n",
      "Epoch 9, Batch 750, Loss: 0.532933132648468\n",
      "Epoch 9, Batch 800, Loss: 0.4795238357782364\n",
      "Epoch 9, Batch 850, Loss: 0.4955106908082962\n",
      "Epoch 9, Batch 900, Loss: 0.5056897175312042\n",
      "Epoch 10, Batch 50, Loss: 0.5127291798591613\n",
      "Epoch 10, Batch 100, Loss: 0.5053442758321762\n",
      "Epoch 10, Batch 150, Loss: 0.5052261245250702\n",
      "Epoch 10, Batch 200, Loss: 0.4929206132888794\n",
      "Epoch 10, Batch 250, Loss: 0.514066988825798\n",
      "Epoch 10, Batch 300, Loss: 0.5231971949338913\n",
      "Epoch 10, Batch 350, Loss: 0.5080589878559113\n",
      "Epoch 10, Batch 400, Loss: 0.47771667182445526\n",
      "Epoch 10, Batch 450, Loss: 0.49652162492275237\n",
      "Epoch 10, Batch 500, Loss: 0.49270247876644135\n",
      "Epoch 10, Batch 550, Loss: 0.5266103708744049\n",
      "Epoch 10, Batch 600, Loss: 0.5052261143922806\n",
      "Epoch 10, Batch 650, Loss: 0.4907271301746368\n",
      "Epoch 10, Batch 700, Loss: 0.5300798171758652\n",
      "Epoch 10, Batch 750, Loss: 0.5089957481622696\n",
      "Epoch 10, Batch 800, Loss: 0.4838313341140747\n",
      "Epoch 10, Batch 850, Loss: 0.4813640558719635\n",
      "Epoch 10, Batch 900, Loss: 0.4765941923856735\n",
      "Epoch 11, Batch 50, Loss: 0.5137358534336091\n",
      "Epoch 11, Batch 100, Loss: 0.5005168682336807\n",
      "Epoch 11, Batch 150, Loss: 0.4764346534013748\n",
      "Epoch 11, Batch 200, Loss: 0.5151173430681228\n",
      "Epoch 11, Batch 250, Loss: 0.49932965874671936\n",
      "Epoch 11, Batch 300, Loss: 0.5066678154468537\n",
      "Epoch 11, Batch 350, Loss: 0.5314721757173538\n",
      "Epoch 11, Batch 400, Loss: 0.4980138784646988\n",
      "Epoch 11, Batch 450, Loss: 0.4864744967222214\n",
      "Epoch 11, Batch 500, Loss: 0.4753892010450363\n",
      "Epoch 11, Batch 550, Loss: 0.48355925440788267\n",
      "Epoch 11, Batch 600, Loss: 0.5079181796312332\n",
      "Epoch 11, Batch 650, Loss: 0.4944710922241211\n",
      "Epoch 11, Batch 700, Loss: 0.5076376909017563\n",
      "Epoch 11, Batch 750, Loss: 0.49714520752429964\n",
      "Epoch 11, Batch 800, Loss: 0.4804960709810257\n",
      "Epoch 11, Batch 850, Loss: 0.4877470326423645\n",
      "Epoch 11, Batch 900, Loss: 0.513020145893097\n",
      "Epoch 12, Batch 50, Loss: 0.47886727392673495\n",
      "Epoch 12, Batch 100, Loss: 0.4924223637580872\n",
      "Epoch 12, Batch 150, Loss: 0.5030275684595108\n",
      "Epoch 12, Batch 200, Loss: 0.5018641090393067\n",
      "Epoch 12, Batch 250, Loss: 0.48779831767082216\n",
      "Epoch 12, Batch 300, Loss: 0.5011613076925278\n",
      "Epoch 12, Batch 350, Loss: 0.5088744139671326\n",
      "Epoch 12, Batch 400, Loss: 0.5153335881233215\n",
      "Epoch 12, Batch 450, Loss: 0.5194058448076249\n",
      "Epoch 12, Batch 500, Loss: 0.501512102484703\n",
      "Epoch 12, Batch 550, Loss: 0.5115734285116196\n",
      "Epoch 12, Batch 600, Loss: 0.5098593527078629\n",
      "Epoch 12, Batch 650, Loss: 0.48711214363574984\n",
      "Epoch 12, Batch 700, Loss: 0.5034208607673645\n",
      "Epoch 12, Batch 750, Loss: 0.4949832004308701\n",
      "Epoch 12, Batch 800, Loss: 0.525767365694046\n",
      "Epoch 12, Batch 850, Loss: 0.49381356716156005\n",
      "Epoch 12, Batch 900, Loss: 0.4881179004907608\n",
      "Epoch 13, Batch 50, Loss: 0.5195522022247314\n",
      "Epoch 13, Batch 100, Loss: 0.5094746321439743\n",
      "Epoch 13, Batch 150, Loss: 0.510417315363884\n",
      "Epoch 13, Batch 200, Loss: 0.4983358532190323\n",
      "Epoch 13, Batch 250, Loss: 0.5140394365787506\n",
      "Epoch 13, Batch 300, Loss: 0.48615037977695463\n",
      "Epoch 13, Batch 350, Loss: 0.4928484660387039\n",
      "Epoch 13, Batch 400, Loss: 0.5019787663221359\n",
      "Epoch 13, Batch 450, Loss: 0.4884838229417801\n",
      "Epoch 13, Batch 500, Loss: 0.5124109846353531\n",
      "Epoch 13, Batch 550, Loss: 0.5194228827953339\n",
      "Epoch 13, Batch 600, Loss: 0.47778028190135957\n",
      "Epoch 13, Batch 650, Loss: 0.4946194314956665\n",
      "Epoch 13, Batch 700, Loss: 0.48754747688770295\n",
      "Epoch 13, Batch 750, Loss: 0.49940017819404603\n",
      "Epoch 13, Batch 800, Loss: 0.5048719668388366\n",
      "Epoch 13, Batch 850, Loss: 0.5020913219451905\n",
      "Epoch 13, Batch 900, Loss: 0.4912160742282867\n",
      "Epoch 14, Batch 50, Loss: 0.5094099342823029\n",
      "Epoch 14, Batch 100, Loss: 0.49499156534671784\n",
      "Epoch 14, Batch 150, Loss: 0.49543430030345914\n",
      "Epoch 14, Batch 200, Loss: 0.49604399502277374\n",
      "Epoch 14, Batch 250, Loss: 0.4835538160800934\n",
      "Epoch 14, Batch 300, Loss: 0.49087621867656706\n",
      "Epoch 14, Batch 350, Loss: 0.48731592535972595\n",
      "Epoch 14, Batch 400, Loss: 0.48095672607421874\n",
      "Epoch 14, Batch 450, Loss: 0.49280792713165283\n",
      "Epoch 14, Batch 500, Loss: 0.5120936149358749\n",
      "Epoch 14, Batch 550, Loss: 0.4999378931522369\n",
      "Epoch 14, Batch 600, Loss: 0.5161549031734467\n",
      "Epoch 14, Batch 650, Loss: 0.4905917400121689\n",
      "Epoch 14, Batch 700, Loss: 0.5044106066226959\n",
      "Epoch 14, Batch 750, Loss: 0.49521749794483183\n",
      "Epoch 14, Batch 800, Loss: 0.48670503437519075\n",
      "Epoch 14, Batch 850, Loss: 0.49837820649147035\n",
      "Epoch 14, Batch 900, Loss: 0.5170207637548446\n",
      "Epoch 15, Batch 50, Loss: 0.4988986444473267\n",
      "Epoch 15, Batch 100, Loss: 0.48696922183036806\n",
      "Epoch 15, Batch 150, Loss: 0.48873332917690276\n",
      "Epoch 15, Batch 200, Loss: 0.5091730856895447\n",
      "Epoch 15, Batch 250, Loss: 0.5173346942663193\n",
      "Epoch 15, Batch 300, Loss: 0.5029342567920685\n",
      "Epoch 15, Batch 350, Loss: 0.49577121376991273\n",
      "Epoch 15, Batch 400, Loss: 0.5306537801027298\n",
      "Epoch 15, Batch 450, Loss: 0.5226313257217408\n",
      "Epoch 15, Batch 500, Loss: 0.5205736869573593\n",
      "Epoch 15, Batch 550, Loss: 0.49109829545021055\n",
      "Epoch 15, Batch 600, Loss: 0.5017287474870682\n",
      "Epoch 15, Batch 650, Loss: 0.5030481988191604\n",
      "Epoch 15, Batch 700, Loss: 0.5017751622200012\n",
      "Epoch 15, Batch 750, Loss: 0.4933505940437317\n",
      "Epoch 15, Batch 800, Loss: 0.5099632263183593\n",
      "Epoch 15, Batch 850, Loss: 0.4990768092870712\n",
      "Epoch 15, Batch 900, Loss: 0.49090389132499695\n",
      "Epoch 16, Batch 50, Loss: 0.5013420259952546\n",
      "Epoch 16, Batch 100, Loss: 0.48269876301288606\n",
      "Epoch 16, Batch 150, Loss: 0.5051761269569397\n",
      "Epoch 16, Batch 200, Loss: 0.52023632645607\n",
      "Epoch 16, Batch 250, Loss: 0.48689616203308106\n",
      "Epoch 16, Batch 300, Loss: 0.5069633841514587\n",
      "Epoch 16, Batch 350, Loss: 0.5113571488857269\n",
      "Epoch 16, Batch 400, Loss: 0.4872853863239288\n",
      "Epoch 16, Batch 450, Loss: 0.5174793237447739\n",
      "Epoch 16, Batch 500, Loss: 0.5192517387866974\n",
      "Epoch 16, Batch 550, Loss: 0.5198408031463623\n",
      "Epoch 16, Batch 600, Loss: 0.5249945193529129\n",
      "Epoch 16, Batch 650, Loss: 0.5111429345607758\n",
      "Epoch 16, Batch 700, Loss: 0.504112430214882\n",
      "Epoch 16, Batch 750, Loss: 0.4933648818731308\n",
      "Epoch 16, Batch 800, Loss: 0.5001425492763519\n",
      "Epoch 16, Batch 850, Loss: 0.505563749074936\n",
      "Epoch 16, Batch 900, Loss: 0.49250368297100067\n",
      "Epoch 17, Batch 50, Loss: 0.4880230897665024\n",
      "Epoch 17, Batch 100, Loss: 0.4740504676103592\n",
      "Epoch 17, Batch 150, Loss: 0.5110845762491226\n",
      "Epoch 17, Batch 200, Loss: 0.5093041300773621\n",
      "Epoch 17, Batch 250, Loss: 0.5048306703567504\n",
      "Epoch 17, Batch 300, Loss: 0.5060790598392486\n",
      "Epoch 17, Batch 350, Loss: 0.48836337447166445\n",
      "Epoch 17, Batch 400, Loss: 0.5106853860616684\n",
      "Epoch 17, Batch 450, Loss: 0.5024104285240173\n",
      "Epoch 17, Batch 500, Loss: 0.500641570687294\n",
      "Epoch 17, Batch 550, Loss: 0.48427192389965057\n",
      "Epoch 17, Batch 600, Loss: 0.5055113357305526\n",
      "Epoch 17, Batch 650, Loss: 0.5017401385307312\n",
      "Epoch 17, Batch 700, Loss: 0.501916766166687\n",
      "Epoch 17, Batch 750, Loss: 0.5028823518753052\n",
      "Epoch 17, Batch 800, Loss: 0.49652314484119414\n",
      "Epoch 17, Batch 850, Loss: 0.4859733772277832\n",
      "Epoch 17, Batch 900, Loss: 0.5155701303482055\n",
      "Epoch 18, Batch 50, Loss: 0.4938537836074829\n",
      "Epoch 18, Batch 100, Loss: 0.4976757204532623\n",
      "Epoch 18, Batch 150, Loss: 0.5020957279205323\n",
      "Epoch 18, Batch 200, Loss: 0.4829304027557373\n",
      "Epoch 18, Batch 250, Loss: 0.49115372240543365\n",
      "Epoch 18, Batch 300, Loss: 0.5092933291196823\n",
      "Epoch 18, Batch 350, Loss: 0.5438275927305222\n",
      "Epoch 18, Batch 400, Loss: 0.49664766490459444\n",
      "Epoch 18, Batch 450, Loss: 0.4950045096874237\n",
      "Epoch 18, Batch 500, Loss: 0.5286680287122727\n",
      "Epoch 18, Batch 550, Loss: 0.47968626141548154\n",
      "Epoch 18, Batch 600, Loss: 0.5287734806537628\n",
      "Epoch 18, Batch 650, Loss: 0.5206949234008789\n",
      "Epoch 18, Batch 700, Loss: 0.4934500581026077\n",
      "Epoch 18, Batch 750, Loss: 0.49733051776885984\n",
      "Epoch 18, Batch 800, Loss: 0.476666561961174\n",
      "Epoch 18, Batch 850, Loss: 0.48866264283657074\n",
      "Epoch 18, Batch 900, Loss: 0.507461473941803\n",
      "Epoch 19, Batch 50, Loss: 0.5117124330997467\n",
      "Epoch 19, Batch 100, Loss: 0.5119919335842132\n",
      "Epoch 19, Batch 150, Loss: 0.5098369824886322\n",
      "Epoch 19, Batch 200, Loss: 0.500186151266098\n",
      "Epoch 19, Batch 250, Loss: 0.5331933510303497\n",
      "Epoch 19, Batch 300, Loss: 0.49327725529670713\n",
      "Epoch 19, Batch 350, Loss: 0.5362563174962998\n",
      "Epoch 19, Batch 400, Loss: 0.5239949202537537\n",
      "Epoch 19, Batch 450, Loss: 0.4901644974946976\n",
      "Epoch 19, Batch 500, Loss: 0.49033922374248506\n",
      "Epoch 19, Batch 550, Loss: 0.48534296452999115\n",
      "Epoch 19, Batch 600, Loss: 0.48810113191604615\n",
      "Epoch 19, Batch 650, Loss: 0.4953743541240692\n",
      "Epoch 19, Batch 700, Loss: 0.492242551445961\n",
      "Epoch 19, Batch 750, Loss: 0.49567442715168\n",
      "Epoch 19, Batch 800, Loss: 0.49000090956687925\n",
      "Epoch 19, Batch 850, Loss: 0.5021649444103241\n",
      "Epoch 19, Batch 900, Loss: 0.5024921083450318\n",
      "Epoch 20, Batch 50, Loss: 0.5069854062795639\n",
      "Epoch 20, Batch 100, Loss: 0.5232973617315292\n",
      "Epoch 20, Batch 150, Loss: 0.49189319014549254\n",
      "Epoch 20, Batch 200, Loss: 0.506939308643341\n",
      "Epoch 20, Batch 250, Loss: 0.49241395175457003\n",
      "Epoch 20, Batch 300, Loss: 0.5084213137626648\n",
      "Epoch 20, Batch 350, Loss: 0.5152181512117386\n",
      "Epoch 20, Batch 400, Loss: 0.5006643348932266\n",
      "Epoch 20, Batch 450, Loss: 0.4912413841485977\n",
      "Epoch 20, Batch 500, Loss: 0.4802048444747925\n",
      "Epoch 20, Batch 550, Loss: 0.4920015692710876\n",
      "Epoch 20, Batch 600, Loss: 0.48041822493076325\n",
      "Epoch 20, Batch 650, Loss: 0.5077422213554382\n",
      "Epoch 20, Batch 700, Loss: 0.48619404911994935\n",
      "Epoch 20, Batch 750, Loss: 0.5077802687883377\n",
      "Epoch 20, Batch 800, Loss: 0.49116225957870485\n",
      "Epoch 20, Batch 850, Loss: 0.5198097753524781\n",
      "Epoch 20, Batch 900, Loss: 0.48248356580734253\n",
      "Epoch 21, Batch 50, Loss: 0.5017930042743682\n",
      "Epoch 21, Batch 100, Loss: 0.529497082233429\n",
      "Epoch 21, Batch 150, Loss: 0.49059100151062013\n",
      "Epoch 21, Batch 200, Loss: 0.5020500165224075\n",
      "Epoch 21, Batch 250, Loss: 0.5037583893537522\n",
      "Epoch 21, Batch 300, Loss: 0.49783349394798276\n",
      "Epoch 21, Batch 350, Loss: 0.4938254398107529\n",
      "Epoch 21, Batch 400, Loss: 0.5151695239543915\n",
      "Epoch 21, Batch 450, Loss: 0.4982195383310318\n",
      "Epoch 21, Batch 500, Loss: 0.5057693767547607\n",
      "Epoch 21, Batch 550, Loss: 0.49241065442562104\n",
      "Epoch 21, Batch 600, Loss: 0.4911549764871597\n",
      "Epoch 21, Batch 650, Loss: 0.48617684841156006\n",
      "Epoch 21, Batch 700, Loss: 0.49919690310955045\n",
      "Epoch 21, Batch 750, Loss: 0.5138265734910965\n",
      "Epoch 21, Batch 800, Loss: 0.49120884001255033\n",
      "Epoch 21, Batch 850, Loss: 0.48322256803512575\n",
      "Epoch 21, Batch 900, Loss: 0.48249256253242495\n",
      "Epoch 22, Batch 50, Loss: 0.5056676697731018\n",
      "Epoch 22, Batch 100, Loss: 0.5071524226665497\n",
      "Epoch 22, Batch 150, Loss: 0.48918968200683594\n",
      "Epoch 22, Batch 200, Loss: 0.48908967614173887\n",
      "Epoch 22, Batch 250, Loss: 0.5071049398183822\n",
      "Epoch 22, Batch 300, Loss: 0.5015254598855973\n",
      "Epoch 22, Batch 350, Loss: 0.5024066507816315\n",
      "Epoch 22, Batch 400, Loss: 0.49003307342529295\n",
      "Epoch 22, Batch 450, Loss: 0.496956587433815\n",
      "Epoch 22, Batch 500, Loss: 0.48143026292324065\n",
      "Epoch 22, Batch 550, Loss: 0.4878054803609848\n",
      "Epoch 22, Batch 600, Loss: 0.48856624603271487\n",
      "Epoch 22, Batch 650, Loss: 0.49219952583312987\n",
      "Epoch 22, Batch 700, Loss: 0.48888213992118834\n",
      "Epoch 22, Batch 750, Loss: 0.5001376402378083\n",
      "Epoch 22, Batch 800, Loss: 0.5003115904331207\n",
      "Epoch 22, Batch 850, Loss: 0.5137967795133591\n",
      "Epoch 22, Batch 900, Loss: 0.5009344363212586\n",
      "Epoch 23, Batch 50, Loss: 0.5055497050285339\n",
      "Epoch 23, Batch 100, Loss: 0.5086186915636063\n",
      "Epoch 23, Batch 150, Loss: 0.5148009127378463\n",
      "Epoch 23, Batch 200, Loss: 0.5034419995546341\n",
      "Epoch 23, Batch 250, Loss: 0.4874169999361038\n",
      "Epoch 23, Batch 300, Loss: 0.5138172394037247\n",
      "Epoch 23, Batch 350, Loss: 0.494371092915535\n",
      "Epoch 23, Batch 400, Loss: 0.4859951293468475\n",
      "Epoch 23, Batch 450, Loss: 0.49286200404167174\n",
      "Epoch 23, Batch 500, Loss: 0.5161458343267441\n",
      "Epoch 23, Batch 550, Loss: 0.5437352931499482\n",
      "Epoch 23, Batch 600, Loss: 0.4834717208147049\n",
      "Epoch 23, Batch 650, Loss: 0.5057829368114471\n",
      "Epoch 23, Batch 700, Loss: 0.5057110172510147\n",
      "Epoch 23, Batch 750, Loss: 0.4984521508216858\n",
      "Epoch 23, Batch 800, Loss: 0.5134612172842026\n",
      "Epoch 23, Batch 850, Loss: 0.5072920054197312\n",
      "Epoch 23, Batch 900, Loss: 0.5038933873176574\n",
      "Epoch 24, Batch 50, Loss: 0.4862105458974838\n",
      "Epoch 24, Batch 100, Loss: 0.5158347034454346\n",
      "Epoch 24, Batch 150, Loss: 0.4907410031557083\n",
      "Epoch 24, Batch 200, Loss: 0.4896407753229141\n",
      "Epoch 24, Batch 250, Loss: 0.4743257027864456\n",
      "Epoch 24, Batch 300, Loss: 0.5038524532318115\n",
      "Epoch 24, Batch 350, Loss: 0.4941129833459854\n",
      "Epoch 24, Batch 400, Loss: 0.501180248260498\n",
      "Epoch 24, Batch 450, Loss: 0.4916049909591675\n",
      "Epoch 24, Batch 500, Loss: 0.4969334447383881\n",
      "Epoch 24, Batch 550, Loss: 0.523260703086853\n",
      "Epoch 24, Batch 600, Loss: 0.5013411295413971\n",
      "Epoch 24, Batch 650, Loss: 0.49059504091739653\n",
      "Epoch 24, Batch 700, Loss: 0.49780250787734986\n",
      "Epoch 24, Batch 750, Loss: 0.5226688069105149\n",
      "Epoch 24, Batch 800, Loss: 0.5031720906496048\n",
      "Epoch 24, Batch 850, Loss: 0.4920845001935959\n",
      "Epoch 24, Batch 900, Loss: 0.49791178047657014\n",
      "Epoch 25, Batch 50, Loss: 0.5045881950855255\n",
      "Epoch 25, Batch 100, Loss: 0.5201399034261703\n",
      "Epoch 25, Batch 150, Loss: 0.5281172341108322\n",
      "Epoch 25, Batch 200, Loss: 0.5152928388118744\n",
      "Epoch 25, Batch 250, Loss: 0.4949412536621094\n",
      "Epoch 25, Batch 300, Loss: 0.5008774238824845\n",
      "Epoch 25, Batch 350, Loss: 0.5066832482814789\n",
      "Epoch 25, Batch 400, Loss: 0.4919894593954086\n",
      "Epoch 25, Batch 450, Loss: 0.49331227123737337\n",
      "Epoch 25, Batch 500, Loss: 0.503958780169487\n",
      "Epoch 25, Batch 550, Loss: 0.49168440759181975\n",
      "Epoch 25, Batch 600, Loss: 0.49467970252037047\n",
      "Epoch 25, Batch 650, Loss: 0.4891111934185028\n",
      "Epoch 25, Batch 700, Loss: 0.4788957679271698\n",
      "Epoch 25, Batch 750, Loss: 0.48945983588695524\n",
      "Epoch 25, Batch 800, Loss: 0.5360911011695861\n",
      "Epoch 25, Batch 850, Loss: 0.49565039813518524\n",
      "Epoch 25, Batch 900, Loss: 0.5190247172117233\n",
      "Accuracy on test set: 0.6679%\n",
      "Fitting for combination 33\n",
      "784\n",
      "2\n",
      "10\n",
      "[40, 40, 10]\n",
      "False\n",
      "['relu', 'tanh']\n",
      "SGD\n",
      "0.1\n",
      "0.03\n",
      "CrossEntropyLoss\n",
      "50\n",
      "25\n",
      "0\n",
      "Epoch 1, Batch 50, Loss: 0.7641143584251404\n",
      "Epoch 1, Batch 100, Loss: 0.4638924741744995\n",
      "Epoch 1, Batch 150, Loss: 0.39699926137924196\n",
      "Epoch 1, Batch 200, Loss: 0.35903413832187653\n",
      "Epoch 1, Batch 250, Loss: 0.354816854596138\n",
      "Epoch 1, Batch 300, Loss: 0.34159578144550323\n",
      "Epoch 1, Batch 350, Loss: 0.33320315897464753\n",
      "Epoch 1, Batch 400, Loss: 0.34289346158504486\n",
      "Epoch 1, Batch 450, Loss: 0.31971305787563326\n",
      "Epoch 1, Batch 500, Loss: 0.31727164596319196\n",
      "Epoch 1, Batch 550, Loss: 0.3317814555764198\n",
      "Epoch 1, Batch 600, Loss: 0.32092055439949035\n",
      "Epoch 1, Batch 650, Loss: 0.31743591398000714\n",
      "Epoch 1, Batch 700, Loss: 0.3117334744334221\n",
      "Epoch 1, Batch 750, Loss: 0.31445792615413665\n",
      "Epoch 1, Batch 800, Loss: 0.3140401366353035\n",
      "Epoch 1, Batch 850, Loss: 0.32703796952962877\n",
      "Epoch 1, Batch 900, Loss: 0.3233139431476593\n",
      "Epoch 2, Batch 50, Loss: 0.32116228967905047\n",
      "Epoch 2, Batch 100, Loss: 0.3040735304355621\n",
      "Epoch 2, Batch 150, Loss: 0.3169663941860199\n",
      "Epoch 2, Batch 200, Loss: 0.30924899578094484\n",
      "Epoch 2, Batch 250, Loss: 0.310205383002758\n",
      "Epoch 2, Batch 300, Loss: 0.3027628657221794\n",
      "Epoch 2, Batch 350, Loss: 0.30477775931358336\n",
      "Epoch 2, Batch 400, Loss: 0.30903890401124956\n",
      "Epoch 2, Batch 450, Loss: 0.3200706797838211\n",
      "Epoch 2, Batch 500, Loss: 0.30292644411325453\n",
      "Epoch 2, Batch 550, Loss: 0.30810670137405394\n",
      "Epoch 2, Batch 600, Loss: 0.30833734780550004\n",
      "Epoch 2, Batch 650, Loss: 0.3059717133641243\n",
      "Epoch 2, Batch 700, Loss: 0.2975513824820519\n",
      "Epoch 2, Batch 750, Loss: 0.30779207825660704\n",
      "Epoch 2, Batch 800, Loss: 0.3118049630522728\n",
      "Epoch 2, Batch 850, Loss: 0.30666771113872526\n",
      "Epoch 2, Batch 900, Loss: 0.311769931614399\n",
      "Epoch 3, Batch 50, Loss: 0.2988365158438683\n",
      "Epoch 3, Batch 100, Loss: 0.3184899619221687\n",
      "Epoch 3, Batch 150, Loss: 0.3175954148173332\n",
      "Epoch 3, Batch 200, Loss: 0.3013203585147858\n",
      "Epoch 3, Batch 250, Loss: 0.2953010493516922\n",
      "Epoch 3, Batch 300, Loss: 0.3079638919234276\n",
      "Epoch 3, Batch 350, Loss: 0.3096655336022377\n",
      "Epoch 3, Batch 400, Loss: 0.30079574555158617\n",
      "Epoch 3, Batch 450, Loss: 0.3091648414731026\n",
      "Epoch 3, Batch 500, Loss: 0.31473991453647615\n",
      "Epoch 3, Batch 550, Loss: 0.3069970577955246\n",
      "Epoch 3, Batch 600, Loss: 0.30337423115968704\n",
      "Epoch 3, Batch 650, Loss: 0.29874502509832385\n",
      "Epoch 3, Batch 700, Loss: 0.31390201151371\n",
      "Epoch 3, Batch 750, Loss: 0.31376404941082003\n",
      "Epoch 3, Batch 800, Loss: 0.3111485368013382\n",
      "Epoch 3, Batch 850, Loss: 0.3193206638097763\n",
      "Epoch 3, Batch 900, Loss: 0.3034334871172905\n",
      "Epoch 4, Batch 50, Loss: 0.3016724342107773\n",
      "Epoch 4, Batch 100, Loss: 0.29571699500083926\n",
      "Epoch 4, Batch 150, Loss: 0.3013217240571976\n",
      "Epoch 4, Batch 200, Loss: 0.3175297075510025\n",
      "Epoch 4, Batch 250, Loss: 0.3037563678622246\n",
      "Epoch 4, Batch 300, Loss: 0.30040271520614625\n",
      "Epoch 4, Batch 350, Loss: 0.30965817034244536\n",
      "Epoch 4, Batch 400, Loss: 0.33004915714263916\n",
      "Epoch 4, Batch 450, Loss: 0.29596717447042464\n",
      "Epoch 4, Batch 500, Loss: 0.3114808803796768\n",
      "Epoch 4, Batch 550, Loss: 0.29923754632472993\n",
      "Epoch 4, Batch 600, Loss: 0.29392319440841674\n",
      "Epoch 4, Batch 650, Loss: 0.30491837561130525\n",
      "Epoch 4, Batch 700, Loss: 0.29397959262132645\n",
      "Epoch 4, Batch 750, Loss: 0.3046115070581436\n",
      "Epoch 4, Batch 800, Loss: 0.3145486450195312\n",
      "Epoch 4, Batch 850, Loss: 0.3070961073040962\n",
      "Epoch 4, Batch 900, Loss: 0.30540597945451736\n",
      "Epoch 5, Batch 50, Loss: 0.3013351780176163\n",
      "Epoch 5, Batch 100, Loss: 0.3065977221727371\n",
      "Epoch 5, Batch 150, Loss: 0.3102574247121811\n",
      "Epoch 5, Batch 200, Loss: 0.31779912501573565\n",
      "Epoch 5, Batch 250, Loss: 0.32246665596961976\n",
      "Epoch 5, Batch 300, Loss: 0.30510233998298647\n",
      "Epoch 5, Batch 350, Loss: 0.3047958198189735\n",
      "Epoch 5, Batch 400, Loss: 0.3114195713400841\n",
      "Epoch 5, Batch 450, Loss: 0.3078564989566803\n",
      "Epoch 5, Batch 500, Loss: 0.296084506213665\n",
      "Epoch 5, Batch 550, Loss: 0.30225505769252775\n",
      "Epoch 5, Batch 600, Loss: 0.29673353403806685\n",
      "Epoch 5, Batch 650, Loss: 0.31884677827358243\n",
      "Epoch 5, Batch 700, Loss: 0.3019947066903114\n",
      "Epoch 5, Batch 750, Loss: 0.29076336055994034\n",
      "Epoch 5, Batch 800, Loss: 0.29171978384256364\n",
      "Epoch 5, Batch 850, Loss: 0.2953557249903679\n",
      "Epoch 5, Batch 900, Loss: 0.29357513546943664\n",
      "Epoch 6, Batch 50, Loss: 0.3099899205565453\n",
      "Epoch 6, Batch 100, Loss: 0.29035676807165145\n",
      "Epoch 6, Batch 150, Loss: 0.30546139866113664\n",
      "Epoch 6, Batch 200, Loss: 0.313405744433403\n",
      "Epoch 6, Batch 250, Loss: 0.28820680767297746\n",
      "Epoch 6, Batch 300, Loss: 0.30065138965845106\n",
      "Epoch 6, Batch 350, Loss: 0.3119762086868286\n",
      "Epoch 6, Batch 400, Loss: 0.2998276674747467\n",
      "Epoch 6, Batch 450, Loss: 0.30425323635339735\n",
      "Epoch 6, Batch 500, Loss: 0.31144163697957994\n",
      "Epoch 6, Batch 550, Loss: 0.30440348833799363\n",
      "Epoch 6, Batch 600, Loss: 0.2962693077325821\n",
      "Epoch 6, Batch 650, Loss: 0.29780261546373366\n",
      "Epoch 6, Batch 700, Loss: 0.3038273617625237\n",
      "Epoch 6, Batch 750, Loss: 0.3072704938054085\n",
      "Epoch 6, Batch 800, Loss: 0.31306772768497465\n",
      "Epoch 6, Batch 850, Loss: 0.2919277495145798\n",
      "Epoch 6, Batch 900, Loss: 0.3060364294052124\n",
      "Epoch 7, Batch 50, Loss: 0.30117382407188414\n",
      "Epoch 7, Batch 100, Loss: 0.29563959062099454\n",
      "Epoch 7, Batch 150, Loss: 0.3109683042764664\n",
      "Epoch 7, Batch 200, Loss: 0.29214905977249145\n",
      "Epoch 7, Batch 250, Loss: 0.30521852016448975\n",
      "Epoch 7, Batch 300, Loss: 0.3048297479748726\n",
      "Epoch 7, Batch 350, Loss: 0.3055279326438904\n",
      "Epoch 7, Batch 400, Loss: 0.3070034655928612\n",
      "Epoch 7, Batch 450, Loss: 0.30482580065727233\n",
      "Epoch 7, Batch 500, Loss: 0.30061156749725343\n",
      "Epoch 7, Batch 550, Loss: 0.3086101007461548\n",
      "Epoch 7, Batch 600, Loss: 0.30698206841945647\n",
      "Epoch 7, Batch 650, Loss: 0.30481639713048936\n",
      "Epoch 7, Batch 700, Loss: 0.3031144055724144\n",
      "Epoch 7, Batch 750, Loss: 0.29628412932157516\n",
      "Epoch 7, Batch 800, Loss: 0.3016861057281494\n",
      "Epoch 7, Batch 850, Loss: 0.29521140217781067\n",
      "Epoch 7, Batch 900, Loss: 0.31087033420801163\n",
      "Epoch 8, Batch 50, Loss: 0.3102515262365341\n",
      "Epoch 8, Batch 100, Loss: 0.29929208278656005\n",
      "Epoch 8, Batch 150, Loss: 0.2984461137652397\n",
      "Epoch 8, Batch 200, Loss: 0.30239034950733185\n",
      "Epoch 8, Batch 250, Loss: 0.30130931168794634\n",
      "Epoch 8, Batch 300, Loss: 0.3193635120987892\n",
      "Epoch 8, Batch 350, Loss: 0.29291449606418607\n",
      "Epoch 8, Batch 400, Loss: 0.29160769820213317\n",
      "Epoch 8, Batch 450, Loss: 0.30886437475681305\n",
      "Epoch 8, Batch 500, Loss: 0.29089540868997577\n",
      "Epoch 8, Batch 550, Loss: 0.28579806685447695\n",
      "Epoch 8, Batch 600, Loss: 0.3166162168979645\n",
      "Epoch 8, Batch 650, Loss: 0.3080985236167908\n",
      "Epoch 8, Batch 700, Loss: 0.30015194714069365\n",
      "Epoch 8, Batch 750, Loss: 0.29435189813375473\n",
      "Epoch 8, Batch 800, Loss: 0.31489998459815977\n",
      "Epoch 8, Batch 850, Loss: 0.29724364042282103\n",
      "Epoch 8, Batch 900, Loss: 0.3078287148475647\n",
      "Epoch 9, Batch 50, Loss: 0.3040607398748398\n",
      "Epoch 9, Batch 100, Loss: 0.29708837836980817\n",
      "Epoch 9, Batch 150, Loss: 0.3082916212081909\n",
      "Epoch 9, Batch 200, Loss: 0.30141617834568024\n",
      "Epoch 9, Batch 250, Loss: 0.30969743311405185\n",
      "Epoch 9, Batch 300, Loss: 0.30327908545732496\n",
      "Epoch 9, Batch 350, Loss: 0.31015054881572723\n",
      "Epoch 9, Batch 400, Loss: 0.2959048068523407\n",
      "Epoch 9, Batch 450, Loss: 0.3067559415102005\n",
      "Epoch 9, Batch 500, Loss: 0.29968911230564116\n",
      "Epoch 9, Batch 550, Loss: 0.30775332272052763\n",
      "Epoch 9, Batch 600, Loss: 0.3067368298768997\n",
      "Epoch 9, Batch 650, Loss: 0.29002864569425585\n",
      "Epoch 9, Batch 700, Loss: 0.30645401000976563\n",
      "Epoch 9, Batch 750, Loss: 0.3083657917380333\n",
      "Epoch 9, Batch 800, Loss: 0.29964529007673263\n",
      "Epoch 9, Batch 850, Loss: 0.3044534820318222\n",
      "Epoch 9, Batch 900, Loss: 0.29011136084795\n",
      "Epoch 10, Batch 50, Loss: 0.30782042354345324\n",
      "Epoch 10, Batch 100, Loss: 0.32156476080417634\n",
      "Epoch 10, Batch 150, Loss: 0.29038239508867264\n",
      "Epoch 10, Batch 200, Loss: 0.3112733620405197\n",
      "Epoch 10, Batch 250, Loss: 0.30797771155834197\n",
      "Epoch 10, Batch 300, Loss: 0.2957166317105293\n",
      "Epoch 10, Batch 350, Loss: 0.3016917818784714\n",
      "Epoch 10, Batch 400, Loss: 0.302943617105484\n",
      "Epoch 10, Batch 450, Loss: 0.3148101472854614\n",
      "Epoch 10, Batch 500, Loss: 0.3054478946328163\n",
      "Epoch 10, Batch 550, Loss: 0.31154188215732576\n",
      "Epoch 10, Batch 600, Loss: 0.30286217361688617\n",
      "Epoch 10, Batch 650, Loss: 0.2983321905136108\n",
      "Epoch 10, Batch 700, Loss: 0.30873940587043763\n",
      "Epoch 10, Batch 750, Loss: 0.2965807166695595\n",
      "Epoch 10, Batch 800, Loss: 0.3016384893655777\n",
      "Epoch 10, Batch 850, Loss: 0.29383118093013766\n",
      "Epoch 10, Batch 900, Loss: 0.3085003474354744\n",
      "Epoch 11, Batch 50, Loss: 0.30004171580076217\n",
      "Epoch 11, Batch 100, Loss: 0.30591093599796293\n",
      "Epoch 11, Batch 150, Loss: 0.3074168786406517\n",
      "Epoch 11, Batch 200, Loss: 0.3109823486208916\n",
      "Epoch 11, Batch 250, Loss: 0.29997545152902605\n",
      "Epoch 11, Batch 300, Loss: 0.2898787888884544\n",
      "Epoch 11, Batch 350, Loss: 0.2971869629621506\n",
      "Epoch 11, Batch 400, Loss: 0.2912533643841744\n",
      "Epoch 11, Batch 450, Loss: 0.3040587058663368\n",
      "Epoch 11, Batch 500, Loss: 0.299535276889801\n",
      "Epoch 11, Batch 550, Loss: 0.3149569937586784\n",
      "Epoch 11, Batch 600, Loss: 0.30231889367103576\n",
      "Epoch 11, Batch 650, Loss: 0.3077772051095963\n",
      "Epoch 11, Batch 700, Loss: 0.31337634444236756\n",
      "Epoch 11, Batch 750, Loss: 0.286483453810215\n",
      "Epoch 11, Batch 800, Loss: 0.30064664483070375\n",
      "Epoch 11, Batch 850, Loss: 0.3052939677238464\n",
      "Epoch 11, Batch 900, Loss: 0.30003324776887896\n",
      "Epoch 12, Batch 50, Loss: 0.2976613029837608\n",
      "Epoch 12, Batch 100, Loss: 0.3148979139328003\n",
      "Epoch 12, Batch 150, Loss: 0.2922132954001427\n",
      "Epoch 12, Batch 200, Loss: 0.30389233589172365\n",
      "Epoch 12, Batch 250, Loss: 0.3009510967135429\n",
      "Epoch 12, Batch 300, Loss: 0.30010329335927965\n",
      "Epoch 12, Batch 350, Loss: 0.3050610953569412\n",
      "Epoch 12, Batch 400, Loss: 0.30393505573272706\n",
      "Epoch 12, Batch 450, Loss: 0.3077136835455894\n",
      "Epoch 12, Batch 500, Loss: 0.2892488497495651\n",
      "Epoch 12, Batch 550, Loss: 0.31108514934778214\n",
      "Epoch 12, Batch 600, Loss: 0.3061386451125145\n",
      "Epoch 12, Batch 650, Loss: 0.29387767881155014\n",
      "Epoch 12, Batch 700, Loss: 0.30031034976243975\n",
      "Epoch 12, Batch 750, Loss: 0.3066290754079819\n",
      "Epoch 12, Batch 800, Loss: 0.29422581404447556\n",
      "Epoch 12, Batch 850, Loss: 0.3056926181912422\n",
      "Epoch 12, Batch 900, Loss: 0.31457565903663637\n",
      "Epoch 13, Batch 50, Loss: 0.3042641508579254\n",
      "Epoch 13, Batch 100, Loss: 0.30520021498203276\n",
      "Epoch 13, Batch 150, Loss: 0.29965038269758226\n",
      "Epoch 13, Batch 200, Loss: 0.29588492155075075\n",
      "Epoch 13, Batch 250, Loss: 0.29851080566644667\n",
      "Epoch 13, Batch 300, Loss: 0.31062062859535217\n",
      "Epoch 13, Batch 350, Loss: 0.30675256699323655\n",
      "Epoch 13, Batch 400, Loss: 0.30402673959732057\n",
      "Epoch 13, Batch 450, Loss: 0.29258735686540605\n",
      "Epoch 13, Batch 500, Loss: 0.3058978250622749\n",
      "Epoch 13, Batch 550, Loss: 0.3015630063414574\n",
      "Epoch 13, Batch 600, Loss: 0.31431194722652434\n",
      "Epoch 13, Batch 650, Loss: 0.31042327255010604\n",
      "Epoch 13, Batch 700, Loss: 0.3160519880056381\n",
      "Epoch 13, Batch 750, Loss: 0.2991151565313339\n",
      "Epoch 13, Batch 800, Loss: 0.30024787664413455\n",
      "Epoch 13, Batch 850, Loss: 0.30716229259967803\n",
      "Epoch 13, Batch 900, Loss: 0.31587766110897064\n",
      "Epoch 14, Batch 50, Loss: 0.2898129400610924\n",
      "Epoch 14, Batch 100, Loss: 0.30414315819740295\n",
      "Epoch 14, Batch 150, Loss: 0.3031289556622505\n",
      "Epoch 14, Batch 200, Loss: 0.29806120932102204\n",
      "Epoch 14, Batch 250, Loss: 0.30753787100315094\n",
      "Epoch 14, Batch 300, Loss: 0.31242457330226897\n",
      "Epoch 14, Batch 350, Loss: 0.29020897775888443\n",
      "Epoch 14, Batch 400, Loss: 0.30596265256404875\n",
      "Epoch 14, Batch 450, Loss: 0.3000991460680962\n",
      "Epoch 14, Batch 500, Loss: 0.3019305232167244\n",
      "Epoch 14, Batch 550, Loss: 0.29677057087421416\n",
      "Epoch 14, Batch 600, Loss: 0.3149497517943382\n",
      "Epoch 14, Batch 650, Loss: 0.30874726802110675\n",
      "Epoch 14, Batch 700, Loss: 0.2999259865283966\n",
      "Epoch 14, Batch 750, Loss: 0.29739089161157606\n",
      "Epoch 14, Batch 800, Loss: 0.3070502945780754\n",
      "Epoch 14, Batch 850, Loss: 0.29696772903203966\n",
      "Epoch 14, Batch 900, Loss: 0.307054677605629\n",
      "Epoch 15, Batch 50, Loss: 0.3013810727000237\n",
      "Epoch 15, Batch 100, Loss: 0.29473965674638747\n",
      "Epoch 15, Batch 150, Loss: 0.30179460018873216\n",
      "Epoch 15, Batch 200, Loss: 0.30373966932296753\n",
      "Epoch 15, Batch 250, Loss: 0.3106405580043793\n",
      "Epoch 15, Batch 300, Loss: 0.2981892734766006\n",
      "Epoch 15, Batch 350, Loss: 0.2871637427806854\n",
      "Epoch 15, Batch 400, Loss: 0.3057188221812248\n",
      "Epoch 15, Batch 450, Loss: 0.3004129260778427\n",
      "Epoch 15, Batch 500, Loss: 0.30782811880111693\n",
      "Epoch 15, Batch 550, Loss: 0.3069989389181137\n",
      "Epoch 15, Batch 600, Loss: 0.3130190145969391\n",
      "Epoch 15, Batch 650, Loss: 0.29167385190725326\n",
      "Epoch 15, Batch 700, Loss: 0.30096085339784623\n",
      "Epoch 15, Batch 750, Loss: 0.30803403675556185\n",
      "Epoch 15, Batch 800, Loss: 0.30310056239366534\n",
      "Epoch 15, Batch 850, Loss: 0.2983940514922142\n",
      "Epoch 15, Batch 900, Loss: 0.29743438839912417\n",
      "Epoch 16, Batch 50, Loss: 0.2992779353260994\n",
      "Epoch 16, Batch 100, Loss: 0.2990507671236992\n",
      "Epoch 16, Batch 150, Loss: 0.2975391909480095\n",
      "Epoch 16, Batch 200, Loss: 0.294838407933712\n",
      "Epoch 16, Batch 250, Loss: 0.30557764440774915\n",
      "Epoch 16, Batch 300, Loss: 0.30125537544488906\n",
      "Epoch 16, Batch 350, Loss: 0.3090564465522766\n",
      "Epoch 16, Batch 400, Loss: 0.2914438945055008\n",
      "Epoch 16, Batch 450, Loss: 0.2988608652353287\n",
      "Epoch 16, Batch 500, Loss: 0.3074927657842636\n",
      "Epoch 16, Batch 550, Loss: 0.3107494479417801\n",
      "Epoch 16, Batch 600, Loss: 0.28997327893972397\n",
      "Epoch 16, Batch 650, Loss: 0.2907607573270798\n",
      "Epoch 16, Batch 700, Loss: 0.29909857869148254\n",
      "Epoch 16, Batch 750, Loss: 0.3144382494688034\n",
      "Epoch 16, Batch 800, Loss: 0.3014430883526802\n",
      "Epoch 16, Batch 850, Loss: 0.313274572789669\n",
      "Epoch 16, Batch 900, Loss: 0.3079618260264397\n",
      "Epoch 17, Batch 50, Loss: 0.29421932727098465\n",
      "Epoch 17, Batch 100, Loss: 0.30208525627851485\n",
      "Epoch 17, Batch 150, Loss: 0.3047133809328079\n",
      "Epoch 17, Batch 200, Loss: 0.30121779441833496\n",
      "Epoch 17, Batch 250, Loss: 0.3069676870107651\n",
      "Epoch 17, Batch 300, Loss: 0.30043435394763945\n",
      "Epoch 17, Batch 350, Loss: 0.30168008744716646\n",
      "Epoch 17, Batch 400, Loss: 0.30334095150232315\n",
      "Epoch 17, Batch 450, Loss: 0.29761415243148803\n",
      "Epoch 17, Batch 500, Loss: 0.3010786470770836\n",
      "Epoch 17, Batch 550, Loss: 0.30798701107501986\n",
      "Epoch 17, Batch 600, Loss: 0.29706338673830035\n",
      "Epoch 17, Batch 650, Loss: 0.30291089296340945\n",
      "Epoch 17, Batch 700, Loss: 0.30326955765485764\n",
      "Epoch 17, Batch 750, Loss: 0.3039600187540054\n",
      "Epoch 17, Batch 800, Loss: 0.3015605819225311\n",
      "Epoch 17, Batch 850, Loss: 0.30653494596481323\n",
      "Epoch 17, Batch 900, Loss: 0.3150689348578453\n",
      "Epoch 18, Batch 50, Loss: 0.2994919994473457\n",
      "Epoch 18, Batch 100, Loss: 0.29506033539772036\n",
      "Epoch 18, Batch 150, Loss: 0.29725939124822615\n",
      "Epoch 18, Batch 200, Loss: 0.3060990011692047\n",
      "Epoch 18, Batch 250, Loss: 0.2869555589556694\n",
      "Epoch 18, Batch 300, Loss: 0.3192672824859619\n",
      "Epoch 18, Batch 350, Loss: 0.30298215031623843\n",
      "Epoch 18, Batch 400, Loss: 0.30576474994421005\n",
      "Epoch 18, Batch 450, Loss: 0.30582782208919523\n",
      "Epoch 18, Batch 500, Loss: 0.2955763438344002\n",
      "Epoch 18, Batch 550, Loss: 0.2944567158818245\n",
      "Epoch 18, Batch 600, Loss: 0.2858149594068527\n",
      "Epoch 18, Batch 650, Loss: 0.30076374679803847\n",
      "Epoch 18, Batch 700, Loss: 0.3087331596016884\n",
      "Epoch 18, Batch 750, Loss: 0.30807296097278597\n",
      "Epoch 18, Batch 800, Loss: 0.3013839256763458\n",
      "Epoch 18, Batch 850, Loss: 0.31669244199991226\n",
      "Epoch 18, Batch 900, Loss: 0.31799417436122895\n",
      "Epoch 19, Batch 50, Loss: 0.29084013134241105\n",
      "Epoch 19, Batch 100, Loss: 0.29659910321235655\n",
      "Epoch 19, Batch 150, Loss: 0.29502799540758134\n",
      "Epoch 19, Batch 200, Loss: 0.30532349705696105\n",
      "Epoch 19, Batch 250, Loss: 0.30897568225860594\n",
      "Epoch 19, Batch 300, Loss: 0.2964730721712112\n",
      "Epoch 19, Batch 350, Loss: 0.29771347910165785\n",
      "Epoch 19, Batch 400, Loss: 0.3099380186200142\n",
      "Epoch 19, Batch 450, Loss: 0.2989374217391014\n",
      "Epoch 19, Batch 500, Loss: 0.2967964258790016\n",
      "Epoch 19, Batch 550, Loss: 0.31298625469207764\n",
      "Epoch 19, Batch 600, Loss: 0.29120709031820297\n",
      "Epoch 19, Batch 650, Loss: 0.2872683098912239\n",
      "Epoch 19, Batch 700, Loss: 0.29424119800329207\n",
      "Epoch 19, Batch 750, Loss: 0.31767517894506453\n",
      "Epoch 19, Batch 800, Loss: 0.3063157084584236\n",
      "Epoch 19, Batch 850, Loss: 0.3125832149386406\n",
      "Epoch 19, Batch 900, Loss: 0.30044198244810105\n",
      "Epoch 20, Batch 50, Loss: 0.2966745883226395\n",
      "Epoch 20, Batch 100, Loss: 0.30790424793958665\n",
      "Epoch 20, Batch 150, Loss: 0.30898656755685805\n",
      "Epoch 20, Batch 200, Loss: 0.3149759918451309\n",
      "Epoch 20, Batch 250, Loss: 0.2930824086070061\n",
      "Epoch 20, Batch 300, Loss: 0.3069968780875206\n",
      "Epoch 20, Batch 350, Loss: 0.31439639151096344\n",
      "Epoch 20, Batch 400, Loss: 0.3030597853660584\n",
      "Epoch 20, Batch 450, Loss: 0.3068965557217598\n",
      "Epoch 20, Batch 500, Loss: 0.30674917459487916\n",
      "Epoch 20, Batch 550, Loss: 0.3007151088118553\n",
      "Epoch 20, Batch 600, Loss: 0.2921365821361542\n",
      "Epoch 20, Batch 650, Loss: 0.3122313818335533\n",
      "Epoch 20, Batch 700, Loss: 0.3074844044446945\n",
      "Epoch 20, Batch 750, Loss: 0.3019350016117096\n",
      "Epoch 20, Batch 800, Loss: 0.28592461466789243\n",
      "Epoch 20, Batch 850, Loss: 0.29208420306444166\n",
      "Epoch 20, Batch 900, Loss: 0.29857990026474\n",
      "Epoch 21, Batch 50, Loss: 0.30973968386650086\n",
      "Epoch 21, Batch 100, Loss: 0.30872073888778684\n",
      "Epoch 21, Batch 150, Loss: 0.31511873036623\n",
      "Epoch 21, Batch 200, Loss: 0.3021047618985176\n",
      "Epoch 21, Batch 250, Loss: 0.28800614953041076\n",
      "Epoch 21, Batch 300, Loss: 0.30598276644945144\n",
      "Epoch 21, Batch 350, Loss: 0.30683512061834334\n",
      "Epoch 21, Batch 400, Loss: 0.3003436502814293\n",
      "Epoch 21, Batch 450, Loss: 0.30734347462654116\n",
      "Epoch 21, Batch 500, Loss: 0.31478889524936676\n",
      "Epoch 21, Batch 550, Loss: 0.2948309442400932\n",
      "Epoch 21, Batch 600, Loss: 0.3113028264045715\n",
      "Epoch 21, Batch 650, Loss: 0.29134552270174024\n",
      "Epoch 21, Batch 700, Loss: 0.2930590859055519\n",
      "Epoch 21, Batch 750, Loss: 0.30528330743312837\n",
      "Epoch 21, Batch 800, Loss: 0.2902620902657509\n",
      "Epoch 21, Batch 850, Loss: 0.30250054836273194\n",
      "Epoch 21, Batch 900, Loss: 0.29714125394821167\n",
      "Epoch 22, Batch 50, Loss: 0.299570392370224\n",
      "Epoch 22, Batch 100, Loss: 0.30446637243032454\n",
      "Epoch 22, Batch 150, Loss: 0.31436165779829023\n",
      "Epoch 22, Batch 200, Loss: 0.3030087193846703\n",
      "Epoch 22, Batch 250, Loss: 0.3120696249604225\n",
      "Epoch 22, Batch 300, Loss: 0.3137414473295212\n",
      "Epoch 22, Batch 350, Loss: 0.3123992130160332\n",
      "Epoch 22, Batch 400, Loss: 0.3111366304755211\n",
      "Epoch 22, Batch 450, Loss: 0.2967432153224945\n",
      "Epoch 22, Batch 500, Loss: 0.29377102553844453\n",
      "Epoch 22, Batch 550, Loss: 0.3010385924577713\n",
      "Epoch 22, Batch 600, Loss: 0.30958654612302783\n",
      "Epoch 22, Batch 650, Loss: 0.3082842952013016\n",
      "Epoch 22, Batch 700, Loss: 0.2993340364098549\n",
      "Epoch 22, Batch 750, Loss: 0.30939623177051545\n",
      "Epoch 22, Batch 800, Loss: 0.2947669878602028\n",
      "Epoch 22, Batch 850, Loss: 0.2856059432029724\n",
      "Epoch 22, Batch 900, Loss: 0.29273418128490447\n",
      "Epoch 23, Batch 50, Loss: 0.30125143349170685\n",
      "Epoch 23, Batch 100, Loss: 0.30963140696287156\n",
      "Epoch 23, Batch 150, Loss: 0.29662204414606097\n",
      "Epoch 23, Batch 200, Loss: 0.3095665255188942\n",
      "Epoch 23, Batch 250, Loss: 0.2928358092904091\n",
      "Epoch 23, Batch 300, Loss: 0.29419129073619843\n",
      "Epoch 23, Batch 350, Loss: 0.30844849586486817\n",
      "Epoch 23, Batch 400, Loss: 0.2961284589767456\n",
      "Epoch 23, Batch 450, Loss: 0.297347831428051\n",
      "Epoch 23, Batch 500, Loss: 0.3038816037774086\n",
      "Epoch 23, Batch 550, Loss: 0.29859484821557997\n",
      "Epoch 23, Batch 600, Loss: 0.3085165846347809\n",
      "Epoch 23, Batch 650, Loss: 0.29949185967445374\n",
      "Epoch 23, Batch 700, Loss: 0.31161351889371874\n",
      "Epoch 23, Batch 750, Loss: 0.30495068699121475\n",
      "Epoch 23, Batch 800, Loss: 0.30127779811620714\n",
      "Epoch 23, Batch 850, Loss: 0.29946622282266616\n",
      "Epoch 23, Batch 900, Loss: 0.308149556517601\n",
      "Epoch 24, Batch 50, Loss: 0.30542112320661546\n",
      "Epoch 24, Batch 100, Loss: 0.2950352755188942\n",
      "Epoch 24, Batch 150, Loss: 0.3058382728695869\n",
      "Epoch 24, Batch 200, Loss: 0.3071573132276535\n",
      "Epoch 24, Batch 250, Loss: 0.29683863908052444\n",
      "Epoch 24, Batch 300, Loss: 0.29596930027008056\n",
      "Epoch 24, Batch 350, Loss: 0.30055522710084914\n",
      "Epoch 24, Batch 400, Loss: 0.3032622146606445\n",
      "Epoch 24, Batch 450, Loss: 0.2928371146321297\n",
      "Epoch 24, Batch 500, Loss: 0.30323570758104323\n",
      "Epoch 24, Batch 550, Loss: 0.30070163488388063\n",
      "Epoch 24, Batch 600, Loss: 0.30003969073295594\n",
      "Epoch 24, Batch 650, Loss: 0.3022422620654106\n",
      "Epoch 24, Batch 700, Loss: 0.31386508166790006\n",
      "Epoch 24, Batch 750, Loss: 0.30936189711093903\n",
      "Epoch 24, Batch 800, Loss: 0.300451343357563\n",
      "Epoch 24, Batch 850, Loss: 0.29799759775400164\n",
      "Epoch 24, Batch 900, Loss: 0.29522917300462725\n",
      "Epoch 25, Batch 50, Loss: 0.29805122822523117\n",
      "Epoch 25, Batch 100, Loss: 0.28810843974351885\n",
      "Epoch 25, Batch 150, Loss: 0.29743963927030564\n",
      "Epoch 25, Batch 200, Loss: 0.2939433124661446\n",
      "Epoch 25, Batch 250, Loss: 0.30294466465711595\n",
      "Epoch 25, Batch 300, Loss: 0.3052091410756111\n",
      "Epoch 25, Batch 350, Loss: 0.3009012275934219\n",
      "Epoch 25, Batch 400, Loss: 0.30142930448055266\n",
      "Epoch 25, Batch 450, Loss: 0.2970969665050507\n",
      "Epoch 25, Batch 500, Loss: 0.29184418499469755\n",
      "Epoch 25, Batch 550, Loss: 0.30690080910921097\n",
      "Epoch 25, Batch 600, Loss: 0.30406166285276415\n",
      "Epoch 25, Batch 650, Loss: 0.2985405284166336\n",
      "Epoch 25, Batch 700, Loss: 0.3027780231833458\n",
      "Epoch 25, Batch 750, Loss: 0.29841147392988204\n",
      "Epoch 25, Batch 800, Loss: 0.3044627603888512\n",
      "Epoch 25, Batch 850, Loss: 0.30385889410972594\n",
      "Epoch 25, Batch 900, Loss: 0.3114570298790932\n",
      "Accuracy on test set: 0.7189%\n",
      "Fitting for combination 34\n",
      "784\n",
      "2\n",
      "10\n",
      "[40, 40, 10]\n",
      "False\n",
      "['relu', 'tanh']\n",
      "Adam\n",
      "0.03\n",
      "0.01\n",
      "CrossEntropyLoss\n",
      "100\n",
      "25\n",
      "0\n",
      "Epoch 1, Batch 100, Loss: 1.1472524482011794\n",
      "Epoch 1, Batch 200, Loss: 0.9203357595205307\n",
      "Epoch 1, Batch 300, Loss: 0.8413053721189498\n",
      "Epoch 1, Batch 400, Loss: 0.9048012503981591\n",
      "Epoch 1, Batch 500, Loss: 0.8369640177488327\n",
      "Epoch 1, Batch 600, Loss: 0.8535764837265014\n",
      "Epoch 1, Batch 700, Loss: 0.8563745960593223\n",
      "Epoch 1, Batch 800, Loss: 0.8502035537362098\n",
      "Epoch 1, Batch 900, Loss: 0.8153855592012406\n",
      "Epoch 2, Batch 100, Loss: 0.8224464881420136\n",
      "Epoch 2, Batch 200, Loss: 0.8389221328496933\n",
      "Epoch 2, Batch 300, Loss: 0.8287451422214508\n",
      "Epoch 2, Batch 400, Loss: 0.8345565235614777\n",
      "Epoch 2, Batch 500, Loss: 0.8126188027858734\n",
      "Epoch 2, Batch 600, Loss: 0.8060137522220612\n",
      "Epoch 2, Batch 700, Loss: 0.8070808497071266\n",
      "Epoch 2, Batch 800, Loss: 0.8091757702827453\n",
      "Epoch 2, Batch 900, Loss: 0.8200733104348182\n",
      "Epoch 3, Batch 100, Loss: 0.7804935532808304\n",
      "Epoch 3, Batch 200, Loss: 0.8528687340021134\n",
      "Epoch 3, Batch 300, Loss: 0.8745634213089943\n",
      "Epoch 3, Batch 400, Loss: 0.8225475704669952\n",
      "Epoch 3, Batch 500, Loss: 0.793121095597744\n",
      "Epoch 3, Batch 600, Loss: 0.8156193059682846\n",
      "Epoch 3, Batch 700, Loss: 0.8406725645065307\n",
      "Epoch 3, Batch 800, Loss: 0.8424587017297744\n",
      "Epoch 3, Batch 900, Loss: 0.8267210680246353\n",
      "Epoch 4, Batch 100, Loss: 0.8233898729085922\n",
      "Epoch 4, Batch 200, Loss: 0.8066847360134125\n",
      "Epoch 4, Batch 300, Loss: 0.8257599157094956\n",
      "Epoch 4, Batch 400, Loss: 0.8420888555049896\n",
      "Epoch 4, Batch 500, Loss: 0.8322627538442612\n",
      "Epoch 4, Batch 600, Loss: 0.7716587111353874\n",
      "Epoch 4, Batch 700, Loss: 0.827078207731247\n",
      "Epoch 4, Batch 800, Loss: 0.8335684829950333\n",
      "Epoch 4, Batch 900, Loss: 0.8647855007648468\n",
      "Epoch 5, Batch 100, Loss: 0.8484450232982635\n",
      "Epoch 5, Batch 200, Loss: 0.7896514493227005\n",
      "Epoch 5, Batch 300, Loss: 0.8073844701051712\n",
      "Epoch 5, Batch 400, Loss: 0.8314499241113663\n",
      "Epoch 5, Batch 500, Loss: 0.8714450192451477\n",
      "Epoch 5, Batch 600, Loss: 0.8307985901832581\n",
      "Epoch 5, Batch 700, Loss: 0.8395966729521751\n",
      "Epoch 5, Batch 800, Loss: 0.8369508570432663\n",
      "Epoch 5, Batch 900, Loss: 0.8604801991581916\n",
      "Epoch 6, Batch 100, Loss: 0.8322464871406555\n",
      "Epoch 6, Batch 200, Loss: 0.8038553217053414\n",
      "Epoch 6, Batch 300, Loss: 0.8579180830717087\n",
      "Epoch 6, Batch 400, Loss: 0.792127251625061\n",
      "Epoch 6, Batch 500, Loss: 0.8046363174915314\n",
      "Epoch 6, Batch 600, Loss: 0.814869195818901\n",
      "Epoch 6, Batch 700, Loss: 0.8015574529767037\n",
      "Epoch 6, Batch 800, Loss: 0.7985950374603271\n",
      "Epoch 6, Batch 900, Loss: 0.807245192527771\n",
      "Epoch 7, Batch 100, Loss: 0.7844361567497253\n",
      "Epoch 7, Batch 200, Loss: 0.8337297451496124\n",
      "Epoch 7, Batch 300, Loss: 0.8246329480409622\n",
      "Epoch 7, Batch 400, Loss: 0.8086192119121551\n",
      "Epoch 7, Batch 500, Loss: 0.8103470611572265\n",
      "Epoch 7, Batch 600, Loss: 0.8234700798988343\n",
      "Epoch 7, Batch 700, Loss: 0.7995316159725189\n",
      "Epoch 7, Batch 800, Loss: 0.7756929185986519\n",
      "Epoch 7, Batch 900, Loss: 0.8168239879608155\n",
      "Epoch 8, Batch 100, Loss: 0.8035793721675872\n",
      "Epoch 8, Batch 200, Loss: 0.8508260291814804\n",
      "Epoch 8, Batch 300, Loss: 0.7792661759257317\n",
      "Epoch 8, Batch 400, Loss: 0.8101871579885482\n",
      "Epoch 8, Batch 500, Loss: 0.8782891452312469\n",
      "Epoch 8, Batch 600, Loss: 0.8741574478149414\n",
      "Epoch 8, Batch 700, Loss: 0.7932734274864197\n",
      "Epoch 8, Batch 800, Loss: 0.8000554025173188\n",
      "Epoch 8, Batch 900, Loss: 0.839155843257904\n",
      "Epoch 9, Batch 100, Loss: 0.8169216620922088\n",
      "Epoch 9, Batch 200, Loss: 0.8327222433686257\n",
      "Epoch 9, Batch 300, Loss: 0.8602416497468949\n",
      "Epoch 9, Batch 400, Loss: 0.8711818259954452\n",
      "Epoch 9, Batch 500, Loss: 0.7936919993162155\n",
      "Epoch 9, Batch 600, Loss: 0.8137698221206665\n",
      "Epoch 9, Batch 700, Loss: 0.7742693781852722\n",
      "Epoch 9, Batch 800, Loss: 0.8139821940660477\n",
      "Epoch 9, Batch 900, Loss: 0.8095706045627594\n",
      "Epoch 10, Batch 100, Loss: 0.799946959912777\n",
      "Epoch 10, Batch 200, Loss: 0.8006509125232697\n",
      "Epoch 10, Batch 300, Loss: 0.8319701474905014\n",
      "Epoch 10, Batch 400, Loss: 0.7968854123353958\n",
      "Epoch 10, Batch 500, Loss: 0.8060661053657532\n",
      "Epoch 10, Batch 600, Loss: 0.8276579606533051\n",
      "Epoch 10, Batch 700, Loss: 0.8452026504278183\n",
      "Epoch 10, Batch 800, Loss: 0.8059651011228561\n",
      "Epoch 10, Batch 900, Loss: 0.8657685130834579\n",
      "Epoch 11, Batch 100, Loss: 0.8051141422986984\n",
      "Epoch 11, Batch 200, Loss: 0.8611543223261833\n",
      "Epoch 11, Batch 300, Loss: 0.8350440752506256\n",
      "Epoch 11, Batch 400, Loss: 0.7962310627102852\n",
      "Epoch 11, Batch 500, Loss: 0.8751674395799637\n",
      "Epoch 11, Batch 600, Loss: 0.8722353142499923\n",
      "Epoch 11, Batch 700, Loss: 0.8706297147274017\n",
      "Epoch 11, Batch 800, Loss: 0.8190599405765533\n",
      "Epoch 11, Batch 900, Loss: 0.851563327908516\n",
      "Epoch 12, Batch 100, Loss: 0.7882818114757538\n",
      "Epoch 12, Batch 200, Loss: 0.7699763751029969\n",
      "Epoch 12, Batch 300, Loss: 0.8437217402458191\n",
      "Epoch 12, Batch 400, Loss: 0.8279708278179169\n",
      "Epoch 12, Batch 500, Loss: 0.8187328958511353\n",
      "Epoch 12, Batch 600, Loss: 0.838949863910675\n",
      "Epoch 12, Batch 700, Loss: 0.8121377837657928\n",
      "Epoch 12, Batch 800, Loss: 0.8510248231887817\n",
      "Epoch 12, Batch 900, Loss: 0.8291357105970383\n",
      "Epoch 13, Batch 100, Loss: 0.8228668200969697\n",
      "Epoch 13, Batch 200, Loss: 0.8178725081682205\n",
      "Epoch 13, Batch 300, Loss: 0.8005435141921043\n",
      "Epoch 13, Batch 400, Loss: 0.828024054467678\n",
      "Epoch 13, Batch 500, Loss: 0.8144298216700554\n",
      "Epoch 13, Batch 600, Loss: 0.8472347509860992\n",
      "Epoch 13, Batch 700, Loss: 0.8696000489592552\n",
      "Epoch 13, Batch 800, Loss: 0.8128600656986237\n",
      "Epoch 13, Batch 900, Loss: 0.8195744374394417\n",
      "Epoch 14, Batch 100, Loss: 0.7894104260206223\n",
      "Epoch 14, Batch 200, Loss: 0.8621862477064133\n",
      "Epoch 14, Batch 300, Loss: 0.8488595819473267\n",
      "Epoch 14, Batch 400, Loss: 0.8489133575558663\n",
      "Epoch 14, Batch 500, Loss: 0.8250691920518876\n",
      "Epoch 14, Batch 600, Loss: 0.8179298907518386\n",
      "Epoch 14, Batch 700, Loss: 0.8457192671298981\n",
      "Epoch 14, Batch 800, Loss: 0.8355854350328445\n",
      "Epoch 14, Batch 900, Loss: 0.8135533577203751\n",
      "Epoch 15, Batch 100, Loss: 0.8479029521346092\n",
      "Epoch 15, Batch 200, Loss: 0.801475909948349\n",
      "Epoch 15, Batch 300, Loss: 0.8661586529016495\n",
      "Epoch 15, Batch 400, Loss: 0.7963377198576927\n",
      "Epoch 15, Batch 500, Loss: 0.7953467166423798\n",
      "Epoch 15, Batch 600, Loss: 0.7882716634869575\n",
      "Epoch 15, Batch 700, Loss: 0.7939851546287536\n",
      "Epoch 15, Batch 800, Loss: 0.8309092479944229\n",
      "Epoch 15, Batch 900, Loss: 0.8644226914644242\n",
      "Epoch 16, Batch 100, Loss: 0.8162733775377273\n",
      "Epoch 16, Batch 200, Loss: 0.8970218127965928\n",
      "Epoch 16, Batch 300, Loss: 0.8839656168222427\n",
      "Epoch 16, Batch 400, Loss: 0.8710370415449142\n",
      "Epoch 16, Batch 500, Loss: 0.8070926958322525\n",
      "Epoch 16, Batch 600, Loss: 0.8227547311782837\n",
      "Epoch 16, Batch 700, Loss: 0.7987400573492051\n",
      "Epoch 16, Batch 800, Loss: 0.8097905606031418\n",
      "Epoch 16, Batch 900, Loss: 0.7956892314553261\n",
      "Epoch 17, Batch 100, Loss: 0.845633434355259\n",
      "Epoch 17, Batch 200, Loss: 0.8206525939702988\n",
      "Epoch 17, Batch 300, Loss: 0.8242946183681488\n",
      "Epoch 17, Batch 400, Loss: 0.8205604779720307\n",
      "Epoch 17, Batch 500, Loss: 0.8554367905855179\n",
      "Epoch 17, Batch 600, Loss: 0.8695615309476853\n",
      "Epoch 17, Batch 700, Loss: 0.7891772568225861\n",
      "Epoch 17, Batch 800, Loss: 0.8084284842014313\n",
      "Epoch 17, Batch 900, Loss: 0.8118769317865372\n",
      "Epoch 18, Batch 100, Loss: 0.7977494019269943\n",
      "Epoch 18, Batch 200, Loss: 0.7809750586748123\n",
      "Epoch 18, Batch 300, Loss: 0.8009569624066353\n",
      "Epoch 18, Batch 400, Loss: 0.8035316550731659\n",
      "Epoch 18, Batch 500, Loss: 0.838322759270668\n",
      "Epoch 18, Batch 600, Loss: 0.8184767705202103\n",
      "Epoch 18, Batch 700, Loss: 0.8581668663024903\n",
      "Epoch 18, Batch 800, Loss: 0.8484025472402572\n",
      "Epoch 18, Batch 900, Loss: 0.8052891626954078\n",
      "Epoch 19, Batch 100, Loss: 0.8236208987236023\n",
      "Epoch 19, Batch 200, Loss: 0.8301864993572236\n",
      "Epoch 19, Batch 300, Loss: 0.8342520421743393\n",
      "Epoch 19, Batch 400, Loss: 0.8739610880613327\n",
      "Epoch 19, Batch 500, Loss: 0.7866676983237266\n",
      "Epoch 19, Batch 600, Loss: 0.7970321056246757\n",
      "Epoch 19, Batch 700, Loss: 0.8190769135951996\n",
      "Epoch 19, Batch 800, Loss: 0.8469161105155945\n",
      "Epoch 19, Batch 900, Loss: 0.8406129854917527\n",
      "Epoch 20, Batch 100, Loss: 0.8276690357923507\n",
      "Epoch 20, Batch 200, Loss: 0.830597332417965\n",
      "Epoch 20, Batch 300, Loss: 0.8286397448182106\n",
      "Epoch 20, Batch 400, Loss: 0.8458875873684883\n",
      "Epoch 20, Batch 500, Loss: 0.8256728529930115\n",
      "Epoch 20, Batch 600, Loss: 0.7978612351417541\n",
      "Epoch 20, Batch 700, Loss: 0.7898920869827271\n",
      "Epoch 20, Batch 800, Loss: 0.8218292739987373\n",
      "Epoch 20, Batch 900, Loss: 0.8396928936243058\n",
      "Epoch 21, Batch 100, Loss: 0.8093049550056457\n",
      "Epoch 21, Batch 200, Loss: 0.7900066390633583\n",
      "Epoch 21, Batch 300, Loss: 0.8315380096435547\n",
      "Epoch 21, Batch 400, Loss: 0.8562091356515884\n",
      "Epoch 21, Batch 500, Loss: 0.8305077904462814\n",
      "Epoch 21, Batch 600, Loss: 0.8116368570923805\n",
      "Epoch 21, Batch 700, Loss: 0.8155691266059876\n",
      "Epoch 21, Batch 800, Loss: 0.7936024498939515\n",
      "Epoch 21, Batch 900, Loss: 0.8039547881484032\n",
      "Epoch 22, Batch 100, Loss: 0.8285337674617768\n",
      "Epoch 22, Batch 200, Loss: 0.8502765837311744\n",
      "Epoch 22, Batch 300, Loss: 0.8235026282072068\n",
      "Epoch 22, Batch 400, Loss: 0.7921304750442505\n",
      "Epoch 22, Batch 500, Loss: 0.8418028563261032\n",
      "Epoch 22, Batch 600, Loss: 0.8133579951524734\n",
      "Epoch 22, Batch 700, Loss: 0.815356371998787\n",
      "Epoch 22, Batch 800, Loss: 0.8150949466228485\n",
      "Epoch 22, Batch 900, Loss: 0.806788564324379\n",
      "Epoch 23, Batch 100, Loss: 0.8420052641630172\n",
      "Epoch 23, Batch 200, Loss: 0.7922885295748711\n",
      "Epoch 23, Batch 300, Loss: 0.8102879777550698\n",
      "Epoch 23, Batch 400, Loss: 0.8232467240095138\n",
      "Epoch 23, Batch 500, Loss: 0.8508473736047745\n",
      "Epoch 23, Batch 600, Loss: 0.8146385106444359\n",
      "Epoch 23, Batch 700, Loss: 0.8123895114660263\n",
      "Epoch 23, Batch 800, Loss: 0.8374380999803543\n",
      "Epoch 23, Batch 900, Loss: 0.8000893306732177\n",
      "Epoch 24, Batch 100, Loss: 0.8126805120706558\n",
      "Epoch 24, Batch 200, Loss: 0.8456864255666733\n",
      "Epoch 24, Batch 300, Loss: 0.8917797720432281\n",
      "Epoch 24, Batch 400, Loss: 0.8435109043121338\n",
      "Epoch 24, Batch 500, Loss: 0.8193007522821426\n",
      "Epoch 24, Batch 600, Loss: 0.8312407237291336\n",
      "Epoch 24, Batch 700, Loss: 0.8228949528932571\n",
      "Epoch 24, Batch 800, Loss: 0.8175824147462845\n",
      "Epoch 24, Batch 900, Loss: 0.782026641368866\n",
      "Epoch 25, Batch 100, Loss: 0.7908397901058197\n",
      "Epoch 25, Batch 200, Loss: 0.7917307162284851\n",
      "Epoch 25, Batch 300, Loss: 0.7982559955120087\n",
      "Epoch 25, Batch 400, Loss: 0.8261590987443924\n",
      "Epoch 25, Batch 500, Loss: 0.7834065389633179\n",
      "Epoch 25, Batch 600, Loss: 0.818210887312889\n",
      "Epoch 25, Batch 700, Loss: 0.8144747054576874\n",
      "Epoch 25, Batch 800, Loss: 0.7575985792279244\n",
      "Epoch 25, Batch 900, Loss: 0.7869628185033798\n",
      "Accuracy on test set: 0.6881%\n",
      "Fitting for combination 35\n",
      "784\n",
      "2\n",
      "10\n",
      "[40, 40, 10]\n",
      "True\n",
      "['relu', 'sigmoid']\n",
      "SGD\n",
      "0.3\n",
      "0.1\n",
      "CrossEntropyLoss\n",
      "50\n",
      "25\n",
      "0\n",
      "Epoch 1, Batch 50, Loss: 1.0547069036960601\n",
      "Epoch 1, Batch 100, Loss: 0.8885690212249756\n",
      "Epoch 1, Batch 150, Loss: 0.8277813422679902\n",
      "Epoch 1, Batch 200, Loss: 0.8015393948554993\n",
      "Epoch 1, Batch 250, Loss: 0.7989808630943298\n",
      "Epoch 1, Batch 300, Loss: 0.7898479855060577\n",
      "Epoch 1, Batch 350, Loss: 0.7865754973888397\n",
      "Epoch 1, Batch 400, Loss: 0.7918880021572113\n",
      "Epoch 1, Batch 450, Loss: 0.7965395033359528\n",
      "Epoch 1, Batch 500, Loss: 0.7915563440322876\n",
      "Epoch 1, Batch 550, Loss: 0.7875464403629303\n",
      "Epoch 1, Batch 600, Loss: 0.7833190846443177\n",
      "Epoch 1, Batch 650, Loss: 0.78442911028862\n",
      "Epoch 1, Batch 700, Loss: 0.7888868427276612\n",
      "Epoch 1, Batch 750, Loss: 0.7900665986537934\n",
      "Epoch 1, Batch 800, Loss: 0.7897660899162292\n",
      "Epoch 1, Batch 850, Loss: 0.7847731626033783\n",
      "Epoch 1, Batch 900, Loss: 0.7869956719875336\n",
      "Epoch 2, Batch 50, Loss: 0.7934459519386291\n",
      "Epoch 2, Batch 100, Loss: 0.7882758808135987\n",
      "Epoch 2, Batch 150, Loss: 0.7864746391773224\n",
      "Epoch 2, Batch 200, Loss: 0.7906189477443695\n",
      "Epoch 2, Batch 250, Loss: 0.7882495701313019\n",
      "Epoch 2, Batch 300, Loss: 0.7803303337097168\n",
      "Epoch 2, Batch 350, Loss: 0.7898666679859161\n",
      "Epoch 2, Batch 400, Loss: 0.7863069319725037\n",
      "Epoch 2, Batch 450, Loss: 0.7853906059265137\n",
      "Epoch 2, Batch 500, Loss: 0.7859937751293182\n",
      "Epoch 2, Batch 550, Loss: 0.778863525390625\n",
      "Epoch 2, Batch 600, Loss: 0.788460830450058\n",
      "Epoch 2, Batch 650, Loss: 0.788078886270523\n",
      "Epoch 2, Batch 700, Loss: 0.7879163241386413\n",
      "Epoch 2, Batch 750, Loss: 0.7909428441524505\n",
      "Epoch 2, Batch 800, Loss: 0.7824280202388764\n",
      "Epoch 2, Batch 850, Loss: 0.7846555018424988\n",
      "Epoch 2, Batch 900, Loss: 0.7867150986194611\n",
      "Epoch 3, Batch 50, Loss: 0.7884524726867675\n",
      "Epoch 3, Batch 100, Loss: 0.788921766281128\n",
      "Epoch 3, Batch 150, Loss: 0.7776795554161072\n",
      "Epoch 3, Batch 200, Loss: 0.7875572288036347\n",
      "Epoch 3, Batch 250, Loss: 0.7880414962768555\n",
      "Epoch 3, Batch 300, Loss: 0.7887911224365234\n",
      "Epoch 3, Batch 350, Loss: 0.7835223507881165\n",
      "Epoch 3, Batch 400, Loss: 0.7904361701011657\n",
      "Epoch 3, Batch 450, Loss: 0.7864263904094696\n",
      "Epoch 3, Batch 500, Loss: 0.7890212559700012\n",
      "Epoch 3, Batch 550, Loss: 0.7850856053829193\n",
      "Epoch 3, Batch 600, Loss: 0.780950620174408\n",
      "Epoch 3, Batch 650, Loss: 0.7855321621894836\n",
      "Epoch 3, Batch 700, Loss: 0.7850988864898681\n",
      "Epoch 3, Batch 750, Loss: 0.7915134656429291\n",
      "Epoch 3, Batch 800, Loss: 0.7927077734470367\n",
      "Epoch 3, Batch 850, Loss: 0.7863945853710175\n",
      "Epoch 3, Batch 900, Loss: 0.7901480281352997\n",
      "Epoch 4, Batch 50, Loss: 0.7900732576847076\n",
      "Epoch 4, Batch 100, Loss: 0.7877347815036774\n",
      "Epoch 4, Batch 150, Loss: 0.7879636359214782\n",
      "Epoch 4, Batch 200, Loss: 0.7816249811649323\n",
      "Epoch 4, Batch 250, Loss: 0.7926951050758362\n",
      "Epoch 4, Batch 300, Loss: 0.7847089850902558\n",
      "Epoch 4, Batch 350, Loss: 0.7812701773643493\n",
      "Epoch 4, Batch 400, Loss: 0.7839318442344666\n",
      "Epoch 4, Batch 450, Loss: 0.7857219696044921\n",
      "Epoch 4, Batch 500, Loss: 0.7860119712352752\n",
      "Epoch 4, Batch 550, Loss: 0.791123834848404\n",
      "Epoch 4, Batch 600, Loss: 0.7884054052829742\n",
      "Epoch 4, Batch 650, Loss: 0.7825736856460571\n",
      "Epoch 4, Batch 700, Loss: 0.7916838550567626\n",
      "Epoch 4, Batch 750, Loss: 0.7824154996871948\n",
      "Epoch 4, Batch 800, Loss: 0.7845404350757599\n",
      "Epoch 4, Batch 850, Loss: 0.7863356256484986\n",
      "Epoch 4, Batch 900, Loss: 0.7862608349323272\n",
      "Epoch 5, Batch 50, Loss: 0.7842651760578155\n",
      "Epoch 5, Batch 100, Loss: 0.7856848442554474\n",
      "Epoch 5, Batch 150, Loss: 0.7865220379829406\n",
      "Epoch 5, Batch 200, Loss: 0.7816850900650024\n",
      "Epoch 5, Batch 250, Loss: 0.7811384761333465\n",
      "Epoch 5, Batch 300, Loss: 0.7846756768226624\n",
      "Epoch 5, Batch 350, Loss: 0.7851613593101502\n",
      "Epoch 5, Batch 400, Loss: 0.7924896168708802\n",
      "Epoch 5, Batch 450, Loss: 0.7851327538490296\n",
      "Epoch 5, Batch 500, Loss: 0.7802761852741241\n",
      "Epoch 5, Batch 550, Loss: 0.7873377311229706\n",
      "Epoch 5, Batch 600, Loss: 0.7946367812156677\n",
      "Epoch 5, Batch 650, Loss: 0.7906122088432312\n",
      "Epoch 5, Batch 700, Loss: 0.7957132470607757\n",
      "Epoch 5, Batch 750, Loss: 0.7952627217769623\n",
      "Epoch 5, Batch 800, Loss: 0.7804483842849731\n",
      "Epoch 5, Batch 850, Loss: 0.7888092780113221\n",
      "Epoch 5, Batch 900, Loss: 0.7801471102237701\n",
      "Epoch 6, Batch 50, Loss: 0.7800749170780182\n",
      "Epoch 6, Batch 100, Loss: 0.7858441615104675\n",
      "Epoch 6, Batch 150, Loss: 0.78175590634346\n",
      "Epoch 6, Batch 200, Loss: 0.7769467496871948\n",
      "Epoch 6, Batch 250, Loss: 0.7874875724315643\n",
      "Epoch 6, Batch 300, Loss: 0.7849218666553497\n",
      "Epoch 6, Batch 350, Loss: 0.7903113079071045\n",
      "Epoch 6, Batch 400, Loss: 0.777872428894043\n",
      "Epoch 6, Batch 450, Loss: 0.7874232387542724\n",
      "Epoch 6, Batch 500, Loss: 0.7891964411735535\n",
      "Epoch 6, Batch 550, Loss: 0.785491737127304\n",
      "Epoch 6, Batch 600, Loss: 0.7932660567760468\n",
      "Epoch 6, Batch 650, Loss: 0.7882026994228363\n",
      "Epoch 6, Batch 700, Loss: 0.7899097275733947\n",
      "Epoch 6, Batch 750, Loss: 0.7903703594207764\n",
      "Epoch 6, Batch 800, Loss: 0.787099221944809\n",
      "Epoch 6, Batch 850, Loss: 0.7880085206031799\n",
      "Epoch 6, Batch 900, Loss: 0.786527647972107\n",
      "Epoch 7, Batch 50, Loss: 0.7833938264846801\n",
      "Epoch 7, Batch 100, Loss: 0.7826353824138641\n",
      "Epoch 7, Batch 150, Loss: 0.7885713970661163\n",
      "Epoch 7, Batch 200, Loss: 0.787683846950531\n",
      "Epoch 7, Batch 250, Loss: 0.7886737716197968\n",
      "Epoch 7, Batch 300, Loss: 0.7888812923431396\n",
      "Epoch 7, Batch 350, Loss: 0.7860568356513977\n",
      "Epoch 7, Batch 400, Loss: 0.7872976529598236\n",
      "Epoch 7, Batch 450, Loss: 0.7904480004310608\n",
      "Epoch 7, Batch 500, Loss: 0.7903665781021119\n",
      "Epoch 7, Batch 550, Loss: 0.784825781583786\n",
      "Epoch 7, Batch 600, Loss: 0.7909812927246094\n",
      "Epoch 7, Batch 650, Loss: 0.7824323439598083\n",
      "Epoch 7, Batch 700, Loss: 0.7829740035533905\n",
      "Epoch 7, Batch 750, Loss: 0.7869817852973938\n",
      "Epoch 7, Batch 800, Loss: 0.789224443435669\n",
      "Epoch 7, Batch 850, Loss: 0.7860666632652282\n",
      "Epoch 7, Batch 900, Loss: 0.7829850900173188\n",
      "Epoch 8, Batch 50, Loss: 0.7749023866653443\n",
      "Epoch 8, Batch 100, Loss: 0.7848657727241516\n",
      "Epoch 8, Batch 150, Loss: 0.7887858319282531\n",
      "Epoch 8, Batch 200, Loss: 0.7937393891811371\n",
      "Epoch 8, Batch 250, Loss: 0.7850193440914154\n",
      "Epoch 8, Batch 300, Loss: 0.7856086015701294\n",
      "Epoch 8, Batch 350, Loss: 0.7957204484939575\n",
      "Epoch 8, Batch 400, Loss: 0.7933541798591613\n",
      "Epoch 8, Batch 450, Loss: 0.7869781506061554\n",
      "Epoch 8, Batch 500, Loss: 0.7814299845695496\n",
      "Epoch 8, Batch 550, Loss: 0.7782592177391052\n",
      "Epoch 8, Batch 600, Loss: 0.7815826559066772\n",
      "Epoch 8, Batch 650, Loss: 0.7850639379024505\n",
      "Epoch 8, Batch 700, Loss: 0.7930301547050476\n",
      "Epoch 8, Batch 750, Loss: 0.7915723168849945\n",
      "Epoch 8, Batch 800, Loss: 0.788534779548645\n",
      "Epoch 8, Batch 850, Loss: 0.7905345010757446\n",
      "Epoch 8, Batch 900, Loss: 0.786260930299759\n",
      "Epoch 9, Batch 50, Loss: 0.7971120572090149\n",
      "Epoch 9, Batch 100, Loss: 0.7925755655765534\n",
      "Epoch 9, Batch 150, Loss: 0.7852126252651215\n",
      "Epoch 9, Batch 200, Loss: 0.7812781703472137\n",
      "Epoch 9, Batch 250, Loss: 0.7840199327468872\n",
      "Epoch 9, Batch 300, Loss: 0.7821210718154907\n",
      "Epoch 9, Batch 350, Loss: 0.7874183559417725\n",
      "Epoch 9, Batch 400, Loss: 0.7941557383537292\n",
      "Epoch 9, Batch 450, Loss: 0.7834042537212372\n",
      "Epoch 9, Batch 500, Loss: 0.786544201374054\n",
      "Epoch 9, Batch 550, Loss: 0.7800385451316834\n",
      "Epoch 9, Batch 600, Loss: 0.7819401276111603\n",
      "Epoch 9, Batch 650, Loss: 0.7919785583019257\n",
      "Epoch 9, Batch 700, Loss: 0.7832801854610443\n",
      "Epoch 9, Batch 750, Loss: 0.7823944008350372\n",
      "Epoch 9, Batch 800, Loss: 0.7773561882972717\n",
      "Epoch 9, Batch 850, Loss: 0.7861429488658905\n",
      "Epoch 9, Batch 900, Loss: 0.797360326051712\n",
      "Epoch 10, Batch 50, Loss: 0.7929732966423034\n",
      "Epoch 10, Batch 100, Loss: 0.7797693383693695\n",
      "Epoch 10, Batch 150, Loss: 0.7865240025520325\n",
      "Epoch 10, Batch 200, Loss: 0.7935065412521363\n",
      "Epoch 10, Batch 250, Loss: 0.788061021566391\n",
      "Epoch 10, Batch 300, Loss: 0.7931742084026336\n",
      "Epoch 10, Batch 350, Loss: 0.7853213047981262\n",
      "Epoch 10, Batch 400, Loss: 0.7868161392211914\n",
      "Epoch 10, Batch 450, Loss: 0.7899407255649566\n",
      "Epoch 10, Batch 500, Loss: 0.7813381135463715\n",
      "Epoch 10, Batch 550, Loss: 0.7870453202724457\n",
      "Epoch 10, Batch 600, Loss: 0.790265154838562\n",
      "Epoch 10, Batch 650, Loss: 0.7878061866760254\n",
      "Epoch 10, Batch 700, Loss: 0.7866503751277923\n",
      "Epoch 10, Batch 750, Loss: 0.7917453527450562\n",
      "Epoch 10, Batch 800, Loss: 0.7881134271621704\n",
      "Epoch 10, Batch 850, Loss: 0.7820152413845062\n",
      "Epoch 10, Batch 900, Loss: 0.7900087022781372\n",
      "Epoch 11, Batch 50, Loss: 0.7878465056419373\n",
      "Epoch 11, Batch 100, Loss: 0.7864328014850617\n",
      "Epoch 11, Batch 150, Loss: 0.7821778333187104\n",
      "Epoch 11, Batch 200, Loss: 0.7848209512233734\n",
      "Epoch 11, Batch 250, Loss: 0.7904789543151856\n",
      "Epoch 11, Batch 300, Loss: 0.783906797170639\n",
      "Epoch 11, Batch 350, Loss: 0.7858551359176635\n",
      "Epoch 11, Batch 400, Loss: 0.7909805524349213\n",
      "Epoch 11, Batch 450, Loss: 0.7855357944965362\n",
      "Epoch 11, Batch 500, Loss: 0.7842068159580231\n",
      "Epoch 11, Batch 550, Loss: 0.7868564510345459\n",
      "Epoch 11, Batch 600, Loss: 0.7861651921272278\n",
      "Epoch 11, Batch 650, Loss: 0.7843185687065124\n",
      "Epoch 11, Batch 700, Loss: 0.7883400082588196\n",
      "Epoch 11, Batch 750, Loss: 0.7862397944927215\n",
      "Epoch 11, Batch 800, Loss: 0.7855736088752746\n",
      "Epoch 11, Batch 850, Loss: 0.7885109066963196\n",
      "Epoch 11, Batch 900, Loss: 0.7895344316959381\n",
      "Epoch 12, Batch 50, Loss: 0.7930812120437623\n",
      "Epoch 12, Batch 100, Loss: 0.7905810093879699\n",
      "Epoch 12, Batch 150, Loss: 0.7822425591945649\n",
      "Epoch 12, Batch 200, Loss: 0.7817148888111114\n",
      "Epoch 12, Batch 250, Loss: 0.7815381574630738\n",
      "Epoch 12, Batch 300, Loss: 0.7869276165962219\n",
      "Epoch 12, Batch 350, Loss: 0.7829794037342072\n",
      "Epoch 12, Batch 400, Loss: 0.7846895158290863\n",
      "Epoch 12, Batch 450, Loss: 0.7937475717067719\n",
      "Epoch 12, Batch 500, Loss: 0.7936132955551147\n",
      "Epoch 12, Batch 550, Loss: 0.7858709037303925\n",
      "Epoch 12, Batch 600, Loss: 0.7901165497303009\n",
      "Epoch 12, Batch 650, Loss: 0.7857970106601715\n",
      "Epoch 12, Batch 700, Loss: 0.7802905631065369\n",
      "Epoch 12, Batch 750, Loss: 0.7865544605255127\n",
      "Epoch 12, Batch 800, Loss: 0.7882797861099243\n",
      "Epoch 12, Batch 850, Loss: 0.7943468809127807\n",
      "Epoch 12, Batch 900, Loss: 0.7919660019874573\n",
      "Epoch 13, Batch 50, Loss: 0.7821857726573944\n",
      "Epoch 13, Batch 100, Loss: 0.7919426500797272\n",
      "Epoch 13, Batch 150, Loss: 0.7874814522266388\n",
      "Epoch 13, Batch 200, Loss: 0.7923536241054535\n",
      "Epoch 13, Batch 250, Loss: 0.7852811276912689\n",
      "Epoch 13, Batch 300, Loss: 0.7867374992370606\n",
      "Epoch 13, Batch 350, Loss: 0.7924743962287902\n",
      "Epoch 13, Batch 400, Loss: 0.7874403095245361\n",
      "Epoch 13, Batch 450, Loss: 0.7903915894031525\n",
      "Epoch 13, Batch 500, Loss: 0.7963549208641052\n",
      "Epoch 13, Batch 550, Loss: 0.7940490627288819\n",
      "Epoch 13, Batch 600, Loss: 0.7845148348808288\n",
      "Epoch 13, Batch 650, Loss: 0.7861645925045013\n",
      "Epoch 13, Batch 700, Loss: 0.7834233260154724\n",
      "Epoch 13, Batch 750, Loss: 0.7873037922382354\n",
      "Epoch 13, Batch 800, Loss: 0.7848500311374664\n",
      "Epoch 13, Batch 850, Loss: 0.7856587445735932\n",
      "Epoch 13, Batch 900, Loss: 0.7876021301746369\n",
      "Epoch 14, Batch 50, Loss: 0.7876386845111847\n",
      "Epoch 14, Batch 100, Loss: 0.790729318857193\n",
      "Epoch 14, Batch 150, Loss: 0.7955046987533569\n",
      "Epoch 14, Batch 200, Loss: 0.7897626888751984\n",
      "Epoch 14, Batch 250, Loss: 0.7923181116580963\n",
      "Epoch 14, Batch 300, Loss: 0.792193990945816\n",
      "Epoch 14, Batch 350, Loss: 0.7903888559341431\n",
      "Epoch 14, Batch 400, Loss: 0.7841654682159424\n",
      "Epoch 14, Batch 450, Loss: 0.7919974148273468\n",
      "Epoch 14, Batch 500, Loss: 0.7890426850318909\n",
      "Epoch 14, Batch 550, Loss: 0.7789012289047241\n",
      "Epoch 14, Batch 600, Loss: 0.7825646960735321\n",
      "Epoch 14, Batch 650, Loss: 0.7850657844543457\n",
      "Epoch 14, Batch 700, Loss: 0.7864880156517029\n",
      "Epoch 14, Batch 750, Loss: 0.782925922870636\n",
      "Epoch 14, Batch 800, Loss: 0.7849968659877777\n",
      "Epoch 14, Batch 850, Loss: 0.7780234253406525\n",
      "Epoch 14, Batch 900, Loss: 0.7830650997161865\n",
      "Epoch 15, Batch 50, Loss: 0.7861752080917358\n",
      "Epoch 15, Batch 100, Loss: 0.7883979272842407\n",
      "Epoch 15, Batch 150, Loss: 0.7995522177219391\n",
      "Epoch 15, Batch 200, Loss: 0.7856292343139648\n",
      "Epoch 15, Batch 250, Loss: 0.7902881801128387\n",
      "Epoch 15, Batch 300, Loss: 0.7866292524337769\n",
      "Epoch 15, Batch 350, Loss: 0.7828279185295105\n",
      "Epoch 15, Batch 400, Loss: 0.7868257856369019\n",
      "Epoch 15, Batch 450, Loss: 0.7890859258174896\n",
      "Epoch 15, Batch 500, Loss: 0.7874552822113037\n",
      "Epoch 15, Batch 550, Loss: 0.7830090868473053\n",
      "Epoch 15, Batch 600, Loss: 0.7861396539211273\n",
      "Epoch 15, Batch 650, Loss: 0.7881268656253815\n",
      "Epoch 15, Batch 700, Loss: 0.7910399448871612\n",
      "Epoch 15, Batch 750, Loss: 0.7889207792282105\n",
      "Epoch 15, Batch 800, Loss: 0.7844782209396363\n",
      "Epoch 15, Batch 850, Loss: 0.7918250286579132\n",
      "Epoch 15, Batch 900, Loss: 0.7886307668685913\n",
      "Epoch 16, Batch 50, Loss: 0.783743155002594\n",
      "Epoch 16, Batch 100, Loss: 0.778951153755188\n",
      "Epoch 16, Batch 150, Loss: 0.7856297361850738\n",
      "Epoch 16, Batch 200, Loss: 0.7899197733402252\n",
      "Epoch 16, Batch 250, Loss: 0.785362651348114\n",
      "Epoch 16, Batch 300, Loss: 0.793546952009201\n",
      "Epoch 16, Batch 350, Loss: 0.7969286346435547\n",
      "Epoch 16, Batch 400, Loss: 0.7822971105575561\n",
      "Epoch 16, Batch 450, Loss: 0.7864718663692475\n",
      "Epoch 16, Batch 500, Loss: 0.7913387787342071\n",
      "Epoch 16, Batch 550, Loss: 0.7858972513675689\n",
      "Epoch 16, Batch 600, Loss: 0.7873904073238372\n",
      "Epoch 16, Batch 650, Loss: 0.7938923633098602\n",
      "Epoch 16, Batch 700, Loss: 0.7941981494426728\n",
      "Epoch 16, Batch 750, Loss: 0.7838810682296753\n",
      "Epoch 16, Batch 800, Loss: 0.7873259055614471\n",
      "Epoch 16, Batch 850, Loss: 0.7971632218360901\n",
      "Epoch 16, Batch 900, Loss: 0.7821856045722961\n",
      "Epoch 17, Batch 50, Loss: 0.7848454928398132\n",
      "Epoch 17, Batch 100, Loss: 0.7868362152576447\n",
      "Epoch 17, Batch 150, Loss: 0.7856162917613984\n",
      "Epoch 17, Batch 200, Loss: 0.7932353925704956\n",
      "Epoch 17, Batch 250, Loss: 0.7953713655471801\n",
      "Epoch 17, Batch 300, Loss: 0.7855488574504852\n",
      "Epoch 17, Batch 350, Loss: 0.7870191204547882\n",
      "Epoch 17, Batch 400, Loss: 0.7903308379650116\n",
      "Epoch 17, Batch 450, Loss: 0.7881252551078797\n",
      "Epoch 17, Batch 500, Loss: 0.7881083941459656\n",
      "Epoch 17, Batch 550, Loss: 0.7845806646347045\n",
      "Epoch 17, Batch 600, Loss: 0.7934498405456543\n",
      "Epoch 17, Batch 650, Loss: 0.7896638536453247\n",
      "Epoch 17, Batch 700, Loss: 0.7870680105686187\n",
      "Epoch 17, Batch 750, Loss: 0.7871491277217865\n",
      "Epoch 17, Batch 800, Loss: 0.7871196591854095\n",
      "Epoch 17, Batch 850, Loss: 0.7764295601844787\n",
      "Epoch 17, Batch 900, Loss: 0.7823464047908782\n",
      "Epoch 18, Batch 50, Loss: 0.7774506080150604\n",
      "Epoch 18, Batch 100, Loss: 0.7907043647766113\n",
      "Epoch 18, Batch 150, Loss: 0.7922976303100586\n",
      "Epoch 18, Batch 200, Loss: 0.7954073011875152\n",
      "Epoch 18, Batch 250, Loss: 0.7952607631683349\n",
      "Epoch 18, Batch 300, Loss: 0.7837177574634552\n",
      "Epoch 18, Batch 350, Loss: 0.7909368598461151\n",
      "Epoch 18, Batch 400, Loss: 0.7920596671104431\n",
      "Epoch 18, Batch 450, Loss: 0.7886300599575042\n",
      "Epoch 18, Batch 500, Loss: 0.7888175475597382\n",
      "Epoch 18, Batch 550, Loss: 0.7787116539478302\n",
      "Epoch 18, Batch 600, Loss: 0.7785721206665039\n",
      "Epoch 18, Batch 650, Loss: 0.7840240943431854\n",
      "Epoch 18, Batch 700, Loss: 0.7823356997966766\n",
      "Epoch 18, Batch 750, Loss: 0.7851226377487183\n",
      "Epoch 18, Batch 800, Loss: 0.7892601084709168\n",
      "Epoch 18, Batch 850, Loss: 0.7815085089206696\n",
      "Epoch 18, Batch 900, Loss: 0.7945329535007477\n",
      "Epoch 19, Batch 50, Loss: 0.7923325407505035\n",
      "Epoch 19, Batch 100, Loss: 0.7859004843235016\n",
      "Epoch 19, Batch 150, Loss: 0.7864085149765014\n",
      "Epoch 19, Batch 200, Loss: 0.7830839216709137\n",
      "Epoch 19, Batch 250, Loss: 0.7867945456504821\n",
      "Epoch 19, Batch 300, Loss: 0.7855150282382966\n",
      "Epoch 19, Batch 350, Loss: 0.7878507328033447\n",
      "Epoch 19, Batch 400, Loss: 0.787229392528534\n",
      "Epoch 19, Batch 450, Loss: 0.7843469405174255\n",
      "Epoch 19, Batch 500, Loss: 0.7881538474559784\n",
      "Epoch 19, Batch 550, Loss: 0.7877822923660278\n",
      "Epoch 19, Batch 600, Loss: 0.7865501236915589\n",
      "Epoch 19, Batch 650, Loss: 0.7858812177181244\n",
      "Epoch 19, Batch 700, Loss: 0.7774903452396393\n",
      "Epoch 19, Batch 750, Loss: 0.7905731928348542\n",
      "Epoch 19, Batch 800, Loss: 0.7921287298202515\n",
      "Epoch 19, Batch 850, Loss: 0.7835897397994995\n",
      "Epoch 19, Batch 900, Loss: 0.7873147404193879\n",
      "Epoch 20, Batch 50, Loss: 0.7900316870212555\n",
      "Epoch 20, Batch 100, Loss: 0.7893950128555298\n",
      "Epoch 20, Batch 150, Loss: 0.7814300060272217\n",
      "Epoch 20, Batch 200, Loss: 0.7914352333545684\n",
      "Epoch 20, Batch 250, Loss: 0.7759612941741943\n",
      "Epoch 20, Batch 300, Loss: 0.7869736731052399\n",
      "Epoch 20, Batch 350, Loss: 0.7835872220993042\n",
      "Epoch 20, Batch 400, Loss: 0.7964242386817932\n",
      "Epoch 20, Batch 450, Loss: 0.7830780375003815\n",
      "Epoch 20, Batch 500, Loss: 0.7869734370708465\n",
      "Epoch 20, Batch 550, Loss: 0.7888016700744629\n",
      "Epoch 20, Batch 600, Loss: 0.7769056463241577\n",
      "Epoch 20, Batch 650, Loss: 0.7895338118076325\n",
      "Epoch 20, Batch 700, Loss: 0.7870554983615875\n",
      "Epoch 20, Batch 750, Loss: 0.787287893295288\n",
      "Epoch 20, Batch 800, Loss: 0.7838419604301453\n",
      "Epoch 20, Batch 850, Loss: 0.7900102853775024\n",
      "Epoch 20, Batch 900, Loss: 0.7863591182231903\n",
      "Epoch 21, Batch 50, Loss: 0.7890681970119476\n",
      "Epoch 21, Batch 100, Loss: 0.7927863132953644\n",
      "Epoch 21, Batch 150, Loss: 0.7887650060653687\n",
      "Epoch 21, Batch 200, Loss: 0.7850411677360535\n",
      "Epoch 21, Batch 250, Loss: 0.7914618051052094\n",
      "Epoch 21, Batch 300, Loss: 0.7864846134185791\n",
      "Epoch 21, Batch 350, Loss: 0.7844525349140167\n",
      "Epoch 21, Batch 400, Loss: 0.7873224353790284\n",
      "Epoch 21, Batch 450, Loss: 0.7890214514732361\n",
      "Epoch 21, Batch 500, Loss: 0.7818777441978455\n",
      "Epoch 21, Batch 550, Loss: 0.7909474050998688\n",
      "Epoch 21, Batch 600, Loss: 0.7956486999988556\n",
      "Epoch 21, Batch 650, Loss: 0.7867558240890503\n",
      "Epoch 21, Batch 700, Loss: 0.795010677576065\n",
      "Epoch 21, Batch 750, Loss: 0.787074648141861\n",
      "Epoch 21, Batch 800, Loss: 0.7863623249530792\n",
      "Epoch 21, Batch 850, Loss: 0.783412892818451\n",
      "Epoch 21, Batch 900, Loss: 0.779205596446991\n",
      "Epoch 22, Batch 50, Loss: 0.7887479829788208\n",
      "Epoch 22, Batch 100, Loss: 0.7849844300746918\n",
      "Epoch 22, Batch 150, Loss: 0.7879063773155213\n",
      "Epoch 22, Batch 200, Loss: 0.7899021017551422\n",
      "Epoch 22, Batch 250, Loss: 0.787733587026596\n",
      "Epoch 22, Batch 300, Loss: 0.7841476869583129\n",
      "Epoch 22, Batch 350, Loss: 0.7880080926418305\n",
      "Epoch 22, Batch 400, Loss: 0.7877860867977142\n",
      "Epoch 22, Batch 450, Loss: 0.7813490235805511\n",
      "Epoch 22, Batch 500, Loss: 0.7842970561981201\n",
      "Epoch 22, Batch 550, Loss: 0.790279871225357\n",
      "Epoch 22, Batch 600, Loss: 0.783826824426651\n",
      "Epoch 22, Batch 650, Loss: 0.7837122535705566\n",
      "Epoch 22, Batch 700, Loss: 0.7829832482337952\n",
      "Epoch 22, Batch 750, Loss: 0.7923361468315124\n",
      "Epoch 22, Batch 800, Loss: 0.7961091017723083\n",
      "Epoch 22, Batch 850, Loss: 0.7855040788650512\n",
      "Epoch 22, Batch 900, Loss: 0.7828083419799805\n",
      "Epoch 23, Batch 50, Loss: 0.7931756854057312\n",
      "Epoch 23, Batch 100, Loss: 0.7829069554805755\n",
      "Epoch 23, Batch 150, Loss: 0.7905660450458527\n",
      "Epoch 23, Batch 200, Loss: 0.7872883975505829\n",
      "Epoch 23, Batch 250, Loss: 0.7838499176502228\n",
      "Epoch 23, Batch 300, Loss: 0.7848284840583801\n",
      "Epoch 23, Batch 350, Loss: 0.7832967972755432\n",
      "Epoch 23, Batch 400, Loss: 0.7890782034397126\n",
      "Epoch 23, Batch 450, Loss: 0.7774305927753449\n",
      "Epoch 23, Batch 500, Loss: 0.7793311417102814\n",
      "Epoch 23, Batch 550, Loss: 0.7873478651046752\n",
      "Epoch 23, Batch 600, Loss: 0.7862129616737366\n",
      "Epoch 23, Batch 650, Loss: 0.7879133772850037\n",
      "Epoch 23, Batch 700, Loss: 0.7803294420242309\n",
      "Epoch 23, Batch 750, Loss: 0.7871680295467377\n",
      "Epoch 23, Batch 800, Loss: 0.7858014154434204\n",
      "Epoch 23, Batch 850, Loss: 0.7870061755180359\n",
      "Epoch 23, Batch 900, Loss: 0.7937058246135712\n",
      "Epoch 24, Batch 50, Loss: 0.7858145141601562\n",
      "Epoch 24, Batch 100, Loss: 0.7849238634109497\n",
      "Epoch 24, Batch 150, Loss: 0.7829137194156647\n",
      "Epoch 24, Batch 200, Loss: 0.786879096031189\n",
      "Epoch 24, Batch 250, Loss: 0.7898364663124084\n",
      "Epoch 24, Batch 300, Loss: 0.7896784830093384\n",
      "Epoch 24, Batch 350, Loss: 0.7867342472076416\n",
      "Epoch 24, Batch 400, Loss: 0.7911076068878173\n",
      "Epoch 24, Batch 450, Loss: 0.7868726813793182\n",
      "Epoch 24, Batch 500, Loss: 0.7889345896244049\n",
      "Epoch 24, Batch 550, Loss: 0.7884601187705994\n",
      "Epoch 24, Batch 600, Loss: 0.7848890864849091\n",
      "Epoch 24, Batch 650, Loss: 0.7916930472850799\n",
      "Epoch 24, Batch 700, Loss: 0.7895529103279114\n",
      "Epoch 24, Batch 750, Loss: 0.7823303616046906\n",
      "Epoch 24, Batch 800, Loss: 0.7903267610073089\n",
      "Epoch 24, Batch 850, Loss: 0.7875162303447724\n",
      "Epoch 24, Batch 900, Loss: 0.7919352746009827\n",
      "Epoch 25, Batch 50, Loss: 0.7865579497814178\n",
      "Epoch 25, Batch 100, Loss: 0.7902687108516693\n",
      "Epoch 25, Batch 150, Loss: 0.7880796706676483\n",
      "Epoch 25, Batch 200, Loss: 0.7942748773097992\n",
      "Epoch 25, Batch 250, Loss: 0.7843790233135224\n",
      "Epoch 25, Batch 300, Loss: 0.7887666559219361\n",
      "Epoch 25, Batch 350, Loss: 0.7884897017478942\n",
      "Epoch 25, Batch 400, Loss: 0.7788977015018463\n",
      "Epoch 25, Batch 450, Loss: 0.7906540501117706\n",
      "Epoch 25, Batch 500, Loss: 0.7849041938781738\n",
      "Epoch 25, Batch 550, Loss: 0.7844608974456787\n",
      "Epoch 25, Batch 600, Loss: 0.7867615365982056\n",
      "Epoch 25, Batch 650, Loss: 0.7917683565616608\n",
      "Epoch 25, Batch 700, Loss: 0.7873788571357727\n",
      "Epoch 25, Batch 750, Loss: 0.7783245277404786\n",
      "Epoch 25, Batch 800, Loss: 0.7869259762763977\n",
      "Epoch 25, Batch 850, Loss: 0.7899975442886352\n",
      "Epoch 25, Batch 900, Loss: 0.7900368356704712\n",
      "Accuracy on test set: 0.4198%\n",
      "Fitting for combination 36\n",
      "784\n",
      "2\n",
      "10\n",
      "[40, 50, 10]\n",
      "True\n",
      "['relu', 'sigmoid']\n",
      "Adam\n",
      "0.01\n",
      "0.3\n",
      "CrossEntropyLoss\n",
      "100\n",
      "25\n",
      "0\n",
      "Epoch 1, Batch 100, Loss: 2.314304883480072\n",
      "Epoch 1, Batch 200, Loss: 2.318014180660248\n",
      "Epoch 1, Batch 300, Loss: 2.3214721512794494\n",
      "Epoch 1, Batch 400, Loss: 2.319822659492493\n",
      "Epoch 1, Batch 500, Loss: 2.316104829311371\n",
      "Epoch 1, Batch 600, Loss: 2.3183174109458924\n",
      "Epoch 1, Batch 700, Loss: 2.319613211154938\n",
      "Epoch 1, Batch 800, Loss: 2.3238948607444763\n",
      "Epoch 1, Batch 900, Loss: 2.321279351711273\n",
      "Epoch 2, Batch 100, Loss: 2.321110942363739\n",
      "Epoch 2, Batch 200, Loss: 2.3218671703338623\n",
      "Epoch 2, Batch 300, Loss: 2.3183460903167723\n",
      "Epoch 2, Batch 400, Loss: 2.318325228691101\n",
      "Epoch 2, Batch 500, Loss: 2.326621994972229\n",
      "Epoch 2, Batch 600, Loss: 2.320088818073273\n",
      "Epoch 2, Batch 700, Loss: 2.3185801196098326\n",
      "Epoch 2, Batch 800, Loss: 2.318634150028229\n",
      "Epoch 2, Batch 900, Loss: 2.316986792087555\n",
      "Epoch 3, Batch 100, Loss: 2.3180817651748655\n",
      "Epoch 3, Batch 200, Loss: 2.3252839255332947\n",
      "Epoch 3, Batch 300, Loss: 2.3191979360580444\n",
      "Epoch 3, Batch 400, Loss: 2.3183134460449217\n",
      "Epoch 3, Batch 500, Loss: 2.322843325138092\n",
      "Epoch 3, Batch 600, Loss: 2.3191362023353577\n",
      "Epoch 3, Batch 700, Loss: 2.3176842665672304\n",
      "Epoch 3, Batch 800, Loss: 2.3222993516922\n",
      "Epoch 3, Batch 900, Loss: 2.318340787887573\n",
      "Epoch 4, Batch 100, Loss: 2.321434671878815\n",
      "Epoch 4, Batch 200, Loss: 2.3213021540641785\n",
      "Epoch 4, Batch 300, Loss: 2.318467514514923\n",
      "Epoch 4, Batch 400, Loss: 2.3194234228134154\n",
      "Epoch 4, Batch 500, Loss: 2.3211464190483095\n",
      "Epoch 4, Batch 600, Loss: 2.3201973748207094\n",
      "Epoch 4, Batch 700, Loss: 2.324143488407135\n",
      "Epoch 4, Batch 800, Loss: 2.3168129897117615\n",
      "Epoch 4, Batch 900, Loss: 2.3179587745666503\n",
      "Epoch 5, Batch 100, Loss: 2.319278497695923\n",
      "Epoch 5, Batch 200, Loss: 2.317311623096466\n",
      "Epoch 5, Batch 300, Loss: 2.3251596093177795\n",
      "Epoch 5, Batch 400, Loss: 2.320578672885895\n",
      "Epoch 5, Batch 500, Loss: 2.322996451854706\n",
      "Epoch 5, Batch 600, Loss: 2.3190670228004455\n",
      "Epoch 5, Batch 700, Loss: 2.318458688259125\n",
      "Epoch 5, Batch 800, Loss: 2.3170673036575318\n",
      "Epoch 5, Batch 900, Loss: 2.319391739368439\n",
      "Epoch 6, Batch 100, Loss: 2.3228454613685607\n",
      "Epoch 6, Batch 200, Loss: 2.321160373687744\n",
      "Epoch 6, Batch 300, Loss: 2.3252701210975646\n",
      "Epoch 6, Batch 400, Loss: 2.3209400725364686\n",
      "Epoch 6, Batch 500, Loss: 2.3251550149917604\n",
      "Epoch 6, Batch 600, Loss: 2.32439035654068\n",
      "Epoch 6, Batch 700, Loss: 2.3185701894760133\n",
      "Epoch 6, Batch 800, Loss: 2.3157580661773682\n",
      "Epoch 6, Batch 900, Loss: 2.323788731098175\n",
      "Epoch 7, Batch 100, Loss: 2.3176188182830813\n",
      "Epoch 7, Batch 200, Loss: 2.3204249787330626\n",
      "Epoch 7, Batch 300, Loss: 2.3211529803276063\n",
      "Epoch 7, Batch 400, Loss: 2.316472454071045\n",
      "Epoch 7, Batch 500, Loss: 2.3147862195968627\n",
      "Epoch 7, Batch 600, Loss: 2.3194541478157045\n",
      "Epoch 7, Batch 700, Loss: 2.320836486816406\n",
      "Epoch 7, Batch 800, Loss: 2.321350326538086\n",
      "Epoch 7, Batch 900, Loss: 2.3188043594360352\n",
      "Epoch 8, Batch 100, Loss: 2.323081476688385\n",
      "Epoch 8, Batch 200, Loss: 2.319334032535553\n",
      "Epoch 8, Batch 300, Loss: 2.323339195251465\n",
      "Epoch 8, Batch 400, Loss: 2.326522316932678\n",
      "Epoch 8, Batch 500, Loss: 2.3190182423591614\n",
      "Epoch 8, Batch 600, Loss: 2.3132430624961855\n",
      "Epoch 8, Batch 700, Loss: 2.3131611824035643\n",
      "Epoch 8, Batch 800, Loss: 2.3197627234458924\n",
      "Epoch 8, Batch 900, Loss: 2.3184922623634336\n",
      "Epoch 9, Batch 100, Loss: 2.321443672180176\n",
      "Epoch 9, Batch 200, Loss: 2.319535880088806\n",
      "Epoch 9, Batch 300, Loss: 2.319340219497681\n",
      "Epoch 9, Batch 400, Loss: 2.323322992324829\n",
      "Epoch 9, Batch 500, Loss: 2.319993314743042\n",
      "Epoch 9, Batch 600, Loss: 2.3161626172065737\n",
      "Epoch 9, Batch 700, Loss: 2.3188552927970885\n",
      "Epoch 9, Batch 800, Loss: 2.3192545866966245\n",
      "Epoch 9, Batch 900, Loss: 2.317313086986542\n",
      "Epoch 10, Batch 100, Loss: 2.330635042190552\n",
      "Epoch 10, Batch 200, Loss: 2.3234172368049624\n",
      "Epoch 10, Batch 300, Loss: 2.325642111301422\n",
      "Epoch 10, Batch 400, Loss: 2.318517384529114\n",
      "Epoch 10, Batch 500, Loss: 2.3134207248687746\n",
      "Epoch 10, Batch 600, Loss: 2.3142329144477842\n",
      "Epoch 10, Batch 700, Loss: 2.320046055316925\n",
      "Epoch 10, Batch 800, Loss: 2.320783705711365\n",
      "Epoch 10, Batch 900, Loss: 2.3208430099487303\n",
      "Epoch 11, Batch 100, Loss: 2.32375661611557\n",
      "Epoch 11, Batch 200, Loss: 2.315740249156952\n",
      "Epoch 11, Batch 300, Loss: 2.3215357875823974\n",
      "Epoch 11, Batch 400, Loss: 2.3171733975410462\n",
      "Epoch 11, Batch 500, Loss: 2.3219867587089538\n",
      "Epoch 11, Batch 600, Loss: 2.320319182872772\n",
      "Epoch 11, Batch 700, Loss: 2.3182878923416137\n",
      "Epoch 11, Batch 800, Loss: 2.3204652571678164\n",
      "Epoch 11, Batch 900, Loss: 2.3190944266319273\n",
      "Epoch 12, Batch 100, Loss: 2.317763032913208\n",
      "Epoch 12, Batch 200, Loss: 2.3203978538513184\n",
      "Epoch 12, Batch 300, Loss: 2.3187101769447325\n",
      "Epoch 12, Batch 400, Loss: 2.3185435056686403\n",
      "Epoch 12, Batch 500, Loss: 2.3177141332626343\n",
      "Epoch 12, Batch 600, Loss: 2.3244471597671508\n",
      "Epoch 12, Batch 700, Loss: 2.319308919906616\n",
      "Epoch 12, Batch 800, Loss: 2.31780531167984\n",
      "Epoch 12, Batch 900, Loss: 2.3229953241348267\n",
      "Epoch 13, Batch 100, Loss: 2.3196131372451783\n",
      "Epoch 13, Batch 200, Loss: 2.3190273022651673\n",
      "Epoch 13, Batch 300, Loss: 2.3178602647781372\n",
      "Epoch 13, Batch 400, Loss: 2.32312105178833\n",
      "Epoch 13, Batch 500, Loss: 2.3181627702713015\n",
      "Epoch 13, Batch 600, Loss: 2.3168919920921325\n",
      "Epoch 13, Batch 700, Loss: 2.325083627700806\n",
      "Epoch 13, Batch 800, Loss: 2.319094889163971\n",
      "Epoch 13, Batch 900, Loss: 2.320338203907013\n",
      "Epoch 14, Batch 100, Loss: 2.3195246767997744\n",
      "Epoch 14, Batch 200, Loss: 2.3202545714378355\n",
      "Epoch 14, Batch 300, Loss: 2.3185935592651368\n",
      "Epoch 14, Batch 400, Loss: 2.3218025302886964\n",
      "Epoch 14, Batch 500, Loss: 2.3163630962371826\n",
      "Epoch 14, Batch 600, Loss: 2.318256719112396\n",
      "Epoch 14, Batch 700, Loss: 2.3179075360298156\n",
      "Epoch 14, Batch 800, Loss: 2.3181359124183656\n",
      "Epoch 14, Batch 900, Loss: 2.3175080108642576\n",
      "Epoch 15, Batch 100, Loss: 2.3183285641670226\n",
      "Epoch 15, Batch 200, Loss: 2.3202318143844605\n",
      "Epoch 15, Batch 300, Loss: 2.3141279435157776\n",
      "Epoch 15, Batch 400, Loss: 2.318819673061371\n",
      "Epoch 15, Batch 500, Loss: 2.3187913513183593\n",
      "Epoch 15, Batch 600, Loss: 2.3189601612091066\n",
      "Epoch 15, Batch 700, Loss: 2.3196702146530153\n",
      "Epoch 15, Batch 800, Loss: 2.3199848413467405\n",
      "Epoch 15, Batch 900, Loss: 2.3191838383674623\n",
      "Epoch 16, Batch 100, Loss: 2.3220860624313353\n",
      "Epoch 16, Batch 200, Loss: 2.320639975070953\n",
      "Epoch 16, Batch 300, Loss: 2.319935176372528\n",
      "Epoch 16, Batch 400, Loss: 2.323353409767151\n",
      "Epoch 16, Batch 500, Loss: 2.320213243961334\n",
      "Epoch 16, Batch 600, Loss: 2.324815521240234\n",
      "Epoch 16, Batch 700, Loss: 2.320448384284973\n",
      "Epoch 16, Batch 800, Loss: 2.323326072692871\n",
      "Epoch 16, Batch 900, Loss: 2.32412606716156\n",
      "Epoch 17, Batch 100, Loss: 2.319295961856842\n",
      "Epoch 17, Batch 200, Loss: 2.314818866252899\n",
      "Epoch 17, Batch 300, Loss: 2.3171265959739684\n",
      "Epoch 17, Batch 400, Loss: 2.319557800292969\n",
      "Epoch 17, Batch 500, Loss: 2.324017150402069\n",
      "Epoch 17, Batch 600, Loss: 2.3144054794311524\n",
      "Epoch 17, Batch 700, Loss: 2.3158471417427062\n",
      "Epoch 17, Batch 800, Loss: 2.3206133008003236\n",
      "Epoch 17, Batch 900, Loss: 2.314969279766083\n",
      "Epoch 18, Batch 100, Loss: 2.3218324971199036\n",
      "Epoch 18, Batch 200, Loss: 2.316461992263794\n",
      "Epoch 18, Batch 300, Loss: 2.319557225704193\n",
      "Epoch 18, Batch 400, Loss: 2.3249037003517152\n",
      "Epoch 18, Batch 500, Loss: 2.3208990597724917\n",
      "Epoch 18, Batch 600, Loss: 2.3185744643211366\n",
      "Epoch 18, Batch 700, Loss: 2.321581633090973\n",
      "Epoch 18, Batch 800, Loss: 2.3234466886520386\n",
      "Epoch 18, Batch 900, Loss: 2.323647372722626\n",
      "Epoch 19, Batch 100, Loss: 2.3227617716789246\n",
      "Epoch 19, Batch 200, Loss: 2.3216995763778687\n",
      "Epoch 19, Batch 300, Loss: 2.316398284435272\n",
      "Epoch 19, Batch 400, Loss: 2.3188993239402773\n",
      "Epoch 19, Batch 500, Loss: 2.324589719772339\n",
      "Epoch 19, Batch 600, Loss: 2.317952573299408\n",
      "Epoch 19, Batch 700, Loss: 2.321282045841217\n",
      "Epoch 19, Batch 800, Loss: 2.3223979592323305\n",
      "Epoch 19, Batch 900, Loss: 2.3247589468955994\n",
      "Epoch 20, Batch 100, Loss: 2.3207947397232056\n",
      "Epoch 20, Batch 200, Loss: 2.3164877319335937\n",
      "Epoch 20, Batch 300, Loss: 2.3239336967468263\n",
      "Epoch 20, Batch 400, Loss: 2.3236133575439455\n",
      "Epoch 20, Batch 500, Loss: 2.3194636940956115\n",
      "Epoch 20, Batch 600, Loss: 2.32010605096817\n",
      "Epoch 20, Batch 700, Loss: 2.317444088459015\n",
      "Epoch 20, Batch 800, Loss: 2.326944944858551\n",
      "Epoch 20, Batch 900, Loss: 2.319984784126282\n",
      "Epoch 21, Batch 100, Loss: 2.3227892827987673\n",
      "Epoch 21, Batch 200, Loss: 2.321424126625061\n",
      "Epoch 21, Batch 300, Loss: 2.3178413009643553\n",
      "Epoch 21, Batch 400, Loss: 2.320804181098938\n",
      "Epoch 21, Batch 500, Loss: 2.3214363169670107\n",
      "Epoch 21, Batch 600, Loss: 2.3230723428726194\n",
      "Epoch 21, Batch 700, Loss: 2.317678270339966\n",
      "Epoch 21, Batch 800, Loss: 2.3215408325195312\n",
      "Epoch 21, Batch 900, Loss: 2.3202681589126586\n",
      "Epoch 22, Batch 100, Loss: 2.322554233074188\n",
      "Epoch 22, Batch 200, Loss: 2.3193081831932068\n",
      "Epoch 22, Batch 300, Loss: 2.3210119223594665\n",
      "Epoch 22, Batch 400, Loss: 2.3212279677391052\n",
      "Epoch 22, Batch 500, Loss: 2.321083273887634\n",
      "Epoch 22, Batch 600, Loss: 2.3162472581863405\n",
      "Epoch 22, Batch 700, Loss: 2.320963969230652\n",
      "Epoch 22, Batch 800, Loss: 2.321002938747406\n",
      "Epoch 22, Batch 900, Loss: 2.324141809940338\n",
      "Epoch 23, Batch 100, Loss: 2.3273353385925293\n",
      "Epoch 23, Batch 200, Loss: 2.3237171816825866\n",
      "Epoch 23, Batch 300, Loss: 2.3191375613212584\n",
      "Epoch 23, Batch 400, Loss: 2.3146715211868285\n",
      "Epoch 23, Batch 500, Loss: 2.32509884595871\n",
      "Epoch 23, Batch 600, Loss: 2.3198239970207215\n",
      "Epoch 23, Batch 700, Loss: 2.319336016178131\n",
      "Epoch 23, Batch 800, Loss: 2.32070725440979\n",
      "Epoch 23, Batch 900, Loss: 2.3212283062934875\n",
      "Epoch 24, Batch 100, Loss: 2.325701265335083\n",
      "Epoch 24, Batch 200, Loss: 2.318236434459686\n",
      "Epoch 24, Batch 300, Loss: 2.316140830516815\n",
      "Epoch 24, Batch 400, Loss: 2.321716237068176\n",
      "Epoch 24, Batch 500, Loss: 2.318200330734253\n",
      "Epoch 24, Batch 600, Loss: 2.3217242479324343\n",
      "Epoch 24, Batch 700, Loss: 2.3175552821159364\n",
      "Epoch 24, Batch 800, Loss: 2.3167180705070494\n",
      "Epoch 24, Batch 900, Loss: 2.3161780190467836\n",
      "Epoch 25, Batch 100, Loss: 2.319592368602753\n",
      "Epoch 25, Batch 200, Loss: 2.318482415676117\n",
      "Epoch 25, Batch 300, Loss: 2.3226435351371766\n",
      "Epoch 25, Batch 400, Loss: 2.3235701894760132\n",
      "Epoch 25, Batch 500, Loss: 2.3224070858955383\n",
      "Epoch 25, Batch 600, Loss: 2.3191367411613464\n",
      "Epoch 25, Batch 700, Loss: 2.3178538250923157\n",
      "Epoch 25, Batch 800, Loss: 2.318591034412384\n",
      "Epoch 25, Batch 900, Loss: 2.3237892460823057\n",
      "Accuracy on test set: 0.1%\n",
      "Fitting for combination 37\n",
      "784\n",
      "2\n",
      "10\n",
      "[40, 50, 10]\n",
      "True\n",
      "['relu', 'tanh']\n",
      "SGD\n",
      "0.3\n",
      "0.1\n",
      "CrossEntropyLoss\n",
      "100\n",
      "25\n",
      "0\n",
      "Epoch 1, Batch 100, Loss: 1.3078096896409988\n",
      "Epoch 1, Batch 200, Loss: 1.1357720506191253\n",
      "Epoch 1, Batch 300, Loss: 1.1595702892541886\n",
      "Epoch 1, Batch 400, Loss: 1.1572473549842834\n",
      "Epoch 1, Batch 500, Loss: 1.1421710729599\n",
      "Epoch 1, Batch 600, Loss: 1.1139623266458512\n",
      "Epoch 1, Batch 700, Loss: 1.124140229821205\n",
      "Epoch 1, Batch 800, Loss: 1.1319237011671066\n",
      "Epoch 1, Batch 900, Loss: 1.1049305844306945\n",
      "Epoch 2, Batch 100, Loss: 1.0978569048643112\n",
      "Epoch 2, Batch 200, Loss: 1.1384315878152846\n",
      "Epoch 2, Batch 300, Loss: 1.1349009549617768\n",
      "Epoch 2, Batch 400, Loss: 1.1276747620105743\n",
      "Epoch 2, Batch 500, Loss: 1.0965735340118408\n",
      "Epoch 2, Batch 600, Loss: 1.1513944268226624\n",
      "Epoch 2, Batch 700, Loss: 1.1167408692836762\n",
      "Epoch 2, Batch 800, Loss: 1.1359246426820755\n",
      "Epoch 2, Batch 900, Loss: 1.13195820748806\n",
      "Epoch 3, Batch 100, Loss: 1.1621582210063934\n",
      "Epoch 3, Batch 200, Loss: 1.142288755774498\n",
      "Epoch 3, Batch 300, Loss: 1.119277405142784\n",
      "Epoch 3, Batch 400, Loss: 1.1055892544984818\n",
      "Epoch 3, Batch 500, Loss: 1.1050403356552123\n",
      "Epoch 3, Batch 600, Loss: 1.1532620882987976\n",
      "Epoch 3, Batch 700, Loss: 1.1070663338899613\n",
      "Epoch 3, Batch 800, Loss: 1.1434469795227051\n",
      "Epoch 3, Batch 900, Loss: 1.1481865507364273\n",
      "Epoch 4, Batch 100, Loss: 1.1285818642377854\n",
      "Epoch 4, Batch 200, Loss: 1.1397177541255952\n",
      "Epoch 4, Batch 300, Loss: 1.1296309947967529\n",
      "Epoch 4, Batch 400, Loss: 1.1509803944826127\n",
      "Epoch 4, Batch 500, Loss: 1.1259868425130843\n",
      "Epoch 4, Batch 600, Loss: 1.1258724522590637\n",
      "Epoch 4, Batch 700, Loss: 1.1231229299306869\n",
      "Epoch 4, Batch 800, Loss: 1.1175207340717315\n",
      "Epoch 4, Batch 900, Loss: 1.1581071388721467\n",
      "Epoch 5, Batch 100, Loss: 1.1031358826160431\n",
      "Epoch 5, Batch 200, Loss: 1.1017732751369476\n",
      "Epoch 5, Batch 300, Loss: 1.135004295706749\n",
      "Epoch 5, Batch 400, Loss: 1.1258321958780289\n",
      "Epoch 5, Batch 500, Loss: 1.1574092876911164\n",
      "Epoch 5, Batch 600, Loss: 1.137279638648033\n",
      "Epoch 5, Batch 700, Loss: 1.115174491405487\n",
      "Epoch 5, Batch 800, Loss: 1.1241491162776946\n",
      "Epoch 5, Batch 900, Loss: 1.1201940613985062\n",
      "Epoch 6, Batch 100, Loss: 1.1042234057188034\n",
      "Epoch 6, Batch 200, Loss: 1.1101185721158982\n",
      "Epoch 6, Batch 300, Loss: 1.135896224975586\n",
      "Epoch 6, Batch 400, Loss: 1.1240470218658447\n",
      "Epoch 6, Batch 500, Loss: 1.1449349695444107\n",
      "Epoch 6, Batch 600, Loss: 1.1337983858585359\n",
      "Epoch 6, Batch 700, Loss: 1.1587263226509095\n",
      "Epoch 6, Batch 800, Loss: 1.12239452958107\n",
      "Epoch 6, Batch 900, Loss: 1.1221675819158554\n",
      "Epoch 7, Batch 100, Loss: 1.1362034916877746\n",
      "Epoch 7, Batch 200, Loss: 1.1159610652923584\n",
      "Epoch 7, Batch 300, Loss: 1.15202722966671\n",
      "Epoch 7, Batch 400, Loss: 1.132718397974968\n",
      "Epoch 7, Batch 500, Loss: 1.1296686899662018\n",
      "Epoch 7, Batch 600, Loss: 1.1073802423477173\n",
      "Epoch 7, Batch 700, Loss: 1.129619613289833\n",
      "Epoch 7, Batch 800, Loss: 1.1237228029966355\n",
      "Epoch 7, Batch 900, Loss: 1.1479607498645783\n",
      "Epoch 8, Batch 100, Loss: 1.1016422724723816\n",
      "Epoch 8, Batch 200, Loss: 1.129906058907509\n",
      "Epoch 8, Batch 300, Loss: 1.1725812250375747\n",
      "Epoch 8, Batch 400, Loss: 1.1008117198944092\n",
      "Epoch 8, Batch 500, Loss: 1.1154328233003616\n",
      "Epoch 8, Batch 600, Loss: 1.1195849448442459\n",
      "Epoch 8, Batch 700, Loss: 1.102235538959503\n",
      "Epoch 8, Batch 800, Loss: 1.115447461605072\n",
      "Epoch 8, Batch 900, Loss: 1.1153394758701325\n",
      "Epoch 9, Batch 100, Loss: 1.126347509622574\n",
      "Epoch 9, Batch 200, Loss: 1.1027392154932023\n",
      "Epoch 9, Batch 300, Loss: 1.1154392045736312\n",
      "Epoch 9, Batch 400, Loss: 1.1282301831245423\n",
      "Epoch 9, Batch 500, Loss: 1.1264541977643967\n",
      "Epoch 9, Batch 600, Loss: 1.1219829338788987\n",
      "Epoch 9, Batch 700, Loss: 1.1056181526184081\n",
      "Epoch 9, Batch 800, Loss: 1.1093249160051346\n",
      "Epoch 9, Batch 900, Loss: 1.1086829072237014\n",
      "Epoch 10, Batch 100, Loss: 1.1353071802854537\n",
      "Epoch 10, Batch 200, Loss: 1.1492885822057723\n",
      "Epoch 10, Batch 300, Loss: 1.101723731160164\n",
      "Epoch 10, Batch 400, Loss: 1.131211969256401\n",
      "Epoch 10, Batch 500, Loss: 1.1308743411302566\n",
      "Epoch 10, Batch 600, Loss: 1.1194326263666152\n",
      "Epoch 10, Batch 700, Loss: 1.1072579771280289\n",
      "Epoch 10, Batch 800, Loss: 1.1161995232105255\n",
      "Epoch 10, Batch 900, Loss: 1.1167002189159394\n",
      "Epoch 11, Batch 100, Loss: 1.1079242682456971\n",
      "Epoch 11, Batch 200, Loss: 1.1242780393362046\n",
      "Epoch 11, Batch 300, Loss: 1.124663091301918\n",
      "Epoch 11, Batch 400, Loss: 1.1214326268434525\n",
      "Epoch 11, Batch 500, Loss: 1.1041556310653686\n",
      "Epoch 11, Batch 600, Loss: 1.1330311846733094\n",
      "Epoch 11, Batch 700, Loss: 1.1185899269580841\n",
      "Epoch 11, Batch 800, Loss: 1.1291443955898286\n",
      "Epoch 11, Batch 900, Loss: 1.1377120512723922\n",
      "Epoch 12, Batch 100, Loss: 1.1570091170072556\n",
      "Epoch 12, Batch 200, Loss: 1.1212835383415223\n",
      "Epoch 12, Batch 300, Loss: 1.1337402522563935\n",
      "Epoch 12, Batch 400, Loss: 1.0997394317388534\n",
      "Epoch 12, Batch 500, Loss: 1.1298958891630173\n",
      "Epoch 12, Batch 600, Loss: 1.161846039891243\n",
      "Epoch 12, Batch 700, Loss: 1.0999955177307128\n",
      "Epoch 12, Batch 800, Loss: 1.1361669820547105\n",
      "Epoch 12, Batch 900, Loss: 1.141565845012665\n",
      "Epoch 13, Batch 100, Loss: 1.1349951720237732\n",
      "Epoch 13, Batch 200, Loss: 1.1508675020933152\n",
      "Epoch 13, Batch 300, Loss: 1.1080095422267915\n",
      "Epoch 13, Batch 400, Loss: 1.113257047533989\n",
      "Epoch 13, Batch 500, Loss: 1.1096922445297241\n",
      "Epoch 13, Batch 600, Loss: 1.1162048441171646\n",
      "Epoch 13, Batch 700, Loss: 1.1576589274406432\n",
      "Epoch 13, Batch 800, Loss: 1.1633875435590744\n",
      "Epoch 13, Batch 900, Loss: 1.1392789316177367\n",
      "Epoch 14, Batch 100, Loss: 1.1461565625667571\n",
      "Epoch 14, Batch 200, Loss: 1.1050719314813613\n",
      "Epoch 14, Batch 300, Loss: 1.1263117396831512\n",
      "Epoch 14, Batch 400, Loss: 1.119210107922554\n",
      "Epoch 14, Batch 500, Loss: 1.1539470303058623\n",
      "Epoch 14, Batch 600, Loss: 1.1198664969205856\n",
      "Epoch 14, Batch 700, Loss: 1.1121936494112015\n",
      "Epoch 14, Batch 800, Loss: 1.1337117564678192\n",
      "Epoch 14, Batch 900, Loss: 1.1773886859416962\n",
      "Epoch 15, Batch 100, Loss: 1.155743471980095\n",
      "Epoch 15, Batch 200, Loss: 1.1246879935264587\n",
      "Epoch 15, Batch 300, Loss: 1.1450140780210496\n",
      "Epoch 15, Batch 400, Loss: 1.1425617921352387\n",
      "Epoch 15, Batch 500, Loss: 1.1388345497846604\n",
      "Epoch 15, Batch 600, Loss: 1.1331981056928635\n",
      "Epoch 15, Batch 700, Loss: 1.1212621796131135\n",
      "Epoch 15, Batch 800, Loss: 1.101457656621933\n",
      "Epoch 15, Batch 900, Loss: 1.1185197466611863\n",
      "Epoch 16, Batch 100, Loss: 1.1316814982891084\n",
      "Epoch 16, Batch 200, Loss: 1.1137485432624816\n",
      "Epoch 16, Batch 300, Loss: 1.1372720503807068\n",
      "Epoch 16, Batch 400, Loss: 1.1192294538021088\n",
      "Epoch 16, Batch 500, Loss: 1.1248272168636322\n",
      "Epoch 16, Batch 600, Loss: 1.1346193087100982\n",
      "Epoch 16, Batch 700, Loss: 1.1463029879331588\n",
      "Epoch 16, Batch 800, Loss: 1.1244911938905715\n",
      "Epoch 16, Batch 900, Loss: 1.1258539360761644\n",
      "Epoch 17, Batch 100, Loss: 1.1170326828956605\n",
      "Epoch 17, Batch 200, Loss: 1.1418919187784196\n",
      "Epoch 17, Batch 300, Loss: 1.1127201175689698\n",
      "Epoch 17, Batch 400, Loss: 1.1497257989645004\n",
      "Epoch 17, Batch 500, Loss: 1.1155594110488891\n",
      "Epoch 17, Batch 600, Loss: 1.1241842168569565\n",
      "Epoch 17, Batch 700, Loss: 1.1317543762922286\n",
      "Epoch 17, Batch 800, Loss: 1.121420688033104\n",
      "Epoch 17, Batch 900, Loss: 1.1338478654623032\n",
      "Epoch 18, Batch 100, Loss: 1.110246468782425\n",
      "Epoch 18, Batch 200, Loss: 1.1337256193161012\n",
      "Epoch 18, Batch 300, Loss: 1.115067913532257\n",
      "Epoch 18, Batch 400, Loss: 1.1419878143072129\n",
      "Epoch 18, Batch 500, Loss: 1.1107682728767394\n",
      "Epoch 18, Batch 600, Loss: 1.1195868587493896\n",
      "Epoch 18, Batch 700, Loss: 1.1234028661251068\n",
      "Epoch 18, Batch 800, Loss: 1.1368410712480546\n",
      "Epoch 18, Batch 900, Loss: 1.1236125272512436\n",
      "Epoch 19, Batch 100, Loss: 1.1487928181886673\n",
      "Epoch 19, Batch 200, Loss: 1.1084242469072343\n",
      "Epoch 19, Batch 300, Loss: 1.1297239595651627\n",
      "Epoch 19, Batch 400, Loss: 1.1257070285081863\n",
      "Epoch 19, Batch 500, Loss: 1.1625627028942107\n",
      "Epoch 19, Batch 600, Loss: 1.15386694252491\n",
      "Epoch 19, Batch 700, Loss: 1.1241220688819886\n",
      "Epoch 19, Batch 800, Loss: 1.1347859287261963\n",
      "Epoch 19, Batch 900, Loss: 1.1095275175571442\n",
      "Epoch 20, Batch 100, Loss: 1.1285084617137908\n",
      "Epoch 20, Batch 200, Loss: 1.1362077564001083\n",
      "Epoch 20, Batch 300, Loss: 1.1233466494083404\n",
      "Epoch 20, Batch 400, Loss: 1.098906711935997\n",
      "Epoch 20, Batch 500, Loss: 1.13485471367836\n",
      "Epoch 20, Batch 600, Loss: 1.1387190270423888\n",
      "Epoch 20, Batch 700, Loss: 1.1424336433410645\n",
      "Epoch 20, Batch 800, Loss: 1.1492635595798493\n",
      "Epoch 20, Batch 900, Loss: 1.1172670441865922\n",
      "Epoch 21, Batch 100, Loss: 1.1028965228796006\n",
      "Epoch 21, Batch 200, Loss: 1.1331985020637512\n",
      "Epoch 21, Batch 300, Loss: 1.1084957987070083\n",
      "Epoch 21, Batch 400, Loss: 1.1378863656520843\n",
      "Epoch 21, Batch 500, Loss: 1.1155725336074829\n",
      "Epoch 21, Batch 600, Loss: 1.1073921209573745\n",
      "Epoch 21, Batch 700, Loss: 1.1184976142644882\n",
      "Epoch 21, Batch 800, Loss: 1.124263543486595\n",
      "Epoch 21, Batch 900, Loss: 1.1387934517860412\n",
      "Epoch 22, Batch 100, Loss: 1.1241486650705337\n",
      "Epoch 22, Batch 200, Loss: 1.1200540679693223\n",
      "Epoch 22, Batch 300, Loss: 1.117737380862236\n",
      "Epoch 22, Batch 400, Loss: 1.1209862130880355\n",
      "Epoch 22, Batch 500, Loss: 1.097312622666359\n",
      "Epoch 22, Batch 600, Loss: 1.1295646232366563\n",
      "Epoch 22, Batch 700, Loss: 1.1536501955986023\n",
      "Epoch 22, Batch 800, Loss: 1.1295824939012526\n",
      "Epoch 22, Batch 900, Loss: 1.144736578464508\n",
      "Epoch 23, Batch 100, Loss: 1.1440758669376374\n",
      "Epoch 23, Batch 200, Loss: 1.1654199171066284\n",
      "Epoch 23, Batch 300, Loss: 1.1275069856643676\n",
      "Epoch 23, Batch 400, Loss: 1.112599236369133\n",
      "Epoch 23, Batch 500, Loss: 1.1179215925931931\n",
      "Epoch 23, Batch 600, Loss: 1.1251953160762787\n",
      "Epoch 23, Batch 700, Loss: 1.135234009027481\n",
      "Epoch 23, Batch 800, Loss: 1.1287781274318696\n",
      "Epoch 23, Batch 900, Loss: 1.122368612885475\n",
      "Epoch 24, Batch 100, Loss: 1.18138197183609\n",
      "Epoch 24, Batch 200, Loss: 1.124862841963768\n",
      "Epoch 24, Batch 300, Loss: 1.146136240363121\n",
      "Epoch 24, Batch 400, Loss: 1.1275564420223236\n",
      "Epoch 24, Batch 500, Loss: 1.1204678773880006\n",
      "Epoch 24, Batch 600, Loss: 1.110840106010437\n",
      "Epoch 24, Batch 700, Loss: 1.1308619636297226\n",
      "Epoch 24, Batch 800, Loss: 1.1508080220222474\n",
      "Epoch 24, Batch 900, Loss: 1.1046688586473465\n",
      "Epoch 25, Batch 100, Loss: 1.1229831725358963\n",
      "Epoch 25, Batch 200, Loss: 1.1157443022727966\n",
      "Epoch 25, Batch 300, Loss: 1.0865818792581559\n",
      "Epoch 25, Batch 400, Loss: 1.117339888215065\n",
      "Epoch 25, Batch 500, Loss: 1.1267835754156112\n",
      "Epoch 25, Batch 600, Loss: 1.1601983088254928\n",
      "Epoch 25, Batch 700, Loss: 1.1445708686113358\n",
      "Epoch 25, Batch 800, Loss: 1.1100786221027374\n",
      "Epoch 25, Batch 900, Loss: 1.1461519426107407\n",
      "Accuracy on test set: 0.4753%\n",
      "Fitting for combination 38\n",
      "784\n",
      "2\n",
      "10\n",
      "[40, 50, 10]\n",
      "True\n",
      "['relu', 'tanh']\n",
      "Adam\n",
      "0.3\n",
      "0\n",
      "CrossEntropyLoss\n",
      "100\n",
      "25\n",
      "0\n",
      "Epoch 1, Batch 100, Loss: 4.080216398239136\n",
      "Epoch 1, Batch 200, Loss: 3.713877747058868\n",
      "Epoch 1, Batch 300, Loss: 3.093638467788696\n",
      "Epoch 1, Batch 400, Loss: 5.217328810691834\n",
      "Epoch 1, Batch 500, Loss: 6.267468376159668\n",
      "Epoch 1, Batch 600, Loss: 3.3961311984062195\n",
      "Epoch 1, Batch 700, Loss: 2.9855523824691774\n",
      "Epoch 1, Batch 800, Loss: 3.6300720286369326\n",
      "Epoch 1, Batch 900, Loss: 5.819001107215882\n",
      "Epoch 2, Batch 100, Loss: 3.219764242172241\n",
      "Epoch 2, Batch 200, Loss: 3.177452600002289\n",
      "Epoch 2, Batch 300, Loss: 3.347241311073303\n",
      "Epoch 2, Batch 400, Loss: 2.993501591682434\n",
      "Epoch 2, Batch 500, Loss: 4.496612339019776\n",
      "Epoch 2, Batch 600, Loss: 4.278289337158203\n",
      "Epoch 2, Batch 700, Loss: 4.369505579471588\n",
      "Epoch 2, Batch 800, Loss: 3.5352830600738527\n",
      "Epoch 2, Batch 900, Loss: 3.561177418231964\n",
      "Epoch 3, Batch 100, Loss: 4.865950820446014\n",
      "Epoch 3, Batch 200, Loss: 5.0738199830055235\n",
      "Epoch 3, Batch 300, Loss: 2.85596684217453\n",
      "Epoch 3, Batch 400, Loss: 3.5796871471405027\n",
      "Epoch 3, Batch 500, Loss: 3.524772336483002\n",
      "Epoch 3, Batch 600, Loss: 3.777734820842743\n",
      "Epoch 3, Batch 700, Loss: 3.240819571018219\n",
      "Epoch 3, Batch 800, Loss: 3.5119638657569885\n",
      "Epoch 3, Batch 900, Loss: 5.201350984573364\n",
      "Epoch 4, Batch 100, Loss: 3.0472233748435973\n",
      "Epoch 4, Batch 200, Loss: 4.519828794002533\n",
      "Epoch 4, Batch 300, Loss: 4.320576529502869\n",
      "Epoch 4, Batch 400, Loss: 3.517365152835846\n",
      "Epoch 4, Batch 500, Loss: 4.146212284564972\n",
      "Epoch 4, Batch 600, Loss: 3.272597870826721\n",
      "Epoch 4, Batch 700, Loss: 3.3365802502632143\n",
      "Epoch 4, Batch 800, Loss: 3.321790888309479\n",
      "Epoch 4, Batch 900, Loss: 4.0326846814155575\n",
      "Epoch 5, Batch 100, Loss: 3.5581813192367555\n",
      "Epoch 5, Batch 200, Loss: 3.042422580718994\n",
      "Epoch 5, Batch 300, Loss: 5.230621991157531\n",
      "Epoch 5, Batch 400, Loss: 5.7567431831359865\n",
      "Epoch 5, Batch 500, Loss: 3.145844223499298\n",
      "Epoch 5, Batch 600, Loss: 3.6542062401771545\n",
      "Epoch 5, Batch 700, Loss: 3.572144477367401\n",
      "Epoch 5, Batch 800, Loss: 3.502008888721466\n",
      "Epoch 5, Batch 900, Loss: 5.643726835250854\n",
      "Epoch 6, Batch 100, Loss: 4.229676220417023\n",
      "Epoch 6, Batch 200, Loss: 5.327805852890014\n",
      "Epoch 6, Batch 300, Loss: 3.3434767866134645\n",
      "Epoch 6, Batch 400, Loss: 3.476623363494873\n",
      "Epoch 6, Batch 500, Loss: 3.514304494857788\n",
      "Epoch 6, Batch 600, Loss: 4.068131320476532\n",
      "Epoch 6, Batch 700, Loss: 3.7153468775749205\n",
      "Epoch 6, Batch 800, Loss: 3.44860294342041\n",
      "Epoch 6, Batch 900, Loss: 3.5148204016685485\n",
      "Epoch 7, Batch 100, Loss: 3.974100344181061\n",
      "Epoch 7, Batch 200, Loss: 3.1898876523971555\n",
      "Epoch 7, Batch 300, Loss: 4.286387002468109\n",
      "Epoch 7, Batch 400, Loss: 4.801686353683472\n",
      "Epoch 7, Batch 500, Loss: 3.6513222789764406\n",
      "Epoch 7, Batch 600, Loss: 4.569286572933197\n",
      "Epoch 7, Batch 700, Loss: 3.222340211868286\n",
      "Epoch 7, Batch 800, Loss: 3.418571043014526\n",
      "Epoch 7, Batch 900, Loss: 3.1853906083106995\n",
      "Epoch 8, Batch 100, Loss: 3.1719911742210387\n",
      "Epoch 8, Batch 200, Loss: 5.566852881908416\n",
      "Epoch 8, Batch 300, Loss: 3.9246028566360476\n",
      "Epoch 8, Batch 400, Loss: 5.504971873760224\n",
      "Epoch 8, Batch 500, Loss: 3.986381196975708\n",
      "Epoch 8, Batch 600, Loss: 3.064636940956116\n",
      "Epoch 8, Batch 700, Loss: 3.7100731182098388\n",
      "Epoch 8, Batch 800, Loss: 3.365995125770569\n",
      "Epoch 8, Batch 900, Loss: 5.528005828857422\n",
      "Epoch 9, Batch 100, Loss: 3.20562607049942\n",
      "Epoch 9, Batch 200, Loss: 5.10653829574585\n",
      "Epoch 9, Batch 300, Loss: 4.793397381305694\n",
      "Epoch 9, Batch 400, Loss: 2.892785415649414\n",
      "Epoch 9, Batch 500, Loss: 4.1504934120178225\n",
      "Epoch 9, Batch 600, Loss: 3.186827142238617\n",
      "Epoch 9, Batch 700, Loss: 4.434122874736786\n",
      "Epoch 9, Batch 800, Loss: 5.88560099363327\n",
      "Epoch 9, Batch 900, Loss: 3.4694783878326416\n",
      "Epoch 10, Batch 100, Loss: 3.0011037945747376\n",
      "Epoch 10, Batch 200, Loss: 4.28326730966568\n",
      "Epoch 10, Batch 300, Loss: 3.874729311466217\n",
      "Epoch 10, Batch 400, Loss: 3.3257067799568176\n",
      "Epoch 10, Batch 500, Loss: 3.0828006410598756\n",
      "Epoch 10, Batch 600, Loss: 4.416897542476654\n",
      "Epoch 10, Batch 700, Loss: 3.5481409883499144\n",
      "Epoch 10, Batch 800, Loss: 4.857740592956543\n",
      "Epoch 10, Batch 900, Loss: 4.777957489490509\n",
      "Epoch 11, Batch 100, Loss: 3.6975181794166563\n",
      "Epoch 11, Batch 200, Loss: 6.456219174861908\n",
      "Epoch 11, Batch 300, Loss: 3.296555120944977\n",
      "Epoch 11, Batch 400, Loss: 3.0557212352752687\n",
      "Epoch 11, Batch 500, Loss: 3.5583242106437685\n",
      "Epoch 11, Batch 600, Loss: 3.304822773933411\n",
      "Epoch 11, Batch 700, Loss: 4.2618741106987\n",
      "Epoch 11, Batch 800, Loss: 3.267683165073395\n",
      "Epoch 11, Batch 900, Loss: 4.281696279048919\n",
      "Epoch 12, Batch 100, Loss: 3.345924837589264\n",
      "Epoch 12, Batch 200, Loss: 3.2680268263816834\n",
      "Epoch 12, Batch 300, Loss: 3.739784917831421\n",
      "Epoch 12, Batch 400, Loss: 4.092036302089691\n",
      "Epoch 12, Batch 500, Loss: 4.3002125263214115\n",
      "Epoch 12, Batch 600, Loss: 4.079666972160339\n",
      "Epoch 12, Batch 700, Loss: 3.6386104893684386\n",
      "Epoch 12, Batch 800, Loss: 3.875969793796539\n",
      "Epoch 12, Batch 900, Loss: 3.151348054409027\n",
      "Epoch 13, Batch 100, Loss: 3.6831861090660096\n",
      "Epoch 13, Batch 200, Loss: 5.296160895824432\n",
      "Epoch 13, Batch 300, Loss: 3.8960111713409424\n",
      "Epoch 13, Batch 400, Loss: 3.122951171398163\n",
      "Epoch 13, Batch 500, Loss: 4.9200294089317325\n",
      "Epoch 13, Batch 600, Loss: 4.489847855567932\n",
      "Epoch 13, Batch 700, Loss: 5.161573753356934\n",
      "Epoch 13, Batch 800, Loss: 5.029677815437317\n",
      "Epoch 13, Batch 900, Loss: 3.2119657945632936\n",
      "Epoch 14, Batch 100, Loss: 3.524223415851593\n",
      "Epoch 14, Batch 200, Loss: 3.115137269496918\n",
      "Epoch 14, Batch 300, Loss: 3.564055252075195\n",
      "Epoch 14, Batch 400, Loss: 3.775005955696106\n",
      "Epoch 14, Batch 500, Loss: 3.694686849117279\n",
      "Epoch 14, Batch 600, Loss: 5.102267234325409\n",
      "Epoch 14, Batch 700, Loss: 4.506029713153839\n",
      "Epoch 14, Batch 800, Loss: 3.028686444759369\n",
      "Epoch 14, Batch 900, Loss: 3.497660074234009\n",
      "Epoch 15, Batch 100, Loss: 3.276116757392883\n",
      "Epoch 15, Batch 200, Loss: 3.8635835337638853\n",
      "Epoch 15, Batch 300, Loss: 4.115201852321625\n",
      "Epoch 15, Batch 400, Loss: 5.191936628818512\n",
      "Epoch 15, Batch 500, Loss: 3.069640965461731\n",
      "Epoch 15, Batch 600, Loss: 4.0625164151191715\n",
      "Epoch 15, Batch 700, Loss: 3.047271134853363\n",
      "Epoch 15, Batch 800, Loss: 3.284634566307068\n",
      "Epoch 15, Batch 900, Loss: 5.104213721752167\n",
      "Epoch 16, Batch 100, Loss: 3.639562559127808\n",
      "Epoch 16, Batch 200, Loss: 6.433335344791413\n",
      "Epoch 16, Batch 300, Loss: 3.9724818110466003\n",
      "Epoch 16, Batch 400, Loss: 2.980920135974884\n",
      "Epoch 16, Batch 500, Loss: 3.263328580856323\n",
      "Epoch 16, Batch 600, Loss: 3.905159375667572\n",
      "Epoch 16, Batch 700, Loss: 3.628920865058899\n",
      "Epoch 16, Batch 800, Loss: 3.825066683292389\n",
      "Epoch 16, Batch 900, Loss: 3.448092496395111\n",
      "Epoch 17, Batch 100, Loss: 5.159728107452392\n",
      "Epoch 17, Batch 200, Loss: 4.3906015992164615\n",
      "Epoch 17, Batch 300, Loss: 5.052575831413269\n",
      "Epoch 17, Batch 400, Loss: 3.500965073108673\n",
      "Epoch 17, Batch 500, Loss: 4.025311365127563\n",
      "Epoch 17, Batch 600, Loss: 3.0010342168807984\n",
      "Epoch 17, Batch 700, Loss: 2.9515847969055176\n",
      "Epoch 17, Batch 800, Loss: 3.1332115483283998\n",
      "Epoch 17, Batch 900, Loss: 4.929241936206818\n",
      "Epoch 18, Batch 100, Loss: 4.0726762390136715\n",
      "Epoch 18, Batch 200, Loss: 3.313769130706787\n",
      "Epoch 18, Batch 300, Loss: 4.356207714080811\n",
      "Epoch 18, Batch 400, Loss: 4.327077906131745\n",
      "Epoch 18, Batch 500, Loss: 3.7558569145202636\n",
      "Epoch 18, Batch 600, Loss: 3.844430785179138\n",
      "Epoch 18, Batch 700, Loss: 3.8185007619857787\n",
      "Epoch 18, Batch 800, Loss: 3.6150329875946046\n",
      "Epoch 18, Batch 900, Loss: 5.121776869297028\n",
      "Epoch 19, Batch 100, Loss: 3.1972533011436464\n",
      "Epoch 19, Batch 200, Loss: 3.101788818836212\n",
      "Epoch 19, Batch 300, Loss: 4.37026437997818\n",
      "Epoch 19, Batch 400, Loss: 5.947366890907287\n",
      "Epoch 19, Batch 500, Loss: 3.944081506729126\n",
      "Epoch 19, Batch 600, Loss: 3.048951811790466\n",
      "Epoch 19, Batch 700, Loss: 4.644345037937164\n",
      "Epoch 19, Batch 800, Loss: 3.7179927897453307\n",
      "Epoch 19, Batch 900, Loss: 3.7076514387130737\n",
      "Epoch 20, Batch 100, Loss: 5.034199523925781\n",
      "Epoch 20, Batch 200, Loss: 4.185100903511048\n",
      "Epoch 20, Batch 300, Loss: 4.0536464095115665\n",
      "Epoch 20, Batch 400, Loss: 3.2810379123687743\n",
      "Epoch 20, Batch 500, Loss: 2.9159529972076417\n",
      "Epoch 20, Batch 600, Loss: 3.9987884902954103\n",
      "Epoch 20, Batch 700, Loss: 5.289166789054871\n",
      "Epoch 20, Batch 800, Loss: 4.808292362689972\n",
      "Epoch 20, Batch 900, Loss: 3.239115698337555\n",
      "Epoch 21, Batch 100, Loss: 3.996059675216675\n",
      "Epoch 21, Batch 200, Loss: 4.850812594890595\n",
      "Epoch 21, Batch 300, Loss: 3.196393585205078\n",
      "Epoch 21, Batch 400, Loss: 3.027165389060974\n",
      "Epoch 21, Batch 500, Loss: 2.912481153011322\n",
      "Epoch 21, Batch 600, Loss: 3.6396855425834658\n",
      "Epoch 21, Batch 700, Loss: 4.650153200626374\n",
      "Epoch 21, Batch 800, Loss: 4.951725633144378\n",
      "Epoch 21, Batch 900, Loss: 3.2284718108177186\n",
      "Epoch 22, Batch 100, Loss: 2.8454361510276795\n",
      "Epoch 22, Batch 200, Loss: 3.319600338935852\n",
      "Epoch 22, Batch 300, Loss: 3.518674099445343\n",
      "Epoch 22, Batch 400, Loss: 6.9901057052612305\n",
      "Epoch 22, Batch 500, Loss: 4.502718877792359\n",
      "Epoch 22, Batch 600, Loss: 3.5821353125572206\n",
      "Epoch 22, Batch 700, Loss: 4.613015851974487\n",
      "Epoch 22, Batch 800, Loss: 3.637426612377167\n",
      "Epoch 22, Batch 900, Loss: 4.367407901287079\n",
      "Epoch 23, Batch 100, Loss: 3.1654179382324217\n",
      "Epoch 23, Batch 200, Loss: 3.9732563614845278\n",
      "Epoch 23, Batch 300, Loss: 2.994356381893158\n",
      "Epoch 23, Batch 400, Loss: 3.10562504529953\n",
      "Epoch 23, Batch 500, Loss: 5.983324692249298\n",
      "Epoch 23, Batch 600, Loss: 4.177370371818543\n",
      "Epoch 23, Batch 700, Loss: 5.196975729465485\n",
      "Epoch 23, Batch 800, Loss: 3.5338481736183165\n",
      "Epoch 23, Batch 900, Loss: 3.5515276074409483\n",
      "Epoch 24, Batch 100, Loss: 4.694875798225403\n",
      "Epoch 24, Batch 200, Loss: 3.572359867095947\n",
      "Epoch 24, Batch 300, Loss: 3.537764804363251\n",
      "Epoch 24, Batch 400, Loss: 3.481483905315399\n",
      "Epoch 24, Batch 500, Loss: 3.1335695934295655\n",
      "Epoch 24, Batch 600, Loss: 2.9146972155570983\n",
      "Epoch 24, Batch 700, Loss: 3.4258614349365235\n",
      "Epoch 24, Batch 800, Loss: 3.7384587478637696\n",
      "Epoch 24, Batch 900, Loss: 5.596410660743714\n",
      "Epoch 25, Batch 100, Loss: 5.4385850977897645\n",
      "Epoch 25, Batch 200, Loss: 2.966342053413391\n",
      "Epoch 25, Batch 300, Loss: 3.316506521701813\n",
      "Epoch 25, Batch 400, Loss: 5.624665260314941\n",
      "Epoch 25, Batch 500, Loss: 3.738812036514282\n",
      "Epoch 25, Batch 600, Loss: 3.1494920444488526\n",
      "Epoch 25, Batch 700, Loss: 4.742254016399383\n",
      "Epoch 25, Batch 800, Loss: 3.227908823490143\n",
      "Epoch 25, Batch 900, Loss: 3.063231327533722\n",
      "Accuracy on test set: 0.1043%\n",
      "Fitting for combination 39\n",
      "784\n",
      "2\n",
      "10\n",
      "[40, 50, 10]\n",
      "False\n",
      "['relu', 'tanh']\n",
      "SGD\n",
      "0.03\n",
      "0.3\n",
      "CrossEntropyLoss\n",
      "100\n",
      "25\n",
      "0\n",
      "Epoch 1, Batch 100, Loss: 1.9726743614673614\n",
      "Epoch 1, Batch 200, Loss: 1.7179534232616425\n",
      "Epoch 1, Batch 300, Loss: 1.6446453738212585\n",
      "Epoch 1, Batch 400, Loss: 1.5963522124290466\n",
      "Epoch 1, Batch 500, Loss: 1.5780503237247467\n",
      "Epoch 1, Batch 600, Loss: 1.5703076910972595\n",
      "Epoch 1, Batch 700, Loss: 1.563152507543564\n",
      "Epoch 1, Batch 800, Loss: 1.554606271982193\n",
      "Epoch 1, Batch 900, Loss: 1.553385353088379\n",
      "Epoch 2, Batch 100, Loss: 1.5481444120407104\n",
      "Epoch 2, Batch 200, Loss: 1.553263808488846\n",
      "Epoch 2, Batch 300, Loss: 1.5406218588352203\n",
      "Epoch 2, Batch 400, Loss: 1.5571591567993164\n",
      "Epoch 2, Batch 500, Loss: 1.561226567029953\n",
      "Epoch 2, Batch 600, Loss: 1.5431591010093688\n",
      "Epoch 2, Batch 700, Loss: 1.5516927790641786\n",
      "Epoch 2, Batch 800, Loss: 1.5521263885498047\n",
      "Epoch 2, Batch 900, Loss: 1.5523363304138185\n",
      "Epoch 3, Batch 100, Loss: 1.552788714170456\n",
      "Epoch 3, Batch 200, Loss: 1.543067306280136\n",
      "Epoch 3, Batch 300, Loss: 1.5516744983196258\n",
      "Epoch 3, Batch 400, Loss: 1.5455619156360627\n",
      "Epoch 3, Batch 500, Loss: 1.54587397813797\n",
      "Epoch 3, Batch 600, Loss: 1.5429093396663667\n",
      "Epoch 3, Batch 700, Loss: 1.5435993218421935\n",
      "Epoch 3, Batch 800, Loss: 1.552113689184189\n",
      "Epoch 3, Batch 900, Loss: 1.5555509257316589\n",
      "Epoch 4, Batch 100, Loss: 1.5424870991706847\n",
      "Epoch 4, Batch 200, Loss: 1.5443864452838898\n",
      "Epoch 4, Batch 300, Loss: 1.5317376780509948\n",
      "Epoch 4, Batch 400, Loss: 1.550904951095581\n",
      "Epoch 4, Batch 500, Loss: 1.5318134725093842\n",
      "Epoch 4, Batch 600, Loss: 1.5505950129032136\n",
      "Epoch 4, Batch 700, Loss: 1.5477734482288361\n",
      "Epoch 4, Batch 800, Loss: 1.5595018005371093\n",
      "Epoch 4, Batch 900, Loss: 1.5498330330848693\n",
      "Epoch 5, Batch 100, Loss: 1.5498034620285035\n",
      "Epoch 5, Batch 200, Loss: 1.5311862015724182\n",
      "Epoch 5, Batch 300, Loss: 1.5446909618377687\n",
      "Epoch 5, Batch 400, Loss: 1.5420955121517181\n",
      "Epoch 5, Batch 500, Loss: 1.5449492275714873\n",
      "Epoch 5, Batch 600, Loss: 1.5546740972995758\n",
      "Epoch 5, Batch 700, Loss: 1.547584159374237\n",
      "Epoch 5, Batch 800, Loss: 1.5500289845466613\n",
      "Epoch 5, Batch 900, Loss: 1.5521711277961732\n",
      "Epoch 6, Batch 100, Loss: 1.5469647347927094\n",
      "Epoch 6, Batch 200, Loss: 1.546524840593338\n",
      "Epoch 6, Batch 300, Loss: 1.5513685894012452\n",
      "Epoch 6, Batch 400, Loss: 1.5515090095996857\n",
      "Epoch 6, Batch 500, Loss: 1.5474271714687347\n",
      "Epoch 6, Batch 600, Loss: 1.5488707602024079\n",
      "Epoch 6, Batch 700, Loss: 1.546120717525482\n",
      "Epoch 6, Batch 800, Loss: 1.5405727624893188\n",
      "Epoch 6, Batch 900, Loss: 1.5322626972198485\n",
      "Epoch 7, Batch 100, Loss: 1.544441304206848\n",
      "Epoch 7, Batch 200, Loss: 1.534021555185318\n",
      "Epoch 7, Batch 300, Loss: 1.5503233921527864\n",
      "Epoch 7, Batch 400, Loss: 1.5557207548618317\n",
      "Epoch 7, Batch 500, Loss: 1.5458492064476013\n",
      "Epoch 7, Batch 600, Loss: 1.5505637073516845\n",
      "Epoch 7, Batch 700, Loss: 1.5482927763462067\n",
      "Epoch 7, Batch 800, Loss: 1.5459244465827942\n",
      "Epoch 7, Batch 900, Loss: 1.5405892717838288\n",
      "Epoch 8, Batch 100, Loss: 1.5413455414772033\n",
      "Epoch 8, Batch 200, Loss: 1.5423453080654144\n",
      "Epoch 8, Batch 300, Loss: 1.5423737478256225\n",
      "Epoch 8, Batch 400, Loss: 1.5527320647239684\n",
      "Epoch 8, Batch 500, Loss: 1.5529340529441833\n",
      "Epoch 8, Batch 600, Loss: 1.5327586436271667\n",
      "Epoch 8, Batch 700, Loss: 1.5380551493167878\n",
      "Epoch 8, Batch 800, Loss: 1.5512985718250274\n",
      "Epoch 8, Batch 900, Loss: 1.5456814873218536\n",
      "Epoch 9, Batch 100, Loss: 1.5452745044231415\n",
      "Epoch 9, Batch 200, Loss: 1.5470565378665924\n",
      "Epoch 9, Batch 300, Loss: 1.5579225528240204\n",
      "Epoch 9, Batch 400, Loss: 1.5488441014289855\n",
      "Epoch 9, Batch 500, Loss: 1.5418984627723693\n",
      "Epoch 9, Batch 600, Loss: 1.5404845333099366\n",
      "Epoch 9, Batch 700, Loss: 1.5495066666603088\n",
      "Epoch 9, Batch 800, Loss: 1.5466419994831084\n",
      "Epoch 9, Batch 900, Loss: 1.5390859043598175\n",
      "Epoch 10, Batch 100, Loss: 1.5501790261268615\n",
      "Epoch 10, Batch 200, Loss: 1.5437203598022462\n",
      "Epoch 10, Batch 300, Loss: 1.5439346766471862\n",
      "Epoch 10, Batch 400, Loss: 1.5441520881652833\n",
      "Epoch 10, Batch 500, Loss: 1.5499079144001007\n",
      "Epoch 10, Batch 600, Loss: 1.5482102680206298\n",
      "Epoch 10, Batch 700, Loss: 1.5426961135864259\n",
      "Epoch 10, Batch 800, Loss: 1.546370894908905\n",
      "Epoch 10, Batch 900, Loss: 1.5360346388816835\n",
      "Epoch 11, Batch 100, Loss: 1.5481508362293244\n",
      "Epoch 11, Batch 200, Loss: 1.5542650878429414\n",
      "Epoch 11, Batch 300, Loss: 1.5438056087493897\n",
      "Epoch 11, Batch 400, Loss: 1.5467996299266815\n",
      "Epoch 11, Batch 500, Loss: 1.5365745639801025\n",
      "Epoch 11, Batch 600, Loss: 1.549197564125061\n",
      "Epoch 11, Batch 700, Loss: 1.5382217037677766\n",
      "Epoch 11, Batch 800, Loss: 1.5418855786323546\n",
      "Epoch 11, Batch 900, Loss: 1.5438888454437256\n",
      "Epoch 12, Batch 100, Loss: 1.5514710605144502\n",
      "Epoch 12, Batch 200, Loss: 1.5389599704742432\n",
      "Epoch 12, Batch 300, Loss: 1.5398770534992219\n",
      "Epoch 12, Batch 400, Loss: 1.5536899220943452\n",
      "Epoch 12, Batch 500, Loss: 1.5394279849529267\n",
      "Epoch 12, Batch 600, Loss: 1.5370595502853392\n",
      "Epoch 12, Batch 700, Loss: 1.551573076248169\n",
      "Epoch 12, Batch 800, Loss: 1.5480608344078064\n",
      "Epoch 12, Batch 900, Loss: 1.5475932121276856\n",
      "Epoch 13, Batch 100, Loss: 1.5428967690467834\n",
      "Epoch 13, Batch 200, Loss: 1.5384335112571716\n",
      "Epoch 13, Batch 300, Loss: 1.5495025849342345\n",
      "Epoch 13, Batch 400, Loss: 1.5353052484989167\n",
      "Epoch 13, Batch 500, Loss: 1.5541857314109802\n",
      "Epoch 13, Batch 600, Loss: 1.5550022828578949\n",
      "Epoch 13, Batch 700, Loss: 1.543778840303421\n",
      "Epoch 13, Batch 800, Loss: 1.5502103745937348\n",
      "Epoch 13, Batch 900, Loss: 1.5366435551643371\n",
      "Epoch 14, Batch 100, Loss: 1.5492459654808044\n",
      "Epoch 14, Batch 200, Loss: 1.5528826522827148\n",
      "Epoch 14, Batch 300, Loss: 1.5435457980632783\n",
      "Epoch 14, Batch 400, Loss: 1.5428351736068726\n",
      "Epoch 14, Batch 500, Loss: 1.546991138458252\n",
      "Epoch 14, Batch 600, Loss: 1.5474874043464661\n",
      "Epoch 14, Batch 700, Loss: 1.5330544936656951\n",
      "Epoch 14, Batch 800, Loss: 1.5384109354019164\n",
      "Epoch 14, Batch 900, Loss: 1.546103639602661\n",
      "Epoch 15, Batch 100, Loss: 1.552744426727295\n",
      "Epoch 15, Batch 200, Loss: 1.5457998275756837\n",
      "Epoch 15, Batch 300, Loss: 1.5457692980766295\n",
      "Epoch 15, Batch 400, Loss: 1.5422891092300415\n",
      "Epoch 15, Batch 500, Loss: 1.5447814428806306\n",
      "Epoch 15, Batch 600, Loss: 1.5436057913303376\n",
      "Epoch 15, Batch 700, Loss: 1.5367145562171936\n",
      "Epoch 15, Batch 800, Loss: 1.5509952569007874\n",
      "Epoch 15, Batch 900, Loss: 1.5514767575263977\n",
      "Epoch 16, Batch 100, Loss: 1.5482190537452698\n",
      "Epoch 16, Batch 200, Loss: 1.5509262073040009\n",
      "Epoch 16, Batch 300, Loss: 1.5451251327991486\n",
      "Epoch 16, Batch 400, Loss: 1.5518681192398072\n",
      "Epoch 16, Batch 500, Loss: 1.5446448814868927\n",
      "Epoch 16, Batch 600, Loss: 1.546471836566925\n",
      "Epoch 16, Batch 700, Loss: 1.542168390750885\n",
      "Epoch 16, Batch 800, Loss: 1.5466134071350097\n",
      "Epoch 16, Batch 900, Loss: 1.5430370938777924\n",
      "Epoch 17, Batch 100, Loss: 1.5418882620334626\n",
      "Epoch 17, Batch 200, Loss: 1.5337980854511262\n",
      "Epoch 17, Batch 300, Loss: 1.5506030523777008\n",
      "Epoch 17, Batch 400, Loss: 1.5432454311847688\n",
      "Epoch 17, Batch 500, Loss: 1.548890643119812\n",
      "Epoch 17, Batch 600, Loss: 1.544822369813919\n",
      "Epoch 17, Batch 700, Loss: 1.546094343662262\n",
      "Epoch 17, Batch 800, Loss: 1.5469970750808715\n",
      "Epoch 17, Batch 900, Loss: 1.546354559659958\n",
      "Epoch 18, Batch 100, Loss: 1.5488512694835663\n",
      "Epoch 18, Batch 200, Loss: 1.5502785396575929\n",
      "Epoch 18, Batch 300, Loss: 1.5535134208202361\n",
      "Epoch 18, Batch 400, Loss: 1.5458830082416535\n",
      "Epoch 18, Batch 500, Loss: 1.5423868227005004\n",
      "Epoch 18, Batch 600, Loss: 1.5456007790565491\n",
      "Epoch 18, Batch 700, Loss: 1.5409649407863617\n",
      "Epoch 18, Batch 800, Loss: 1.5363044333457947\n",
      "Epoch 18, Batch 900, Loss: 1.5524089574813842\n",
      "Epoch 19, Batch 100, Loss: 1.5415320205688476\n",
      "Epoch 19, Batch 200, Loss: 1.5506196999549866\n",
      "Epoch 19, Batch 300, Loss: 1.5435211312770845\n",
      "Epoch 19, Batch 400, Loss: 1.549525250196457\n",
      "Epoch 19, Batch 500, Loss: 1.5373110628128053\n",
      "Epoch 19, Batch 600, Loss: 1.5482162237167358\n",
      "Epoch 19, Batch 700, Loss: 1.5504830026626586\n",
      "Epoch 19, Batch 800, Loss: 1.5439937257766723\n",
      "Epoch 19, Batch 900, Loss: 1.5456241464614868\n",
      "Epoch 20, Batch 100, Loss: 1.5435880720615387\n",
      "Epoch 20, Batch 200, Loss: 1.5392419719696044\n",
      "Epoch 20, Batch 300, Loss: 1.5447502291202546\n",
      "Epoch 20, Batch 400, Loss: 1.5434215378761291\n",
      "Epoch 20, Batch 500, Loss: 1.5541555905342102\n",
      "Epoch 20, Batch 600, Loss: 1.5400928008556365\n",
      "Epoch 20, Batch 700, Loss: 1.5484594988822937\n",
      "Epoch 20, Batch 800, Loss: 1.5425585663318635\n",
      "Epoch 20, Batch 900, Loss: 1.5501950871944428\n",
      "Epoch 21, Batch 100, Loss: 1.5441479110717773\n",
      "Epoch 21, Batch 200, Loss: 1.5491751825809479\n",
      "Epoch 21, Batch 300, Loss: 1.5391768777370454\n",
      "Epoch 21, Batch 400, Loss: 1.5386094677448272\n",
      "Epoch 21, Batch 500, Loss: 1.556590392589569\n",
      "Epoch 21, Batch 600, Loss: 1.5520231962203979\n",
      "Epoch 21, Batch 700, Loss: 1.5414239621162416\n",
      "Epoch 21, Batch 800, Loss: 1.5489161324501037\n",
      "Epoch 21, Batch 900, Loss: 1.5365300643444062\n",
      "Epoch 22, Batch 100, Loss: 1.5482859659194945\n",
      "Epoch 22, Batch 200, Loss: 1.5426149189472198\n",
      "Epoch 22, Batch 300, Loss: 1.5401714527606964\n",
      "Epoch 22, Batch 400, Loss: 1.55064523935318\n",
      "Epoch 22, Batch 500, Loss: 1.546246954202652\n",
      "Epoch 22, Batch 600, Loss: 1.543001434803009\n",
      "Epoch 22, Batch 700, Loss: 1.5486696541309357\n",
      "Epoch 22, Batch 800, Loss: 1.5524285221099854\n",
      "Epoch 22, Batch 900, Loss: 1.5431527864933015\n",
      "Epoch 23, Batch 100, Loss: 1.551280564069748\n",
      "Epoch 23, Batch 200, Loss: 1.5424342441558838\n",
      "Epoch 23, Batch 300, Loss: 1.5516530323028563\n",
      "Epoch 23, Batch 400, Loss: 1.5327238595485688\n",
      "Epoch 23, Batch 500, Loss: 1.541385155916214\n",
      "Epoch 23, Batch 600, Loss: 1.5537019848823548\n",
      "Epoch 23, Batch 700, Loss: 1.5528306555747986\n",
      "Epoch 23, Batch 800, Loss: 1.5409265017509461\n",
      "Epoch 23, Batch 900, Loss: 1.5388809776306152\n",
      "Epoch 24, Batch 100, Loss: 1.548135175704956\n",
      "Epoch 24, Batch 200, Loss: 1.5363920748233795\n",
      "Epoch 24, Batch 300, Loss: 1.5467129373550415\n",
      "Epoch 24, Batch 400, Loss: 1.5487068223953246\n",
      "Epoch 24, Batch 500, Loss: 1.5463337886333466\n",
      "Epoch 24, Batch 600, Loss: 1.5523252630233764\n",
      "Epoch 24, Batch 700, Loss: 1.5333750939369202\n",
      "Epoch 24, Batch 800, Loss: 1.5443133926391601\n",
      "Epoch 24, Batch 900, Loss: 1.5538037943840026\n",
      "Epoch 25, Batch 100, Loss: 1.556122967004776\n",
      "Epoch 25, Batch 200, Loss: 1.5492382740974426\n",
      "Epoch 25, Batch 300, Loss: 1.5331089198589325\n",
      "Epoch 25, Batch 400, Loss: 1.5450737535953523\n",
      "Epoch 25, Batch 500, Loss: 1.5450837123394012\n",
      "Epoch 25, Batch 600, Loss: 1.544088237285614\n",
      "Epoch 25, Batch 700, Loss: 1.5385256350040435\n",
      "Epoch 25, Batch 800, Loss: 1.5489062440395356\n",
      "Epoch 25, Batch 900, Loss: 1.5453171467781066\n",
      "Accuracy on test set: 0.4113%\n",
      "Fitting for combination 40\n",
      "784\n",
      "3\n",
      "10\n",
      "[40, 10, 10, 10]\n",
      "False\n",
      "['relu', 'sigmoid', 'sigmoid']\n",
      "Adam\n",
      "0.3\n",
      "0.01\n",
      "CrossEntropyLoss\n",
      "50\n",
      "25\n",
      "0\n",
      "Epoch 1, Batch 50, Loss: 1.1863630771636964\n",
      "Epoch 1, Batch 100, Loss: 1.1523220181465148\n",
      "Epoch 1, Batch 150, Loss: 1.1644569778442382\n",
      "Epoch 1, Batch 200, Loss: 1.170405833721161\n",
      "Epoch 1, Batch 250, Loss: 1.1665649962425233\n",
      "Epoch 1, Batch 300, Loss: 1.1725013089179992\n",
      "Epoch 1, Batch 350, Loss: 1.1630716037750244\n",
      "Epoch 1, Batch 400, Loss: 1.1720271587371827\n",
      "Epoch 1, Batch 450, Loss: 1.165920970439911\n",
      "Epoch 1, Batch 500, Loss: 1.167004280090332\n",
      "Epoch 1, Batch 550, Loss: 1.1663341164588927\n",
      "Epoch 1, Batch 600, Loss: 1.1708839058876037\n",
      "Epoch 1, Batch 650, Loss: 1.1678720498085022\n",
      "Epoch 1, Batch 700, Loss: 1.1676037883758545\n",
      "Epoch 1, Batch 750, Loss: 1.16816330909729\n",
      "Epoch 1, Batch 800, Loss: 1.1672329354286193\n",
      "Epoch 1, Batch 850, Loss: 1.172461929321289\n",
      "Epoch 1, Batch 900, Loss: 1.172736213207245\n",
      "Epoch 2, Batch 50, Loss: 1.1679466438293458\n",
      "Epoch 2, Batch 100, Loss: 1.1676133060455323\n",
      "Epoch 2, Batch 150, Loss: 1.165368413925171\n",
      "Epoch 2, Batch 200, Loss: 1.1689520573616028\n",
      "Epoch 2, Batch 250, Loss: 1.1729264187812805\n",
      "Epoch 2, Batch 300, Loss: 1.1697888493537902\n",
      "Epoch 2, Batch 350, Loss: 1.1763264322280884\n",
      "Epoch 2, Batch 400, Loss: 1.1713813710212708\n",
      "Epoch 2, Batch 450, Loss: 1.1726778435707093\n",
      "Epoch 2, Batch 500, Loss: 1.171366398334503\n",
      "Epoch 2, Batch 550, Loss: 1.1694067931175232\n",
      "Epoch 2, Batch 600, Loss: 1.1716892337799072\n",
      "Epoch 2, Batch 650, Loss: 1.1661756920814514\n",
      "Epoch 2, Batch 700, Loss: 1.1697355222702026\n",
      "Epoch 2, Batch 750, Loss: 1.16761789560318\n",
      "Epoch 2, Batch 800, Loss: 1.1706686186790467\n",
      "Epoch 2, Batch 850, Loss: 1.1673797035217286\n",
      "Epoch 2, Batch 900, Loss: 1.1720160746574402\n",
      "Epoch 3, Batch 50, Loss: 1.166673219203949\n",
      "Epoch 3, Batch 100, Loss: 1.1673895287513734\n",
      "Epoch 3, Batch 150, Loss: 1.1684740591049194\n",
      "Epoch 3, Batch 200, Loss: 1.1740638780593873\n",
      "Epoch 3, Batch 250, Loss: 1.1644536542892456\n",
      "Epoch 3, Batch 300, Loss: 1.1675274443626404\n",
      "Epoch 3, Batch 350, Loss: 1.1683384323120116\n",
      "Epoch 3, Batch 400, Loss: 1.1669362473487854\n",
      "Epoch 3, Batch 450, Loss: 1.169259831905365\n",
      "Epoch 3, Batch 500, Loss: 1.165438606739044\n",
      "Epoch 3, Batch 550, Loss: 1.1655446481704712\n",
      "Epoch 3, Batch 600, Loss: 1.1653960680961608\n",
      "Epoch 3, Batch 650, Loss: 1.1678779530525207\n",
      "Epoch 3, Batch 700, Loss: 1.169702432155609\n",
      "Epoch 3, Batch 750, Loss: 1.1667048025131226\n",
      "Epoch 3, Batch 800, Loss: 1.16795729637146\n",
      "Epoch 3, Batch 850, Loss: 1.1723675918579102\n",
      "Epoch 3, Batch 900, Loss: 1.1664693856239319\n",
      "Epoch 4, Batch 50, Loss: 1.1632839131355286\n",
      "Epoch 4, Batch 100, Loss: 1.1700423288345336\n",
      "Epoch 4, Batch 150, Loss: 1.1694670629501343\n",
      "Epoch 4, Batch 200, Loss: 1.170559482574463\n",
      "Epoch 4, Batch 250, Loss: 1.1698950123786926\n",
      "Epoch 4, Batch 300, Loss: 1.1677562189102173\n",
      "Epoch 4, Batch 350, Loss: 1.1706261467933654\n",
      "Epoch 4, Batch 400, Loss: 1.1695768642425537\n",
      "Epoch 4, Batch 450, Loss: 1.1758517265319823\n",
      "Epoch 4, Batch 500, Loss: 1.171142520904541\n",
      "Epoch 4, Batch 550, Loss: 1.168363757133484\n",
      "Epoch 4, Batch 600, Loss: 1.1689892959594728\n",
      "Epoch 4, Batch 650, Loss: 1.1694848227500916\n",
      "Epoch 4, Batch 700, Loss: 1.1683046436309814\n",
      "Epoch 4, Batch 750, Loss: 1.1745757842063904\n",
      "Epoch 4, Batch 800, Loss: 1.1670512294769286\n",
      "Epoch 4, Batch 850, Loss: 1.172001643180847\n",
      "Epoch 4, Batch 900, Loss: 1.1703347277641296\n",
      "Epoch 5, Batch 50, Loss: 1.1681660175323487\n",
      "Epoch 5, Batch 100, Loss: 1.1726410508155822\n",
      "Epoch 5, Batch 150, Loss: 1.168734974861145\n",
      "Epoch 5, Batch 200, Loss: 1.1753694367408754\n",
      "Epoch 5, Batch 250, Loss: 1.1681821990013121\n",
      "Epoch 5, Batch 300, Loss: 1.1705300998687744\n",
      "Epoch 5, Batch 350, Loss: 1.1666531944274903\n",
      "Epoch 5, Batch 400, Loss: 1.1681529760360718\n",
      "Epoch 5, Batch 450, Loss: 1.169765167236328\n",
      "Epoch 5, Batch 500, Loss: 1.1700126647949218\n",
      "Epoch 5, Batch 550, Loss: 1.171127758026123\n",
      "Epoch 5, Batch 600, Loss: 1.1697177481651306\n",
      "Epoch 5, Batch 650, Loss: 1.173328411579132\n",
      "Epoch 5, Batch 700, Loss: 1.1710516381263734\n",
      "Epoch 5, Batch 750, Loss: 1.1680305409431457\n",
      "Epoch 5, Batch 800, Loss: 1.173037621974945\n",
      "Epoch 5, Batch 850, Loss: 1.1677332186698914\n",
      "Epoch 5, Batch 900, Loss: 1.1711418056488037\n",
      "Epoch 6, Batch 50, Loss: 1.1706822443008422\n",
      "Epoch 6, Batch 100, Loss: 1.168509876728058\n",
      "Epoch 6, Batch 150, Loss: 1.1724668836593628\n",
      "Epoch 6, Batch 200, Loss: 1.1660766839981078\n",
      "Epoch 6, Batch 250, Loss: 1.1668330430984497\n",
      "Epoch 6, Batch 300, Loss: 1.172405915260315\n",
      "Epoch 6, Batch 350, Loss: 1.1769694232940673\n",
      "Epoch 6, Batch 400, Loss: 1.1713408613204956\n",
      "Epoch 6, Batch 450, Loss: 1.1685157060623168\n",
      "Epoch 6, Batch 500, Loss: 1.1644451642036437\n",
      "Epoch 6, Batch 550, Loss: 1.1671183156967162\n",
      "Epoch 6, Batch 600, Loss: 1.1707958388328552\n",
      "Epoch 6, Batch 650, Loss: 1.1703200459480285\n",
      "Epoch 6, Batch 700, Loss: 1.1754538345336913\n",
      "Epoch 6, Batch 750, Loss: 1.1699775338172913\n",
      "Epoch 6, Batch 800, Loss: 1.179672269821167\n",
      "Epoch 6, Batch 850, Loss: 1.1713417768478394\n",
      "Epoch 6, Batch 900, Loss: 1.1668809103965758\n",
      "Epoch 7, Batch 50, Loss: 1.168234302997589\n",
      "Epoch 7, Batch 100, Loss: 1.1757026958465575\n",
      "Epoch 7, Batch 150, Loss: 1.169300844669342\n",
      "Epoch 7, Batch 200, Loss: 1.1679736137390138\n",
      "Epoch 7, Batch 250, Loss: 1.1713128447532655\n",
      "Epoch 7, Batch 300, Loss: 1.1663772439956666\n",
      "Epoch 7, Batch 350, Loss: 1.1707304000854493\n",
      "Epoch 7, Batch 400, Loss: 1.1704345321655274\n",
      "Epoch 7, Batch 450, Loss: 1.169764437675476\n",
      "Epoch 7, Batch 500, Loss: 1.1713021206855774\n",
      "Epoch 7, Batch 550, Loss: 1.1671415376663208\n",
      "Epoch 7, Batch 600, Loss: 1.1703247332572937\n",
      "Epoch 7, Batch 650, Loss: 1.17842027425766\n",
      "Epoch 7, Batch 700, Loss: 1.1715797543525697\n",
      "Epoch 7, Batch 750, Loss: 1.1698795175552368\n",
      "Epoch 7, Batch 800, Loss: 1.1682773208618165\n",
      "Epoch 7, Batch 850, Loss: 1.1627899193763733\n",
      "Epoch 7, Batch 900, Loss: 1.1655674481391907\n",
      "Epoch 8, Batch 50, Loss: 1.1701536202430725\n",
      "Epoch 8, Batch 100, Loss: 1.1641988015174867\n",
      "Epoch 8, Batch 150, Loss: 1.1687261486053466\n",
      "Epoch 8, Batch 200, Loss: 1.1774117422103882\n",
      "Epoch 8, Batch 250, Loss: 1.1685326647758485\n",
      "Epoch 8, Batch 300, Loss: 1.1732397842407227\n",
      "Epoch 8, Batch 350, Loss: 1.17460791349411\n",
      "Epoch 8, Batch 400, Loss: 1.1688417720794677\n",
      "Epoch 8, Batch 450, Loss: 1.1722509002685546\n",
      "Epoch 8, Batch 500, Loss: 1.1739362931251527\n",
      "Epoch 8, Batch 550, Loss: 1.1678077125549315\n",
      "Epoch 8, Batch 600, Loss: 1.1738213729858398\n",
      "Epoch 8, Batch 650, Loss: 1.1754449224472046\n",
      "Epoch 8, Batch 700, Loss: 1.16722136259079\n",
      "Epoch 8, Batch 750, Loss: 1.1669567036628723\n",
      "Epoch 8, Batch 800, Loss: 1.1659592247009278\n",
      "Epoch 8, Batch 850, Loss: 1.1719968557357787\n",
      "Epoch 8, Batch 900, Loss: 1.1727702856063842\n",
      "Epoch 9, Batch 50, Loss: 1.1700570821762084\n",
      "Epoch 9, Batch 100, Loss: 1.169660668373108\n",
      "Epoch 9, Batch 150, Loss: 1.1699296283721923\n",
      "Epoch 9, Batch 200, Loss: 1.1716588973999023\n",
      "Epoch 9, Batch 250, Loss: 1.167543008327484\n",
      "Epoch 9, Batch 300, Loss: 1.1684298515319824\n",
      "Epoch 9, Batch 350, Loss: 1.1657736921310424\n",
      "Epoch 9, Batch 400, Loss: 1.1672351479530334\n",
      "Epoch 9, Batch 450, Loss: 1.1721468138694764\n",
      "Epoch 9, Batch 500, Loss: 1.1700753164291382\n",
      "Epoch 9, Batch 550, Loss: 1.1729584789276124\n",
      "Epoch 9, Batch 600, Loss: 1.1733545923233033\n",
      "Epoch 9, Batch 650, Loss: 1.1689327335357667\n",
      "Epoch 9, Batch 700, Loss: 1.1683063411712646\n",
      "Epoch 9, Batch 750, Loss: 1.1670798587799072\n",
      "Epoch 9, Batch 800, Loss: 1.1688275742530823\n",
      "Epoch 9, Batch 850, Loss: 1.171041796207428\n",
      "Epoch 9, Batch 900, Loss: 1.1668097853660584\n",
      "Epoch 10, Batch 50, Loss: 1.1703669714927674\n",
      "Epoch 10, Batch 100, Loss: 1.1690249395370484\n",
      "Epoch 10, Batch 150, Loss: 1.171444845199585\n",
      "Epoch 10, Batch 200, Loss: 1.1721561241149903\n",
      "Epoch 10, Batch 250, Loss: 1.1688516426086426\n",
      "Epoch 10, Batch 300, Loss: 1.1718071961402894\n",
      "Epoch 10, Batch 350, Loss: 1.1711539268493651\n",
      "Epoch 10, Batch 400, Loss: 1.1692981386184693\n",
      "Epoch 10, Batch 450, Loss: 1.1699426651000977\n",
      "Epoch 10, Batch 500, Loss: 1.167419469356537\n",
      "Epoch 10, Batch 550, Loss: 1.17250146150589\n",
      "Epoch 10, Batch 600, Loss: 1.1697016549110413\n",
      "Epoch 10, Batch 650, Loss: 1.1713741540908813\n",
      "Epoch 10, Batch 700, Loss: 1.1717219519615174\n",
      "Epoch 10, Batch 750, Loss: 1.1702616572380067\n",
      "Epoch 10, Batch 800, Loss: 1.1712989449501037\n",
      "Epoch 10, Batch 850, Loss: 1.1673065853118896\n",
      "Epoch 10, Batch 900, Loss: 1.1690189576148986\n",
      "Epoch 11, Batch 50, Loss: 1.1742376804351806\n",
      "Epoch 11, Batch 100, Loss: 1.1716394925117493\n",
      "Epoch 11, Batch 150, Loss: 1.1697917246818543\n",
      "Epoch 11, Batch 200, Loss: 1.1687445950508117\n",
      "Epoch 11, Batch 250, Loss: 1.1645605635643006\n",
      "Epoch 11, Batch 300, Loss: 1.1662586426734924\n",
      "Epoch 11, Batch 350, Loss: 1.1762493109703065\n",
      "Epoch 11, Batch 400, Loss: 1.173628146648407\n",
      "Epoch 11, Batch 450, Loss: 1.1739743876457214\n",
      "Epoch 11, Batch 500, Loss: 1.1681219506263734\n",
      "Epoch 11, Batch 550, Loss: 1.165216670036316\n",
      "Epoch 11, Batch 600, Loss: 1.1681746196746827\n",
      "Epoch 11, Batch 650, Loss: 1.1684449052810668\n",
      "Epoch 11, Batch 700, Loss: 1.1692542028427124\n",
      "Epoch 11, Batch 750, Loss: 1.1708209109306336\n",
      "Epoch 11, Batch 800, Loss: 1.1681767702102661\n",
      "Epoch 11, Batch 850, Loss: 1.164525294303894\n",
      "Epoch 11, Batch 900, Loss: 1.1644101214408875\n",
      "Epoch 12, Batch 50, Loss: 1.1770953154563903\n",
      "Epoch 12, Batch 100, Loss: 1.164874813556671\n",
      "Epoch 12, Batch 150, Loss: 1.1686667060852052\n",
      "Epoch 12, Batch 200, Loss: 1.168370931148529\n",
      "Epoch 12, Batch 250, Loss: 1.1667704725265502\n",
      "Epoch 12, Batch 300, Loss: 1.1689590525627136\n",
      "Epoch 12, Batch 350, Loss: 1.1718982791900634\n",
      "Epoch 12, Batch 400, Loss: 1.1716939115524292\n",
      "Epoch 12, Batch 450, Loss: 1.1721805119514466\n",
      "Epoch 12, Batch 500, Loss: 1.1685872626304628\n",
      "Epoch 12, Batch 550, Loss: 1.1705023002624513\n",
      "Epoch 12, Batch 600, Loss: 1.168979184627533\n",
      "Epoch 12, Batch 650, Loss: 1.170336833000183\n",
      "Epoch 12, Batch 700, Loss: 1.1768563079833985\n",
      "Epoch 12, Batch 750, Loss: 1.172415430545807\n",
      "Epoch 12, Batch 800, Loss: 1.166864047050476\n",
      "Epoch 12, Batch 850, Loss: 1.1680254054069519\n",
      "Epoch 12, Batch 900, Loss: 1.1705377197265625\n",
      "Epoch 13, Batch 50, Loss: 1.1697805666923522\n",
      "Epoch 13, Batch 100, Loss: 1.1674022555351258\n",
      "Epoch 13, Batch 150, Loss: 1.1693929600715638\n",
      "Epoch 13, Batch 200, Loss: 1.170122275352478\n",
      "Epoch 13, Batch 250, Loss: 1.1668075156211852\n",
      "Epoch 13, Batch 300, Loss: 1.1718163466453553\n",
      "Epoch 13, Batch 350, Loss: 1.1741856503486634\n",
      "Epoch 13, Batch 400, Loss: 1.171865894794464\n",
      "Epoch 13, Batch 450, Loss: 1.167402024269104\n",
      "Epoch 13, Batch 500, Loss: 1.1687575840950013\n",
      "Epoch 13, Batch 550, Loss: 1.1721503639221191\n",
      "Epoch 13, Batch 600, Loss: 1.1721884775161744\n",
      "Epoch 13, Batch 650, Loss: 1.1663444232940674\n",
      "Epoch 13, Batch 700, Loss: 1.1712304520606995\n",
      "Epoch 13, Batch 750, Loss: 1.1745217442512512\n",
      "Epoch 13, Batch 800, Loss: 1.1700200057029724\n",
      "Epoch 13, Batch 850, Loss: 1.1693172287940978\n",
      "Epoch 13, Batch 900, Loss: 1.1697139573097228\n",
      "Epoch 14, Batch 50, Loss: 1.1717370462417602\n",
      "Epoch 14, Batch 100, Loss: 1.16739235162735\n",
      "Epoch 14, Batch 150, Loss: 1.1703740882873535\n",
      "Epoch 14, Batch 200, Loss: 1.1668537187576293\n",
      "Epoch 14, Batch 250, Loss: 1.1696201682090759\n",
      "Epoch 14, Batch 300, Loss: 1.1661620616912842\n",
      "Epoch 14, Batch 350, Loss: 1.1631103897094726\n",
      "Epoch 14, Batch 400, Loss: 1.1674756145477294\n",
      "Epoch 14, Batch 450, Loss: 1.1698261380195618\n",
      "Epoch 14, Batch 500, Loss: 1.1684193754196166\n",
      "Epoch 14, Batch 550, Loss: 1.1732561635971068\n",
      "Epoch 14, Batch 600, Loss: 1.1684948134422302\n",
      "Epoch 14, Batch 650, Loss: 1.166948721408844\n",
      "Epoch 14, Batch 700, Loss: 1.171521668434143\n",
      "Epoch 14, Batch 750, Loss: 1.1714565229415894\n",
      "Epoch 14, Batch 800, Loss: 1.1649964690208434\n",
      "Epoch 14, Batch 850, Loss: 1.1692750096321105\n",
      "Epoch 14, Batch 900, Loss: 1.1757930707931519\n",
      "Epoch 15, Batch 50, Loss: 1.1693613934516907\n",
      "Epoch 15, Batch 100, Loss: 1.1730355167388915\n",
      "Epoch 15, Batch 150, Loss: 1.1675772166252136\n",
      "Epoch 15, Batch 200, Loss: 1.1669289445877076\n",
      "Epoch 15, Batch 250, Loss: 1.1698556303977967\n",
      "Epoch 15, Batch 300, Loss: 1.1698080849647523\n",
      "Epoch 15, Batch 350, Loss: 1.1693495106697083\n",
      "Epoch 15, Batch 400, Loss: 1.1673042225837706\n",
      "Epoch 15, Batch 450, Loss: 1.1652551364898682\n",
      "Epoch 15, Batch 500, Loss: 1.169095802307129\n",
      "Epoch 15, Batch 550, Loss: 1.1708130502700806\n",
      "Epoch 15, Batch 600, Loss: 1.168326072692871\n",
      "Epoch 15, Batch 650, Loss: 1.1660730504989625\n",
      "Epoch 15, Batch 700, Loss: 1.1688224029541017\n",
      "Epoch 15, Batch 750, Loss: 1.1722759747505187\n",
      "Epoch 15, Batch 800, Loss: 1.1747531652450562\n",
      "Epoch 15, Batch 850, Loss: 1.167352819442749\n",
      "Epoch 15, Batch 900, Loss: 1.1707922625541687\n",
      "Epoch 16, Batch 50, Loss: 1.1649887800216674\n",
      "Epoch 16, Batch 100, Loss: 1.169708490371704\n",
      "Epoch 16, Batch 150, Loss: 1.1720284628868103\n",
      "Epoch 16, Batch 200, Loss: 1.1703042078018189\n",
      "Epoch 16, Batch 250, Loss: 1.16699791431427\n",
      "Epoch 16, Batch 300, Loss: 1.1658229899406434\n",
      "Epoch 16, Batch 350, Loss: 1.1721679306030273\n",
      "Epoch 16, Batch 400, Loss: 1.1700142884254456\n",
      "Epoch 16, Batch 450, Loss: 1.1695709896087647\n",
      "Epoch 16, Batch 500, Loss: 1.1712403345108031\n",
      "Epoch 16, Batch 550, Loss: 1.1704493165016174\n",
      "Epoch 16, Batch 600, Loss: 1.1713759016990661\n",
      "Epoch 16, Batch 650, Loss: 1.1712792253494262\n",
      "Epoch 16, Batch 700, Loss: 1.1693004584312439\n",
      "Epoch 16, Batch 750, Loss: 1.1696190285682677\n",
      "Epoch 16, Batch 800, Loss: 1.1695661997795106\n",
      "Epoch 16, Batch 850, Loss: 1.1694534873962403\n",
      "Epoch 16, Batch 900, Loss: 1.1639432215690613\n",
      "Epoch 17, Batch 50, Loss: 1.1687007069587707\n",
      "Epoch 17, Batch 100, Loss: 1.166121962070465\n",
      "Epoch 17, Batch 150, Loss: 1.1722558951377868\n",
      "Epoch 17, Batch 200, Loss: 1.170134139060974\n",
      "Epoch 17, Batch 250, Loss: 1.1710833644866943\n",
      "Epoch 17, Batch 300, Loss: 1.1715772438049317\n",
      "Epoch 17, Batch 350, Loss: 1.1730612587928773\n",
      "Epoch 17, Batch 400, Loss: 1.1682598352432252\n",
      "Epoch 17, Batch 450, Loss: 1.1682858347892762\n",
      "Epoch 17, Batch 500, Loss: 1.1720660400390626\n",
      "Epoch 17, Batch 550, Loss: 1.1683428931236266\n",
      "Epoch 17, Batch 600, Loss: 1.1704291200637817\n",
      "Epoch 17, Batch 650, Loss: 1.1708279514312745\n",
      "Epoch 17, Batch 700, Loss: 1.1684613251686096\n",
      "Epoch 17, Batch 750, Loss: 1.168196828365326\n",
      "Epoch 17, Batch 800, Loss: 1.1696352696418761\n",
      "Epoch 17, Batch 850, Loss: 1.1708705735206604\n",
      "Epoch 17, Batch 900, Loss: 1.1709463095664978\n",
      "Epoch 18, Batch 50, Loss: 1.1706080412864686\n",
      "Epoch 18, Batch 100, Loss: 1.171752314567566\n",
      "Epoch 18, Batch 150, Loss: 1.166899130344391\n",
      "Epoch 18, Batch 200, Loss: 1.1725771999359131\n",
      "Epoch 18, Batch 250, Loss: 1.1737225222587586\n",
      "Epoch 18, Batch 300, Loss: 1.1721234726905823\n",
      "Epoch 18, Batch 350, Loss: 1.1679902791976928\n",
      "Epoch 18, Batch 400, Loss: 1.1703018212318421\n",
      "Epoch 18, Batch 450, Loss: 1.1670384335517883\n",
      "Epoch 18, Batch 500, Loss: 1.1721558833122254\n",
      "Epoch 18, Batch 550, Loss: 1.1671630692481996\n",
      "Epoch 18, Batch 600, Loss: 1.1679837131500244\n",
      "Epoch 18, Batch 650, Loss: 1.1691788578033446\n",
      "Epoch 18, Batch 700, Loss: 1.1701129150390626\n",
      "Epoch 18, Batch 750, Loss: 1.1669381690025329\n",
      "Epoch 18, Batch 800, Loss: 1.1682700634002685\n",
      "Epoch 18, Batch 850, Loss: 1.172695028781891\n",
      "Epoch 18, Batch 900, Loss: 1.1693521213531495\n",
      "Epoch 19, Batch 50, Loss: 1.1674367356300355\n",
      "Epoch 19, Batch 100, Loss: 1.1711562991142273\n",
      "Epoch 19, Batch 150, Loss: 1.1669933009147644\n",
      "Epoch 19, Batch 200, Loss: 1.1704191875457763\n",
      "Epoch 19, Batch 250, Loss: 1.1699199676513672\n",
      "Epoch 19, Batch 300, Loss: 1.1730926871299743\n",
      "Epoch 19, Batch 350, Loss: 1.1708111119270326\n",
      "Epoch 19, Batch 400, Loss: 1.1685732340812682\n",
      "Epoch 19, Batch 450, Loss: 1.1703613781929016\n",
      "Epoch 19, Batch 500, Loss: 1.1673406386375427\n",
      "Epoch 19, Batch 550, Loss: 1.1744272184371949\n",
      "Epoch 19, Batch 600, Loss: 1.1663663244247438\n",
      "Epoch 19, Batch 650, Loss: 1.1775042009353638\n",
      "Epoch 19, Batch 700, Loss: 1.1676497983932494\n",
      "Epoch 19, Batch 750, Loss: 1.1663417029380798\n",
      "Epoch 19, Batch 800, Loss: 1.166793508529663\n",
      "Epoch 19, Batch 850, Loss: 1.1700323224067688\n",
      "Epoch 19, Batch 900, Loss: 1.1741201710700988\n",
      "Epoch 20, Batch 50, Loss: 1.1715665411949159\n",
      "Epoch 20, Batch 100, Loss: 1.1654740715026854\n",
      "Epoch 20, Batch 150, Loss: 1.172055127620697\n",
      "Epoch 20, Batch 200, Loss: 1.1699697017669677\n",
      "Epoch 20, Batch 250, Loss: 1.1722079300880432\n",
      "Epoch 20, Batch 300, Loss: 1.1705522751808166\n",
      "Epoch 20, Batch 350, Loss: 1.1679695892333983\n",
      "Epoch 20, Batch 400, Loss: 1.1678137397766113\n",
      "Epoch 20, Batch 450, Loss: 1.16811092376709\n",
      "Epoch 20, Batch 500, Loss: 1.1650884318351746\n",
      "Epoch 20, Batch 550, Loss: 1.1672258019447326\n",
      "Epoch 20, Batch 600, Loss: 1.1679966616630555\n",
      "Epoch 20, Batch 650, Loss: 1.1649912333488464\n",
      "Epoch 20, Batch 700, Loss: 1.171941614151001\n",
      "Epoch 20, Batch 750, Loss: 1.1749257278442382\n",
      "Epoch 20, Batch 800, Loss: 1.1674234890937805\n",
      "Epoch 20, Batch 850, Loss: 1.1724988269805907\n",
      "Epoch 20, Batch 900, Loss: 1.1748358726501464\n",
      "Epoch 21, Batch 50, Loss: 1.1680973482131958\n",
      "Epoch 21, Batch 100, Loss: 1.167845571041107\n",
      "Epoch 21, Batch 150, Loss: 1.1801362133026123\n",
      "Epoch 21, Batch 200, Loss: 1.1723452115058899\n",
      "Epoch 21, Batch 250, Loss: 1.1694311118125915\n",
      "Epoch 21, Batch 300, Loss: 1.172839641571045\n",
      "Epoch 21, Batch 350, Loss: 1.1692896723747253\n",
      "Epoch 21, Batch 400, Loss: 1.1698196840286255\n",
      "Epoch 21, Batch 450, Loss: 1.167685525417328\n",
      "Epoch 21, Batch 500, Loss: 1.1681414818763733\n",
      "Epoch 21, Batch 550, Loss: 1.1716509532928467\n",
      "Epoch 21, Batch 600, Loss: 1.1684073877334595\n",
      "Epoch 21, Batch 650, Loss: 1.1646556425094605\n",
      "Epoch 21, Batch 700, Loss: 1.1682658100128174\n",
      "Epoch 21, Batch 750, Loss: 1.1672578525543214\n",
      "Epoch 21, Batch 800, Loss: 1.1671918845176696\n",
      "Epoch 21, Batch 850, Loss: 1.1711518931388856\n",
      "Epoch 21, Batch 900, Loss: 1.174671275615692\n",
      "Epoch 22, Batch 50, Loss: 1.1707845306396485\n",
      "Epoch 22, Batch 100, Loss: 1.1699256587028504\n",
      "Epoch 22, Batch 150, Loss: 1.169263606071472\n",
      "Epoch 22, Batch 200, Loss: 1.1645839428901672\n",
      "Epoch 22, Batch 250, Loss: 1.1638057398796082\n",
      "Epoch 22, Batch 300, Loss: 1.1680399036407472\n",
      "Epoch 22, Batch 350, Loss: 1.1711183786392212\n",
      "Epoch 22, Batch 400, Loss: 1.1681058835983276\n",
      "Epoch 22, Batch 450, Loss: 1.1725416469573975\n",
      "Epoch 22, Batch 500, Loss: 1.1764639854431151\n",
      "Epoch 22, Batch 550, Loss: 1.1672649955749512\n",
      "Epoch 22, Batch 600, Loss: 1.1692161202430724\n",
      "Epoch 22, Batch 650, Loss: 1.1678622078895569\n",
      "Epoch 22, Batch 700, Loss: 1.1756110000610351\n",
      "Epoch 22, Batch 750, Loss: 1.172068214416504\n",
      "Epoch 22, Batch 800, Loss: 1.1674544954299926\n",
      "Epoch 22, Batch 850, Loss: 1.167554223537445\n",
      "Epoch 22, Batch 900, Loss: 1.1721566748619079\n",
      "Epoch 23, Batch 50, Loss: 1.1714321541786195\n",
      "Epoch 23, Batch 100, Loss: 1.1737139129638672\n",
      "Epoch 23, Batch 150, Loss: 1.1765887451171875\n",
      "Epoch 23, Batch 200, Loss: 1.1677024126052857\n",
      "Epoch 23, Batch 250, Loss: 1.1665861439704894\n",
      "Epoch 23, Batch 300, Loss: 1.1678529810905456\n",
      "Epoch 23, Batch 350, Loss: 1.1656639647483826\n",
      "Epoch 23, Batch 400, Loss: 1.1717363667488099\n",
      "Epoch 23, Batch 450, Loss: 1.1725110220909118\n",
      "Epoch 23, Batch 500, Loss: 1.1714952993392944\n",
      "Epoch 23, Batch 550, Loss: 1.171756567955017\n",
      "Epoch 23, Batch 600, Loss: 1.16895583152771\n",
      "Epoch 23, Batch 650, Loss: 1.1689820003509521\n",
      "Epoch 23, Batch 700, Loss: 1.172129855155945\n",
      "Epoch 23, Batch 750, Loss: 1.1704463696479797\n",
      "Epoch 23, Batch 800, Loss: 1.168619544506073\n",
      "Epoch 23, Batch 850, Loss: 1.1751287746429444\n",
      "Epoch 23, Batch 900, Loss: 1.1697959065437318\n",
      "Epoch 24, Batch 50, Loss: 1.1701531863212586\n",
      "Epoch 24, Batch 100, Loss: 1.1753948283195497\n",
      "Epoch 24, Batch 150, Loss: 1.1714164805412293\n",
      "Epoch 24, Batch 200, Loss: 1.174004135131836\n",
      "Epoch 24, Batch 250, Loss: 1.173763062953949\n",
      "Epoch 24, Batch 300, Loss: 1.171177978515625\n",
      "Epoch 24, Batch 350, Loss: 1.1720696806907653\n",
      "Epoch 24, Batch 400, Loss: 1.1686238288879394\n",
      "Epoch 24, Batch 450, Loss: 1.171896162033081\n",
      "Epoch 24, Batch 500, Loss: 1.1698961758613586\n",
      "Epoch 24, Batch 550, Loss: 1.1637640714645385\n",
      "Epoch 24, Batch 600, Loss: 1.1737649130821228\n",
      "Epoch 24, Batch 650, Loss: 1.1730298614501953\n",
      "Epoch 24, Batch 700, Loss: 1.1696883964538574\n",
      "Epoch 24, Batch 750, Loss: 1.1679647135734559\n",
      "Epoch 24, Batch 800, Loss: 1.1653788590431213\n",
      "Epoch 24, Batch 850, Loss: 1.1757148337364196\n",
      "Epoch 24, Batch 900, Loss: 1.1647946882247924\n",
      "Epoch 25, Batch 50, Loss: 1.169886839389801\n",
      "Epoch 25, Batch 100, Loss: 1.1662212657928466\n",
      "Epoch 25, Batch 150, Loss: 1.1669849610328675\n",
      "Epoch 25, Batch 200, Loss: 1.167562448978424\n",
      "Epoch 25, Batch 250, Loss: 1.1737412405014038\n",
      "Epoch 25, Batch 300, Loss: 1.1709752559661866\n",
      "Epoch 25, Batch 350, Loss: 1.1698173451423646\n",
      "Epoch 25, Batch 400, Loss: 1.1714650630950927\n",
      "Epoch 25, Batch 450, Loss: 1.1707588362693786\n",
      "Epoch 25, Batch 500, Loss: 1.1705181097984314\n",
      "Epoch 25, Batch 550, Loss: 1.1748245859146118\n",
      "Epoch 25, Batch 600, Loss: 1.1765808725357056\n",
      "Epoch 25, Batch 650, Loss: 1.1707455801963806\n",
      "Epoch 25, Batch 700, Loss: 1.1672372674942018\n",
      "Epoch 25, Batch 750, Loss: 1.1674648809432984\n",
      "Epoch 25, Batch 800, Loss: 1.1705514359474183\n",
      "Epoch 25, Batch 850, Loss: 1.1732973170280456\n",
      "Epoch 25, Batch 900, Loss: 1.168819420337677\n",
      "Accuracy on test set: 0.1%\n",
      "Fitting for combination 41\n",
      "784\n",
      "3\n",
      "10\n",
      "[40, 10, 10, 10]\n",
      "True\n",
      "['relu', 'sigmoid', 'tanh']\n",
      "SGD\n",
      "0.1\n",
      "0.01\n",
      "CrossEntropyLoss\n",
      "100\n",
      "25\n",
      "0\n",
      "Epoch 1, Batch 100, Loss: 2.206051881313324\n",
      "Epoch 1, Batch 200, Loss: 1.6510993897914887\n",
      "Epoch 1, Batch 300, Loss: 1.3205499231815339\n",
      "Epoch 1, Batch 400, Loss: 1.163208349943161\n",
      "Epoch 1, Batch 500, Loss: 1.0836994278430938\n",
      "Epoch 1, Batch 600, Loss: 1.0227886652946472\n",
      "Epoch 1, Batch 700, Loss: 0.9490789097547531\n",
      "Epoch 1, Batch 800, Loss: 0.935753020644188\n",
      "Epoch 1, Batch 900, Loss: 0.8807417756319046\n",
      "Epoch 2, Batch 100, Loss: 0.852697821855545\n",
      "Epoch 2, Batch 200, Loss: 0.8433412981033325\n",
      "Epoch 2, Batch 300, Loss: 0.8162736737728119\n",
      "Epoch 2, Batch 400, Loss: 0.8220072698593139\n",
      "Epoch 2, Batch 500, Loss: 0.7766962200403214\n",
      "Epoch 2, Batch 600, Loss: 0.7686919575929642\n",
      "Epoch 2, Batch 700, Loss: 0.767012768983841\n",
      "Epoch 2, Batch 800, Loss: 0.748967592716217\n",
      "Epoch 2, Batch 900, Loss: 0.7293870824575425\n",
      "Epoch 3, Batch 100, Loss: 0.7136672830581665\n",
      "Epoch 3, Batch 200, Loss: 0.7035383895039559\n",
      "Epoch 3, Batch 300, Loss: 0.7043009531497956\n",
      "Epoch 3, Batch 400, Loss: 0.6976471340656281\n",
      "Epoch 3, Batch 500, Loss: 0.715387852191925\n",
      "Epoch 3, Batch 600, Loss: 0.696882591843605\n",
      "Epoch 3, Batch 700, Loss: 0.689176918566227\n",
      "Epoch 3, Batch 800, Loss: 0.6893040868639946\n",
      "Epoch 3, Batch 900, Loss: 0.6951390922069549\n",
      "Epoch 4, Batch 100, Loss: 0.6808747416734695\n",
      "Epoch 4, Batch 200, Loss: 0.6821658810973168\n",
      "Epoch 4, Batch 300, Loss: 0.6811023443937302\n",
      "Epoch 4, Batch 400, Loss: 0.6804202282428742\n",
      "Epoch 4, Batch 500, Loss: 0.6848241102695465\n",
      "Epoch 4, Batch 600, Loss: 0.6708140826225281\n",
      "Epoch 4, Batch 700, Loss: 0.6560700145363808\n",
      "Epoch 4, Batch 800, Loss: 0.6670685800909996\n",
      "Epoch 4, Batch 900, Loss: 0.657024259865284\n",
      "Epoch 5, Batch 100, Loss: 0.6675252601504326\n",
      "Epoch 5, Batch 200, Loss: 0.6560105276107788\n",
      "Epoch 5, Batch 300, Loss: 0.6648960167169571\n",
      "Epoch 5, Batch 400, Loss: 0.6622782325744629\n",
      "Epoch 5, Batch 500, Loss: 0.6678882482647895\n",
      "Epoch 5, Batch 600, Loss: 0.6610597097873687\n",
      "Epoch 5, Batch 700, Loss: 0.6300571885704994\n",
      "Epoch 5, Batch 800, Loss: 0.6493458062410354\n",
      "Epoch 5, Batch 900, Loss: 0.6667218866944313\n",
      "Epoch 6, Batch 100, Loss: 0.6463903269171715\n",
      "Epoch 6, Batch 200, Loss: 0.6560643252730369\n",
      "Epoch 6, Batch 300, Loss: 0.6533136302232743\n",
      "Epoch 6, Batch 400, Loss: 0.6643583065271378\n",
      "Epoch 6, Batch 500, Loss: 0.6479612866044044\n",
      "Epoch 6, Batch 600, Loss: 0.6441264262795449\n",
      "Epoch 6, Batch 700, Loss: 0.6525053295493126\n",
      "Epoch 6, Batch 800, Loss: 0.6541509002447128\n",
      "Epoch 6, Batch 900, Loss: 0.6371741795539856\n",
      "Epoch 7, Batch 100, Loss: 0.6343349349498749\n",
      "Epoch 7, Batch 200, Loss: 0.6501862087845802\n",
      "Epoch 7, Batch 300, Loss: 0.652172201871872\n",
      "Epoch 7, Batch 400, Loss: 0.632313195168972\n",
      "Epoch 7, Batch 500, Loss: 0.6296491256356239\n",
      "Epoch 7, Batch 600, Loss: 0.6245088291168213\n",
      "Epoch 7, Batch 700, Loss: 0.6551027068495751\n",
      "Epoch 7, Batch 800, Loss: 0.6403829619288445\n",
      "Epoch 7, Batch 900, Loss: 0.6491432356834411\n",
      "Epoch 8, Batch 100, Loss: 0.6238036927580833\n",
      "Epoch 8, Batch 200, Loss: 0.6186124992370605\n",
      "Epoch 8, Batch 300, Loss: 0.6432031020522118\n",
      "Epoch 8, Batch 400, Loss: 0.619193448126316\n",
      "Epoch 8, Batch 500, Loss: 0.6238259759545326\n",
      "Epoch 8, Batch 600, Loss: 0.6294119000434876\n",
      "Epoch 8, Batch 700, Loss: 0.6228810319304466\n",
      "Epoch 8, Batch 800, Loss: 0.6175256153941154\n",
      "Epoch 8, Batch 900, Loss: 0.6209675869345666\n",
      "Epoch 9, Batch 100, Loss: 0.6221817964315415\n",
      "Epoch 9, Batch 200, Loss: 0.5931895866990089\n",
      "Epoch 9, Batch 300, Loss: 0.5966147965192795\n",
      "Epoch 9, Batch 400, Loss: 0.6028463208675384\n",
      "Epoch 9, Batch 500, Loss: 0.5982497811317444\n",
      "Epoch 9, Batch 600, Loss: 0.6068490025401115\n",
      "Epoch 9, Batch 700, Loss: 0.5799735957384109\n",
      "Epoch 9, Batch 800, Loss: 0.5946900483965873\n",
      "Epoch 9, Batch 900, Loss: 0.6118545815348625\n",
      "Epoch 10, Batch 100, Loss: 0.5985694286227227\n",
      "Epoch 10, Batch 200, Loss: 0.599183936715126\n",
      "Epoch 10, Batch 300, Loss: 0.5765296125411987\n",
      "Epoch 10, Batch 400, Loss: 0.58424575060606\n",
      "Epoch 10, Batch 500, Loss: 0.5855634778738021\n",
      "Epoch 10, Batch 600, Loss: 0.6012351614236832\n",
      "Epoch 10, Batch 700, Loss: 0.5956596091389657\n",
      "Epoch 10, Batch 800, Loss: 0.584578318297863\n",
      "Epoch 10, Batch 900, Loss: 0.5656091219186783\n",
      "Epoch 11, Batch 100, Loss: 0.5853310999274254\n",
      "Epoch 11, Batch 200, Loss: 0.5890532085299491\n",
      "Epoch 11, Batch 300, Loss: 0.5767877614498138\n",
      "Epoch 11, Batch 400, Loss: 0.5852115422487258\n",
      "Epoch 11, Batch 500, Loss: 0.5918019437789916\n",
      "Epoch 11, Batch 600, Loss: 0.5840914541482926\n",
      "Epoch 11, Batch 700, Loss: 0.5751324790716171\n",
      "Epoch 11, Batch 800, Loss: 0.5842089793086052\n",
      "Epoch 11, Batch 900, Loss: 0.5730552655458451\n",
      "Epoch 12, Batch 100, Loss: 0.5706470921635628\n",
      "Epoch 12, Batch 200, Loss: 0.567678801715374\n",
      "Epoch 12, Batch 300, Loss: 0.5929540464282036\n",
      "Epoch 12, Batch 400, Loss: 0.5796723267436028\n",
      "Epoch 12, Batch 500, Loss: 0.5772410699725151\n",
      "Epoch 12, Batch 600, Loss: 0.5645750784873962\n",
      "Epoch 12, Batch 700, Loss: 0.5823554345965385\n",
      "Epoch 12, Batch 800, Loss: 0.5848263454437256\n",
      "Epoch 12, Batch 900, Loss: 0.580788214802742\n",
      "Epoch 13, Batch 100, Loss: 0.5749406465888023\n",
      "Epoch 13, Batch 200, Loss: 0.560354640185833\n",
      "Epoch 13, Batch 300, Loss: 0.5639036732912064\n",
      "Epoch 13, Batch 400, Loss: 0.5857490548491477\n",
      "Epoch 13, Batch 500, Loss: 0.5795989483594894\n",
      "Epoch 13, Batch 600, Loss: 0.5863987058401108\n",
      "Epoch 13, Batch 700, Loss: 0.5630865281820298\n",
      "Epoch 13, Batch 800, Loss: 0.5757640826702118\n",
      "Epoch 13, Batch 900, Loss: 0.5782488590478897\n",
      "Epoch 14, Batch 100, Loss: 0.5881973260641098\n",
      "Epoch 14, Batch 200, Loss: 0.5806418663263321\n",
      "Epoch 14, Batch 300, Loss: 0.5798989844322204\n",
      "Epoch 14, Batch 400, Loss: 0.5795133346319199\n",
      "Epoch 14, Batch 500, Loss: 0.569423745572567\n",
      "Epoch 14, Batch 600, Loss: 0.5630181282758713\n",
      "Epoch 14, Batch 700, Loss: 0.5760236439108849\n",
      "Epoch 14, Batch 800, Loss: 0.5596421328186989\n",
      "Epoch 14, Batch 900, Loss: 0.5954863613843918\n",
      "Epoch 15, Batch 100, Loss: 0.566680251955986\n",
      "Epoch 15, Batch 200, Loss: 0.5834413629770279\n",
      "Epoch 15, Batch 300, Loss: 0.5821244093775749\n",
      "Epoch 15, Batch 400, Loss: 0.5778958290815354\n",
      "Epoch 15, Batch 500, Loss: 0.5672770550847054\n",
      "Epoch 15, Batch 600, Loss: 0.5840704542398453\n",
      "Epoch 15, Batch 700, Loss: 0.5595896807312966\n",
      "Epoch 15, Batch 800, Loss: 0.570697492659092\n",
      "Epoch 15, Batch 900, Loss: 0.5805168727040291\n",
      "Epoch 16, Batch 100, Loss: 0.5745698726177215\n",
      "Epoch 16, Batch 200, Loss: 0.5672008693218231\n",
      "Epoch 16, Batch 300, Loss: 0.570043474137783\n",
      "Epoch 16, Batch 400, Loss: 0.5535712879896164\n",
      "Epoch 16, Batch 500, Loss: 0.5841405019164085\n",
      "Epoch 16, Batch 600, Loss: 0.5746382454037666\n",
      "Epoch 16, Batch 700, Loss: 0.5810740655660629\n",
      "Epoch 16, Batch 800, Loss: 0.5638055989146232\n",
      "Epoch 16, Batch 900, Loss: 0.5674486216902733\n",
      "Epoch 17, Batch 100, Loss: 0.5760381820797921\n",
      "Epoch 17, Batch 200, Loss: 0.5784428456425666\n",
      "Epoch 17, Batch 300, Loss: 0.5425748988986016\n",
      "Epoch 17, Batch 400, Loss: 0.5644272255897522\n",
      "Epoch 17, Batch 500, Loss: 0.5658605054020882\n",
      "Epoch 17, Batch 600, Loss: 0.568591693341732\n",
      "Epoch 17, Batch 700, Loss: 0.5936074620485305\n",
      "Epoch 17, Batch 800, Loss: 0.5644254887104034\n",
      "Epoch 17, Batch 900, Loss: 0.5834253033995629\n",
      "Epoch 18, Batch 100, Loss: 0.5673369470238686\n",
      "Epoch 18, Batch 200, Loss: 0.5676554760336876\n",
      "Epoch 18, Batch 300, Loss: 0.556277274787426\n",
      "Epoch 18, Batch 400, Loss: 0.5713689124584198\n",
      "Epoch 18, Batch 500, Loss: 0.5734745174646377\n",
      "Epoch 18, Batch 600, Loss: 0.5852497035264969\n",
      "Epoch 18, Batch 700, Loss: 0.5897714701294899\n",
      "Epoch 18, Batch 800, Loss: 0.5633000701665878\n",
      "Epoch 18, Batch 900, Loss: 0.5660244995355606\n",
      "Epoch 19, Batch 100, Loss: 0.5625475534796714\n",
      "Epoch 19, Batch 200, Loss: 0.5828592491149902\n",
      "Epoch 19, Batch 300, Loss: 0.5664339023828506\n",
      "Epoch 19, Batch 400, Loss: 0.5665599191188813\n",
      "Epoch 19, Batch 500, Loss: 0.5659112271666527\n",
      "Epoch 19, Batch 600, Loss: 0.5788213580846786\n",
      "Epoch 19, Batch 700, Loss: 0.556342479288578\n",
      "Epoch 19, Batch 800, Loss: 0.5669367450475693\n",
      "Epoch 19, Batch 900, Loss: 0.5571125230193138\n",
      "Epoch 20, Batch 100, Loss: 0.5600734350085258\n",
      "Epoch 20, Batch 200, Loss: 0.5577156296372414\n",
      "Epoch 20, Batch 300, Loss: 0.5731736874580383\n",
      "Epoch 20, Batch 400, Loss: 0.5539220422506332\n",
      "Epoch 20, Batch 500, Loss: 0.5696279481053352\n",
      "Epoch 20, Batch 600, Loss: 0.5605614966154099\n",
      "Epoch 20, Batch 700, Loss: 0.5820286867022514\n",
      "Epoch 20, Batch 800, Loss: 0.5562796461582183\n",
      "Epoch 20, Batch 900, Loss: 0.5812087348103523\n",
      "Epoch 21, Batch 100, Loss: 0.5599129682779312\n",
      "Epoch 21, Batch 200, Loss: 0.5765151315927506\n",
      "Epoch 21, Batch 300, Loss: 0.5564475843310356\n",
      "Epoch 21, Batch 400, Loss: 0.5570118436217308\n",
      "Epoch 21, Batch 500, Loss: 0.5630707398056984\n",
      "Epoch 21, Batch 600, Loss: 0.5648994082212448\n",
      "Epoch 21, Batch 700, Loss: 0.5708580738306046\n",
      "Epoch 21, Batch 800, Loss: 0.5724549308419228\n",
      "Epoch 21, Batch 900, Loss: 0.5644451513886451\n",
      "Epoch 22, Batch 100, Loss: 0.5731746006011963\n",
      "Epoch 22, Batch 200, Loss: 0.5658223363757133\n",
      "Epoch 22, Batch 300, Loss: 0.5620893123745918\n",
      "Epoch 22, Batch 400, Loss: 0.5670150315761566\n",
      "Epoch 22, Batch 500, Loss: 0.56309942394495\n",
      "Epoch 22, Batch 600, Loss: 0.5681931391358376\n",
      "Epoch 22, Batch 700, Loss: 0.5753648197650909\n",
      "Epoch 22, Batch 800, Loss: 0.5652736860513687\n",
      "Epoch 22, Batch 900, Loss: 0.5523985642194748\n",
      "Epoch 23, Batch 100, Loss: 0.5515400576591492\n",
      "Epoch 23, Batch 200, Loss: 0.5454431426525116\n",
      "Epoch 23, Batch 300, Loss: 0.5581469321250916\n",
      "Epoch 23, Batch 400, Loss: 0.5566006627678871\n",
      "Epoch 23, Batch 500, Loss: 0.5528160083293915\n",
      "Epoch 23, Batch 600, Loss: 0.5538345387578011\n",
      "Epoch 23, Batch 700, Loss: 0.5926748037338256\n",
      "Epoch 23, Batch 800, Loss: 0.5645206704735756\n",
      "Epoch 23, Batch 900, Loss: 0.5792214930057525\n",
      "Epoch 24, Batch 100, Loss: 0.5754512166976928\n",
      "Epoch 24, Batch 200, Loss: 0.5724608328938484\n",
      "Epoch 24, Batch 300, Loss: 0.5545860031247138\n",
      "Epoch 24, Batch 400, Loss: 0.5550649112462998\n",
      "Epoch 24, Batch 500, Loss: 0.5426309183239937\n",
      "Epoch 24, Batch 600, Loss: 0.5580192005634308\n",
      "Epoch 24, Batch 700, Loss: 0.5758921411633492\n",
      "Epoch 24, Batch 800, Loss: 0.5755245077610016\n",
      "Epoch 24, Batch 900, Loss: 0.5689754229784012\n",
      "Epoch 25, Batch 100, Loss: 0.5461298251152038\n",
      "Epoch 25, Batch 200, Loss: 0.5699220648407937\n",
      "Epoch 25, Batch 300, Loss: 0.5703997844457627\n",
      "Epoch 25, Batch 400, Loss: 0.570837153494358\n",
      "Epoch 25, Batch 500, Loss: 0.558340035378933\n",
      "Epoch 25, Batch 600, Loss: 0.5481742042303085\n",
      "Epoch 25, Batch 700, Loss: 0.5642957365512848\n",
      "Epoch 25, Batch 800, Loss: 0.5621410477161407\n",
      "Epoch 25, Batch 900, Loss: 0.5547006329894066\n",
      "Accuracy on test set: 0.8186%\n",
      "Fitting for combination 42\n",
      "784\n",
      "3\n",
      "10\n",
      "[40, 10, 10, 10]\n",
      "True\n",
      "['relu', 'sigmoid', 'tanh']\n",
      "Adam\n",
      "0.3\n",
      "0.03\n",
      "CrossEntropyLoss\n",
      "100\n",
      "25\n",
      "0\n",
      "Epoch 1, Batch 100, Loss: 2.3875393891334533\n",
      "Epoch 1, Batch 200, Loss: 2.329117181301117\n",
      "Epoch 1, Batch 300, Loss: 2.3442021799087525\n",
      "Epoch 1, Batch 400, Loss: 2.348763132095337\n",
      "Epoch 1, Batch 500, Loss: 2.3620208930969238\n",
      "Epoch 1, Batch 600, Loss: 2.375482611656189\n",
      "Epoch 1, Batch 700, Loss: 2.3639131879806516\n",
      "Epoch 1, Batch 800, Loss: 2.3319780921936033\n",
      "Epoch 1, Batch 900, Loss: 2.398031270503998\n",
      "Epoch 2, Batch 100, Loss: 2.326088230609894\n",
      "Epoch 2, Batch 200, Loss: 2.336009259223938\n",
      "Epoch 2, Batch 300, Loss: 2.3300603342056276\n",
      "Epoch 2, Batch 400, Loss: 2.3978573799133303\n",
      "Epoch 2, Batch 500, Loss: 2.4042024302482603\n",
      "Epoch 2, Batch 600, Loss: 2.3482704520225526\n",
      "Epoch 2, Batch 700, Loss: 2.317434244155884\n",
      "Epoch 2, Batch 800, Loss: 2.3241774821281433\n",
      "Epoch 2, Batch 900, Loss: 2.393342955112457\n",
      "Epoch 3, Batch 100, Loss: 2.3556661725044252\n",
      "Epoch 3, Batch 200, Loss: 2.343861954212189\n",
      "Epoch 3, Batch 300, Loss: 2.337267460823059\n",
      "Epoch 3, Batch 400, Loss: 2.348809494972229\n",
      "Epoch 3, Batch 500, Loss: 2.384801847934723\n",
      "Epoch 3, Batch 600, Loss: 2.371651463508606\n",
      "Epoch 3, Batch 700, Loss: 2.4055311918258666\n",
      "Epoch 3, Batch 800, Loss: 2.387269449234009\n",
      "Epoch 3, Batch 900, Loss: 2.353502652645111\n",
      "Epoch 4, Batch 100, Loss: 2.379647309780121\n",
      "Epoch 4, Batch 200, Loss: 2.344455590248108\n",
      "Epoch 4, Batch 300, Loss: 2.3584912753105165\n",
      "Epoch 4, Batch 400, Loss: 2.3634495425224302\n",
      "Epoch 4, Batch 500, Loss: 2.3495741653442384\n",
      "Epoch 4, Batch 600, Loss: 2.3797649145126343\n",
      "Epoch 4, Batch 700, Loss: 2.354847993850708\n",
      "Epoch 4, Batch 800, Loss: 2.360946044921875\n",
      "Epoch 4, Batch 900, Loss: 2.338443245887756\n",
      "Epoch 5, Batch 100, Loss: 2.3711640357971193\n",
      "Epoch 5, Batch 200, Loss: 2.344936237335205\n",
      "Epoch 5, Batch 300, Loss: 2.358416128158569\n",
      "Epoch 5, Batch 400, Loss: 2.3449428844451905\n",
      "Epoch 5, Batch 500, Loss: 2.3601053190231323\n",
      "Epoch 5, Batch 600, Loss: 2.409797959327698\n",
      "Epoch 5, Batch 700, Loss: 2.4060869312286375\n",
      "Epoch 5, Batch 800, Loss: 2.32567010641098\n",
      "Epoch 5, Batch 900, Loss: 2.3499512767791746\n",
      "Epoch 6, Batch 100, Loss: 2.330142226219177\n",
      "Epoch 6, Batch 200, Loss: 2.3352969026565553\n",
      "Epoch 6, Batch 300, Loss: 2.3610538172721864\n",
      "Epoch 6, Batch 400, Loss: 2.34016521692276\n",
      "Epoch 6, Batch 500, Loss: 2.3554751205444338\n",
      "Epoch 6, Batch 600, Loss: 2.3332306933403015\n",
      "Epoch 6, Batch 700, Loss: 2.3573667097091673\n",
      "Epoch 6, Batch 800, Loss: 2.3708340692520142\n",
      "Epoch 6, Batch 900, Loss: 2.421694207191467\n",
      "Epoch 7, Batch 100, Loss: 2.3516409492492674\n",
      "Epoch 7, Batch 200, Loss: 2.3585066032409667\n",
      "Epoch 7, Batch 300, Loss: 2.3964704036712647\n",
      "Epoch 7, Batch 400, Loss: 2.3816372966766357\n",
      "Epoch 7, Batch 500, Loss: 2.3411200571060182\n",
      "Epoch 7, Batch 600, Loss: 2.3530995893478392\n",
      "Epoch 7, Batch 700, Loss: 2.3564786195755003\n",
      "Epoch 7, Batch 800, Loss: 2.3385761046409606\n",
      "Epoch 7, Batch 900, Loss: 2.3582717823982238\n",
      "Epoch 8, Batch 100, Loss: 2.379700880050659\n",
      "Epoch 8, Batch 200, Loss: 2.3497990012168883\n",
      "Epoch 8, Batch 300, Loss: 2.3477388834953308\n",
      "Epoch 8, Batch 400, Loss: 2.3524391984939577\n",
      "Epoch 8, Batch 500, Loss: 2.364876079559326\n",
      "Epoch 8, Batch 600, Loss: 2.3822015047073366\n",
      "Epoch 8, Batch 700, Loss: 2.3997185611724854\n",
      "Epoch 8, Batch 800, Loss: 2.3382473373413086\n",
      "Epoch 8, Batch 900, Loss: 2.355601181983948\n",
      "Epoch 9, Batch 100, Loss: 2.3482464027404784\n",
      "Epoch 9, Batch 200, Loss: 2.359111764431\n",
      "Epoch 9, Batch 300, Loss: 2.3781695580482483\n",
      "Epoch 9, Batch 400, Loss: 2.3644599175453185\n",
      "Epoch 9, Batch 500, Loss: 2.366179141998291\n",
      "Epoch 9, Batch 600, Loss: 2.3913330125808714\n",
      "Epoch 9, Batch 700, Loss: 2.340466327667236\n",
      "Epoch 9, Batch 800, Loss: 2.360970849990845\n",
      "Epoch 9, Batch 900, Loss: 2.3671869826316834\n",
      "Epoch 10, Batch 100, Loss: 2.4085843563079834\n",
      "Epoch 10, Batch 200, Loss: 2.380201325416565\n",
      "Epoch 10, Batch 300, Loss: 2.335688381195068\n",
      "Epoch 10, Batch 400, Loss: 2.3626771569252014\n",
      "Epoch 10, Batch 500, Loss: 2.350607590675354\n",
      "Epoch 10, Batch 600, Loss: 2.3400677752494814\n",
      "Epoch 10, Batch 700, Loss: 2.3743364882469176\n",
      "Epoch 10, Batch 800, Loss: 2.3795409750938417\n",
      "Epoch 10, Batch 900, Loss: 2.3797966933250425\n",
      "Epoch 11, Batch 100, Loss: 2.3624228191375733\n",
      "Epoch 11, Batch 200, Loss: 2.3745159983634947\n",
      "Epoch 11, Batch 300, Loss: 2.3719088411331177\n",
      "Epoch 11, Batch 400, Loss: 2.329500768184662\n",
      "Epoch 11, Batch 500, Loss: 2.339860279560089\n",
      "Epoch 11, Batch 600, Loss: 2.3685450887680055\n",
      "Epoch 11, Batch 700, Loss: 2.353364276885986\n",
      "Epoch 11, Batch 800, Loss: 2.331886577606201\n",
      "Epoch 11, Batch 900, Loss: 2.3342800974845885\n",
      "Epoch 12, Batch 100, Loss: 2.3686160755157473\n",
      "Epoch 12, Batch 200, Loss: 2.4468381571769715\n",
      "Epoch 12, Batch 300, Loss: 2.403319501876831\n",
      "Epoch 12, Batch 400, Loss: 2.3421838879585266\n",
      "Epoch 12, Batch 500, Loss: 2.355008738040924\n",
      "Epoch 12, Batch 600, Loss: 2.397555482387543\n",
      "Epoch 12, Batch 700, Loss: 2.36132465839386\n",
      "Epoch 12, Batch 800, Loss: 2.343770990371704\n",
      "Epoch 12, Batch 900, Loss: 2.340061526298523\n",
      "Epoch 13, Batch 100, Loss: 2.3364452123641968\n",
      "Epoch 13, Batch 200, Loss: 2.339531328678131\n",
      "Epoch 13, Batch 300, Loss: 2.4173686361312865\n",
      "Epoch 13, Batch 400, Loss: 2.364244115352631\n",
      "Epoch 13, Batch 500, Loss: 2.3632017183303833\n",
      "Epoch 13, Batch 600, Loss: 2.356105856895447\n",
      "Epoch 13, Batch 700, Loss: 2.355531244277954\n",
      "Epoch 13, Batch 800, Loss: 2.344883050918579\n",
      "Epoch 13, Batch 900, Loss: 2.401614713668823\n",
      "Epoch 14, Batch 100, Loss: 2.329476182460785\n",
      "Epoch 14, Batch 200, Loss: 2.326431360244751\n",
      "Epoch 14, Batch 300, Loss: 2.337211673259735\n",
      "Epoch 14, Batch 400, Loss: 2.400116753578186\n",
      "Epoch 14, Batch 500, Loss: 2.3468547010421754\n",
      "Epoch 14, Batch 600, Loss: 2.373306837081909\n",
      "Epoch 14, Batch 700, Loss: 2.345219118595123\n",
      "Epoch 14, Batch 800, Loss: 2.400706295967102\n",
      "Epoch 14, Batch 900, Loss: 2.3579971957206727\n",
      "Epoch 15, Batch 100, Loss: 2.338034133911133\n",
      "Epoch 15, Batch 200, Loss: 2.3822771430015566\n",
      "Epoch 15, Batch 300, Loss: 2.36569180727005\n",
      "Epoch 15, Batch 400, Loss: 2.340894010066986\n",
      "Epoch 15, Batch 500, Loss: 2.3225290536880494\n",
      "Epoch 15, Batch 600, Loss: 2.3227614426612853\n",
      "Epoch 15, Batch 700, Loss: 2.462784180641174\n",
      "Epoch 15, Batch 800, Loss: 2.4903097105026246\n",
      "Epoch 15, Batch 900, Loss: 2.330684108734131\n",
      "Epoch 16, Batch 100, Loss: 2.3403990602493288\n",
      "Epoch 16, Batch 200, Loss: 2.326948082447052\n",
      "Epoch 16, Batch 300, Loss: 2.3564530658721923\n",
      "Epoch 16, Batch 400, Loss: 2.3626142954826355\n",
      "Epoch 16, Batch 500, Loss: 2.355239567756653\n",
      "Epoch 16, Batch 600, Loss: 2.3472300148010254\n",
      "Epoch 16, Batch 700, Loss: 2.376691267490387\n",
      "Epoch 16, Batch 800, Loss: 2.3312460374832153\n",
      "Epoch 16, Batch 900, Loss: 2.374748589992523\n",
      "Epoch 17, Batch 100, Loss: 2.350868031978607\n",
      "Epoch 17, Batch 200, Loss: 2.332876284122467\n",
      "Epoch 17, Batch 300, Loss: 2.348898470401764\n",
      "Epoch 17, Batch 400, Loss: 2.3539797139167784\n",
      "Epoch 17, Batch 500, Loss: 2.3283516693115236\n",
      "Epoch 17, Batch 600, Loss: 2.340388824939728\n",
      "Epoch 17, Batch 700, Loss: 2.368472092151642\n",
      "Epoch 17, Batch 800, Loss: 2.362914514541626\n",
      "Epoch 17, Batch 900, Loss: 2.3465891790390017\n",
      "Epoch 18, Batch 100, Loss: 2.3562384366989138\n",
      "Epoch 18, Batch 200, Loss: 2.354406433105469\n",
      "Epoch 18, Batch 300, Loss: 2.41885130405426\n",
      "Epoch 18, Batch 400, Loss: 2.368117458820343\n",
      "Epoch 18, Batch 500, Loss: 2.3756179785728455\n",
      "Epoch 18, Batch 600, Loss: 2.338974459171295\n",
      "Epoch 18, Batch 700, Loss: 2.3898050665855406\n",
      "Epoch 18, Batch 800, Loss: 2.363010425567627\n",
      "Epoch 18, Batch 900, Loss: 2.330851695537567\n",
      "Epoch 19, Batch 100, Loss: 2.3406633377075194\n",
      "Epoch 19, Batch 200, Loss: 2.3565096426010133\n",
      "Epoch 19, Batch 300, Loss: 2.3521266198158264\n",
      "Epoch 19, Batch 400, Loss: 2.376569230556488\n",
      "Epoch 19, Batch 500, Loss: 2.340665190219879\n",
      "Epoch 19, Batch 600, Loss: 2.349053122997284\n",
      "Epoch 19, Batch 700, Loss: 2.3387828612327577\n",
      "Epoch 19, Batch 800, Loss: 2.3358048582077027\n",
      "Epoch 19, Batch 900, Loss: 2.3261845684051514\n",
      "Epoch 20, Batch 100, Loss: 2.362554495334625\n",
      "Epoch 20, Batch 200, Loss: 2.374819769859314\n",
      "Epoch 20, Batch 300, Loss: 2.3523383283615114\n",
      "Epoch 20, Batch 400, Loss: 2.356845269203186\n",
      "Epoch 20, Batch 500, Loss: 2.3950009512901307\n",
      "Epoch 20, Batch 600, Loss: 2.383009548187256\n",
      "Epoch 20, Batch 700, Loss: 2.3490254640579225\n",
      "Epoch 20, Batch 800, Loss: 2.3807978582382203\n",
      "Epoch 20, Batch 900, Loss: 2.3668538689613343\n",
      "Epoch 21, Batch 100, Loss: 2.3509743475914\n",
      "Epoch 21, Batch 200, Loss: 2.3233441638946535\n",
      "Epoch 21, Batch 300, Loss: 2.345036189556122\n",
      "Epoch 21, Batch 400, Loss: 2.353660349845886\n",
      "Epoch 21, Batch 500, Loss: 2.3853840327262876\n",
      "Epoch 21, Batch 600, Loss: 2.3520766210556032\n",
      "Epoch 21, Batch 700, Loss: 2.3679753613471983\n",
      "Epoch 21, Batch 800, Loss: 2.374914484024048\n",
      "Epoch 21, Batch 900, Loss: 2.3665049862861633\n",
      "Epoch 22, Batch 100, Loss: 2.3445347213745116\n",
      "Epoch 22, Batch 200, Loss: 2.3359914994239808\n",
      "Epoch 22, Batch 300, Loss: 2.3315019464492797\n",
      "Epoch 22, Batch 400, Loss: 2.366283805370331\n",
      "Epoch 22, Batch 500, Loss: 2.41444331407547\n",
      "Epoch 22, Batch 600, Loss: 2.3685505390167236\n",
      "Epoch 22, Batch 700, Loss: 2.351449601650238\n",
      "Epoch 22, Batch 800, Loss: 2.4178783965110777\n",
      "Epoch 22, Batch 900, Loss: 2.3828708338737488\n",
      "Epoch 23, Batch 100, Loss: 2.3363599395751953\n",
      "Epoch 23, Batch 200, Loss: 2.35274817943573\n",
      "Epoch 23, Batch 300, Loss: 2.357765371799469\n",
      "Epoch 23, Batch 400, Loss: 2.345324831008911\n",
      "Epoch 23, Batch 500, Loss: 2.3444528007507324\n",
      "Epoch 23, Batch 600, Loss: 2.3731885242462156\n",
      "Epoch 23, Batch 700, Loss: 2.342586824893951\n",
      "Epoch 23, Batch 800, Loss: 2.3319105052948\n",
      "Epoch 23, Batch 900, Loss: 2.3660837316513064\n",
      "Epoch 24, Batch 100, Loss: 2.48729483127594\n",
      "Epoch 24, Batch 200, Loss: 2.3621611166000367\n",
      "Epoch 24, Batch 300, Loss: 2.3360698199272156\n",
      "Epoch 24, Batch 400, Loss: 2.364444591999054\n",
      "Epoch 24, Batch 500, Loss: 2.35317049741745\n",
      "Epoch 24, Batch 600, Loss: 2.3456070709228514\n",
      "Epoch 24, Batch 700, Loss: 2.343696174621582\n",
      "Epoch 24, Batch 800, Loss: 2.3661255383491517\n",
      "Epoch 24, Batch 900, Loss: 2.368828384876251\n",
      "Epoch 25, Batch 100, Loss: 2.3455438661575316\n",
      "Epoch 25, Batch 200, Loss: 2.4145028495788576\n",
      "Epoch 25, Batch 300, Loss: 2.393265037536621\n",
      "Epoch 25, Batch 400, Loss: 2.3445382213592527\n",
      "Epoch 25, Batch 500, Loss: 2.350197768211365\n",
      "Epoch 25, Batch 600, Loss: 2.3411348342895506\n",
      "Epoch 25, Batch 700, Loss: 2.391214153766632\n",
      "Epoch 25, Batch 800, Loss: 2.408175940513611\n",
      "Epoch 25, Batch 900, Loss: 2.337193808555603\n",
      "Accuracy on test set: 0.1%\n",
      "Fitting for combination 43\n",
      "784\n",
      "3\n",
      "10\n",
      "[40, 10, 10, 10]\n",
      "False\n",
      "['relu', 'sigmoid', 'tanh']\n",
      "SGD\n",
      "0.03\n",
      "0\n",
      "CrossEntropyLoss\n",
      "50\n",
      "25\n",
      "0\n",
      "Epoch 1, Batch 50, Loss: 1.15708087682724\n",
      "Epoch 1, Batch 100, Loss: 1.1401026797294618\n",
      "Epoch 1, Batch 150, Loss: 1.1243151092529298\n",
      "Epoch 1, Batch 200, Loss: 1.0951447701454162\n",
      "Epoch 1, Batch 250, Loss: 1.050901279449463\n",
      "Epoch 1, Batch 300, Loss: 0.9951319909095764\n",
      "Epoch 1, Batch 350, Loss: 0.9394821190834045\n",
      "Epoch 1, Batch 400, Loss: 0.8982623827457428\n",
      "Epoch 1, Batch 450, Loss: 0.859197564125061\n",
      "Epoch 1, Batch 500, Loss: 0.8234068048000336\n",
      "Epoch 1, Batch 550, Loss: 0.7952088820934295\n",
      "Epoch 1, Batch 600, Loss: 0.7710906076431274\n",
      "Epoch 1, Batch 650, Loss: 0.745519847869873\n",
      "Epoch 1, Batch 700, Loss: 0.7280763864517212\n",
      "Epoch 1, Batch 750, Loss: 0.7170745992660522\n",
      "Epoch 1, Batch 800, Loss: 0.6918294703960419\n",
      "Epoch 1, Batch 850, Loss: 0.6692299771308899\n",
      "Epoch 1, Batch 900, Loss: 0.653647004365921\n",
      "Epoch 2, Batch 50, Loss: 0.612377301454544\n",
      "Epoch 2, Batch 100, Loss: 0.5925757837295532\n",
      "Epoch 2, Batch 150, Loss: 0.5812379896640778\n",
      "Epoch 2, Batch 200, Loss: 0.5587687349319458\n",
      "Epoch 2, Batch 250, Loss: 0.5405305016040802\n",
      "Epoch 2, Batch 300, Loss: 0.5274162435531616\n",
      "Epoch 2, Batch 350, Loss: 0.513798081278801\n",
      "Epoch 2, Batch 400, Loss: 0.5118855440616608\n",
      "Epoch 2, Batch 450, Loss: 0.4900102806091309\n",
      "Epoch 2, Batch 500, Loss: 0.48972375571727755\n",
      "Epoch 2, Batch 550, Loss: 0.47254775404930116\n",
      "Epoch 2, Batch 600, Loss: 0.4615395259857178\n",
      "Epoch 2, Batch 650, Loss: 0.4587937265634537\n",
      "Epoch 2, Batch 700, Loss: 0.45058405220508574\n",
      "Epoch 2, Batch 750, Loss: 0.4479751622676849\n",
      "Epoch 2, Batch 800, Loss: 0.4374755251407623\n",
      "Epoch 2, Batch 850, Loss: 0.4372178012132645\n",
      "Epoch 2, Batch 900, Loss: 0.4221949124336243\n",
      "Epoch 3, Batch 50, Loss: 0.4061086815595627\n",
      "Epoch 3, Batch 100, Loss: 0.3972539389133453\n",
      "Epoch 3, Batch 150, Loss: 0.3919358664751053\n",
      "Epoch 3, Batch 200, Loss: 0.4001556271314621\n",
      "Epoch 3, Batch 250, Loss: 0.39841759741306304\n",
      "Epoch 3, Batch 300, Loss: 0.3828297609090805\n",
      "Epoch 3, Batch 350, Loss: 0.39243767559528353\n",
      "Epoch 3, Batch 400, Loss: 0.3961024183034897\n",
      "Epoch 3, Batch 450, Loss: 0.3758552119135857\n",
      "Epoch 3, Batch 500, Loss: 0.3702780437469482\n",
      "Epoch 3, Batch 550, Loss: 0.36073204934597014\n",
      "Epoch 3, Batch 600, Loss: 0.368258011341095\n",
      "Epoch 3, Batch 650, Loss: 0.3696250981092453\n",
      "Epoch 3, Batch 700, Loss: 0.351514535844326\n",
      "Epoch 3, Batch 750, Loss: 0.36396200299263\n",
      "Epoch 3, Batch 800, Loss: 0.36000382900238037\n",
      "Epoch 3, Batch 850, Loss: 0.34932938337326047\n",
      "Epoch 3, Batch 900, Loss: 0.3521412125229835\n",
      "Epoch 4, Batch 50, Loss: 0.33409932017326355\n",
      "Epoch 4, Batch 100, Loss: 0.33587937861680983\n",
      "Epoch 4, Batch 150, Loss: 0.32937302470207214\n",
      "Epoch 4, Batch 200, Loss: 0.3342096981406212\n",
      "Epoch 4, Batch 250, Loss: 0.3295440399646759\n",
      "Epoch 4, Batch 300, Loss: 0.3255017587542534\n",
      "Epoch 4, Batch 350, Loss: 0.31694386422634124\n",
      "Epoch 4, Batch 400, Loss: 0.3293171298503876\n",
      "Epoch 4, Batch 450, Loss: 0.3283957451581955\n",
      "Epoch 4, Batch 500, Loss: 0.31490604758262636\n",
      "Epoch 4, Batch 550, Loss: 0.32817770779132843\n",
      "Epoch 4, Batch 600, Loss: 0.32150685399770734\n",
      "Epoch 4, Batch 650, Loss: 0.31033914983272554\n",
      "Epoch 4, Batch 700, Loss: 0.3179889333248138\n",
      "Epoch 4, Batch 750, Loss: 0.31132638961076736\n",
      "Epoch 4, Batch 800, Loss: 0.3097934341430664\n",
      "Epoch 4, Batch 850, Loss: 0.29911803334951403\n",
      "Epoch 4, Batch 900, Loss: 0.296784510910511\n",
      "Epoch 5, Batch 50, Loss: 0.28855413794517515\n",
      "Epoch 5, Batch 100, Loss: 0.29564128667116163\n",
      "Epoch 5, Batch 150, Loss: 0.2765274754166603\n",
      "Epoch 5, Batch 200, Loss: 0.2921516561508179\n",
      "Epoch 5, Batch 250, Loss: 0.2755356448888779\n",
      "Epoch 5, Batch 300, Loss: 0.28554021537303925\n",
      "Epoch 5, Batch 350, Loss: 0.29403629660606384\n",
      "Epoch 5, Batch 400, Loss: 0.2820542296767235\n",
      "Epoch 5, Batch 450, Loss: 0.282057580947876\n",
      "Epoch 5, Batch 500, Loss: 0.28636828660964964\n",
      "Epoch 5, Batch 550, Loss: 0.2611489963531494\n",
      "Epoch 5, Batch 600, Loss: 0.2577457612752914\n",
      "Epoch 5, Batch 650, Loss: 0.2559326809644699\n",
      "Epoch 5, Batch 700, Loss: 0.2585720008611679\n",
      "Epoch 5, Batch 750, Loss: 0.26292471289634706\n",
      "Epoch 5, Batch 800, Loss: 0.2568443536758423\n",
      "Epoch 5, Batch 850, Loss: 0.26656315743923187\n",
      "Epoch 5, Batch 900, Loss: 0.26024033963680265\n",
      "Epoch 6, Batch 50, Loss: 0.25905232489109037\n",
      "Epoch 6, Batch 100, Loss: 0.25257629066705706\n",
      "Epoch 6, Batch 150, Loss: 0.25187498271465303\n",
      "Epoch 6, Batch 200, Loss: 0.24204011827707292\n",
      "Epoch 6, Batch 250, Loss: 0.24249402135610582\n",
      "Epoch 6, Batch 300, Loss: 0.2525303146243095\n",
      "Epoch 6, Batch 350, Loss: 0.23989125847816467\n",
      "Epoch 6, Batch 400, Loss: 0.24915373504161833\n",
      "Epoch 6, Batch 450, Loss: 0.24131685882806778\n",
      "Epoch 6, Batch 500, Loss: 0.22953742146492004\n",
      "Epoch 6, Batch 550, Loss: 0.25840154170989993\n",
      "Epoch 6, Batch 600, Loss: 0.24784149199724198\n",
      "Epoch 6, Batch 650, Loss: 0.2397904321551323\n",
      "Epoch 6, Batch 700, Loss: 0.23869240760803223\n",
      "Epoch 6, Batch 750, Loss: 0.2311248791217804\n",
      "Epoch 6, Batch 800, Loss: 0.2461291193962097\n",
      "Epoch 6, Batch 850, Loss: 0.24208882182836533\n",
      "Epoch 6, Batch 900, Loss: 0.23924936175346376\n",
      "Epoch 7, Batch 50, Loss: 0.2298377674818039\n",
      "Epoch 7, Batch 100, Loss: 0.2407454439997673\n",
      "Epoch 7, Batch 150, Loss: 0.22830411344766616\n",
      "Epoch 7, Batch 200, Loss: 0.23429691046476364\n",
      "Epoch 7, Batch 250, Loss: 0.22786444723606109\n",
      "Epoch 7, Batch 300, Loss: 0.23329047828912736\n",
      "Epoch 7, Batch 350, Loss: 0.2228538879752159\n",
      "Epoch 7, Batch 400, Loss: 0.21645255506038666\n",
      "Epoch 7, Batch 450, Loss: 0.22828131049871445\n",
      "Epoch 7, Batch 500, Loss: 0.23445785745978356\n",
      "Epoch 7, Batch 550, Loss: 0.22455088943243026\n",
      "Epoch 7, Batch 600, Loss: 0.2147579026222229\n",
      "Epoch 7, Batch 650, Loss: 0.22247417017817497\n",
      "Epoch 7, Batch 700, Loss: 0.21043846428394317\n",
      "Epoch 7, Batch 750, Loss: 0.21760475441813468\n",
      "Epoch 7, Batch 800, Loss: 0.21987954914569854\n",
      "Epoch 7, Batch 850, Loss: 0.2377919787168503\n",
      "Epoch 7, Batch 900, Loss: 0.20821751847863199\n",
      "Epoch 8, Batch 50, Loss: 0.21956896603107454\n",
      "Epoch 8, Batch 100, Loss: 0.21198463886976243\n",
      "Epoch 8, Batch 150, Loss: 0.21435191065073014\n",
      "Epoch 8, Batch 200, Loss: 0.22447232991456986\n",
      "Epoch 8, Batch 250, Loss: 0.2167541640996933\n",
      "Epoch 8, Batch 300, Loss: 0.21238518819212915\n",
      "Epoch 8, Batch 350, Loss: 0.1906517432630062\n",
      "Epoch 8, Batch 400, Loss: 0.21181023508310318\n",
      "Epoch 8, Batch 450, Loss: 0.2234497831761837\n",
      "Epoch 8, Batch 500, Loss: 0.20640703573822974\n",
      "Epoch 8, Batch 550, Loss: 0.20081397518515587\n",
      "Epoch 8, Batch 600, Loss: 0.2139968079328537\n",
      "Epoch 8, Batch 650, Loss: 0.21544035032391548\n",
      "Epoch 8, Batch 700, Loss: 0.20995195522904397\n",
      "Epoch 8, Batch 750, Loss: 0.20639280647039412\n",
      "Epoch 8, Batch 800, Loss: 0.2180073441565037\n",
      "Epoch 8, Batch 850, Loss: 0.20508700758218765\n",
      "Epoch 8, Batch 900, Loss: 0.20211947202682495\n",
      "Epoch 9, Batch 50, Loss: 0.19915109172463416\n",
      "Epoch 9, Batch 100, Loss: 0.2081487335264683\n",
      "Epoch 9, Batch 150, Loss: 0.2134148073196411\n",
      "Epoch 9, Batch 200, Loss: 0.2035712368786335\n",
      "Epoch 9, Batch 250, Loss: 0.19471472427248954\n",
      "Epoch 9, Batch 300, Loss: 0.20509730964899064\n",
      "Epoch 9, Batch 350, Loss: 0.2022016905248165\n",
      "Epoch 9, Batch 400, Loss: 0.2002747391164303\n",
      "Epoch 9, Batch 450, Loss: 0.20097679689526557\n",
      "Epoch 9, Batch 500, Loss: 0.20342583879828452\n",
      "Epoch 9, Batch 550, Loss: 0.20090048223733903\n",
      "Epoch 9, Batch 600, Loss: 0.19957956701517104\n",
      "Epoch 9, Batch 650, Loss: 0.18771947383880616\n",
      "Epoch 9, Batch 700, Loss: 0.2018636144697666\n",
      "Epoch 9, Batch 750, Loss: 0.1958111506700516\n",
      "Epoch 9, Batch 800, Loss: 0.1907907734811306\n",
      "Epoch 9, Batch 850, Loss: 0.21369525253772736\n",
      "Epoch 9, Batch 900, Loss: 0.191294187605381\n",
      "Epoch 10, Batch 50, Loss: 0.2050401782989502\n",
      "Epoch 10, Batch 100, Loss: 0.19589821979403496\n",
      "Epoch 10, Batch 150, Loss: 0.18886360719799997\n",
      "Epoch 10, Batch 200, Loss: 0.19863153472542763\n",
      "Epoch 10, Batch 250, Loss: 0.18634318619966506\n",
      "Epoch 10, Batch 300, Loss: 0.18456777751445771\n",
      "Epoch 10, Batch 350, Loss: 0.20338415667414667\n",
      "Epoch 10, Batch 400, Loss: 0.2007667538523674\n",
      "Epoch 10, Batch 450, Loss: 0.18710307270288468\n",
      "Epoch 10, Batch 500, Loss: 0.18545779347419739\n",
      "Epoch 10, Batch 550, Loss: 0.18116822123527526\n",
      "Epoch 10, Batch 600, Loss: 0.20382166147232056\n",
      "Epoch 10, Batch 650, Loss: 0.19613681867718696\n",
      "Epoch 10, Batch 700, Loss: 0.1763697797060013\n",
      "Epoch 10, Batch 750, Loss: 0.19022184118628502\n",
      "Epoch 10, Batch 800, Loss: 0.19773271679878235\n",
      "Epoch 10, Batch 850, Loss: 0.19553247705101967\n",
      "Epoch 10, Batch 900, Loss: 0.1966521379351616\n",
      "Epoch 11, Batch 50, Loss: 0.1891816283762455\n",
      "Epoch 11, Batch 100, Loss: 0.1937540665268898\n",
      "Epoch 11, Batch 150, Loss: 0.19274580523371695\n",
      "Epoch 11, Batch 200, Loss: 0.18112987279891968\n",
      "Epoch 11, Batch 250, Loss: 0.19339090250432492\n",
      "Epoch 11, Batch 300, Loss: 0.1918371656537056\n",
      "Epoch 11, Batch 350, Loss: 0.1861314781010151\n",
      "Epoch 11, Batch 400, Loss: 0.18209141850471497\n",
      "Epoch 11, Batch 450, Loss: 0.1907909631729126\n",
      "Epoch 11, Batch 500, Loss: 0.18023424252867698\n",
      "Epoch 11, Batch 550, Loss: 0.17875270575284957\n",
      "Epoch 11, Batch 600, Loss: 0.17189465448260308\n",
      "Epoch 11, Batch 650, Loss: 0.18762365520000457\n",
      "Epoch 11, Batch 700, Loss: 0.19632545739412308\n",
      "Epoch 11, Batch 750, Loss: 0.1803216455876827\n",
      "Epoch 11, Batch 800, Loss: 0.1734189710021019\n",
      "Epoch 11, Batch 850, Loss: 0.18592361465096474\n",
      "Epoch 11, Batch 900, Loss: 0.18252354249358177\n",
      "Epoch 12, Batch 50, Loss: 0.1837880703806877\n",
      "Epoch 12, Batch 100, Loss: 0.18140433758497237\n",
      "Epoch 12, Batch 150, Loss: 0.17660752043128014\n",
      "Epoch 12, Batch 200, Loss: 0.1792766682803631\n",
      "Epoch 12, Batch 250, Loss: 0.18294626951217652\n",
      "Epoch 12, Batch 300, Loss: 0.16984868720173835\n",
      "Epoch 12, Batch 350, Loss: 0.17536849692463874\n",
      "Epoch 12, Batch 400, Loss: 0.18859903186559676\n",
      "Epoch 12, Batch 450, Loss: 0.18582078099250793\n",
      "Epoch 12, Batch 500, Loss: 0.17210844054818153\n",
      "Epoch 12, Batch 550, Loss: 0.18095833748579027\n",
      "Epoch 12, Batch 600, Loss: 0.17784436404705048\n",
      "Epoch 12, Batch 650, Loss: 0.18585998997092246\n",
      "Epoch 12, Batch 700, Loss: 0.18246401876211166\n",
      "Epoch 12, Batch 750, Loss: 0.17788011714816093\n",
      "Epoch 12, Batch 800, Loss: 0.16258484050631522\n",
      "Epoch 12, Batch 850, Loss: 0.18986549064517022\n",
      "Epoch 12, Batch 900, Loss: 0.1670750157535076\n",
      "Epoch 13, Batch 50, Loss: 0.17410968154668807\n",
      "Epoch 13, Batch 100, Loss: 0.1777205301821232\n",
      "Epoch 13, Batch 150, Loss: 0.16997449219226837\n",
      "Epoch 13, Batch 200, Loss: 0.1654488141834736\n",
      "Epoch 13, Batch 250, Loss: 0.1638565666973591\n",
      "Epoch 13, Batch 300, Loss: 0.17975158751010895\n",
      "Epoch 13, Batch 350, Loss: 0.17184952408075332\n",
      "Epoch 13, Batch 400, Loss: 0.1789967454969883\n",
      "Epoch 13, Batch 450, Loss: 0.1664421434700489\n",
      "Epoch 13, Batch 500, Loss: 0.17923179551959037\n",
      "Epoch 13, Batch 550, Loss: 0.17122528851032257\n",
      "Epoch 13, Batch 600, Loss: 0.17148892670869828\n",
      "Epoch 13, Batch 650, Loss: 0.1734243842959404\n",
      "Epoch 13, Batch 700, Loss: 0.1743681874871254\n",
      "Epoch 13, Batch 750, Loss: 0.17884124040603638\n",
      "Epoch 13, Batch 800, Loss: 0.1732669021189213\n",
      "Epoch 13, Batch 850, Loss: 0.17652972757816315\n",
      "Epoch 13, Batch 900, Loss: 0.17785719126462937\n",
      "Epoch 14, Batch 50, Loss: 0.1627974633872509\n",
      "Epoch 14, Batch 100, Loss: 0.16580283090472223\n",
      "Epoch 14, Batch 150, Loss: 0.17659243419766427\n",
      "Epoch 14, Batch 200, Loss: 0.16639709427952767\n",
      "Epoch 14, Batch 250, Loss: 0.1749863861501217\n",
      "Epoch 14, Batch 300, Loss: 0.17106690242886544\n",
      "Epoch 14, Batch 350, Loss: 0.17769801571965219\n",
      "Epoch 14, Batch 400, Loss: 0.1689520847797394\n",
      "Epoch 14, Batch 450, Loss: 0.16994282558560372\n",
      "Epoch 14, Batch 500, Loss: 0.17317928671836852\n",
      "Epoch 14, Batch 550, Loss: 0.17268033549189568\n",
      "Epoch 14, Batch 600, Loss: 0.16739563301205634\n",
      "Epoch 14, Batch 650, Loss: 0.1620875458419323\n",
      "Epoch 14, Batch 700, Loss: 0.17973213419318199\n",
      "Epoch 14, Batch 750, Loss: 0.17194410905241966\n",
      "Epoch 14, Batch 800, Loss: 0.16475110203027726\n",
      "Epoch 14, Batch 850, Loss: 0.1639058670401573\n",
      "Epoch 14, Batch 900, Loss: 0.1696777883172035\n",
      "Epoch 15, Batch 50, Loss: 0.16149734154343606\n",
      "Epoch 15, Batch 100, Loss: 0.169011961966753\n",
      "Epoch 15, Batch 150, Loss: 0.16269826427102088\n",
      "Epoch 15, Batch 200, Loss: 0.15610577613115312\n",
      "Epoch 15, Batch 250, Loss: 0.16734783500432968\n",
      "Epoch 15, Batch 300, Loss: 0.17071016862988472\n",
      "Epoch 15, Batch 350, Loss: 0.150342009216547\n",
      "Epoch 15, Batch 400, Loss: 0.1740328398346901\n",
      "Epoch 15, Batch 450, Loss: 0.16768812090158464\n",
      "Epoch 15, Batch 500, Loss: 0.1758240443468094\n",
      "Epoch 15, Batch 550, Loss: 0.16676033183932304\n",
      "Epoch 15, Batch 600, Loss: 0.16942633852362632\n",
      "Epoch 15, Batch 650, Loss: 0.16990677922964095\n",
      "Epoch 15, Batch 700, Loss: 0.15454194620251654\n",
      "Epoch 15, Batch 750, Loss: 0.17137506917119028\n",
      "Epoch 15, Batch 800, Loss: 0.1570459073781967\n",
      "Epoch 15, Batch 850, Loss: 0.16576218143105506\n",
      "Epoch 15, Batch 900, Loss: 0.17737272396683693\n",
      "Epoch 16, Batch 50, Loss: 0.1491209001839161\n",
      "Epoch 16, Batch 100, Loss: 0.15715185001492502\n",
      "Epoch 16, Batch 150, Loss: 0.16237923502922058\n",
      "Epoch 16, Batch 200, Loss: 0.16761446610093117\n",
      "Epoch 16, Batch 250, Loss: 0.16504234559834002\n",
      "Epoch 16, Batch 300, Loss: 0.15944389700889589\n",
      "Epoch 16, Batch 350, Loss: 0.16040922954678535\n",
      "Epoch 16, Batch 400, Loss: 0.16935356989502906\n",
      "Epoch 16, Batch 450, Loss: 0.1695779387652874\n",
      "Epoch 16, Batch 500, Loss: 0.1659512497484684\n",
      "Epoch 16, Batch 550, Loss: 0.1702824841439724\n",
      "Epoch 16, Batch 600, Loss: 0.15952947810292245\n",
      "Epoch 16, Batch 650, Loss: 0.16972371101379394\n",
      "Epoch 16, Batch 700, Loss: 0.1612018033862114\n",
      "Epoch 16, Batch 750, Loss: 0.16030608952045441\n",
      "Epoch 16, Batch 800, Loss: 0.15912414386868476\n",
      "Epoch 16, Batch 850, Loss: 0.14978641748428345\n",
      "Epoch 16, Batch 900, Loss: 0.1638486321270466\n",
      "Epoch 17, Batch 50, Loss: 0.16545016422867775\n",
      "Epoch 17, Batch 100, Loss: 0.15423110127449036\n",
      "Epoch 17, Batch 150, Loss: 0.16109234020113944\n",
      "Epoch 17, Batch 200, Loss: 0.1542049840092659\n",
      "Epoch 17, Batch 250, Loss: 0.1445441336929798\n",
      "Epoch 17, Batch 300, Loss: 0.1597011797875166\n",
      "Epoch 17, Batch 350, Loss: 0.15667233169078826\n",
      "Epoch 17, Batch 400, Loss: 0.15826731458306312\n",
      "Epoch 17, Batch 450, Loss: 0.16782114043831825\n",
      "Epoch 17, Batch 500, Loss: 0.17050800919532777\n",
      "Epoch 17, Batch 550, Loss: 0.15444371186196804\n",
      "Epoch 17, Batch 600, Loss: 0.15407901927828788\n",
      "Epoch 17, Batch 650, Loss: 0.1621442225575447\n",
      "Epoch 17, Batch 700, Loss: 0.15373994529247284\n",
      "Epoch 17, Batch 750, Loss: 0.14925657764077185\n",
      "Epoch 17, Batch 800, Loss: 0.1525486183166504\n",
      "Epoch 17, Batch 850, Loss: 0.15867674365639686\n",
      "Epoch 17, Batch 900, Loss: 0.1710589498281479\n",
      "Epoch 18, Batch 50, Loss: 0.15696459501981735\n",
      "Epoch 18, Batch 100, Loss: 0.1438884097337723\n",
      "Epoch 18, Batch 150, Loss: 0.16184793084859847\n",
      "Epoch 18, Batch 200, Loss: 0.14404253855347635\n",
      "Epoch 18, Batch 250, Loss: 0.15419027492403983\n",
      "Epoch 18, Batch 300, Loss: 0.15429241940379143\n",
      "Epoch 18, Batch 350, Loss: 0.137497878074646\n",
      "Epoch 18, Batch 400, Loss: 0.15908674478530885\n",
      "Epoch 18, Batch 450, Loss: 0.15152786746621133\n",
      "Epoch 18, Batch 500, Loss: 0.15700260907411576\n",
      "Epoch 18, Batch 550, Loss: 0.15025715544819832\n",
      "Epoch 18, Batch 600, Loss: 0.1475702567398548\n",
      "Epoch 18, Batch 650, Loss: 0.15661376282572748\n",
      "Epoch 18, Batch 700, Loss: 0.1595848299562931\n",
      "Epoch 18, Batch 750, Loss: 0.16135733351111411\n",
      "Epoch 18, Batch 800, Loss: 0.14743887335062028\n",
      "Epoch 18, Batch 850, Loss: 0.16807389125227928\n",
      "Epoch 18, Batch 900, Loss: 0.16555190101265907\n",
      "Epoch 19, Batch 50, Loss: 0.1605530025064945\n",
      "Epoch 19, Batch 100, Loss: 0.15397029004991056\n",
      "Epoch 19, Batch 150, Loss: 0.15038556307554246\n",
      "Epoch 19, Batch 200, Loss: 0.14235990568995477\n",
      "Epoch 19, Batch 250, Loss: 0.15753654286265373\n",
      "Epoch 19, Batch 300, Loss: 0.15940685525536538\n",
      "Epoch 19, Batch 350, Loss: 0.1561729319393635\n",
      "Epoch 19, Batch 400, Loss: 0.16247083008289337\n",
      "Epoch 19, Batch 450, Loss: 0.15215584143996239\n",
      "Epoch 19, Batch 500, Loss: 0.14491275854408742\n",
      "Epoch 19, Batch 550, Loss: 0.1487128149718046\n",
      "Epoch 19, Batch 600, Loss: 0.1542185626924038\n",
      "Epoch 19, Batch 650, Loss: 0.15926099717617034\n",
      "Epoch 19, Batch 700, Loss: 0.14447389140725136\n",
      "Epoch 19, Batch 750, Loss: 0.14876003429293633\n",
      "Epoch 19, Batch 800, Loss: 0.15360238611698152\n",
      "Epoch 19, Batch 850, Loss: 0.14905718505382537\n",
      "Epoch 19, Batch 900, Loss: 0.15182173527777196\n",
      "Epoch 20, Batch 50, Loss: 0.14649808153510094\n",
      "Epoch 20, Batch 100, Loss: 0.15379185929894448\n",
      "Epoch 20, Batch 150, Loss: 0.1476052837073803\n",
      "Epoch 20, Batch 200, Loss: 0.15178286999464036\n",
      "Epoch 20, Batch 250, Loss: 0.15933027237653732\n",
      "Epoch 20, Batch 300, Loss: 0.14829450443387032\n",
      "Epoch 20, Batch 350, Loss: 0.1533503143489361\n",
      "Epoch 20, Batch 400, Loss: 0.13285349413752556\n",
      "Epoch 20, Batch 450, Loss: 0.15259339839220046\n",
      "Epoch 20, Batch 500, Loss: 0.15340618282556534\n",
      "Epoch 20, Batch 550, Loss: 0.14558930963277816\n",
      "Epoch 20, Batch 600, Loss: 0.14461677759885788\n",
      "Epoch 20, Batch 650, Loss: 0.14530766859650612\n",
      "Epoch 20, Batch 700, Loss: 0.16175217658281327\n",
      "Epoch 20, Batch 750, Loss: 0.14277399197220803\n",
      "Epoch 20, Batch 800, Loss: 0.14510517805814743\n",
      "Epoch 20, Batch 850, Loss: 0.1510180139541626\n",
      "Epoch 20, Batch 900, Loss: 0.15450047880411147\n",
      "Epoch 21, Batch 50, Loss: 0.16457755789160727\n",
      "Epoch 21, Batch 100, Loss: 0.15075145930051803\n",
      "Epoch 21, Batch 150, Loss: 0.15690356999635696\n",
      "Epoch 21, Batch 200, Loss: 0.14373918317258358\n",
      "Epoch 21, Batch 250, Loss: 0.15005373492836951\n",
      "Epoch 21, Batch 300, Loss: 0.14209483236074447\n",
      "Epoch 21, Batch 350, Loss: 0.1362713396549225\n",
      "Epoch 21, Batch 400, Loss: 0.14246067732572557\n",
      "Epoch 21, Batch 450, Loss: 0.15190624862909316\n",
      "Epoch 21, Batch 500, Loss: 0.1474893121421337\n",
      "Epoch 21, Batch 550, Loss: 0.13848155982792376\n",
      "Epoch 21, Batch 600, Loss: 0.14515763387084007\n",
      "Epoch 21, Batch 650, Loss: 0.1423148923367262\n",
      "Epoch 21, Batch 700, Loss: 0.14881729185581208\n",
      "Epoch 21, Batch 750, Loss: 0.1420475999265909\n",
      "Epoch 21, Batch 800, Loss: 0.14790022797882557\n",
      "Epoch 21, Batch 850, Loss: 0.15172523200511934\n",
      "Epoch 21, Batch 900, Loss: 0.1599727226793766\n",
      "Epoch 22, Batch 50, Loss: 0.14718025729060172\n",
      "Epoch 22, Batch 100, Loss: 0.12875661581754685\n",
      "Epoch 22, Batch 150, Loss: 0.14596538327634334\n",
      "Epoch 22, Batch 200, Loss: 0.1496254375576973\n",
      "Epoch 22, Batch 250, Loss: 0.1522911885380745\n",
      "Epoch 22, Batch 300, Loss: 0.13004872761666775\n",
      "Epoch 22, Batch 350, Loss: 0.14011120036244393\n",
      "Epoch 22, Batch 400, Loss: 0.14714792415499686\n",
      "Epoch 22, Batch 450, Loss: 0.14975394695997238\n",
      "Epoch 22, Batch 500, Loss: 0.15383495897054672\n",
      "Epoch 22, Batch 550, Loss: 0.15090564340353013\n",
      "Epoch 22, Batch 600, Loss: 0.1530362333357334\n",
      "Epoch 22, Batch 650, Loss: 0.13959261417388916\n",
      "Epoch 22, Batch 700, Loss: 0.13400729835033418\n",
      "Epoch 22, Batch 750, Loss: 0.1482020714879036\n",
      "Epoch 22, Batch 800, Loss: 0.14586047619581222\n",
      "Epoch 22, Batch 850, Loss: 0.13870948076248169\n",
      "Epoch 22, Batch 900, Loss: 0.1471789835393429\n",
      "Epoch 23, Batch 50, Loss: 0.1515363085269928\n",
      "Epoch 23, Batch 100, Loss: 0.14564019858837127\n",
      "Epoch 23, Batch 150, Loss: 0.1312156370282173\n",
      "Epoch 23, Batch 200, Loss: 0.13992371395230294\n",
      "Epoch 23, Batch 250, Loss: 0.15064167246222496\n",
      "Epoch 23, Batch 300, Loss: 0.14764360323548317\n",
      "Epoch 23, Batch 350, Loss: 0.14589797094464302\n",
      "Epoch 23, Batch 400, Loss: 0.13388986058533192\n",
      "Epoch 23, Batch 450, Loss: 0.14500251084566115\n",
      "Epoch 23, Batch 500, Loss: 0.14330024801194668\n",
      "Epoch 23, Batch 550, Loss: 0.15200374960899354\n",
      "Epoch 23, Batch 600, Loss: 0.14582732692360878\n",
      "Epoch 23, Batch 650, Loss: 0.14778662890195846\n",
      "Epoch 23, Batch 700, Loss: 0.14571494154632092\n",
      "Epoch 23, Batch 750, Loss: 0.14262947782874108\n",
      "Epoch 23, Batch 800, Loss: 0.1457771597057581\n",
      "Epoch 23, Batch 850, Loss: 0.13647446691989898\n",
      "Epoch 23, Batch 900, Loss: 0.12873268738389015\n",
      "Epoch 24, Batch 50, Loss: 0.14255803123116492\n",
      "Epoch 24, Batch 100, Loss: 0.14693903960287572\n",
      "Epoch 24, Batch 150, Loss: 0.132931190431118\n",
      "Epoch 24, Batch 200, Loss: 0.13849807292222976\n",
      "Epoch 24, Batch 250, Loss: 0.14405856505036355\n",
      "Epoch 24, Batch 300, Loss: 0.13756039574742318\n",
      "Epoch 24, Batch 350, Loss: 0.14854712694883346\n",
      "Epoch 24, Batch 400, Loss: 0.14125999316573143\n",
      "Epoch 24, Batch 450, Loss: 0.1431560654193163\n",
      "Epoch 24, Batch 500, Loss: 0.14547971948981286\n",
      "Epoch 24, Batch 550, Loss: 0.13945179134607316\n",
      "Epoch 24, Batch 600, Loss: 0.1459231571853161\n",
      "Epoch 24, Batch 650, Loss: 0.13226395323872567\n",
      "Epoch 24, Batch 700, Loss: 0.13218898847699165\n",
      "Epoch 24, Batch 750, Loss: 0.1426077838242054\n",
      "Epoch 24, Batch 800, Loss: 0.14757831618189812\n",
      "Epoch 24, Batch 850, Loss: 0.13160478964447975\n",
      "Epoch 24, Batch 900, Loss: 0.13916742503643037\n",
      "Epoch 25, Batch 50, Loss: 0.14523688673973084\n",
      "Epoch 25, Batch 100, Loss: 0.13830095484852792\n",
      "Epoch 25, Batch 150, Loss: 0.14116159155964852\n",
      "Epoch 25, Batch 200, Loss: 0.13490181915462018\n",
      "Epoch 25, Batch 250, Loss: 0.12920437559485434\n",
      "Epoch 25, Batch 300, Loss: 0.13379314430058004\n",
      "Epoch 25, Batch 350, Loss: 0.14844883374869824\n",
      "Epoch 25, Batch 400, Loss: 0.13968619629740714\n",
      "Epoch 25, Batch 450, Loss: 0.13554269641637803\n",
      "Epoch 25, Batch 500, Loss: 0.1364495934545994\n",
      "Epoch 25, Batch 550, Loss: 0.14156837768852712\n",
      "Epoch 25, Batch 600, Loss: 0.14069682627916336\n",
      "Epoch 25, Batch 650, Loss: 0.14391921572387217\n",
      "Epoch 25, Batch 700, Loss: 0.13153302304446698\n",
      "Epoch 25, Batch 750, Loss: 0.13071994468569756\n",
      "Epoch 25, Batch 800, Loss: 0.13423159450292588\n",
      "Epoch 25, Batch 850, Loss: 0.1495114804804325\n",
      "Epoch 25, Batch 900, Loss: 0.15361720830202102\n",
      "Accuracy on test set: 0.8569%\n",
      "Fitting for combination 44\n",
      "784\n",
      "3\n",
      "10\n",
      "[40, 10, 20, 10]\n",
      "False\n",
      "['relu', 'sigmoid', 'relu']\n",
      "Adam\n",
      "0.03\n",
      "0.01\n",
      "CrossEntropyLoss\n",
      "100\n",
      "25\n",
      "0\n",
      "Epoch 1, Batch 100, Loss: 1.608677237033844\n",
      "Epoch 1, Batch 200, Loss: 1.233922576904297\n",
      "Epoch 1, Batch 300, Loss: 1.105234861969948\n",
      "Epoch 1, Batch 400, Loss: 1.1060974311828613\n",
      "Epoch 1, Batch 500, Loss: 1.1131678956747055\n",
      "Epoch 1, Batch 600, Loss: 1.0659277975559234\n",
      "Epoch 1, Batch 700, Loss: 1.0481641453504562\n",
      "Epoch 1, Batch 800, Loss: 1.043284307718277\n",
      "Epoch 1, Batch 900, Loss: 1.0519252705574036\n",
      "Epoch 2, Batch 100, Loss: 1.0422882431745528\n",
      "Epoch 2, Batch 200, Loss: 0.9670922869443893\n",
      "Epoch 2, Batch 300, Loss: 0.9463808065652848\n",
      "Epoch 2, Batch 400, Loss: 0.9381278866529464\n",
      "Epoch 2, Batch 500, Loss: 0.9439881813526153\n",
      "Epoch 2, Batch 600, Loss: 0.9327156800031662\n",
      "Epoch 2, Batch 700, Loss: 1.0012887340784073\n",
      "Epoch 2, Batch 800, Loss: 0.9618257516622544\n",
      "Epoch 2, Batch 900, Loss: 1.0129827147722243\n",
      "Epoch 3, Batch 100, Loss: 0.9435907953977585\n",
      "Epoch 3, Batch 200, Loss: 0.9376692813634873\n",
      "Epoch 3, Batch 300, Loss: 0.9889929223060608\n",
      "Epoch 3, Batch 400, Loss: 0.9169949507713318\n",
      "Epoch 3, Batch 500, Loss: 0.9395358955860138\n",
      "Epoch 3, Batch 600, Loss: 0.9580819410085678\n",
      "Epoch 3, Batch 700, Loss: 0.9904050523042679\n",
      "Epoch 3, Batch 800, Loss: 0.9608521515130997\n",
      "Epoch 3, Batch 900, Loss: 0.8869269073009491\n",
      "Epoch 4, Batch 100, Loss: 0.9052298754453659\n",
      "Epoch 4, Batch 200, Loss: 0.9445420336723328\n",
      "Epoch 4, Batch 300, Loss: 0.977669090628624\n",
      "Epoch 4, Batch 400, Loss: 0.9824834847450257\n",
      "Epoch 4, Batch 500, Loss: 0.9130107802152634\n",
      "Epoch 4, Batch 600, Loss: 1.0030566549301148\n",
      "Epoch 4, Batch 700, Loss: 0.9477577829360961\n",
      "Epoch 4, Batch 800, Loss: 0.9833056622743607\n",
      "Epoch 4, Batch 900, Loss: 0.8955713617801666\n",
      "Epoch 5, Batch 100, Loss: 0.9141652131080628\n",
      "Epoch 5, Batch 200, Loss: 0.9156656527519226\n",
      "Epoch 5, Batch 300, Loss: 0.9438821649551392\n",
      "Epoch 5, Batch 400, Loss: 0.9085755217075348\n",
      "Epoch 5, Batch 500, Loss: 0.9110446113348007\n",
      "Epoch 5, Batch 600, Loss: 0.9189335316419601\n",
      "Epoch 5, Batch 700, Loss: 0.8988490790128708\n",
      "Epoch 5, Batch 800, Loss: 0.9400863188505173\n",
      "Epoch 5, Batch 900, Loss: 0.896334417462349\n",
      "Epoch 6, Batch 100, Loss: 0.9087490075826645\n",
      "Epoch 6, Batch 200, Loss: 0.8861605697870254\n",
      "Epoch 6, Batch 300, Loss: 0.9371315133571625\n",
      "Epoch 6, Batch 400, Loss: 0.9058841294050217\n",
      "Epoch 6, Batch 500, Loss: 0.9378915697336196\n",
      "Epoch 6, Batch 600, Loss: 0.9115119272470474\n",
      "Epoch 6, Batch 700, Loss: 0.9087684893608093\n",
      "Epoch 6, Batch 800, Loss: 0.9414760285615921\n",
      "Epoch 6, Batch 900, Loss: 0.8997978717088699\n",
      "Epoch 7, Batch 100, Loss: 0.9236841070652008\n",
      "Epoch 7, Batch 200, Loss: 0.9197742956876754\n",
      "Epoch 7, Batch 300, Loss: 0.8969525074958802\n",
      "Epoch 7, Batch 400, Loss: 0.9069890624284744\n",
      "Epoch 7, Batch 500, Loss: 0.9477415335178375\n",
      "Epoch 7, Batch 600, Loss: 0.9655571407079697\n",
      "Epoch 7, Batch 700, Loss: 0.9340204608440399\n",
      "Epoch 7, Batch 800, Loss: 0.9479637080430985\n",
      "Epoch 7, Batch 900, Loss: 0.8892490309476853\n",
      "Epoch 8, Batch 100, Loss: 0.916554514169693\n",
      "Epoch 8, Batch 200, Loss: 0.9160327261686325\n",
      "Epoch 8, Batch 300, Loss: 0.8876171225309372\n",
      "Epoch 8, Batch 400, Loss: 0.9143163132667541\n",
      "Epoch 8, Batch 500, Loss: 0.8798363846540451\n",
      "Epoch 8, Batch 600, Loss: 0.9279220592975617\n",
      "Epoch 8, Batch 700, Loss: 0.9516709864139556\n",
      "Epoch 8, Batch 800, Loss: 0.916230109333992\n",
      "Epoch 8, Batch 900, Loss: 0.9405791997909546\n",
      "Epoch 9, Batch 100, Loss: 0.9029159051179886\n",
      "Epoch 9, Batch 200, Loss: 0.8892594766616821\n",
      "Epoch 9, Batch 300, Loss: 0.9237091165781021\n",
      "Epoch 9, Batch 400, Loss: 0.9385275638103485\n",
      "Epoch 9, Batch 500, Loss: 0.9276236402988434\n",
      "Epoch 9, Batch 600, Loss: 0.9021394258737564\n",
      "Epoch 9, Batch 700, Loss: 0.9383119082450867\n",
      "Epoch 9, Batch 800, Loss: 0.9511251890659332\n",
      "Epoch 9, Batch 900, Loss: 0.9064276224374771\n",
      "Epoch 10, Batch 100, Loss: 0.9163889461755752\n",
      "Epoch 10, Batch 200, Loss: 0.9349874627590179\n",
      "Epoch 10, Batch 300, Loss: 0.9416179490089417\n",
      "Epoch 10, Batch 400, Loss: 0.9198788923025131\n",
      "Epoch 10, Batch 500, Loss: 0.9180649220943451\n",
      "Epoch 10, Batch 600, Loss: 0.9139652878046036\n",
      "Epoch 10, Batch 700, Loss: 0.939717811346054\n",
      "Epoch 10, Batch 800, Loss: 0.8980519860982895\n",
      "Epoch 10, Batch 900, Loss: 0.8371901619434357\n",
      "Epoch 11, Batch 100, Loss: 0.9337828207015991\n",
      "Epoch 11, Batch 200, Loss: 0.9378364849090576\n",
      "Epoch 11, Batch 300, Loss: 0.891874503493309\n",
      "Epoch 11, Batch 400, Loss: 0.8773915380239486\n",
      "Epoch 11, Batch 500, Loss: 0.9175427907705307\n",
      "Epoch 11, Batch 600, Loss: 0.9374068546295166\n",
      "Epoch 11, Batch 700, Loss: 0.911046969294548\n",
      "Epoch 11, Batch 800, Loss: 0.9402753561735153\n",
      "Epoch 11, Batch 900, Loss: 0.8742489582300186\n",
      "Epoch 12, Batch 100, Loss: 0.9351639121770858\n",
      "Epoch 12, Batch 200, Loss: 0.8712923496961593\n",
      "Epoch 12, Batch 300, Loss: 0.9492749434709549\n",
      "Epoch 12, Batch 400, Loss: 0.9003650057315826\n",
      "Epoch 12, Batch 500, Loss: 0.8957469409704208\n",
      "Epoch 12, Batch 600, Loss: 0.8680221968889237\n",
      "Epoch 12, Batch 700, Loss: 0.9070872062444687\n",
      "Epoch 12, Batch 800, Loss: 0.9009164273738861\n",
      "Epoch 12, Batch 900, Loss: 0.8848665016889572\n",
      "Epoch 13, Batch 100, Loss: 0.8829890829324722\n",
      "Epoch 13, Batch 200, Loss: 0.8992833125591279\n",
      "Epoch 13, Batch 300, Loss: 0.9184725916385651\n",
      "Epoch 13, Batch 400, Loss: 0.9246821069717407\n",
      "Epoch 13, Batch 500, Loss: 0.922593024969101\n",
      "Epoch 13, Batch 600, Loss: 0.9133549475669861\n",
      "Epoch 13, Batch 700, Loss: 0.9061256313323974\n",
      "Epoch 13, Batch 800, Loss: 0.9026793301105499\n",
      "Epoch 13, Batch 900, Loss: 0.920067617893219\n",
      "Epoch 14, Batch 100, Loss: 0.922481220960617\n",
      "Epoch 14, Batch 200, Loss: 0.872003762125969\n",
      "Epoch 14, Batch 300, Loss: 0.8762084233760834\n",
      "Epoch 14, Batch 400, Loss: 0.9007146495580673\n",
      "Epoch 14, Batch 500, Loss: 0.9228489470481872\n",
      "Epoch 14, Batch 600, Loss: 0.8692550563812256\n",
      "Epoch 14, Batch 700, Loss: 0.9252032893896103\n",
      "Epoch 14, Batch 800, Loss: 0.8964534455537796\n",
      "Epoch 14, Batch 900, Loss: 0.913473196029663\n",
      "Epoch 15, Batch 100, Loss: 0.8897120571136474\n",
      "Epoch 15, Batch 200, Loss: 0.9323396420478821\n",
      "Epoch 15, Batch 300, Loss: 0.9275542414188385\n",
      "Epoch 15, Batch 400, Loss: 0.9573270267248154\n",
      "Epoch 15, Batch 500, Loss: 0.9066886812448501\n",
      "Epoch 15, Batch 600, Loss: 0.8925759363174438\n",
      "Epoch 15, Batch 700, Loss: 0.8626002705097199\n",
      "Epoch 15, Batch 800, Loss: 0.8890703135728836\n",
      "Epoch 15, Batch 900, Loss: 0.8863500362634659\n",
      "Epoch 16, Batch 100, Loss: 0.921848959326744\n",
      "Epoch 16, Batch 200, Loss: 0.8820454424619675\n",
      "Epoch 16, Batch 300, Loss: 0.8710416233539582\n",
      "Epoch 16, Batch 400, Loss: 0.907057119011879\n",
      "Epoch 16, Batch 500, Loss: 0.8903569984436035\n",
      "Epoch 16, Batch 600, Loss: 0.9027730655670166\n",
      "Epoch 16, Batch 700, Loss: 0.900570655465126\n",
      "Epoch 16, Batch 800, Loss: 0.9447001254558564\n",
      "Epoch 16, Batch 900, Loss: 0.889352851510048\n",
      "Epoch 17, Batch 100, Loss: 0.9060858500003814\n",
      "Epoch 17, Batch 200, Loss: 0.9108557581901551\n",
      "Epoch 17, Batch 300, Loss: 0.8925985449552536\n",
      "Epoch 17, Batch 400, Loss: 0.8834720128774642\n",
      "Epoch 17, Batch 500, Loss: 0.8932761669158935\n",
      "Epoch 17, Batch 600, Loss: 0.8821345061063767\n",
      "Epoch 17, Batch 700, Loss: 0.8770430558919906\n",
      "Epoch 17, Batch 800, Loss: 0.9134509491920472\n",
      "Epoch 17, Batch 900, Loss: 0.9221598118543625\n",
      "Epoch 18, Batch 100, Loss: 0.9101201546192169\n",
      "Epoch 18, Batch 200, Loss: 0.8531448155641556\n",
      "Epoch 18, Batch 300, Loss: 0.9045417362451553\n",
      "Epoch 18, Batch 400, Loss: 0.8803458935022355\n",
      "Epoch 18, Batch 500, Loss: 0.8964223855733872\n",
      "Epoch 18, Batch 600, Loss: 0.8801951462030411\n",
      "Epoch 18, Batch 700, Loss: 0.8953517496585846\n",
      "Epoch 18, Batch 800, Loss: 0.9094682085514069\n",
      "Epoch 18, Batch 900, Loss: 0.9004053443670272\n",
      "Epoch 19, Batch 100, Loss: 0.9106908565759659\n",
      "Epoch 19, Batch 200, Loss: 0.9238970339298248\n",
      "Epoch 19, Batch 300, Loss: 0.9198315244913101\n",
      "Epoch 19, Batch 400, Loss: 0.9330675578117371\n",
      "Epoch 19, Batch 500, Loss: 0.9735434013605118\n",
      "Epoch 19, Batch 600, Loss: 0.9440899425745011\n",
      "Epoch 19, Batch 700, Loss: 0.9061175119876862\n",
      "Epoch 19, Batch 800, Loss: 0.9062282866239548\n",
      "Epoch 19, Batch 900, Loss: 0.9149323308467865\n",
      "Epoch 20, Batch 100, Loss: 0.8972586506605148\n",
      "Epoch 20, Batch 200, Loss: 0.9708390146493912\n",
      "Epoch 20, Batch 300, Loss: 0.8999969619512558\n",
      "Epoch 20, Batch 400, Loss: 0.9373529767990112\n",
      "Epoch 20, Batch 500, Loss: 0.8741113924980164\n",
      "Epoch 20, Batch 600, Loss: 0.9328662884235382\n",
      "Epoch 20, Batch 700, Loss: 0.899957275390625\n",
      "Epoch 20, Batch 800, Loss: 0.8849985486268998\n",
      "Epoch 20, Batch 900, Loss: 0.9059116524457932\n",
      "Epoch 21, Batch 100, Loss: 0.9510163456201554\n",
      "Epoch 21, Batch 200, Loss: 0.870934447646141\n",
      "Epoch 21, Batch 300, Loss: 0.9094069641828537\n",
      "Epoch 21, Batch 400, Loss: 0.8854556906223298\n",
      "Epoch 21, Batch 500, Loss: 0.8883428794145584\n",
      "Epoch 21, Batch 600, Loss: 0.8891898989677429\n",
      "Epoch 21, Batch 700, Loss: 0.882629057765007\n",
      "Epoch 21, Batch 800, Loss: 0.933222039937973\n",
      "Epoch 21, Batch 900, Loss: 0.8863823872804641\n",
      "Epoch 22, Batch 100, Loss: 0.8941425025463104\n",
      "Epoch 22, Batch 200, Loss: 0.9067228001356125\n",
      "Epoch 22, Batch 300, Loss: 0.8911057060956955\n",
      "Epoch 22, Batch 400, Loss: 0.9430000734329224\n",
      "Epoch 22, Batch 500, Loss: 0.9146882551908493\n",
      "Epoch 22, Batch 600, Loss: 0.8686749541759491\n",
      "Epoch 22, Batch 700, Loss: 0.8826058512926102\n",
      "Epoch 22, Batch 800, Loss: 0.9215727108716965\n",
      "Epoch 22, Batch 900, Loss: 0.8930254727602005\n",
      "Epoch 23, Batch 100, Loss: 0.9204814445972442\n",
      "Epoch 23, Batch 200, Loss: 0.9200429224967956\n",
      "Epoch 23, Batch 300, Loss: 0.9267507088184357\n",
      "Epoch 23, Batch 400, Loss: 0.919647998213768\n",
      "Epoch 23, Batch 500, Loss: 0.8601216280460358\n",
      "Epoch 23, Batch 600, Loss: 0.8897527450323105\n",
      "Epoch 23, Batch 700, Loss: 0.9134625428915024\n",
      "Epoch 23, Batch 800, Loss: 0.8962075352668762\n",
      "Epoch 23, Batch 900, Loss: 0.8604614394903183\n",
      "Epoch 24, Batch 100, Loss: 0.9008066195249558\n",
      "Epoch 24, Batch 200, Loss: 0.9543078482151032\n",
      "Epoch 24, Batch 300, Loss: 0.8758579832315445\n",
      "Epoch 24, Batch 400, Loss: 0.9145429730415344\n",
      "Epoch 24, Batch 500, Loss: 0.9032904607057571\n",
      "Epoch 24, Batch 600, Loss: 0.8967050313949585\n",
      "Epoch 24, Batch 700, Loss: 0.9678246611356736\n",
      "Epoch 24, Batch 800, Loss: 0.9106092458963394\n",
      "Epoch 24, Batch 900, Loss: 0.9337755113840103\n",
      "Epoch 25, Batch 100, Loss: 0.8883108222484588\n",
      "Epoch 25, Batch 200, Loss: 0.9163581877946854\n",
      "Epoch 25, Batch 300, Loss: 0.9595219373703003\n",
      "Epoch 25, Batch 400, Loss: 0.9636311483383179\n",
      "Epoch 25, Batch 500, Loss: 0.9437151217460632\n",
      "Epoch 25, Batch 600, Loss: 0.9449598056077957\n",
      "Epoch 25, Batch 700, Loss: 0.9806365096569061\n",
      "Epoch 25, Batch 800, Loss: 0.9460633468627929\n",
      "Epoch 25, Batch 900, Loss: 0.903194613456726\n",
      "Accuracy on test set: 0.6856%\n",
      "Fitting for combination 45\n",
      "784\n",
      "3\n",
      "10\n",
      "[40, 10, 20, 10]\n",
      "False\n",
      "['relu', 'sigmoid', 'tanh']\n",
      "SGD\n",
      "0.1\n",
      "0.03\n",
      "CrossEntropyLoss\n",
      "200\n",
      "25\n",
      "0\n",
      "Epoch 1, Batch 200, Loss: 4.174265592098236\n",
      "Epoch 1, Batch 400, Loss: 3.099633915424347\n",
      "Epoch 1, Batch 600, Loss: 2.6472872507572176\n",
      "Epoch 1, Batch 800, Loss: 2.539733484983444\n",
      "Epoch 2, Batch 200, Loss: 2.4593120646476745\n",
      "Epoch 2, Batch 400, Loss: 2.441330372095108\n",
      "Epoch 2, Batch 600, Loss: 2.4042553877830506\n",
      "Epoch 2, Batch 800, Loss: 2.357563777565956\n",
      "Epoch 3, Batch 200, Loss: 2.2949448019266128\n",
      "Epoch 3, Batch 400, Loss: 2.2679317134618757\n",
      "Epoch 3, Batch 600, Loss: 2.206130332350731\n",
      "Epoch 3, Batch 800, Loss: 2.175747110247612\n",
      "Epoch 4, Batch 200, Loss: 2.096174002289772\n",
      "Epoch 4, Batch 400, Loss: 2.0857593035697937\n",
      "Epoch 4, Batch 600, Loss: 2.0741214215755464\n",
      "Epoch 4, Batch 800, Loss: 2.0209726852178576\n",
      "Epoch 5, Batch 200, Loss: 2.003984971642494\n",
      "Epoch 5, Batch 400, Loss: 1.9898804593086243\n",
      "Epoch 5, Batch 600, Loss: 1.964459074139595\n",
      "Epoch 5, Batch 800, Loss: 1.976769294142723\n",
      "Epoch 6, Batch 200, Loss: 1.961754440665245\n",
      "Epoch 6, Batch 400, Loss: 1.9334305447340012\n",
      "Epoch 6, Batch 600, Loss: 1.9233745855093003\n",
      "Epoch 6, Batch 800, Loss: 1.9272538083791733\n",
      "Epoch 7, Batch 200, Loss: 1.8983443915843963\n",
      "Epoch 7, Batch 400, Loss: 1.9212888556718826\n",
      "Epoch 7, Batch 600, Loss: 1.8982715481519699\n",
      "Epoch 7, Batch 800, Loss: 1.9122082096338273\n",
      "Epoch 8, Batch 200, Loss: 1.8958253484964371\n",
      "Epoch 8, Batch 400, Loss: 1.903962709903717\n",
      "Epoch 8, Batch 600, Loss: 1.8985772502422333\n",
      "Epoch 8, Batch 800, Loss: 1.9087720602750777\n",
      "Epoch 9, Batch 200, Loss: 1.8937348514795302\n",
      "Epoch 9, Batch 400, Loss: 1.9017274463176728\n",
      "Epoch 9, Batch 600, Loss: 1.9127672481536866\n",
      "Epoch 9, Batch 800, Loss: 1.8937309932708741\n",
      "Epoch 10, Batch 200, Loss: 1.9140997761487961\n",
      "Epoch 10, Batch 400, Loss: 1.9070421308279037\n",
      "Epoch 10, Batch 600, Loss: 1.8803153240680694\n",
      "Epoch 10, Batch 800, Loss: 1.8831845927238464\n",
      "Epoch 11, Batch 200, Loss: 1.8866942256689072\n",
      "Epoch 11, Batch 400, Loss: 1.884911567568779\n",
      "Epoch 11, Batch 600, Loss: 1.9005636823177339\n",
      "Epoch 11, Batch 800, Loss: 1.9059130001068114\n",
      "Epoch 12, Batch 200, Loss: 1.8828276991844177\n",
      "Epoch 12, Batch 400, Loss: 1.8838191419839858\n",
      "Epoch 12, Batch 600, Loss: 1.9006979739665986\n",
      "Epoch 12, Batch 800, Loss: 1.8902423560619355\n",
      "Epoch 13, Batch 200, Loss: 1.8767815375328063\n",
      "Epoch 13, Batch 400, Loss: 1.8954593700170517\n",
      "Epoch 13, Batch 600, Loss: 1.9042262852191925\n",
      "Epoch 13, Batch 800, Loss: 1.9019763147830964\n",
      "Epoch 14, Batch 200, Loss: 1.870013313293457\n",
      "Epoch 14, Batch 400, Loss: 1.8745609694719314\n",
      "Epoch 14, Batch 600, Loss: 1.892908280491829\n",
      "Epoch 14, Batch 800, Loss: 1.9024755072593689\n",
      "Epoch 15, Batch 200, Loss: 1.906727740764618\n",
      "Epoch 15, Batch 400, Loss: 1.8868562519550323\n",
      "Epoch 15, Batch 600, Loss: 1.8836790555715561\n",
      "Epoch 15, Batch 800, Loss: 1.8966942024230957\n",
      "Epoch 16, Batch 200, Loss: 1.868245707154274\n",
      "Epoch 16, Batch 400, Loss: 1.894574271440506\n",
      "Epoch 16, Batch 600, Loss: 1.9069696021080018\n",
      "Epoch 16, Batch 800, Loss: 1.880517328977585\n",
      "Epoch 17, Batch 200, Loss: 1.886964654326439\n",
      "Epoch 17, Batch 400, Loss: 1.8681471097469329\n",
      "Epoch 17, Batch 600, Loss: 1.8790291386842728\n",
      "Epoch 17, Batch 800, Loss: 1.908254627585411\n",
      "Epoch 18, Batch 200, Loss: 1.8770777332782744\n",
      "Epoch 18, Batch 400, Loss: 1.9140603429079055\n",
      "Epoch 18, Batch 600, Loss: 1.8902926635742188\n",
      "Epoch 18, Batch 800, Loss: 1.8853045785427094\n",
      "Epoch 19, Batch 200, Loss: 1.8944367396831512\n",
      "Epoch 19, Batch 400, Loss: 1.8589826476573945\n",
      "Epoch 19, Batch 600, Loss: 1.9029421073198318\n",
      "Epoch 19, Batch 800, Loss: 1.9098912566900252\n",
      "Epoch 20, Batch 200, Loss: 1.882238432765007\n",
      "Epoch 20, Batch 400, Loss: 1.8701383399963378\n",
      "Epoch 20, Batch 600, Loss: 1.8723075169324874\n",
      "Epoch 20, Batch 800, Loss: 1.9081751608848572\n",
      "Epoch 21, Batch 200, Loss: 1.8956096452474593\n",
      "Epoch 21, Batch 400, Loss: 1.8852657479047776\n",
      "Epoch 21, Batch 600, Loss: 1.8889718478918076\n",
      "Epoch 21, Batch 800, Loss: 1.8911579233407974\n",
      "Epoch 22, Batch 200, Loss: 1.8700203400850297\n",
      "Epoch 22, Batch 400, Loss: 1.8955581730604172\n",
      "Epoch 22, Batch 600, Loss: 1.898981481194496\n",
      "Epoch 22, Batch 800, Loss: 1.9151267832517624\n",
      "Epoch 23, Batch 200, Loss: 1.8927510404586791\n",
      "Epoch 23, Batch 400, Loss: 1.8716562223434448\n",
      "Epoch 23, Batch 600, Loss: 1.9064647728204727\n",
      "Epoch 23, Batch 800, Loss: 1.8771554553508758\n",
      "Epoch 24, Batch 200, Loss: 1.8994429767131806\n",
      "Epoch 24, Batch 400, Loss: 1.903117899298668\n",
      "Epoch 24, Batch 600, Loss: 1.8866623371839524\n",
      "Epoch 24, Batch 800, Loss: 1.8824180203676224\n",
      "Epoch 25, Batch 200, Loss: 1.9006228017807008\n",
      "Epoch 25, Batch 400, Loss: 1.9010767704248428\n",
      "Epoch 25, Batch 600, Loss: 1.8808171194791794\n",
      "Epoch 25, Batch 800, Loss: 1.8823532032966614\n",
      "Accuracy on test set: 0.7192%\n",
      "Fitting for combination 46\n",
      "784\n",
      "3\n",
      "10\n",
      "[40, 10, 20, 10]\n",
      "False\n",
      "['relu', 'sigmoid', 'tanh']\n",
      "Adam\n",
      "0.01\n",
      "0.3\n",
      "CrossEntropyLoss\n",
      "100\n",
      "25\n",
      "0\n",
      "Epoch 1, Batch 100, Loss: 2.3041640639305117\n",
      "Epoch 1, Batch 200, Loss: 2.302771277427673\n",
      "Epoch 1, Batch 300, Loss: 2.302753601074219\n",
      "Epoch 1, Batch 400, Loss: 2.3031068420410157\n",
      "Epoch 1, Batch 500, Loss: 2.302755389213562\n",
      "Epoch 1, Batch 600, Loss: 2.3028766107559204\n",
      "Epoch 1, Batch 700, Loss: 2.302978537082672\n",
      "Epoch 1, Batch 800, Loss: 2.3024283838272095\n",
      "Epoch 1, Batch 900, Loss: 2.303057053089142\n",
      "Epoch 2, Batch 100, Loss: 2.3025983333587647\n",
      "Epoch 2, Batch 200, Loss: 2.302731466293335\n",
      "Epoch 2, Batch 300, Loss: 2.3027556848526003\n",
      "Epoch 2, Batch 400, Loss: 2.3026589584350585\n",
      "Epoch 2, Batch 500, Loss: 2.302512640953064\n",
      "Epoch 2, Batch 600, Loss: 2.3027846813201904\n",
      "Epoch 2, Batch 700, Loss: 2.3027353143692015\n",
      "Epoch 2, Batch 800, Loss: 2.3026699209213257\n",
      "Epoch 2, Batch 900, Loss: 2.3025303888320923\n",
      "Epoch 3, Batch 100, Loss: 2.3029777932167055\n",
      "Epoch 3, Batch 200, Loss: 2.3028956484794616\n",
      "Epoch 3, Batch 300, Loss: 2.302541363239288\n",
      "Epoch 3, Batch 400, Loss: 2.3027526140213013\n",
      "Epoch 3, Batch 500, Loss: 2.302946186065674\n",
      "Epoch 3, Batch 600, Loss: 2.302417848110199\n",
      "Epoch 3, Batch 700, Loss: 2.302469913959503\n",
      "Epoch 3, Batch 800, Loss: 2.302595708370209\n",
      "Epoch 3, Batch 900, Loss: 2.303034152984619\n",
      "Epoch 4, Batch 100, Loss: 2.302692174911499\n",
      "Epoch 4, Batch 200, Loss: 2.3029052734375\n",
      "Epoch 4, Batch 300, Loss: 2.3032390308380126\n",
      "Epoch 4, Batch 400, Loss: 2.3029584097862243\n",
      "Epoch 4, Batch 500, Loss: 2.30268363237381\n",
      "Epoch 4, Batch 600, Loss: 2.302990081310272\n",
      "Epoch 4, Batch 700, Loss: 2.302754609584808\n",
      "Epoch 4, Batch 800, Loss: 2.3028030824661254\n",
      "Epoch 4, Batch 900, Loss: 2.3031717228889463\n",
      "Epoch 5, Batch 100, Loss: 2.3026908373832704\n",
      "Epoch 5, Batch 200, Loss: 2.3028017807006838\n",
      "Epoch 5, Batch 300, Loss: 2.302683861255646\n",
      "Epoch 5, Batch 400, Loss: 2.3028594923019408\n",
      "Epoch 5, Batch 500, Loss: 2.302970595359802\n",
      "Epoch 5, Batch 600, Loss: 2.302963228225708\n",
      "Epoch 5, Batch 700, Loss: 2.3027776527404784\n",
      "Epoch 5, Batch 800, Loss: 2.3029056572914124\n",
      "Epoch 5, Batch 900, Loss: 2.3028821253776552\n",
      "Epoch 6, Batch 100, Loss: 2.3025641512870787\n",
      "Epoch 6, Batch 200, Loss: 2.302705147266388\n",
      "Epoch 6, Batch 300, Loss: 2.3030283308029174\n",
      "Epoch 6, Batch 400, Loss: 2.302513613700867\n",
      "Epoch 6, Batch 500, Loss: 2.3030866384506226\n",
      "Epoch 6, Batch 600, Loss: 2.302589032649994\n",
      "Epoch 6, Batch 700, Loss: 2.302871911525726\n",
      "Epoch 6, Batch 800, Loss: 2.30256206035614\n",
      "Epoch 6, Batch 900, Loss: 2.3027955675125122\n",
      "Epoch 7, Batch 100, Loss: 2.302842800617218\n",
      "Epoch 7, Batch 200, Loss: 2.302955002784729\n",
      "Epoch 7, Batch 300, Loss: 2.303142395019531\n",
      "Epoch 7, Batch 400, Loss: 2.3028922986984255\n",
      "Epoch 7, Batch 500, Loss: 2.3031060433387758\n",
      "Epoch 7, Batch 600, Loss: 2.3027270531654356\n",
      "Epoch 7, Batch 700, Loss: 2.3030532002449036\n",
      "Epoch 7, Batch 800, Loss: 2.3029932093620302\n",
      "Epoch 7, Batch 900, Loss: 2.302797954082489\n",
      "Epoch 8, Batch 100, Loss: 2.3026813673973083\n",
      "Epoch 8, Batch 200, Loss: 2.3027565336227416\n",
      "Epoch 8, Batch 300, Loss: 2.3032247161865236\n",
      "Epoch 8, Batch 400, Loss: 2.302534964084625\n",
      "Epoch 8, Batch 500, Loss: 2.3029752373695374\n",
      "Epoch 8, Batch 600, Loss: 2.302996509075165\n",
      "Epoch 8, Batch 700, Loss: 2.303127610683441\n",
      "Epoch 8, Batch 800, Loss: 2.302901790142059\n",
      "Epoch 8, Batch 900, Loss: 2.302618978023529\n",
      "Epoch 9, Batch 100, Loss: 2.3027442359924315\n",
      "Epoch 9, Batch 200, Loss: 2.3030965065956117\n",
      "Epoch 9, Batch 300, Loss: 2.3028226852416993\n",
      "Epoch 9, Batch 400, Loss: 2.3028799510002136\n",
      "Epoch 9, Batch 500, Loss: 2.3029960012435913\n",
      "Epoch 9, Batch 600, Loss: 2.303159098625183\n",
      "Epoch 9, Batch 700, Loss: 2.3025509548187255\n",
      "Epoch 9, Batch 800, Loss: 2.3026181983947756\n",
      "Epoch 9, Batch 900, Loss: 2.3025472068786623\n",
      "Epoch 10, Batch 100, Loss: 2.3025785231590272\n",
      "Epoch 10, Batch 200, Loss: 2.3027876377105714\n",
      "Epoch 10, Batch 300, Loss: 2.3027871894836425\n",
      "Epoch 10, Batch 400, Loss: 2.302875208854675\n",
      "Epoch 10, Batch 500, Loss: 2.3025570631027223\n",
      "Epoch 10, Batch 600, Loss: 2.30282345533371\n",
      "Epoch 10, Batch 700, Loss: 2.3026529717445374\n",
      "Epoch 10, Batch 800, Loss: 2.3028416061401367\n",
      "Epoch 10, Batch 900, Loss: 2.303216264247894\n",
      "Epoch 11, Batch 100, Loss: 2.3027282476425173\n",
      "Epoch 11, Batch 200, Loss: 2.3027812337875364\n",
      "Epoch 11, Batch 300, Loss: 2.3028344702720642\n",
      "Epoch 11, Batch 400, Loss: 2.3031070828437805\n",
      "Epoch 11, Batch 500, Loss: 2.3027746844291688\n",
      "Epoch 11, Batch 600, Loss: 2.3031375551223756\n",
      "Epoch 11, Batch 700, Loss: 2.303068907260895\n",
      "Epoch 11, Batch 800, Loss: 2.303111138343811\n",
      "Epoch 11, Batch 900, Loss: 2.3031355476379396\n",
      "Epoch 12, Batch 100, Loss: 2.3024437642097473\n",
      "Epoch 12, Batch 200, Loss: 2.3027399468421934\n",
      "Epoch 12, Batch 300, Loss: 2.3030356764793396\n",
      "Epoch 12, Batch 400, Loss: 2.30245311498642\n",
      "Epoch 12, Batch 500, Loss: 2.3031341648101806\n",
      "Epoch 12, Batch 600, Loss: 2.303098564147949\n",
      "Epoch 12, Batch 700, Loss: 2.3026996517181395\n",
      "Epoch 12, Batch 800, Loss: 2.3028653335571287\n",
      "Epoch 12, Batch 900, Loss: 2.3031179475784302\n",
      "Epoch 13, Batch 100, Loss: 2.302893724441528\n",
      "Epoch 13, Batch 200, Loss: 2.302617609500885\n",
      "Epoch 13, Batch 300, Loss: 2.3027351260185243\n",
      "Epoch 13, Batch 400, Loss: 2.3028135180473326\n",
      "Epoch 13, Batch 500, Loss: 2.303033034801483\n",
      "Epoch 13, Batch 600, Loss: 2.3022814345359803\n",
      "Epoch 13, Batch 700, Loss: 2.3026452708244323\n",
      "Epoch 13, Batch 800, Loss: 2.3026737928390504\n",
      "Epoch 13, Batch 900, Loss: 2.3028813290596006\n",
      "Epoch 14, Batch 100, Loss: 2.3027852392196655\n",
      "Epoch 14, Batch 200, Loss: 2.3024369215965272\n",
      "Epoch 14, Batch 300, Loss: 2.302671666145325\n",
      "Epoch 14, Batch 400, Loss: 2.3027809715270995\n",
      "Epoch 14, Batch 500, Loss: 2.302559537887573\n",
      "Epoch 14, Batch 600, Loss: 2.302753043174744\n",
      "Epoch 14, Batch 700, Loss: 2.3030475330352784\n",
      "Epoch 14, Batch 800, Loss: 2.3027316236495974\n",
      "Epoch 14, Batch 900, Loss: 2.302808012962341\n",
      "Epoch 15, Batch 100, Loss: 2.3029534363746644\n",
      "Epoch 15, Batch 200, Loss: 2.3028701400756835\n",
      "Epoch 15, Batch 300, Loss: 2.302717046737671\n",
      "Epoch 15, Batch 400, Loss: 2.303000338077545\n",
      "Epoch 15, Batch 500, Loss: 2.303012237548828\n",
      "Epoch 15, Batch 600, Loss: 2.302929286956787\n",
      "Epoch 15, Batch 700, Loss: 2.302882716655731\n",
      "Epoch 15, Batch 800, Loss: 2.302978525161743\n",
      "Epoch 15, Batch 900, Loss: 2.3029487109184266\n",
      "Epoch 16, Batch 100, Loss: 2.3031203317642213\n",
      "Epoch 16, Batch 200, Loss: 2.3029051065444945\n",
      "Epoch 16, Batch 300, Loss: 2.3028753900527956\n",
      "Epoch 16, Batch 400, Loss: 2.3029268383979797\n",
      "Epoch 16, Batch 500, Loss: 2.302695245742798\n",
      "Epoch 16, Batch 600, Loss: 2.302985405921936\n",
      "Epoch 16, Batch 700, Loss: 2.303085289001465\n",
      "Epoch 16, Batch 800, Loss: 2.3027824330329896\n",
      "Epoch 16, Batch 900, Loss: 2.3032227754592896\n",
      "Epoch 17, Batch 100, Loss: 2.302579176425934\n",
      "Epoch 17, Batch 200, Loss: 2.3027439546585082\n",
      "Epoch 17, Batch 300, Loss: 2.3028606081008913\n",
      "Epoch 17, Batch 400, Loss: 2.3029242897033693\n",
      "Epoch 17, Batch 500, Loss: 2.30219605922699\n",
      "Epoch 17, Batch 600, Loss: 2.3029705023765565\n",
      "Epoch 17, Batch 700, Loss: 2.302762713432312\n",
      "Epoch 17, Batch 800, Loss: 2.3027508091926574\n",
      "Epoch 17, Batch 900, Loss: 2.3029906344413758\n",
      "Epoch 18, Batch 100, Loss: 2.3027649998664854\n",
      "Epoch 18, Batch 200, Loss: 2.3027264046669007\n",
      "Epoch 18, Batch 300, Loss: 2.303092257976532\n",
      "Epoch 18, Batch 400, Loss: 2.302602982521057\n",
      "Epoch 18, Batch 500, Loss: 2.302681529521942\n",
      "Epoch 18, Batch 600, Loss: 2.3032184529304502\n",
      "Epoch 18, Batch 700, Loss: 2.3027214407920837\n",
      "Epoch 18, Batch 800, Loss: 2.3032292985916136\n",
      "Epoch 18, Batch 900, Loss: 2.3027987337112426\n",
      "Epoch 19, Batch 100, Loss: 2.302624680995941\n",
      "Epoch 19, Batch 200, Loss: 2.3026888084411623\n",
      "Epoch 19, Batch 300, Loss: 2.3029457044601442\n",
      "Epoch 19, Batch 400, Loss: 2.3026825261116026\n",
      "Epoch 19, Batch 500, Loss: 2.3028055524826048\n",
      "Epoch 19, Batch 600, Loss: 2.302744369506836\n",
      "Epoch 19, Batch 700, Loss: 2.3029999899864197\n",
      "Epoch 19, Batch 800, Loss: 2.3029251217842104\n",
      "Epoch 19, Batch 900, Loss: 2.302801744937897\n",
      "Epoch 20, Batch 100, Loss: 2.3028200817108155\n",
      "Epoch 20, Batch 200, Loss: 2.3025384163856506\n",
      "Epoch 20, Batch 300, Loss: 2.3031234169006347\n",
      "Epoch 20, Batch 400, Loss: 2.3027905225753784\n",
      "Epoch 20, Batch 500, Loss: 2.3029562544822695\n",
      "Epoch 20, Batch 600, Loss: 2.3028429174423217\n",
      "Epoch 20, Batch 700, Loss: 2.302694008350372\n",
      "Epoch 20, Batch 800, Loss: 2.303032171726227\n",
      "Epoch 20, Batch 900, Loss: 2.3030871891975404\n",
      "Epoch 21, Batch 100, Loss: 2.3027223539352417\n",
      "Epoch 21, Batch 200, Loss: 2.3035994172096252\n",
      "Epoch 21, Batch 300, Loss: 2.303169231414795\n",
      "Epoch 21, Batch 400, Loss: 2.3021652483940125\n",
      "Epoch 21, Batch 500, Loss: 2.302451214790344\n",
      "Epoch 21, Batch 600, Loss: 2.303045175075531\n",
      "Epoch 21, Batch 700, Loss: 2.3027180671691894\n",
      "Epoch 21, Batch 800, Loss: 2.302840268611908\n",
      "Epoch 21, Batch 900, Loss: 2.3031877422332765\n",
      "Epoch 22, Batch 100, Loss: 2.303115196228027\n",
      "Epoch 22, Batch 200, Loss: 2.3033092331886293\n",
      "Epoch 22, Batch 300, Loss: 2.3028200602531435\n",
      "Epoch 22, Batch 400, Loss: 2.3031753969192503\n",
      "Epoch 22, Batch 500, Loss: 2.3030018115043642\n",
      "Epoch 22, Batch 600, Loss: 2.3029587388038637\n",
      "Epoch 22, Batch 700, Loss: 2.302954919338226\n",
      "Epoch 22, Batch 800, Loss: 2.3027477788925172\n",
      "Epoch 22, Batch 900, Loss: 2.3031437468528746\n",
      "Epoch 23, Batch 100, Loss: 2.3029583954811095\n",
      "Epoch 23, Batch 200, Loss: 2.303080379962921\n",
      "Epoch 23, Batch 300, Loss: 2.3031968998909\n",
      "Epoch 23, Batch 400, Loss: 2.3028301858901976\n",
      "Epoch 23, Batch 500, Loss: 2.3033863592147825\n",
      "Epoch 23, Batch 600, Loss: 2.3033238506317137\n",
      "Epoch 23, Batch 700, Loss: 2.3031026220321653\n",
      "Epoch 23, Batch 800, Loss: 2.302590482234955\n",
      "Epoch 23, Batch 900, Loss: 2.302538044452667\n",
      "Epoch 24, Batch 100, Loss: 2.30276638507843\n",
      "Epoch 24, Batch 200, Loss: 2.302672419548035\n",
      "Epoch 24, Batch 300, Loss: 2.3026759886741637\n",
      "Epoch 24, Batch 400, Loss: 2.3030144643783568\n",
      "Epoch 24, Batch 500, Loss: 2.3028044748306273\n",
      "Epoch 24, Batch 600, Loss: 2.3026525497436525\n",
      "Epoch 24, Batch 700, Loss: 2.3029324626922607\n",
      "Epoch 24, Batch 800, Loss: 2.303074436187744\n",
      "Epoch 24, Batch 900, Loss: 2.3032561850547792\n",
      "Epoch 25, Batch 100, Loss: 2.3025846886634826\n",
      "Epoch 25, Batch 200, Loss: 2.302604842185974\n",
      "Epoch 25, Batch 300, Loss: 2.303445839881897\n",
      "Epoch 25, Batch 400, Loss: 2.3030636739730834\n",
      "Epoch 25, Batch 500, Loss: 2.3028558897972107\n",
      "Epoch 25, Batch 600, Loss: 2.3027147364616396\n",
      "Epoch 25, Batch 700, Loss: 2.3028234958648683\n",
      "Epoch 25, Batch 800, Loss: 2.302657289505005\n",
      "Epoch 25, Batch 900, Loss: 2.3025503063201906\n",
      "Accuracy on test set: 0.1%\n",
      "Fitting for combination 47\n",
      "784\n",
      "3\n",
      "10\n",
      "[40, 10, 20, 10]\n",
      "True\n",
      "['relu', 'sigmoid', 'sigmoid']\n",
      "SGD\n",
      "0.03\n",
      "0\n",
      "CrossEntropyLoss\n",
      "50\n",
      "25\n",
      "0\n",
      "Epoch 1, Batch 50, Loss: 1.1654366374015808\n",
      "Epoch 1, Batch 100, Loss: 1.1527822399139405\n",
      "Epoch 1, Batch 150, Loss: 1.1504303669929505\n",
      "Epoch 1, Batch 200, Loss: 1.1491892910003663\n",
      "Epoch 1, Batch 250, Loss: 1.149131908416748\n",
      "Epoch 1, Batch 300, Loss: 1.1476226782798766\n",
      "Epoch 1, Batch 350, Loss: 1.1471833348274232\n",
      "Epoch 1, Batch 400, Loss: 1.145769579410553\n",
      "Epoch 1, Batch 450, Loss: 1.1437835574150086\n",
      "Epoch 1, Batch 500, Loss: 1.1417183375358582\n",
      "Epoch 1, Batch 550, Loss: 1.1401347184181214\n",
      "Epoch 1, Batch 600, Loss: 1.1357367849349975\n",
      "Epoch 1, Batch 650, Loss: 1.1328260731697082\n",
      "Epoch 1, Batch 700, Loss: 1.1284472155570984\n",
      "Epoch 1, Batch 750, Loss: 1.1219191980361938\n",
      "Epoch 1, Batch 800, Loss: 1.1149330353736877\n",
      "Epoch 1, Batch 850, Loss: 1.1063644123077392\n",
      "Epoch 1, Batch 900, Loss: 1.0956967449188233\n",
      "Epoch 2, Batch 50, Loss: 1.0716829371452332\n",
      "Epoch 2, Batch 100, Loss: 1.060295352935791\n",
      "Epoch 2, Batch 150, Loss: 1.0431337571144104\n",
      "Epoch 2, Batch 200, Loss: 1.0252345418930053\n",
      "Epoch 2, Batch 250, Loss: 1.0059645652770997\n",
      "Epoch 2, Batch 300, Loss: 0.9874528419971466\n",
      "Epoch 2, Batch 350, Loss: 0.9736384606361389\n",
      "Epoch 2, Batch 400, Loss: 0.9574668300151825\n",
      "Epoch 2, Batch 450, Loss: 0.934790575504303\n",
      "Epoch 2, Batch 500, Loss: 0.9212261879444122\n",
      "Epoch 2, Batch 550, Loss: 0.9072724258899689\n",
      "Epoch 2, Batch 600, Loss: 0.8917607235908508\n",
      "Epoch 2, Batch 650, Loss: 0.8819664001464844\n",
      "Epoch 2, Batch 700, Loss: 0.8735078048706054\n",
      "Epoch 2, Batch 750, Loss: 0.8586331272125244\n",
      "Epoch 2, Batch 800, Loss: 0.8469340908527374\n",
      "Epoch 2, Batch 850, Loss: 0.8358877825737\n",
      "Epoch 2, Batch 900, Loss: 0.8194631564617157\n",
      "Epoch 3, Batch 50, Loss: 0.8081985080242157\n",
      "Epoch 3, Batch 100, Loss: 0.7997636806964874\n",
      "Epoch 3, Batch 150, Loss: 0.7917720973491669\n",
      "Epoch 3, Batch 200, Loss: 0.7854307472705842\n",
      "Epoch 3, Batch 250, Loss: 0.7736232900619506\n",
      "Epoch 3, Batch 300, Loss: 0.7720496714115143\n",
      "Epoch 3, Batch 350, Loss: 0.7635655891895294\n",
      "Epoch 3, Batch 400, Loss: 0.754184422492981\n",
      "Epoch 3, Batch 450, Loss: 0.7458019244670868\n",
      "Epoch 3, Batch 500, Loss: 0.7400881052017212\n",
      "Epoch 3, Batch 550, Loss: 0.7364263713359833\n",
      "Epoch 3, Batch 600, Loss: 0.7291352200508118\n",
      "Epoch 3, Batch 650, Loss: 0.7217440354824066\n",
      "Epoch 3, Batch 700, Loss: 0.7199205994606018\n",
      "Epoch 3, Batch 750, Loss: 0.7076101672649383\n",
      "Epoch 3, Batch 800, Loss: 0.7025974881649018\n",
      "Epoch 3, Batch 850, Loss: 0.6956340837478637\n",
      "Epoch 3, Batch 900, Loss: 0.6900657176971435\n",
      "Epoch 4, Batch 50, Loss: 0.6863332402706146\n",
      "Epoch 4, Batch 100, Loss: 0.6778555357456207\n",
      "Epoch 4, Batch 150, Loss: 0.6706704032421112\n",
      "Epoch 4, Batch 200, Loss: 0.6638202023506165\n",
      "Epoch 4, Batch 250, Loss: 0.6605520868301391\n",
      "Epoch 4, Batch 300, Loss: 0.6570794904232025\n",
      "Epoch 4, Batch 350, Loss: 0.6572625541687012\n",
      "Epoch 4, Batch 400, Loss: 0.6446439051628112\n",
      "Epoch 4, Batch 450, Loss: 0.6440028285980225\n",
      "Epoch 4, Batch 500, Loss: 0.6423963391780854\n",
      "Epoch 4, Batch 550, Loss: 0.6303456735610962\n",
      "Epoch 4, Batch 600, Loss: 0.6306221210956573\n",
      "Epoch 4, Batch 650, Loss: 0.6325374901294708\n",
      "Epoch 4, Batch 700, Loss: 0.6268352031707763\n",
      "Epoch 4, Batch 750, Loss: 0.623118052482605\n",
      "Epoch 4, Batch 800, Loss: 0.6221439051628113\n",
      "Epoch 4, Batch 850, Loss: 0.6151663041114808\n",
      "Epoch 4, Batch 900, Loss: 0.6169983386993408\n",
      "Epoch 5, Batch 50, Loss: 0.6125301468372345\n",
      "Epoch 5, Batch 100, Loss: 0.6030532956123352\n",
      "Epoch 5, Batch 150, Loss: 0.5941658270359039\n",
      "Epoch 5, Batch 200, Loss: 0.59638441324234\n",
      "Epoch 5, Batch 250, Loss: 0.589035040140152\n",
      "Epoch 5, Batch 300, Loss: 0.5915546321868896\n",
      "Epoch 5, Batch 350, Loss: 0.5919144856929779\n",
      "Epoch 5, Batch 400, Loss: 0.5912047970294952\n",
      "Epoch 5, Batch 450, Loss: 0.5853171133995057\n",
      "Epoch 5, Batch 500, Loss: 0.5772102689743042\n",
      "Epoch 5, Batch 550, Loss: 0.5675982975959778\n",
      "Epoch 5, Batch 600, Loss: 0.578016402721405\n",
      "Epoch 5, Batch 650, Loss: 0.575603187084198\n",
      "Epoch 5, Batch 700, Loss: 0.5753299343585968\n",
      "Epoch 5, Batch 750, Loss: 0.5629071140289307\n",
      "Epoch 5, Batch 800, Loss: 0.5698576951026917\n",
      "Epoch 5, Batch 850, Loss: 0.5637567710876464\n",
      "Epoch 5, Batch 900, Loss: 0.557914457321167\n",
      "Epoch 6, Batch 50, Loss: 0.5562067329883575\n",
      "Epoch 6, Batch 100, Loss: 0.5484908658266068\n",
      "Epoch 6, Batch 150, Loss: 0.5479635000228882\n",
      "Epoch 6, Batch 200, Loss: 0.5471802562475204\n",
      "Epoch 6, Batch 250, Loss: 0.5499008083343506\n",
      "Epoch 6, Batch 300, Loss: 0.5432366335391998\n",
      "Epoch 6, Batch 350, Loss: 0.5400359243154526\n",
      "Epoch 6, Batch 400, Loss: 0.5325432842969895\n",
      "Epoch 6, Batch 450, Loss: 0.5362978535890579\n",
      "Epoch 6, Batch 500, Loss: 0.5301855808496475\n",
      "Epoch 6, Batch 550, Loss: 0.5261843025684356\n",
      "Epoch 6, Batch 600, Loss: 0.5278536015748978\n",
      "Epoch 6, Batch 650, Loss: 0.5204397505521774\n",
      "Epoch 6, Batch 700, Loss: 0.5260442465543747\n",
      "Epoch 6, Batch 750, Loss: 0.5164493334293365\n",
      "Epoch 6, Batch 800, Loss: 0.512285088300705\n",
      "Epoch 6, Batch 850, Loss: 0.5091827762126923\n",
      "Epoch 6, Batch 900, Loss: 0.5158302998542785\n",
      "Epoch 7, Batch 50, Loss: 0.5053456825017929\n",
      "Epoch 7, Batch 100, Loss: 0.5062703704833984\n",
      "Epoch 7, Batch 150, Loss: 0.49596314013004306\n",
      "Epoch 7, Batch 200, Loss: 0.5039279055595398\n",
      "Epoch 7, Batch 250, Loss: 0.4936447823047638\n",
      "Epoch 7, Batch 300, Loss: 0.48709071934223175\n",
      "Epoch 7, Batch 350, Loss: 0.4916903656721115\n",
      "Epoch 7, Batch 400, Loss: 0.48992504835128786\n",
      "Epoch 7, Batch 450, Loss: 0.4826900947093964\n",
      "Epoch 7, Batch 500, Loss: 0.4924138575792313\n",
      "Epoch 7, Batch 550, Loss: 0.48546396851539614\n",
      "Epoch 7, Batch 600, Loss: 0.4795925801992416\n",
      "Epoch 7, Batch 650, Loss: 0.47734379649162295\n",
      "Epoch 7, Batch 700, Loss: 0.4723091334104538\n",
      "Epoch 7, Batch 750, Loss: 0.4658343189954758\n",
      "Epoch 7, Batch 800, Loss: 0.4808506214618683\n",
      "Epoch 7, Batch 850, Loss: 0.4652998071908951\n",
      "Epoch 7, Batch 900, Loss: 0.46870803654193877\n",
      "Epoch 8, Batch 50, Loss: 0.46280218958854674\n",
      "Epoch 8, Batch 100, Loss: 0.4651317900419235\n",
      "Epoch 8, Batch 150, Loss: 0.4540839052200317\n",
      "Epoch 8, Batch 200, Loss: 0.4672713828086853\n",
      "Epoch 8, Batch 250, Loss: 0.4633373981714249\n",
      "Epoch 8, Batch 300, Loss: 0.4519292086362839\n",
      "Epoch 8, Batch 350, Loss: 0.44946743071079254\n",
      "Epoch 8, Batch 400, Loss: 0.44477016866207125\n",
      "Epoch 8, Batch 450, Loss: 0.45416116833686826\n",
      "Epoch 8, Batch 500, Loss: 0.4300126314163208\n",
      "Epoch 8, Batch 550, Loss: 0.43623401045799254\n",
      "Epoch 8, Batch 600, Loss: 0.44713629841804503\n",
      "Epoch 8, Batch 650, Loss: 0.4380749714374542\n",
      "Epoch 8, Batch 700, Loss: 0.43742658495903014\n",
      "Epoch 8, Batch 750, Loss: 0.4341071736812592\n",
      "Epoch 8, Batch 800, Loss: 0.4381006354093552\n",
      "Epoch 8, Batch 850, Loss: 0.42985089004039767\n",
      "Epoch 8, Batch 900, Loss: 0.43148609876632693\n",
      "Epoch 9, Batch 50, Loss: 0.42240781486034396\n",
      "Epoch 9, Batch 100, Loss: 0.43094249844551086\n",
      "Epoch 9, Batch 150, Loss: 0.42494709610939024\n",
      "Epoch 9, Batch 200, Loss: 0.41610666155815124\n",
      "Epoch 9, Batch 250, Loss: 0.41998847961425784\n",
      "Epoch 9, Batch 300, Loss: 0.4075072127580643\n",
      "Epoch 9, Batch 350, Loss: 0.4212102311849594\n",
      "Epoch 9, Batch 400, Loss: 0.4121911692619324\n",
      "Epoch 9, Batch 450, Loss: 0.41481633961200715\n",
      "Epoch 9, Batch 500, Loss: 0.40877758622169497\n",
      "Epoch 9, Batch 550, Loss: 0.4122220605611801\n",
      "Epoch 9, Batch 600, Loss: 0.41803371846675874\n",
      "Epoch 9, Batch 650, Loss: 0.4117999309301376\n",
      "Epoch 9, Batch 700, Loss: 0.39991711199283597\n",
      "Epoch 9, Batch 750, Loss: 0.39664055705070494\n",
      "Epoch 9, Batch 800, Loss: 0.4124642068147659\n",
      "Epoch 9, Batch 850, Loss: 0.3973892033100128\n",
      "Epoch 9, Batch 900, Loss: 0.4075777107477188\n",
      "Epoch 10, Batch 50, Loss: 0.3928705793619156\n",
      "Epoch 10, Batch 100, Loss: 0.4057317852973938\n",
      "Epoch 10, Batch 150, Loss: 0.3821339696645737\n",
      "Epoch 10, Batch 200, Loss: 0.3887503880262375\n",
      "Epoch 10, Batch 250, Loss: 0.3973055762052536\n",
      "Epoch 10, Batch 300, Loss: 0.3870336675643921\n",
      "Epoch 10, Batch 350, Loss: 0.3809008949995041\n",
      "Epoch 10, Batch 400, Loss: 0.38019267201423645\n",
      "Epoch 10, Batch 450, Loss: 0.3815485417842865\n",
      "Epoch 10, Batch 500, Loss: 0.3953829669952393\n",
      "Epoch 10, Batch 550, Loss: 0.3894894826412201\n",
      "Epoch 10, Batch 600, Loss: 0.38707775831222535\n",
      "Epoch 10, Batch 650, Loss: 0.38348041117191317\n",
      "Epoch 10, Batch 700, Loss: 0.38872903406620024\n",
      "Epoch 10, Batch 750, Loss: 0.3767215120792389\n",
      "Epoch 10, Batch 800, Loss: 0.3741890734434128\n",
      "Epoch 10, Batch 850, Loss: 0.37915533304214477\n",
      "Epoch 10, Batch 900, Loss: 0.3863221937417984\n",
      "Epoch 11, Batch 50, Loss: 0.3786817753314972\n",
      "Epoch 11, Batch 100, Loss: 0.3629347485303879\n",
      "Epoch 11, Batch 150, Loss: 0.3710030627250671\n",
      "Epoch 11, Batch 200, Loss: 0.3876291263103485\n",
      "Epoch 11, Batch 250, Loss: 0.3633958828449249\n",
      "Epoch 11, Batch 300, Loss: 0.3685916894674301\n",
      "Epoch 11, Batch 350, Loss: 0.3631491893529892\n",
      "Epoch 11, Batch 400, Loss: 0.35994463086128237\n",
      "Epoch 11, Batch 450, Loss: 0.3586455762386322\n",
      "Epoch 11, Batch 500, Loss: 0.3606577664613724\n",
      "Epoch 11, Batch 550, Loss: 0.3596380740404129\n",
      "Epoch 11, Batch 600, Loss: 0.35330085813999174\n",
      "Epoch 11, Batch 650, Loss: 0.36654613554477694\n",
      "Epoch 11, Batch 700, Loss: 0.3668019622564316\n",
      "Epoch 11, Batch 750, Loss: 0.36760438919067384\n",
      "Epoch 11, Batch 800, Loss: 0.3670100224018097\n",
      "Epoch 11, Batch 850, Loss: 0.36224076509475706\n",
      "Epoch 11, Batch 900, Loss: 0.356328204870224\n",
      "Epoch 12, Batch 50, Loss: 0.349125971198082\n",
      "Epoch 12, Batch 100, Loss: 0.34259732663631437\n",
      "Epoch 12, Batch 150, Loss: 0.3588849490880966\n",
      "Epoch 12, Batch 200, Loss: 0.3586899650096893\n",
      "Epoch 12, Batch 250, Loss: 0.34821153938770294\n",
      "Epoch 12, Batch 300, Loss: 0.33796523153781893\n",
      "Epoch 12, Batch 350, Loss: 0.3399666476249695\n",
      "Epoch 12, Batch 400, Loss: 0.3435803434252739\n",
      "Epoch 12, Batch 450, Loss: 0.3487980470061302\n",
      "Epoch 12, Batch 500, Loss: 0.3578064334392548\n",
      "Epoch 12, Batch 550, Loss: 0.34730458676815035\n",
      "Epoch 12, Batch 600, Loss: 0.3491641068458557\n",
      "Epoch 12, Batch 650, Loss: 0.33771233886480334\n",
      "Epoch 12, Batch 700, Loss: 0.3478875380754471\n",
      "Epoch 12, Batch 750, Loss: 0.34492592692375185\n",
      "Epoch 12, Batch 800, Loss: 0.3415406119823456\n",
      "Epoch 12, Batch 850, Loss: 0.3351989683508873\n",
      "Epoch 12, Batch 900, Loss: 0.34686219722032546\n",
      "Epoch 13, Batch 50, Loss: 0.3329524165391922\n",
      "Epoch 13, Batch 100, Loss: 0.3279444488883019\n",
      "Epoch 13, Batch 150, Loss: 0.3466320624947548\n",
      "Epoch 13, Batch 200, Loss: 0.3287516292929649\n",
      "Epoch 13, Batch 250, Loss: 0.32923326045274737\n",
      "Epoch 13, Batch 300, Loss: 0.3238109457492828\n",
      "Epoch 13, Batch 350, Loss: 0.33392960429191587\n",
      "Epoch 13, Batch 400, Loss: 0.33277632474899294\n",
      "Epoch 13, Batch 450, Loss: 0.32531800866127014\n",
      "Epoch 13, Batch 500, Loss: 0.3300900033116341\n",
      "Epoch 13, Batch 550, Loss: 0.33266965389251707\n",
      "Epoch 13, Batch 600, Loss: 0.3286155289411545\n",
      "Epoch 13, Batch 650, Loss: 0.31389702498912814\n",
      "Epoch 13, Batch 700, Loss: 0.32174191385507583\n",
      "Epoch 13, Batch 750, Loss: 0.32716534703969957\n",
      "Epoch 13, Batch 800, Loss: 0.32619977414608003\n",
      "Epoch 13, Batch 850, Loss: 0.31807318449020383\n",
      "Epoch 13, Batch 900, Loss: 0.3267890191078186\n",
      "Epoch 14, Batch 50, Loss: 0.31374392718076705\n",
      "Epoch 14, Batch 100, Loss: 0.3162785306572914\n",
      "Epoch 14, Batch 150, Loss: 0.3110063269734383\n",
      "Epoch 14, Batch 200, Loss: 0.32096488565206527\n",
      "Epoch 14, Batch 250, Loss: 0.3159553986787796\n",
      "Epoch 14, Batch 300, Loss: 0.3285766008496285\n",
      "Epoch 14, Batch 350, Loss: 0.3089742332696915\n",
      "Epoch 14, Batch 400, Loss: 0.3187876898050308\n",
      "Epoch 14, Batch 450, Loss: 0.3076834511756897\n",
      "Epoch 14, Batch 500, Loss: 0.3150074249505997\n",
      "Epoch 14, Batch 550, Loss: 0.2953029337525368\n",
      "Epoch 14, Batch 600, Loss: 0.31289922893047334\n",
      "Epoch 14, Batch 650, Loss: 0.31501480609178545\n",
      "Epoch 14, Batch 700, Loss: 0.30332536339759825\n",
      "Epoch 14, Batch 750, Loss: 0.30600777179002764\n",
      "Epoch 14, Batch 800, Loss: 0.3008782580494881\n",
      "Epoch 14, Batch 850, Loss: 0.2919825565814972\n",
      "Epoch 14, Batch 900, Loss: 0.3189936050772667\n",
      "Epoch 15, Batch 50, Loss: 0.29852251380681993\n",
      "Epoch 15, Batch 100, Loss: 0.3045938524603844\n",
      "Epoch 15, Batch 150, Loss: 0.2946686211228371\n",
      "Epoch 15, Batch 200, Loss: 0.29600356727838517\n",
      "Epoch 15, Batch 250, Loss: 0.29096873342990875\n",
      "Epoch 15, Batch 300, Loss: 0.29298041939735414\n",
      "Epoch 15, Batch 350, Loss: 0.2959879678487778\n",
      "Epoch 15, Batch 400, Loss: 0.3055184900760651\n",
      "Epoch 15, Batch 450, Loss: 0.2990467593073845\n",
      "Epoch 15, Batch 500, Loss: 0.28977067828178404\n",
      "Epoch 15, Batch 550, Loss: 0.30740467995405196\n",
      "Epoch 15, Batch 600, Loss: 0.28540450453758237\n",
      "Epoch 15, Batch 650, Loss: 0.2952309402823448\n",
      "Epoch 15, Batch 700, Loss: 0.29850429177284243\n",
      "Epoch 15, Batch 750, Loss: 0.29594969004392624\n",
      "Epoch 15, Batch 800, Loss: 0.2906412422657013\n",
      "Epoch 15, Batch 850, Loss: 0.27740526586771014\n",
      "Epoch 15, Batch 900, Loss: 0.2935113522410393\n",
      "Epoch 16, Batch 50, Loss: 0.28089687287807463\n",
      "Epoch 16, Batch 100, Loss: 0.28666060119867326\n",
      "Epoch 16, Batch 150, Loss: 0.27962745279073714\n",
      "Epoch 16, Batch 200, Loss: 0.2917706328630447\n",
      "Epoch 16, Batch 250, Loss: 0.28216936856508257\n",
      "Epoch 16, Batch 300, Loss: 0.27746405988931655\n",
      "Epoch 16, Batch 350, Loss: 0.27975699573755264\n",
      "Epoch 16, Batch 400, Loss: 0.27928716242313384\n",
      "Epoch 16, Batch 450, Loss: 0.28607415795326235\n",
      "Epoch 16, Batch 500, Loss: 0.287558981180191\n",
      "Epoch 16, Batch 550, Loss: 0.28056253373622897\n",
      "Epoch 16, Batch 600, Loss: 0.28398343801498416\n",
      "Epoch 16, Batch 650, Loss: 0.28078446209430696\n",
      "Epoch 16, Batch 700, Loss: 0.2862967744469643\n",
      "Epoch 16, Batch 750, Loss: 0.26492943465709684\n",
      "Epoch 16, Batch 800, Loss: 0.2769522324204445\n",
      "Epoch 16, Batch 850, Loss: 0.2774367126822472\n",
      "Epoch 16, Batch 900, Loss: 0.2716491684317589\n",
      "Epoch 17, Batch 50, Loss: 0.26625283718109133\n",
      "Epoch 17, Batch 100, Loss: 0.2822041943669319\n",
      "Epoch 17, Batch 150, Loss: 0.2685898718237877\n",
      "Epoch 17, Batch 200, Loss: 0.2685151472687721\n",
      "Epoch 17, Batch 250, Loss: 0.2721103397011757\n",
      "Epoch 17, Batch 300, Loss: 0.2696381819248199\n",
      "Epoch 17, Batch 350, Loss: 0.2655967363715172\n",
      "Epoch 17, Batch 400, Loss: 0.2561070218682289\n",
      "Epoch 17, Batch 450, Loss: 0.26847095251083375\n",
      "Epoch 17, Batch 500, Loss: 0.26464650869369505\n",
      "Epoch 17, Batch 550, Loss: 0.26788277983665465\n",
      "Epoch 17, Batch 600, Loss: 0.2674412128329277\n",
      "Epoch 17, Batch 650, Loss: 0.27169679164886473\n",
      "Epoch 17, Batch 700, Loss: 0.2721824777126312\n",
      "Epoch 17, Batch 750, Loss: 0.257638064622879\n",
      "Epoch 17, Batch 800, Loss: 0.2509327536821365\n",
      "Epoch 17, Batch 850, Loss: 0.257049722969532\n",
      "Epoch 17, Batch 900, Loss: 0.2531167906522751\n",
      "Epoch 18, Batch 50, Loss: 0.2582690194249153\n",
      "Epoch 18, Batch 100, Loss: 0.26180076837539673\n",
      "Epoch 18, Batch 150, Loss: 0.25346501648426056\n",
      "Epoch 18, Batch 200, Loss: 0.2560050195455551\n",
      "Epoch 18, Batch 250, Loss: 0.2656213304400444\n",
      "Epoch 18, Batch 300, Loss: 0.25505328088998797\n",
      "Epoch 18, Batch 350, Loss: 0.26185338348150256\n",
      "Epoch 18, Batch 400, Loss: 0.2546220484375954\n",
      "Epoch 18, Batch 450, Loss: 0.25080284237861633\n",
      "Epoch 18, Batch 500, Loss: 0.24262526720762254\n",
      "Epoch 18, Batch 550, Loss: 0.24777429342269897\n",
      "Epoch 18, Batch 600, Loss: 0.2617124465107918\n",
      "Epoch 18, Batch 650, Loss: 0.24892210841178894\n",
      "Epoch 18, Batch 700, Loss: 0.24086042404174804\n",
      "Epoch 18, Batch 750, Loss: 0.23822408854961397\n",
      "Epoch 18, Batch 800, Loss: 0.2573939934372902\n",
      "Epoch 18, Batch 850, Loss: 0.25155637323856356\n",
      "Epoch 18, Batch 900, Loss: 0.2558479326963425\n",
      "Epoch 19, Batch 50, Loss: 0.24862366050481796\n",
      "Epoch 19, Batch 100, Loss: 0.23503625214099885\n",
      "Epoch 19, Batch 150, Loss: 0.25749229997396467\n",
      "Epoch 19, Batch 200, Loss: 0.2473551994562149\n",
      "Epoch 19, Batch 250, Loss: 0.23904645472764968\n",
      "Epoch 19, Batch 300, Loss: 0.24355957597494127\n",
      "Epoch 19, Batch 350, Loss: 0.23277121365070344\n",
      "Epoch 19, Batch 400, Loss: 0.2349024909734726\n",
      "Epoch 19, Batch 450, Loss: 0.2499256592988968\n",
      "Epoch 19, Batch 500, Loss: 0.2520154622197151\n",
      "Epoch 19, Batch 550, Loss: 0.23584314614534377\n",
      "Epoch 19, Batch 600, Loss: 0.24466996699571608\n",
      "Epoch 19, Batch 650, Loss: 0.23370658069849015\n",
      "Epoch 19, Batch 700, Loss: 0.23311318576335907\n",
      "Epoch 19, Batch 750, Loss: 0.23913148641586304\n",
      "Epoch 19, Batch 800, Loss: 0.2387193349003792\n",
      "Epoch 19, Batch 850, Loss: 0.23318198204040527\n",
      "Epoch 19, Batch 900, Loss: 0.2376306676864624\n",
      "Epoch 20, Batch 50, Loss: 0.22334868848323822\n",
      "Epoch 20, Batch 100, Loss: 0.23028963893651963\n",
      "Epoch 20, Batch 150, Loss: 0.23776871144771575\n",
      "Epoch 20, Batch 200, Loss: 0.22828024327754975\n",
      "Epoch 20, Batch 250, Loss: 0.22131606489419936\n",
      "Epoch 20, Batch 300, Loss: 0.22462792366743087\n",
      "Epoch 20, Batch 350, Loss: 0.24230033457279204\n",
      "Epoch 20, Batch 400, Loss: 0.24080988466739656\n",
      "Epoch 20, Batch 450, Loss: 0.23917371451854705\n",
      "Epoch 20, Batch 500, Loss: 0.23027300268411635\n",
      "Epoch 20, Batch 550, Loss: 0.22848894894123079\n",
      "Epoch 20, Batch 600, Loss: 0.2344752088189125\n",
      "Epoch 20, Batch 650, Loss: 0.23848373234272002\n",
      "Epoch 20, Batch 700, Loss: 0.21970116525888442\n",
      "Epoch 20, Batch 750, Loss: 0.22684253841638566\n",
      "Epoch 20, Batch 800, Loss: 0.2256668135523796\n",
      "Epoch 20, Batch 850, Loss: 0.22823543846607208\n",
      "Epoch 20, Batch 900, Loss: 0.2266034010052681\n",
      "Epoch 21, Batch 50, Loss: 0.2248809164762497\n",
      "Epoch 21, Batch 100, Loss: 0.21954198211431503\n",
      "Epoch 21, Batch 150, Loss: 0.2328842958807945\n",
      "Epoch 21, Batch 200, Loss: 0.22318386122584344\n",
      "Epoch 21, Batch 250, Loss: 0.22096479684114456\n",
      "Epoch 21, Batch 300, Loss: 0.22984366178512572\n",
      "Epoch 21, Batch 350, Loss: 0.22669559508562087\n",
      "Epoch 21, Batch 400, Loss: 0.2267157742381096\n",
      "Epoch 21, Batch 450, Loss: 0.21865802735090256\n",
      "Epoch 21, Batch 500, Loss: 0.21510071843862533\n",
      "Epoch 21, Batch 550, Loss: 0.21383382230997086\n",
      "Epoch 21, Batch 600, Loss: 0.2214469528198242\n",
      "Epoch 21, Batch 650, Loss: 0.22000212252140044\n",
      "Epoch 21, Batch 700, Loss: 0.2194427764415741\n",
      "Epoch 21, Batch 750, Loss: 0.22264041155576705\n",
      "Epoch 21, Batch 800, Loss: 0.19885190695524216\n",
      "Epoch 21, Batch 850, Loss: 0.21803650230169297\n",
      "Epoch 21, Batch 900, Loss: 0.22141102582216263\n",
      "Epoch 22, Batch 50, Loss: 0.20014377415180207\n",
      "Epoch 22, Batch 100, Loss: 0.22434702664613723\n",
      "Epoch 22, Batch 150, Loss: 0.2216785407066345\n",
      "Epoch 22, Batch 200, Loss: 0.2228077931702137\n",
      "Epoch 22, Batch 250, Loss: 0.21497212052345277\n",
      "Epoch 22, Batch 300, Loss: 0.21432310611009597\n",
      "Epoch 22, Batch 350, Loss: 0.20416526585817338\n",
      "Epoch 22, Batch 400, Loss: 0.21034766167402266\n",
      "Epoch 22, Batch 450, Loss: 0.21266362339258194\n",
      "Epoch 22, Batch 500, Loss: 0.1998220883309841\n",
      "Epoch 22, Batch 550, Loss: 0.2084250496327877\n",
      "Epoch 22, Batch 600, Loss: 0.21549291282892227\n",
      "Epoch 22, Batch 650, Loss: 0.20174394845962523\n",
      "Epoch 22, Batch 700, Loss: 0.21344766348600389\n",
      "Epoch 22, Batch 750, Loss: 0.22282903477549554\n",
      "Epoch 22, Batch 800, Loss: 0.21034247428178787\n",
      "Epoch 22, Batch 850, Loss: 0.21900469034910203\n",
      "Epoch 22, Batch 900, Loss: 0.22123088628053666\n",
      "Epoch 23, Batch 50, Loss: 0.201483586281538\n",
      "Epoch 23, Batch 100, Loss: 0.20575825676321982\n",
      "Epoch 23, Batch 150, Loss: 0.20562495186924934\n",
      "Epoch 23, Batch 200, Loss: 0.21693698674440384\n",
      "Epoch 23, Batch 250, Loss: 0.21898582339286804\n",
      "Epoch 23, Batch 300, Loss: 0.19948471292853356\n",
      "Epoch 23, Batch 350, Loss: 0.20787652671337128\n",
      "Epoch 23, Batch 400, Loss: 0.21726641833782195\n",
      "Epoch 23, Batch 450, Loss: 0.22003631949424743\n",
      "Epoch 23, Batch 500, Loss: 0.19804140642285348\n",
      "Epoch 23, Batch 550, Loss: 0.190021171271801\n",
      "Epoch 23, Batch 600, Loss: 0.2091207793354988\n",
      "Epoch 23, Batch 650, Loss: 0.19961830615997314\n",
      "Epoch 23, Batch 700, Loss: 0.19822134986519813\n",
      "Epoch 23, Batch 750, Loss: 0.2091946405172348\n",
      "Epoch 23, Batch 800, Loss: 0.20147704780101777\n",
      "Epoch 23, Batch 850, Loss: 0.20992531031370162\n",
      "Epoch 23, Batch 900, Loss: 0.2022201481461525\n",
      "Epoch 24, Batch 50, Loss: 0.1979782199859619\n",
      "Epoch 24, Batch 100, Loss: 0.21251391381025314\n",
      "Epoch 24, Batch 150, Loss: 0.1976215697824955\n",
      "Epoch 24, Batch 200, Loss: 0.19996935173869132\n",
      "Epoch 24, Batch 250, Loss: 0.18845174759626387\n",
      "Epoch 24, Batch 300, Loss: 0.1931745108962059\n",
      "Epoch 24, Batch 350, Loss: 0.196892571747303\n",
      "Epoch 24, Batch 400, Loss: 0.19670695021748544\n",
      "Epoch 24, Batch 450, Loss: 0.2036214616894722\n",
      "Epoch 24, Batch 500, Loss: 0.20418904066085816\n",
      "Epoch 24, Batch 550, Loss: 0.20660623520612717\n",
      "Epoch 24, Batch 600, Loss: 0.19686220988631248\n",
      "Epoch 24, Batch 650, Loss: 0.20900334134697915\n",
      "Epoch 24, Batch 700, Loss: 0.19264258325099945\n",
      "Epoch 24, Batch 750, Loss: 0.20291855156421662\n",
      "Epoch 24, Batch 800, Loss: 0.20006944999098777\n",
      "Epoch 24, Batch 850, Loss: 0.20100213915109635\n",
      "Epoch 24, Batch 900, Loss: 0.1918951053917408\n",
      "Epoch 25, Batch 50, Loss: 0.18291519001126288\n",
      "Epoch 25, Batch 100, Loss: 0.18531819447875023\n",
      "Epoch 25, Batch 150, Loss: 0.19106512784957885\n",
      "Epoch 25, Batch 200, Loss: 0.19270438864827155\n",
      "Epoch 25, Batch 250, Loss: 0.20133827924728392\n",
      "Epoch 25, Batch 300, Loss: 0.20746983259916305\n",
      "Epoch 25, Batch 350, Loss: 0.20078425616025924\n",
      "Epoch 25, Batch 400, Loss: 0.2003770124912262\n",
      "Epoch 25, Batch 450, Loss: 0.1993473133444786\n",
      "Epoch 25, Batch 500, Loss: 0.2081363382935524\n",
      "Epoch 25, Batch 550, Loss: 0.2008439728617668\n",
      "Epoch 25, Batch 600, Loss: 0.1900548008084297\n",
      "Epoch 25, Batch 650, Loss: 0.20102598115801812\n",
      "Epoch 25, Batch 700, Loss: 0.197680104970932\n",
      "Epoch 25, Batch 750, Loss: 0.179868400990963\n",
      "Epoch 25, Batch 800, Loss: 0.18506498545408248\n",
      "Epoch 25, Batch 850, Loss: 0.19093346565961838\n",
      "Epoch 25, Batch 900, Loss: 0.19580606162548064\n",
      "Accuracy on test set: 0.8494%\n",
      "Fitting for combination 48\n",
      "784\n",
      "3\n",
      "10\n",
      "[40, 10, 30, 10]\n",
      "False\n",
      "['relu', 'sigmoid', 'sigmoid']\n",
      "Adam\n",
      "0.01\n",
      "0.03\n",
      "CrossEntropyLoss\n",
      "200\n",
      "25\n",
      "0\n",
      "Epoch 1, Batch 200, Loss: 4.634421639442444\n",
      "Epoch 1, Batch 400, Loss: 4.629714694023132\n",
      "Epoch 1, Batch 600, Loss: 4.634463088512421\n",
      "Epoch 1, Batch 800, Loss: 4.636254394054413\n",
      "Epoch 2, Batch 200, Loss: 4.629915256500244\n",
      "Epoch 2, Batch 400, Loss: 4.634249668121338\n",
      "Epoch 2, Batch 600, Loss: 4.634738314151764\n",
      "Epoch 2, Batch 800, Loss: 4.634428215026856\n",
      "Epoch 3, Batch 200, Loss: 4.629694945812226\n",
      "Epoch 3, Batch 400, Loss: 4.628106846809387\n",
      "Epoch 3, Batch 600, Loss: 4.628016676902771\n",
      "Epoch 3, Batch 800, Loss: 4.628333153724671\n",
      "Epoch 4, Batch 200, Loss: 4.626860673427582\n",
      "Epoch 4, Batch 400, Loss: 4.632050745487213\n",
      "Epoch 4, Batch 600, Loss: 4.632590596675873\n",
      "Epoch 4, Batch 800, Loss: 4.631379947662354\n",
      "Epoch 5, Batch 200, Loss: 4.627020869255066\n",
      "Epoch 5, Batch 400, Loss: 4.634891970157623\n",
      "Epoch 5, Batch 600, Loss: 4.633977048397064\n",
      "Epoch 5, Batch 800, Loss: 4.629492075443268\n",
      "Epoch 6, Batch 200, Loss: 4.632783191204071\n",
      "Epoch 6, Batch 400, Loss: 4.627449400424958\n",
      "Epoch 6, Batch 600, Loss: 4.633466753959656\n",
      "Epoch 6, Batch 800, Loss: 4.6280028033256535\n",
      "Epoch 7, Batch 200, Loss: 4.634629981517792\n",
      "Epoch 7, Batch 400, Loss: 4.6350151062011715\n",
      "Epoch 7, Batch 600, Loss: 4.628091595172882\n",
      "Epoch 7, Batch 800, Loss: 4.631776053905487\n",
      "Epoch 8, Batch 200, Loss: 4.632292273044587\n",
      "Epoch 8, Batch 400, Loss: 4.631133224964142\n",
      "Epoch 8, Batch 600, Loss: 4.628229432106018\n",
      "Epoch 8, Batch 800, Loss: 4.631817493438721\n",
      "Epoch 9, Batch 200, Loss: 4.631299698352814\n",
      "Epoch 9, Batch 400, Loss: 4.6300872659683225\n",
      "Epoch 9, Batch 600, Loss: 4.629555184841156\n",
      "Epoch 9, Batch 800, Loss: 4.635922799110412\n",
      "Epoch 10, Batch 200, Loss: 4.626834168434143\n",
      "Epoch 10, Batch 400, Loss: 4.631482110023499\n",
      "Epoch 10, Batch 600, Loss: 4.630825381278992\n",
      "Epoch 10, Batch 800, Loss: 4.632869148254395\n",
      "Epoch 11, Batch 200, Loss: 4.63011156797409\n",
      "Epoch 11, Batch 400, Loss: 4.631685972213745\n",
      "Epoch 11, Batch 600, Loss: 4.630087196826935\n",
      "Epoch 11, Batch 800, Loss: 4.632291326522827\n",
      "Epoch 12, Batch 200, Loss: 4.629025096893311\n",
      "Epoch 12, Batch 400, Loss: 4.629628782272339\n",
      "Epoch 12, Batch 600, Loss: 4.631477024555206\n",
      "Epoch 12, Batch 800, Loss: 4.629762358665467\n",
      "Epoch 13, Batch 200, Loss: 4.630375411510467\n",
      "Epoch 13, Batch 400, Loss: 4.630521914958954\n",
      "Epoch 13, Batch 600, Loss: 4.631830041408539\n",
      "Epoch 13, Batch 800, Loss: 4.630144114494324\n",
      "Epoch 14, Batch 200, Loss: 4.6366636848449705\n",
      "Epoch 14, Batch 400, Loss: 4.62843309879303\n",
      "Epoch 14, Batch 600, Loss: 4.635473740100861\n",
      "Epoch 14, Batch 800, Loss: 4.631181406974792\n",
      "Epoch 15, Batch 200, Loss: 4.635849523544311\n",
      "Epoch 15, Batch 400, Loss: 4.633274652957916\n",
      "Epoch 15, Batch 600, Loss: 4.630662038326263\n",
      "Epoch 15, Batch 800, Loss: 4.631304376125335\n",
      "Epoch 16, Batch 200, Loss: 4.629535942077637\n",
      "Epoch 16, Batch 400, Loss: 4.63530038356781\n",
      "Epoch 16, Batch 600, Loss: 4.632731015682221\n",
      "Epoch 16, Batch 800, Loss: 4.630009763240814\n",
      "Epoch 17, Batch 200, Loss: 4.631922197341919\n",
      "Epoch 17, Batch 400, Loss: 4.630179722309112\n",
      "Epoch 17, Batch 600, Loss: 4.636775732040405\n",
      "Epoch 17, Batch 800, Loss: 4.637040235996246\n",
      "Epoch 18, Batch 200, Loss: 4.633830599784851\n",
      "Epoch 18, Batch 400, Loss: 4.6287240886688235\n",
      "Epoch 18, Batch 600, Loss: 4.6294499373435976\n",
      "Epoch 18, Batch 800, Loss: 4.6358228778839115\n",
      "Epoch 19, Batch 200, Loss: 4.6358293485641475\n",
      "Epoch 19, Batch 400, Loss: 4.627638568878174\n",
      "Epoch 19, Batch 600, Loss: 4.635304455757141\n",
      "Epoch 19, Batch 800, Loss: 4.628139140605927\n",
      "Epoch 20, Batch 200, Loss: 4.628650562763214\n",
      "Epoch 20, Batch 400, Loss: 4.632141425609588\n",
      "Epoch 20, Batch 600, Loss: 4.6303661346435545\n",
      "Epoch 20, Batch 800, Loss: 4.632355780601501\n",
      "Epoch 21, Batch 200, Loss: 4.634487116336823\n",
      "Epoch 21, Batch 400, Loss: 4.634017636775971\n",
      "Epoch 21, Batch 600, Loss: 4.628693599700927\n",
      "Epoch 21, Batch 800, Loss: 4.627688639163971\n",
      "Epoch 22, Batch 200, Loss: 4.634235444068909\n",
      "Epoch 22, Batch 400, Loss: 4.632683899402618\n",
      "Epoch 22, Batch 600, Loss: 4.632925324440002\n",
      "Epoch 22, Batch 800, Loss: 4.628422598838807\n",
      "Epoch 23, Batch 200, Loss: 4.6319585800170895\n",
      "Epoch 23, Batch 400, Loss: 4.633300898075103\n",
      "Epoch 23, Batch 600, Loss: 4.630535163879395\n",
      "Epoch 23, Batch 800, Loss: 4.627797303199768\n",
      "Epoch 24, Batch 200, Loss: 4.6336941814422605\n",
      "Epoch 24, Batch 400, Loss: 4.6360397100448605\n",
      "Epoch 24, Batch 600, Loss: 4.633163349628449\n",
      "Epoch 24, Batch 800, Loss: 4.633820013999939\n",
      "Epoch 25, Batch 200, Loss: 4.6286701226234435\n",
      "Epoch 25, Batch 400, Loss: 4.6274638795852665\n",
      "Epoch 25, Batch 600, Loss: 4.630696301460266\n",
      "Epoch 25, Batch 800, Loss: 4.62763252735138\n",
      "Accuracy on test set: 0.1%\n",
      "Fitting for combination 49\n",
      "784\n",
      "3\n",
      "10\n",
      "[40, 10, 30, 10]\n",
      "False\n",
      "['relu', 'sigmoid', 'sigmoid']\n",
      "SGD\n",
      "0.1\n",
      "0\n",
      "CrossEntropyLoss\n",
      "100\n",
      "25\n",
      "0\n",
      "Epoch 1, Batch 100, Loss: 2.3038030791282655\n",
      "Epoch 1, Batch 200, Loss: 2.292152078151703\n",
      "Epoch 1, Batch 300, Loss: 2.2369411969184876\n",
      "Epoch 1, Batch 400, Loss: 2.0602629351615906\n",
      "Epoch 1, Batch 500, Loss: 1.8387115287780762\n",
      "Epoch 1, Batch 600, Loss: 1.7085113549232482\n",
      "Epoch 1, Batch 700, Loss: 1.6275430953502654\n",
      "Epoch 1, Batch 800, Loss: 1.5455999541282655\n",
      "Epoch 1, Batch 900, Loss: 1.4856100654602051\n",
      "Epoch 2, Batch 100, Loss: 1.409637976884842\n",
      "Epoch 2, Batch 200, Loss: 1.3717396545410157\n",
      "Epoch 2, Batch 300, Loss: 1.3227931296825408\n",
      "Epoch 2, Batch 400, Loss: 1.283739470243454\n",
      "Epoch 2, Batch 500, Loss: 1.2536359548568725\n",
      "Epoch 2, Batch 600, Loss: 1.212389497756958\n",
      "Epoch 2, Batch 700, Loss: 1.1845711827278138\n",
      "Epoch 2, Batch 800, Loss: 1.1497433227300644\n",
      "Epoch 2, Batch 900, Loss: 1.1268571496009827\n",
      "Epoch 3, Batch 100, Loss: 1.0790885680913924\n",
      "Epoch 3, Batch 200, Loss: 1.0289556926488876\n",
      "Epoch 3, Batch 300, Loss: 0.972406457066536\n",
      "Epoch 3, Batch 400, Loss: 0.9338871031999588\n",
      "Epoch 3, Batch 500, Loss: 0.8775614899396896\n",
      "Epoch 3, Batch 600, Loss: 0.8359149265289306\n",
      "Epoch 3, Batch 700, Loss: 0.8031464028358459\n",
      "Epoch 3, Batch 800, Loss: 0.7781702810525895\n",
      "Epoch 3, Batch 900, Loss: 0.771384888291359\n",
      "Epoch 4, Batch 100, Loss: 0.7284695130586624\n",
      "Epoch 4, Batch 200, Loss: 0.7018057250976563\n",
      "Epoch 4, Batch 300, Loss: 0.6839019107818604\n",
      "Epoch 4, Batch 400, Loss: 0.661341724395752\n",
      "Epoch 4, Batch 500, Loss: 0.6739812529087067\n",
      "Epoch 4, Batch 600, Loss: 0.6308320450782776\n",
      "Epoch 4, Batch 700, Loss: 0.6301169016957283\n",
      "Epoch 4, Batch 800, Loss: 0.6334351477026939\n",
      "Epoch 4, Batch 900, Loss: 0.6247006571292877\n",
      "Epoch 5, Batch 100, Loss: 0.5897023990750313\n",
      "Epoch 5, Batch 200, Loss: 0.5860834851861\n",
      "Epoch 5, Batch 300, Loss: 0.5447960394620895\n",
      "Epoch 5, Batch 400, Loss: 0.5497737374901771\n",
      "Epoch 5, Batch 500, Loss: 0.5721163567900658\n",
      "Epoch 5, Batch 600, Loss: 0.5461482721567154\n",
      "Epoch 5, Batch 700, Loss: 0.5210638290643692\n",
      "Epoch 5, Batch 800, Loss: 0.5203470893204212\n",
      "Epoch 5, Batch 900, Loss: 0.5147043904662132\n",
      "Epoch 6, Batch 100, Loss: 0.4987936225533485\n",
      "Epoch 6, Batch 200, Loss: 0.4912235760688782\n",
      "Epoch 6, Batch 300, Loss: 0.4797893109917641\n",
      "Epoch 6, Batch 400, Loss: 0.46816516667604446\n",
      "Epoch 6, Batch 500, Loss: 0.472931095212698\n",
      "Epoch 6, Batch 600, Loss: 0.45554829329252244\n",
      "Epoch 6, Batch 700, Loss: 0.46507292658090593\n",
      "Epoch 6, Batch 800, Loss: 0.4654755079746246\n",
      "Epoch 6, Batch 900, Loss: 0.454108645170927\n",
      "Epoch 7, Batch 100, Loss: 0.4508614085614681\n",
      "Epoch 7, Batch 200, Loss: 0.41641213685274125\n",
      "Epoch 7, Batch 300, Loss: 0.43426277965307236\n",
      "Epoch 7, Batch 400, Loss: 0.4323709730803966\n",
      "Epoch 7, Batch 500, Loss: 0.42130388572812083\n",
      "Epoch 7, Batch 600, Loss: 0.42630870297551154\n",
      "Epoch 7, Batch 700, Loss: 0.43299668446183204\n",
      "Epoch 7, Batch 800, Loss: 0.43025542706251146\n",
      "Epoch 7, Batch 900, Loss: 0.4402866327762604\n",
      "Epoch 8, Batch 100, Loss: 0.4129539792239666\n",
      "Epoch 8, Batch 200, Loss: 0.41505515679717064\n",
      "Epoch 8, Batch 300, Loss: 0.39542861163616183\n",
      "Epoch 8, Batch 400, Loss: 0.42418835759162904\n",
      "Epoch 8, Batch 500, Loss: 0.40824542328715324\n",
      "Epoch 8, Batch 600, Loss: 0.41521396428346635\n",
      "Epoch 8, Batch 700, Loss: 0.4049855647981167\n",
      "Epoch 8, Batch 800, Loss: 0.40019462078809737\n",
      "Epoch 8, Batch 900, Loss: 0.38726763784885404\n",
      "Epoch 9, Batch 100, Loss: 0.38949633449316023\n",
      "Epoch 9, Batch 200, Loss: 0.37495324537158015\n",
      "Epoch 9, Batch 300, Loss: 0.3952433101832867\n",
      "Epoch 9, Batch 400, Loss: 0.3985368165373802\n",
      "Epoch 9, Batch 500, Loss: 0.38305681630969046\n",
      "Epoch 9, Batch 600, Loss: 0.3866787648200989\n",
      "Epoch 9, Batch 700, Loss: 0.3899487693607807\n",
      "Epoch 9, Batch 800, Loss: 0.3811896400153637\n",
      "Epoch 9, Batch 900, Loss: 0.3840384064614773\n",
      "Epoch 10, Batch 100, Loss: 0.3664009226858616\n",
      "Epoch 10, Batch 200, Loss: 0.3831870852410793\n",
      "Epoch 10, Batch 300, Loss: 0.3778233768045902\n",
      "Epoch 10, Batch 400, Loss: 0.36299477607011793\n",
      "Epoch 10, Batch 500, Loss: 0.37804927811026573\n",
      "Epoch 10, Batch 600, Loss: 0.3670478178560734\n",
      "Epoch 10, Batch 700, Loss: 0.37509976252913474\n",
      "Epoch 10, Batch 800, Loss: 0.3790056322515011\n",
      "Epoch 10, Batch 900, Loss: 0.374000643491745\n",
      "Epoch 11, Batch 100, Loss: 0.3618581852316856\n",
      "Epoch 11, Batch 200, Loss: 0.37216161653399465\n",
      "Epoch 11, Batch 300, Loss: 0.3835734434425831\n",
      "Epoch 11, Batch 400, Loss: 0.34672098368406296\n",
      "Epoch 11, Batch 500, Loss: 0.3647466284036636\n",
      "Epoch 11, Batch 600, Loss: 0.36119604736566546\n",
      "Epoch 11, Batch 700, Loss: 0.35135502979159355\n",
      "Epoch 11, Batch 800, Loss: 0.34866911321878435\n",
      "Epoch 11, Batch 900, Loss: 0.3487847377359867\n",
      "Epoch 12, Batch 100, Loss: 0.34022263750433923\n",
      "Epoch 12, Batch 200, Loss: 0.35838704273104666\n",
      "Epoch 12, Batch 300, Loss: 0.3317058798670769\n",
      "Epoch 12, Batch 400, Loss: 0.3321354314684868\n",
      "Epoch 12, Batch 500, Loss: 0.3605682773888111\n",
      "Epoch 12, Batch 600, Loss: 0.3573592133820057\n",
      "Epoch 12, Batch 700, Loss: 0.3361392305791378\n",
      "Epoch 12, Batch 800, Loss: 0.36328581675887106\n",
      "Epoch 12, Batch 900, Loss: 0.3439033211022615\n",
      "Epoch 13, Batch 100, Loss: 0.3452376540750265\n",
      "Epoch 13, Batch 200, Loss: 0.32764918252825737\n",
      "Epoch 13, Batch 300, Loss: 0.3249982014298439\n",
      "Epoch 13, Batch 400, Loss: 0.31613261714577673\n",
      "Epoch 13, Batch 500, Loss: 0.34452337823808193\n",
      "Epoch 13, Batch 600, Loss: 0.3342698784172535\n",
      "Epoch 13, Batch 700, Loss: 0.35057939887046813\n",
      "Epoch 13, Batch 800, Loss: 0.3316296051442623\n",
      "Epoch 13, Batch 900, Loss: 0.33002417653799054\n",
      "Epoch 14, Batch 100, Loss: 0.3169186244904995\n",
      "Epoch 14, Batch 200, Loss: 0.32915322601795194\n",
      "Epoch 14, Batch 300, Loss: 0.33467451244592666\n",
      "Epoch 14, Batch 400, Loss: 0.32704009547829627\n",
      "Epoch 14, Batch 500, Loss: 0.32466954305768014\n",
      "Epoch 14, Batch 600, Loss: 0.3197401911020279\n",
      "Epoch 14, Batch 700, Loss: 0.3353103681653738\n",
      "Epoch 14, Batch 800, Loss: 0.3336256255209446\n",
      "Epoch 14, Batch 900, Loss: 0.3397465369105339\n",
      "Epoch 15, Batch 100, Loss: 0.31983160056173804\n",
      "Epoch 15, Batch 200, Loss: 0.31499548375606534\n",
      "Epoch 15, Batch 300, Loss: 0.3296696121990681\n",
      "Epoch 15, Batch 400, Loss: 0.3161515586078167\n",
      "Epoch 15, Batch 500, Loss: 0.3302085903286934\n",
      "Epoch 15, Batch 600, Loss: 0.3125733332335949\n",
      "Epoch 15, Batch 700, Loss: 0.3264363469183445\n",
      "Epoch 15, Batch 800, Loss: 0.30780690237879754\n",
      "Epoch 15, Batch 900, Loss: 0.32647991582751273\n",
      "Epoch 16, Batch 100, Loss: 0.3162676661461592\n",
      "Epoch 16, Batch 200, Loss: 0.31957062423229216\n",
      "Epoch 16, Batch 300, Loss: 0.29740036226809025\n",
      "Epoch 16, Batch 400, Loss: 0.32422630071640013\n",
      "Epoch 16, Batch 500, Loss: 0.32349416717886925\n",
      "Epoch 16, Batch 600, Loss: 0.3125382509827614\n",
      "Epoch 16, Batch 700, Loss: 0.3053481450676918\n",
      "Epoch 16, Batch 800, Loss: 0.3072506931424141\n",
      "Epoch 16, Batch 900, Loss: 0.31871738404035566\n",
      "Epoch 17, Batch 100, Loss: 0.2994603879004717\n",
      "Epoch 17, Batch 200, Loss: 0.3064204577356577\n",
      "Epoch 17, Batch 300, Loss: 0.28610673919320107\n",
      "Epoch 17, Batch 400, Loss: 0.2961508469283581\n",
      "Epoch 17, Batch 500, Loss: 0.33671795420348644\n",
      "Epoch 17, Batch 600, Loss: 0.30439653277397155\n",
      "Epoch 17, Batch 700, Loss: 0.30790489926934245\n",
      "Epoch 17, Batch 800, Loss: 0.3085899291187525\n",
      "Epoch 17, Batch 900, Loss: 0.3179937280714512\n",
      "Epoch 18, Batch 100, Loss: 0.29870688274502755\n",
      "Epoch 18, Batch 200, Loss: 0.30984058022499084\n",
      "Epoch 18, Batch 300, Loss: 0.29100762121379375\n",
      "Epoch 18, Batch 400, Loss: 0.2928431089967489\n",
      "Epoch 18, Batch 500, Loss: 0.30507513262331487\n",
      "Epoch 18, Batch 600, Loss: 0.2768967945128679\n",
      "Epoch 18, Batch 700, Loss: 0.29644601345062255\n",
      "Epoch 18, Batch 800, Loss: 0.30081041403114794\n",
      "Epoch 18, Batch 900, Loss: 0.3174067039787769\n",
      "Epoch 19, Batch 100, Loss: 0.29540818378329275\n",
      "Epoch 19, Batch 200, Loss: 0.29795663319528104\n",
      "Epoch 19, Batch 300, Loss: 0.29265365555882455\n",
      "Epoch 19, Batch 400, Loss: 0.28241624020040035\n",
      "Epoch 19, Batch 500, Loss: 0.30598380871117115\n",
      "Epoch 19, Batch 600, Loss: 0.30037950180470946\n",
      "Epoch 19, Batch 700, Loss: 0.29775421142578123\n",
      "Epoch 19, Batch 800, Loss: 0.291543477922678\n",
      "Epoch 19, Batch 900, Loss: 0.31058880761265756\n",
      "Epoch 20, Batch 100, Loss: 0.2859605585038662\n",
      "Epoch 20, Batch 200, Loss: 0.28864720955491063\n",
      "Epoch 20, Batch 300, Loss: 0.27719623148441314\n",
      "Epoch 20, Batch 400, Loss: 0.2836290042102337\n",
      "Epoch 20, Batch 500, Loss: 0.29564033918082716\n",
      "Epoch 20, Batch 600, Loss: 0.29063850596547125\n",
      "Epoch 20, Batch 700, Loss: 0.2949847061932087\n",
      "Epoch 20, Batch 800, Loss: 0.29024720050394537\n",
      "Epoch 20, Batch 900, Loss: 0.28542686864733696\n",
      "Epoch 21, Batch 100, Loss: 0.28367978490889073\n",
      "Epoch 21, Batch 200, Loss: 0.28521262064576147\n",
      "Epoch 21, Batch 300, Loss: 0.3113154188543558\n",
      "Epoch 21, Batch 400, Loss: 0.2758397915959358\n",
      "Epoch 21, Batch 500, Loss: 0.2697470949590206\n",
      "Epoch 21, Batch 600, Loss: 0.2946721221506596\n",
      "Epoch 21, Batch 700, Loss: 0.2813309497386217\n",
      "Epoch 21, Batch 800, Loss: 0.2795060475170612\n",
      "Epoch 21, Batch 900, Loss: 0.2929591302573681\n",
      "Epoch 22, Batch 100, Loss: 0.27859773829579354\n",
      "Epoch 22, Batch 200, Loss: 0.2696582332253456\n",
      "Epoch 22, Batch 300, Loss: 0.2886626313626766\n",
      "Epoch 22, Batch 400, Loss: 0.2774086320400238\n",
      "Epoch 22, Batch 500, Loss: 0.2930880211293697\n",
      "Epoch 22, Batch 600, Loss: 0.27220276951789857\n",
      "Epoch 22, Batch 700, Loss: 0.28295824460685254\n",
      "Epoch 22, Batch 800, Loss: 0.2843427512049675\n",
      "Epoch 22, Batch 900, Loss: 0.2892333686351776\n",
      "Epoch 23, Batch 100, Loss: 0.2621999184042215\n",
      "Epoch 23, Batch 200, Loss: 0.2895701138675213\n",
      "Epoch 23, Batch 300, Loss: 0.28205187641084195\n",
      "Epoch 23, Batch 400, Loss: 0.2802226092666388\n",
      "Epoch 23, Batch 500, Loss: 0.2602692633494735\n",
      "Epoch 23, Batch 600, Loss: 0.27917127378284934\n",
      "Epoch 23, Batch 700, Loss: 0.27053263448178766\n",
      "Epoch 23, Batch 800, Loss: 0.28251817598938944\n",
      "Epoch 23, Batch 900, Loss: 0.27696082651615145\n",
      "Epoch 24, Batch 100, Loss: 0.27178723335266114\n",
      "Epoch 24, Batch 200, Loss: 0.2683950401842594\n",
      "Epoch 24, Batch 300, Loss: 0.2896481837332249\n",
      "Epoch 24, Batch 400, Loss: 0.2748907049745321\n",
      "Epoch 24, Batch 500, Loss: 0.2686592762172222\n",
      "Epoch 24, Batch 600, Loss: 0.2692689197510481\n",
      "Epoch 24, Batch 700, Loss: 0.279579798951745\n",
      "Epoch 24, Batch 800, Loss: 0.2802662216126919\n",
      "Epoch 24, Batch 900, Loss: 0.2633153463900089\n",
      "Epoch 25, Batch 100, Loss: 0.2732598569989204\n",
      "Epoch 25, Batch 200, Loss: 0.2637926851958036\n",
      "Epoch 25, Batch 300, Loss: 0.2673654816299677\n",
      "Epoch 25, Batch 400, Loss: 0.2568549836426973\n",
      "Epoch 25, Batch 500, Loss: 0.28000473938882353\n",
      "Epoch 25, Batch 600, Loss: 0.2658407562226057\n",
      "Epoch 25, Batch 700, Loss: 0.2652325440943241\n",
      "Epoch 25, Batch 800, Loss: 0.2784719680994749\n",
      "Epoch 25, Batch 900, Loss: 0.27527239695191386\n",
      "Accuracy on test set: 0.8736%\n",
      "Fitting for combination 50\n",
      "784\n",
      "3\n",
      "10\n",
      "[40, 10, 30, 10]\n",
      "True\n",
      "['relu', 'sigmoid', 'tanh']\n",
      "Adam\n",
      "0.3\n",
      "0.01\n",
      "CrossEntropyLoss\n",
      "50\n",
      "25\n",
      "0\n",
      "Epoch 1, Batch 50, Loss: 1.5525913834571838\n",
      "Epoch 1, Batch 100, Loss: 1.2686218285560609\n",
      "Epoch 1, Batch 150, Loss: 1.497313449382782\n",
      "Epoch 1, Batch 200, Loss: 2.198224976062775\n",
      "Epoch 1, Batch 250, Loss: 1.287569251060486\n",
      "Epoch 1, Batch 300, Loss: 1.1878112268447876\n",
      "Epoch 1, Batch 350, Loss: 1.1758937382698058\n",
      "Epoch 1, Batch 400, Loss: 1.1805729627609254\n",
      "Epoch 1, Batch 450, Loss: 1.2177213740348816\n",
      "Epoch 1, Batch 500, Loss: 1.2583107984066009\n",
      "Epoch 1, Batch 550, Loss: 1.7189789247512817\n",
      "Epoch 1, Batch 600, Loss: 1.9119737696647645\n",
      "Epoch 1, Batch 650, Loss: 1.263005061149597\n",
      "Epoch 1, Batch 700, Loss: 1.187296016216278\n",
      "Epoch 1, Batch 750, Loss: 1.2213794326782226\n",
      "Epoch 1, Batch 800, Loss: 1.166944657564163\n",
      "Epoch 1, Batch 850, Loss: 1.2190628099441527\n",
      "Epoch 1, Batch 900, Loss: 1.2125461459159852\n",
      "Epoch 2, Batch 50, Loss: 1.2460943269729614\n",
      "Epoch 2, Batch 100, Loss: 1.191254804134369\n",
      "Epoch 2, Batch 150, Loss: 1.2513113951683044\n",
      "Epoch 2, Batch 200, Loss: 1.2397142696380614\n",
      "Epoch 2, Batch 250, Loss: 1.2866368007659912\n",
      "Epoch 2, Batch 300, Loss: 1.3068386888504029\n",
      "Epoch 2, Batch 350, Loss: 1.19504958152771\n",
      "Epoch 2, Batch 400, Loss: 1.571152560710907\n",
      "Epoch 2, Batch 450, Loss: 3.305527379512787\n",
      "Epoch 2, Batch 500, Loss: 3.416244261264801\n",
      "Epoch 2, Batch 550, Loss: 1.5943203687667846\n",
      "Epoch 2, Batch 600, Loss: 1.2266170001029968\n",
      "Epoch 2, Batch 650, Loss: 1.1789845061302184\n",
      "Epoch 2, Batch 700, Loss: 1.166938602924347\n",
      "Epoch 2, Batch 750, Loss: 1.1903803253173828\n",
      "Epoch 2, Batch 800, Loss: 1.1672379136085511\n",
      "Epoch 2, Batch 850, Loss: 1.1883546280860902\n",
      "Epoch 2, Batch 900, Loss: 1.2213823008537292\n",
      "Epoch 3, Batch 50, Loss: 1.207833104133606\n",
      "Epoch 3, Batch 100, Loss: 1.2560748910903932\n",
      "Epoch 3, Batch 150, Loss: 1.2098096537590026\n",
      "Epoch 3, Batch 200, Loss: 1.2417120432853699\n",
      "Epoch 3, Batch 250, Loss: 1.195748689174652\n",
      "Epoch 3, Batch 300, Loss: 1.242870478630066\n",
      "Epoch 3, Batch 350, Loss: 1.2342549300193786\n",
      "Epoch 3, Batch 400, Loss: 1.2576629281044007\n",
      "Epoch 3, Batch 450, Loss: 1.6073180747032165\n",
      "Epoch 3, Batch 500, Loss: 3.82486701965332\n",
      "Epoch 3, Batch 550, Loss: 2.4680808568000794\n",
      "Epoch 3, Batch 600, Loss: 1.490793318748474\n",
      "Epoch 3, Batch 650, Loss: 1.2049135041236878\n",
      "Epoch 3, Batch 700, Loss: 1.2100993275642395\n",
      "Epoch 3, Batch 750, Loss: 1.1914249277114868\n",
      "Epoch 3, Batch 800, Loss: 1.1585447072982789\n",
      "Epoch 3, Batch 850, Loss: 1.1658716917037963\n",
      "Epoch 3, Batch 900, Loss: 1.2096571445465087\n",
      "Epoch 4, Batch 50, Loss: 1.2499616980552672\n",
      "Epoch 4, Batch 100, Loss: 1.2276205682754517\n",
      "Epoch 4, Batch 150, Loss: 1.1864045548439026\n",
      "Epoch 4, Batch 200, Loss: 1.2563180041313171\n",
      "Epoch 4, Batch 250, Loss: 1.2424939155578614\n",
      "Epoch 4, Batch 300, Loss: 1.2231901001930237\n",
      "Epoch 4, Batch 350, Loss: 1.1638425314426422\n",
      "Epoch 4, Batch 400, Loss: 1.2476596868038177\n",
      "Epoch 4, Batch 450, Loss: 1.219785873889923\n",
      "Epoch 4, Batch 500, Loss: 1.2985126638412476\n",
      "Epoch 4, Batch 550, Loss: 1.4520519614219665\n",
      "Epoch 4, Batch 600, Loss: 1.7219133234024049\n",
      "Epoch 4, Batch 650, Loss: 2.355213267803192\n",
      "Epoch 4, Batch 700, Loss: 2.767537453174591\n",
      "Epoch 4, Batch 750, Loss: 1.4742286443710326\n",
      "Epoch 4, Batch 800, Loss: 1.2407445061206817\n",
      "Epoch 4, Batch 850, Loss: 1.1886668801307678\n",
      "Epoch 4, Batch 900, Loss: 1.1836252307891846\n",
      "Epoch 5, Batch 50, Loss: 1.20957946062088\n",
      "Epoch 5, Batch 100, Loss: 1.3075246143341064\n",
      "Epoch 5, Batch 150, Loss: 1.2376107788085937\n",
      "Epoch 5, Batch 200, Loss: 1.1977677178382873\n",
      "Epoch 5, Batch 250, Loss: 1.1933872771263123\n",
      "Epoch 5, Batch 300, Loss: 1.1979530930519104\n",
      "Epoch 5, Batch 350, Loss: 1.2124910867214203\n",
      "Epoch 5, Batch 400, Loss: 1.2129986810684203\n",
      "Epoch 5, Batch 450, Loss: 1.1925880026817322\n",
      "Epoch 5, Batch 500, Loss: 1.252440824508667\n",
      "Epoch 5, Batch 550, Loss: 1.6413854908943177\n",
      "Epoch 5, Batch 600, Loss: 3.2057917642593385\n",
      "Epoch 5, Batch 650, Loss: 2.7321543383598326\n",
      "Epoch 5, Batch 700, Loss: 1.4348463320732117\n",
      "Epoch 5, Batch 750, Loss: 1.2222068738937377\n",
      "Epoch 5, Batch 800, Loss: 1.1774243712425232\n",
      "Epoch 5, Batch 850, Loss: 1.1655827260017395\n",
      "Epoch 5, Batch 900, Loss: 1.1585159659385682\n",
      "Epoch 6, Batch 50, Loss: 1.2066841268539428\n",
      "Epoch 6, Batch 100, Loss: 1.1996599531173706\n",
      "Epoch 6, Batch 150, Loss: 1.235301296710968\n",
      "Epoch 6, Batch 200, Loss: 1.1991102504730224\n",
      "Epoch 6, Batch 250, Loss: 1.1674649095535279\n",
      "Epoch 6, Batch 300, Loss: 1.1813153672218322\n",
      "Epoch 6, Batch 350, Loss: 1.2147854137420655\n",
      "Epoch 6, Batch 400, Loss: 1.2399747109413146\n",
      "Epoch 6, Batch 450, Loss: 1.5220800685882567\n",
      "Epoch 6, Batch 500, Loss: 1.9594824194908143\n",
      "Epoch 6, Batch 550, Loss: 3.1083301591873167\n",
      "Epoch 6, Batch 600, Loss: 2.314710745811462\n",
      "Epoch 6, Batch 650, Loss: 1.5204706501960754\n",
      "Epoch 6, Batch 700, Loss: 1.214429726600647\n",
      "Epoch 6, Batch 750, Loss: 1.212206473350525\n",
      "Epoch 6, Batch 800, Loss: 1.2163482403755188\n",
      "Epoch 6, Batch 850, Loss: 1.1865751504898072\n",
      "Epoch 6, Batch 900, Loss: 1.1795153892040253\n",
      "Epoch 7, Batch 50, Loss: 1.1891838359832763\n",
      "Epoch 7, Batch 100, Loss: 1.2365791749954225\n",
      "Epoch 7, Batch 150, Loss: 1.2932037997245789\n",
      "Epoch 7, Batch 200, Loss: 1.255541787147522\n",
      "Epoch 7, Batch 250, Loss: 1.2882006192207336\n",
      "Epoch 7, Batch 300, Loss: 1.2132511615753174\n",
      "Epoch 7, Batch 350, Loss: 1.225613877773285\n",
      "Epoch 7, Batch 400, Loss: 1.2250774812698364\n",
      "Epoch 7, Batch 450, Loss: 1.1884386587142943\n",
      "Epoch 7, Batch 500, Loss: 1.2281676650047302\n",
      "Epoch 7, Batch 550, Loss: 1.255597059726715\n",
      "Epoch 7, Batch 600, Loss: 1.2005777072906494\n",
      "Epoch 7, Batch 650, Loss: 1.1865978717803956\n",
      "Epoch 7, Batch 700, Loss: 1.1768162739276886\n",
      "Epoch 7, Batch 750, Loss: 1.2208914804458617\n",
      "Epoch 7, Batch 800, Loss: 1.269807252883911\n",
      "Epoch 7, Batch 850, Loss: 1.2031482923030854\n",
      "Epoch 7, Batch 900, Loss: 1.2794938397407531\n",
      "Epoch 8, Batch 50, Loss: 4.478691115379333\n",
      "Epoch 8, Batch 100, Loss: 2.3734672975540163\n",
      "Epoch 8, Batch 150, Loss: 1.6578406190872192\n",
      "Epoch 8, Batch 200, Loss: 1.3075720620155336\n",
      "Epoch 8, Batch 250, Loss: 1.1908118891716004\n",
      "Epoch 8, Batch 300, Loss: 1.158960120677948\n",
      "Epoch 8, Batch 350, Loss: 1.2057173919677735\n",
      "Epoch 8, Batch 400, Loss: 1.17761666059494\n",
      "Epoch 8, Batch 450, Loss: 1.158359546661377\n",
      "Epoch 8, Batch 500, Loss: 1.1634378457069396\n",
      "Epoch 8, Batch 550, Loss: 1.175426448583603\n",
      "Epoch 8, Batch 600, Loss: 1.2148367619514466\n",
      "Epoch 8, Batch 650, Loss: 1.2971537446975707\n",
      "Epoch 8, Batch 700, Loss: 1.3273935627937317\n",
      "Epoch 8, Batch 750, Loss: 1.306409432888031\n",
      "Epoch 8, Batch 800, Loss: 1.341672945022583\n",
      "Epoch 8, Batch 850, Loss: 1.3666233682632447\n",
      "Epoch 8, Batch 900, Loss: 1.214171597957611\n",
      "Epoch 9, Batch 50, Loss: 1.243026978969574\n",
      "Epoch 9, Batch 100, Loss: 1.214237871170044\n",
      "Epoch 9, Batch 150, Loss: 1.239847023487091\n",
      "Epoch 9, Batch 200, Loss: 1.4500654816627503\n",
      "Epoch 9, Batch 250, Loss: 2.8759514188766477\n",
      "Epoch 9, Batch 300, Loss: 3.442824292182922\n",
      "Epoch 9, Batch 350, Loss: 2.169247930049896\n",
      "Epoch 9, Batch 400, Loss: 1.3192856526374817\n",
      "Epoch 9, Batch 450, Loss: 1.1650803971290589\n",
      "Epoch 9, Batch 500, Loss: 1.1889180374145507\n",
      "Epoch 9, Batch 550, Loss: 1.1772007036209107\n",
      "Epoch 9, Batch 600, Loss: 1.159365825653076\n",
      "Epoch 9, Batch 650, Loss: 1.1813874280452727\n",
      "Epoch 9, Batch 700, Loss: 1.1783911395072937\n",
      "Epoch 9, Batch 750, Loss: 1.1726522290706634\n",
      "Epoch 9, Batch 800, Loss: 1.2098145031929015\n",
      "Epoch 9, Batch 850, Loss: 1.2630146718025208\n",
      "Epoch 9, Batch 900, Loss: 1.3214974260330201\n",
      "Epoch 10, Batch 50, Loss: 1.2327282452583312\n",
      "Epoch 10, Batch 100, Loss: 1.2924613237380982\n",
      "Epoch 10, Batch 150, Loss: 1.4247121238708496\n",
      "Epoch 10, Batch 200, Loss: 1.2452134776115418\n",
      "Epoch 10, Batch 250, Loss: 1.2252258896827697\n",
      "Epoch 10, Batch 300, Loss: 1.1811622428894042\n",
      "Epoch 10, Batch 350, Loss: 1.2036697006225585\n",
      "Epoch 10, Batch 400, Loss: 1.2141454434394836\n",
      "Epoch 10, Batch 450, Loss: 1.2075439620018005\n",
      "Epoch 10, Batch 500, Loss: 1.562226688861847\n",
      "Epoch 10, Batch 550, Loss: 2.70167644739151\n",
      "Epoch 10, Batch 600, Loss: 1.8810307836532594\n",
      "Epoch 10, Batch 650, Loss: 1.8452199912071228\n",
      "Epoch 10, Batch 700, Loss: 1.8211951708793641\n",
      "Epoch 10, Batch 750, Loss: 1.349470226764679\n",
      "Epoch 10, Batch 800, Loss: 1.2258259963989258\n",
      "Epoch 10, Batch 850, Loss: 1.2582422757148743\n",
      "Epoch 10, Batch 900, Loss: 1.2391780638694763\n",
      "Epoch 11, Batch 50, Loss: 1.1820589458942414\n",
      "Epoch 11, Batch 100, Loss: 1.1972763705253602\n",
      "Epoch 11, Batch 150, Loss: 1.2464498209953307\n",
      "Epoch 11, Batch 200, Loss: 1.2253600645065308\n",
      "Epoch 11, Batch 250, Loss: 1.2013528537750244\n",
      "Epoch 11, Batch 300, Loss: 1.1932923364639283\n",
      "Epoch 11, Batch 350, Loss: 1.2022612261772156\n",
      "Epoch 11, Batch 400, Loss: 1.1747318148612975\n",
      "Epoch 11, Batch 450, Loss: 1.1801325500011444\n",
      "Epoch 11, Batch 500, Loss: 1.2016396641731262\n",
      "Epoch 11, Batch 550, Loss: 1.2518120694160462\n",
      "Epoch 11, Batch 600, Loss: 1.1926365780830384\n",
      "Epoch 11, Batch 650, Loss: 1.186187710762024\n",
      "Epoch 11, Batch 700, Loss: 1.3595844960212708\n",
      "Epoch 11, Batch 750, Loss: 2.262547028064728\n",
      "Epoch 11, Batch 800, Loss: 1.283177876472473\n",
      "Epoch 11, Batch 850, Loss: 1.4326970744132996\n",
      "Epoch 11, Batch 900, Loss: 2.1944123458862306\n",
      "Epoch 12, Batch 50, Loss: 1.633017771244049\n",
      "Epoch 12, Batch 100, Loss: 1.3905769300460815\n",
      "Epoch 12, Batch 150, Loss: 1.3697135686874389\n",
      "Epoch 12, Batch 200, Loss: 1.2776815128326415\n",
      "Epoch 12, Batch 250, Loss: 1.3861498737335205\n",
      "Epoch 12, Batch 300, Loss: 1.199241201877594\n",
      "Epoch 12, Batch 350, Loss: 1.1876081919670105\n",
      "Epoch 12, Batch 400, Loss: 1.172239489555359\n",
      "Epoch 12, Batch 450, Loss: 1.2152517175674438\n",
      "Epoch 12, Batch 500, Loss: 1.1895681595802308\n",
      "Epoch 12, Batch 550, Loss: 1.3057872819900513\n",
      "Epoch 12, Batch 600, Loss: 1.2207191920280456\n",
      "Epoch 12, Batch 650, Loss: 1.2412917017936707\n",
      "Epoch 12, Batch 700, Loss: 1.2011241602897644\n",
      "Epoch 12, Batch 750, Loss: 1.2142731213569642\n",
      "Epoch 12, Batch 800, Loss: 1.4033240222930907\n",
      "Epoch 12, Batch 850, Loss: 1.2794282460212707\n",
      "Epoch 12, Batch 900, Loss: 1.2261396908760072\n",
      "Epoch 13, Batch 50, Loss: 1.2392210483551025\n",
      "Epoch 13, Batch 100, Loss: 1.1776169335842133\n",
      "Epoch 13, Batch 150, Loss: 1.261691961288452\n",
      "Epoch 13, Batch 200, Loss: 1.234958963394165\n",
      "Epoch 13, Batch 250, Loss: 1.9196626019477845\n",
      "Epoch 13, Batch 300, Loss: 3.3676186752319337\n",
      "Epoch 13, Batch 350, Loss: 2.9377166485786437\n",
      "Epoch 13, Batch 400, Loss: 1.783850486278534\n",
      "Epoch 13, Batch 450, Loss: 1.175814139842987\n",
      "Epoch 13, Batch 500, Loss: 1.2280988550186158\n",
      "Epoch 13, Batch 550, Loss: 1.261280300617218\n",
      "Epoch 13, Batch 600, Loss: 1.1741921210289001\n",
      "Epoch 13, Batch 650, Loss: 1.187464485168457\n",
      "Epoch 13, Batch 700, Loss: 1.2481026065349579\n",
      "Epoch 13, Batch 750, Loss: 1.2071611189842224\n",
      "Epoch 13, Batch 800, Loss: 1.1776601076126099\n",
      "Epoch 13, Batch 850, Loss: 1.2089719033241273\n",
      "Epoch 13, Batch 900, Loss: 1.2336075186729432\n",
      "Epoch 14, Batch 50, Loss: 1.2091356325149536\n",
      "Epoch 14, Batch 100, Loss: 1.2390669989585876\n",
      "Epoch 14, Batch 150, Loss: 1.252267816066742\n",
      "Epoch 14, Batch 200, Loss: 1.2352190160751342\n",
      "Epoch 14, Batch 250, Loss: 1.2446161150932311\n",
      "Epoch 14, Batch 300, Loss: 1.203794767856598\n",
      "Epoch 14, Batch 350, Loss: 1.1782078909873963\n",
      "Epoch 14, Batch 400, Loss: 1.20216162443161\n",
      "Epoch 14, Batch 450, Loss: 1.191984043121338\n",
      "Epoch 14, Batch 500, Loss: 1.2687592196464539\n",
      "Epoch 14, Batch 550, Loss: 1.2727256655693053\n",
      "Epoch 14, Batch 600, Loss: 1.212770435810089\n",
      "Epoch 14, Batch 650, Loss: 1.2346794891357422\n",
      "Epoch 14, Batch 700, Loss: 2.0591168069839476\n",
      "Epoch 14, Batch 750, Loss: 3.458567891120911\n",
      "Epoch 14, Batch 800, Loss: 3.4927244663238524\n",
      "Epoch 14, Batch 850, Loss: 2.299112195968628\n",
      "Epoch 14, Batch 900, Loss: 1.2561697697639465\n",
      "Epoch 15, Batch 50, Loss: 1.190868046283722\n",
      "Epoch 15, Batch 100, Loss: 1.1613766050338745\n",
      "Epoch 15, Batch 150, Loss: 1.2104946303367614\n",
      "Epoch 15, Batch 200, Loss: 1.1815978240966798\n",
      "Epoch 15, Batch 250, Loss: 1.1767686712741852\n",
      "Epoch 15, Batch 300, Loss: 1.1991541028022765\n",
      "Epoch 15, Batch 350, Loss: 1.1785016345977783\n",
      "Epoch 15, Batch 400, Loss: 1.1727274346351624\n",
      "Epoch 15, Batch 450, Loss: 1.266754505634308\n",
      "Epoch 15, Batch 500, Loss: 1.337235791683197\n",
      "Epoch 15, Batch 550, Loss: 1.2432845497131348\n",
      "Epoch 15, Batch 600, Loss: 1.3355463290214538\n",
      "Epoch 15, Batch 650, Loss: 1.2155690598487854\n",
      "Epoch 15, Batch 700, Loss: 1.3102209043502808\n",
      "Epoch 15, Batch 750, Loss: 1.2618172979354858\n",
      "Epoch 15, Batch 800, Loss: 1.204039340019226\n",
      "Epoch 15, Batch 850, Loss: 1.2043717312812805\n",
      "Epoch 15, Batch 900, Loss: 1.200106885433197\n",
      "Epoch 16, Batch 50, Loss: 1.2802956771850587\n",
      "Epoch 16, Batch 100, Loss: 2.244179072380066\n",
      "Epoch 16, Batch 150, Loss: 2.356474032402039\n",
      "Epoch 16, Batch 200, Loss: 2.223584461212158\n",
      "Epoch 16, Batch 250, Loss: 1.3626323628425598\n",
      "Epoch 16, Batch 300, Loss: 1.1955345726013185\n",
      "Epoch 16, Batch 350, Loss: 1.220334873199463\n",
      "Epoch 16, Batch 400, Loss: 1.1984932589530946\n",
      "Epoch 16, Batch 450, Loss: 1.1940030765533447\n",
      "Epoch 16, Batch 500, Loss: 1.1720491933822632\n",
      "Epoch 16, Batch 550, Loss: 1.1967744278907775\n",
      "Epoch 16, Batch 600, Loss: 1.1733678483963013\n",
      "Epoch 16, Batch 650, Loss: 1.2186224675178527\n",
      "Epoch 16, Batch 700, Loss: 1.2475797867774963\n",
      "Epoch 16, Batch 750, Loss: 1.2262458896636963\n",
      "Epoch 16, Batch 800, Loss: 1.2117639684677124\n",
      "Epoch 16, Batch 850, Loss: 1.1960858488082886\n",
      "Epoch 16, Batch 900, Loss: 1.2770236682891847\n",
      "Epoch 17, Batch 50, Loss: 1.5295307350158691\n",
      "Epoch 17, Batch 100, Loss: 1.2717458629608154\n",
      "Epoch 17, Batch 150, Loss: 1.1986015272140502\n",
      "Epoch 17, Batch 200, Loss: 1.157770230770111\n",
      "Epoch 17, Batch 250, Loss: 1.1998939657211303\n",
      "Epoch 17, Batch 300, Loss: 1.2574204015731811\n",
      "Epoch 17, Batch 350, Loss: 1.3230308604240417\n",
      "Epoch 17, Batch 400, Loss: 1.4209434366226197\n",
      "Epoch 17, Batch 450, Loss: 1.6122542786598206\n",
      "Epoch 17, Batch 500, Loss: 1.6036645984649658\n",
      "Epoch 17, Batch 550, Loss: 1.9223620748519898\n",
      "Epoch 17, Batch 600, Loss: 1.3943830847740173\n",
      "Epoch 17, Batch 650, Loss: 1.7808134436607361\n",
      "Epoch 17, Batch 700, Loss: 2.8021144700050353\n",
      "Epoch 17, Batch 750, Loss: 1.6591953468322753\n",
      "Epoch 17, Batch 800, Loss: 1.2958092784881592\n",
      "Epoch 17, Batch 850, Loss: 1.2033873915672302\n",
      "Epoch 17, Batch 900, Loss: 1.2100236773490907\n",
      "Epoch 18, Batch 50, Loss: 1.272322428226471\n",
      "Epoch 18, Batch 100, Loss: 1.1873363423347474\n",
      "Epoch 18, Batch 150, Loss: 1.3513314461708068\n",
      "Epoch 18, Batch 200, Loss: 1.2282491850852966\n",
      "Epoch 18, Batch 250, Loss: 1.1743588590621947\n",
      "Epoch 18, Batch 300, Loss: 1.239196882247925\n",
      "Epoch 18, Batch 350, Loss: 1.321031677722931\n",
      "Epoch 18, Batch 400, Loss: 1.1803717708587647\n",
      "Epoch 18, Batch 450, Loss: 1.193868019580841\n",
      "Epoch 18, Batch 500, Loss: 1.2739563965797425\n",
      "Epoch 18, Batch 550, Loss: 1.233949384689331\n",
      "Epoch 18, Batch 600, Loss: 1.414798789024353\n",
      "Epoch 18, Batch 650, Loss: 1.2699879717826843\n",
      "Epoch 18, Batch 700, Loss: 1.2702855396270751\n",
      "Epoch 18, Batch 750, Loss: 1.3483901739120483\n",
      "Epoch 18, Batch 800, Loss: 1.2817710638046265\n",
      "Epoch 18, Batch 850, Loss: 1.3390007042884826\n",
      "Epoch 18, Batch 900, Loss: 1.8271200847625733\n",
      "Epoch 19, Batch 50, Loss: 1.2518106150627135\n",
      "Epoch 19, Batch 100, Loss: 1.3860442924499512\n",
      "Epoch 19, Batch 150, Loss: 1.3483779883384706\n",
      "Epoch 19, Batch 200, Loss: 1.1964769434928895\n",
      "Epoch 19, Batch 250, Loss: 1.1709696626663209\n",
      "Epoch 19, Batch 300, Loss: 1.2101661348342896\n",
      "Epoch 19, Batch 350, Loss: 1.2150683426856994\n",
      "Epoch 19, Batch 400, Loss: 1.185999722480774\n",
      "Epoch 19, Batch 450, Loss: 1.1973039937019347\n",
      "Epoch 19, Batch 500, Loss: 1.208103804588318\n",
      "Epoch 19, Batch 550, Loss: 1.188843457698822\n",
      "Epoch 19, Batch 600, Loss: 1.2010007548332213\n",
      "Epoch 19, Batch 650, Loss: 1.2421544408798217\n",
      "Epoch 19, Batch 700, Loss: 1.2224196219444274\n",
      "Epoch 19, Batch 750, Loss: 1.5736781024932862\n",
      "Epoch 19, Batch 800, Loss: 3.993132407665253\n",
      "Epoch 19, Batch 850, Loss: 2.502010934352875\n",
      "Epoch 19, Batch 900, Loss: 2.1042004776000978\n",
      "Epoch 20, Batch 50, Loss: 1.2634015107154846\n",
      "Epoch 20, Batch 100, Loss: 1.1894647836685182\n",
      "Epoch 20, Batch 150, Loss: 1.213699152469635\n",
      "Epoch 20, Batch 200, Loss: 1.1964799606800078\n",
      "Epoch 20, Batch 250, Loss: 1.2420708751678466\n",
      "Epoch 20, Batch 300, Loss: 1.2086635756492614\n",
      "Epoch 20, Batch 350, Loss: 1.1822474813461303\n",
      "Epoch 20, Batch 400, Loss: 1.269778904914856\n",
      "Epoch 20, Batch 450, Loss: 1.251211702823639\n",
      "Epoch 20, Batch 500, Loss: 1.1983541643619537\n",
      "Epoch 20, Batch 550, Loss: 1.218822057247162\n",
      "Epoch 20, Batch 600, Loss: 1.2285845732688905\n",
      "Epoch 20, Batch 650, Loss: 1.2895849299430848\n",
      "Epoch 20, Batch 700, Loss: 1.3311669373512267\n",
      "Epoch 20, Batch 750, Loss: 1.1844000244140624\n",
      "Epoch 20, Batch 800, Loss: 1.1677988839149476\n",
      "Epoch 20, Batch 850, Loss: 1.2691843605041504\n",
      "Epoch 20, Batch 900, Loss: 1.2956176233291625\n",
      "Epoch 21, Batch 50, Loss: 3.907110435962677\n",
      "Epoch 21, Batch 100, Loss: 2.924191975593567\n",
      "Epoch 21, Batch 150, Loss: 1.9600752639770507\n",
      "Epoch 21, Batch 200, Loss: 1.192595932483673\n",
      "Epoch 21, Batch 250, Loss: 1.1539908266067505\n",
      "Epoch 21, Batch 300, Loss: 1.1935086607933045\n",
      "Epoch 21, Batch 350, Loss: 1.1842495799064636\n",
      "Epoch 21, Batch 400, Loss: 1.1767628288269043\n",
      "Epoch 21, Batch 450, Loss: 1.1691501641273498\n",
      "Epoch 21, Batch 500, Loss: 1.2120037579536438\n",
      "Epoch 21, Batch 550, Loss: 1.165338008403778\n",
      "Epoch 21, Batch 600, Loss: 1.1820443606376647\n",
      "Epoch 21, Batch 650, Loss: 1.2074166131019592\n",
      "Epoch 21, Batch 700, Loss: 1.2025864458084106\n",
      "Epoch 21, Batch 750, Loss: 1.3064809012413026\n",
      "Epoch 21, Batch 800, Loss: 1.2416697239875794\n",
      "Epoch 21, Batch 850, Loss: 1.236980962753296\n",
      "Epoch 21, Batch 900, Loss: 1.2573465633392333\n",
      "Epoch 22, Batch 50, Loss: 1.2384034848213197\n",
      "Epoch 22, Batch 100, Loss: 1.252406644821167\n",
      "Epoch 22, Batch 150, Loss: 1.1803117156028748\n",
      "Epoch 22, Batch 200, Loss: 1.188334059715271\n",
      "Epoch 22, Batch 250, Loss: 1.21957888007164\n",
      "Epoch 22, Batch 300, Loss: 1.39230206489563\n",
      "Epoch 22, Batch 350, Loss: 1.595675368309021\n",
      "Epoch 22, Batch 400, Loss: 4.531604633331299\n",
      "Epoch 22, Batch 450, Loss: 3.2339078783988953\n",
      "Epoch 22, Batch 500, Loss: 1.4698422694206237\n",
      "Epoch 22, Batch 550, Loss: 1.1816093707084656\n",
      "Epoch 22, Batch 600, Loss: 1.1839338088035583\n",
      "Epoch 22, Batch 650, Loss: 1.194981586933136\n",
      "Epoch 22, Batch 700, Loss: 1.228879508972168\n",
      "Epoch 22, Batch 750, Loss: 1.3337261366844178\n",
      "Epoch 22, Batch 800, Loss: 1.2920461893081665\n",
      "Epoch 22, Batch 850, Loss: 1.2147401666641235\n",
      "Epoch 22, Batch 900, Loss: 1.2338140726089477\n",
      "Epoch 23, Batch 50, Loss: 1.2088124871253967\n",
      "Epoch 23, Batch 100, Loss: 1.1777888822555542\n",
      "Epoch 23, Batch 150, Loss: 1.2012139678001403\n",
      "Epoch 23, Batch 200, Loss: 1.181129628419876\n",
      "Epoch 23, Batch 250, Loss: 1.2343680429458619\n",
      "Epoch 23, Batch 300, Loss: 1.2021406030654906\n",
      "Epoch 23, Batch 350, Loss: 1.1885342049598693\n",
      "Epoch 23, Batch 400, Loss: 1.2026090359687804\n",
      "Epoch 23, Batch 450, Loss: 1.3205279207229614\n",
      "Epoch 23, Batch 500, Loss: 1.2288983607292174\n",
      "Epoch 23, Batch 550, Loss: 1.2161354041099548\n",
      "Epoch 23, Batch 600, Loss: 1.2076820206642152\n",
      "Epoch 23, Batch 650, Loss: 1.2523179364204406\n",
      "Epoch 23, Batch 700, Loss: 1.3456796789169312\n",
      "Epoch 23, Batch 750, Loss: 1.9255566310882568\n",
      "Epoch 23, Batch 800, Loss: 3.036953535079956\n",
      "Epoch 23, Batch 850, Loss: 3.4016272401809693\n",
      "Epoch 23, Batch 900, Loss: 2.1272582626342773\n",
      "Epoch 24, Batch 50, Loss: 1.2053089785575866\n",
      "Epoch 24, Batch 100, Loss: 1.1806554460525513\n",
      "Epoch 24, Batch 150, Loss: 1.21128648519516\n",
      "Epoch 24, Batch 200, Loss: 1.2232176959514618\n",
      "Epoch 24, Batch 250, Loss: 1.186633243560791\n",
      "Epoch 24, Batch 300, Loss: 1.2042757844924927\n",
      "Epoch 24, Batch 350, Loss: 1.1896152305603027\n",
      "Epoch 24, Batch 400, Loss: 1.1974293279647827\n",
      "Epoch 24, Batch 450, Loss: 1.1785656595230103\n",
      "Epoch 24, Batch 500, Loss: 1.2075274550914765\n",
      "Epoch 24, Batch 550, Loss: 1.1905616211891175\n",
      "Epoch 24, Batch 600, Loss: 1.1916984963417052\n",
      "Epoch 24, Batch 650, Loss: 1.2196078872680665\n",
      "Epoch 24, Batch 700, Loss: 1.19129869222641\n",
      "Epoch 24, Batch 750, Loss: 1.213236699104309\n",
      "Epoch 24, Batch 800, Loss: 1.2508030247688293\n",
      "Epoch 24, Batch 850, Loss: 1.4378802585601806\n",
      "Epoch 24, Batch 900, Loss: 1.225940053462982\n",
      "Epoch 25, Batch 50, Loss: 1.5793675804138183\n",
      "Epoch 25, Batch 100, Loss: 1.43662832736969\n",
      "Epoch 25, Batch 150, Loss: 1.3411225414276122\n",
      "Epoch 25, Batch 200, Loss: 1.352873785495758\n",
      "Epoch 25, Batch 250, Loss: 1.3693160033226013\n",
      "Epoch 25, Batch 300, Loss: 1.4711359357833862\n",
      "Epoch 25, Batch 350, Loss: 1.2496017003059388\n",
      "Epoch 25, Batch 400, Loss: 1.2094020652770996\n",
      "Epoch 25, Batch 450, Loss: 1.2119527602195739\n",
      "Epoch 25, Batch 500, Loss: 1.298235387802124\n",
      "Epoch 25, Batch 550, Loss: 1.3317367136478424\n",
      "Epoch 25, Batch 600, Loss: 1.3196269035339356\n",
      "Epoch 25, Batch 650, Loss: 2.1974920988082887\n",
      "Epoch 25, Batch 700, Loss: 2.185191249847412\n",
      "Epoch 25, Batch 750, Loss: 3.4206240510940553\n",
      "Epoch 25, Batch 800, Loss: 1.8487263345718383\n",
      "Epoch 25, Batch 850, Loss: 1.2133403992652894\n",
      "Epoch 25, Batch 900, Loss: 1.182599468231201\n",
      "Accuracy on test set: 0.1%\n",
      "Fitting for combination 51\n",
      "784\n",
      "3\n",
      "10\n",
      "[40, 10, 30, 10]\n",
      "True\n",
      "['relu', 'sigmoid', 'tanh']\n",
      "SGD\n",
      "0.3\n",
      "0.3\n",
      "CrossEntropyLoss\n",
      "100\n",
      "25\n",
      "0\n",
      "Epoch 1, Batch 100, Loss: 2.3034796023368838\n",
      "Epoch 1, Batch 200, Loss: 2.3027376747131347\n",
      "Epoch 1, Batch 300, Loss: 2.302802996635437\n",
      "Epoch 1, Batch 400, Loss: 2.3029938316345215\n",
      "Epoch 1, Batch 500, Loss: 2.3027716374397276\n",
      "Epoch 1, Batch 600, Loss: 2.303034346103668\n",
      "Epoch 1, Batch 700, Loss: 2.3022303080558775\n",
      "Epoch 1, Batch 800, Loss: 2.3029310274124146\n",
      "Epoch 1, Batch 900, Loss: 2.3027827095985414\n",
      "Epoch 2, Batch 100, Loss: 2.302836766242981\n",
      "Epoch 2, Batch 200, Loss: 2.302898094654083\n",
      "Epoch 2, Batch 300, Loss: 2.3032148838043214\n",
      "Epoch 2, Batch 400, Loss: 2.303239333629608\n",
      "Epoch 2, Batch 500, Loss: 2.302614381313324\n",
      "Epoch 2, Batch 600, Loss: 2.3030090284347535\n",
      "Epoch 2, Batch 700, Loss: 2.302766056060791\n",
      "Epoch 2, Batch 800, Loss: 2.302584388256073\n",
      "Epoch 2, Batch 900, Loss: 2.302874193191528\n",
      "Epoch 3, Batch 100, Loss: 2.3026563262939455\n",
      "Epoch 3, Batch 200, Loss: 2.303077516555786\n",
      "Epoch 3, Batch 300, Loss: 2.3031070494651793\n",
      "Epoch 3, Batch 400, Loss: 2.302833960056305\n",
      "Epoch 3, Batch 500, Loss: 2.302447257041931\n",
      "Epoch 3, Batch 600, Loss: 2.3027371907234193\n",
      "Epoch 3, Batch 700, Loss: 2.3028770112991332\n",
      "Epoch 3, Batch 800, Loss: 2.3027785062789916\n",
      "Epoch 3, Batch 900, Loss: 2.3027514219284058\n",
      "Epoch 4, Batch 100, Loss: 2.3028460001945494\n",
      "Epoch 4, Batch 200, Loss: 2.3030366134643554\n",
      "Epoch 4, Batch 300, Loss: 2.302963333129883\n",
      "Epoch 4, Batch 400, Loss: 2.302726743221283\n",
      "Epoch 4, Batch 500, Loss: 2.303245906829834\n",
      "Epoch 4, Batch 600, Loss: 2.3027083396911623\n",
      "Epoch 4, Batch 700, Loss: 2.303045241832733\n",
      "Epoch 4, Batch 800, Loss: 2.3028165340423583\n",
      "Epoch 4, Batch 900, Loss: 2.302818419933319\n",
      "Epoch 5, Batch 100, Loss: 2.3028565001487733\n",
      "Epoch 5, Batch 200, Loss: 2.302873923778534\n",
      "Epoch 5, Batch 300, Loss: 2.3029501104354857\n",
      "Epoch 5, Batch 400, Loss: 2.302928650379181\n",
      "Epoch 5, Batch 500, Loss: 2.3028532695770263\n",
      "Epoch 5, Batch 600, Loss: 2.302558116912842\n",
      "Epoch 5, Batch 700, Loss: 2.3025609636306763\n",
      "Epoch 5, Batch 800, Loss: 2.303208866119385\n",
      "Epoch 5, Batch 900, Loss: 2.302722692489624\n",
      "Epoch 6, Batch 100, Loss: 2.302195198535919\n",
      "Epoch 6, Batch 200, Loss: 2.302637004852295\n",
      "Epoch 6, Batch 300, Loss: 2.3026618361473083\n",
      "Epoch 6, Batch 400, Loss: 2.3024363136291504\n",
      "Epoch 6, Batch 500, Loss: 2.3031117296218873\n",
      "Epoch 6, Batch 600, Loss: 2.3027298855781555\n",
      "Epoch 6, Batch 700, Loss: 2.3029788279533387\n",
      "Epoch 6, Batch 800, Loss: 2.302416594028473\n",
      "Epoch 6, Batch 900, Loss: 2.303020339012146\n",
      "Epoch 7, Batch 100, Loss: 2.3030334162712096\n",
      "Epoch 7, Batch 200, Loss: 2.3030737900733946\n",
      "Epoch 7, Batch 300, Loss: 2.302950868606567\n",
      "Epoch 7, Batch 400, Loss: 2.303174006938934\n",
      "Epoch 7, Batch 500, Loss: 2.302680594921112\n",
      "Epoch 7, Batch 600, Loss: 2.3023135328292845\n",
      "Epoch 7, Batch 700, Loss: 2.3030789136886596\n",
      "Epoch 7, Batch 800, Loss: 2.303144626617432\n",
      "Epoch 7, Batch 900, Loss: 2.302682702541351\n",
      "Epoch 8, Batch 100, Loss: 2.3032669258117675\n",
      "Epoch 8, Batch 200, Loss: 2.302664396762848\n",
      "Epoch 8, Batch 300, Loss: 2.302994849681854\n",
      "Epoch 8, Batch 400, Loss: 2.3027051854133607\n",
      "Epoch 8, Batch 500, Loss: 2.303153202533722\n",
      "Epoch 8, Batch 600, Loss: 2.3030801010131836\n",
      "Epoch 8, Batch 700, Loss: 2.302924835681915\n",
      "Epoch 8, Batch 800, Loss: 2.302934474945068\n",
      "Epoch 8, Batch 900, Loss: 2.302691810131073\n",
      "Epoch 9, Batch 100, Loss: 2.302997076511383\n",
      "Epoch 9, Batch 200, Loss: 2.3022802591323854\n",
      "Epoch 9, Batch 300, Loss: 2.3030580496788025\n",
      "Epoch 9, Batch 400, Loss: 2.3033618831634524\n",
      "Epoch 9, Batch 500, Loss: 2.3030612778663637\n",
      "Epoch 9, Batch 600, Loss: 2.3028502297401428\n",
      "Epoch 9, Batch 700, Loss: 2.303084259033203\n",
      "Epoch 9, Batch 800, Loss: 2.30292542219162\n",
      "Epoch 9, Batch 900, Loss: 2.3027838039398194\n",
      "Epoch 10, Batch 100, Loss: 2.303182051181793\n",
      "Epoch 10, Batch 200, Loss: 2.302506914138794\n",
      "Epoch 10, Batch 300, Loss: 2.303126723766327\n",
      "Epoch 10, Batch 400, Loss: 2.302901017665863\n",
      "Epoch 10, Batch 500, Loss: 2.3028211784362793\n",
      "Epoch 10, Batch 600, Loss: 2.3029404950141905\n",
      "Epoch 10, Batch 700, Loss: 2.3027866864204407\n",
      "Epoch 10, Batch 800, Loss: 2.303158309459686\n",
      "Epoch 10, Batch 900, Loss: 2.3028674054145815\n",
      "Epoch 11, Batch 100, Loss: 2.303168246746063\n",
      "Epoch 11, Batch 200, Loss: 2.3024086475372316\n",
      "Epoch 11, Batch 300, Loss: 2.3030447864532473\n",
      "Epoch 11, Batch 400, Loss: 2.3026647686958315\n",
      "Epoch 11, Batch 500, Loss: 2.303188674449921\n",
      "Epoch 11, Batch 600, Loss: 2.3030098986625673\n",
      "Epoch 11, Batch 700, Loss: 2.3030939888954163\n",
      "Epoch 11, Batch 800, Loss: 2.3027645993232726\n",
      "Epoch 11, Batch 900, Loss: 2.302713444232941\n",
      "Epoch 12, Batch 100, Loss: 2.303144373893738\n",
      "Epoch 12, Batch 200, Loss: 2.303097641468048\n",
      "Epoch 12, Batch 300, Loss: 2.302507584095001\n",
      "Epoch 12, Batch 400, Loss: 2.3031360483169556\n",
      "Epoch 12, Batch 500, Loss: 2.3028500628471376\n",
      "Epoch 12, Batch 600, Loss: 2.3029694747924805\n",
      "Epoch 12, Batch 700, Loss: 2.303061330318451\n",
      "Epoch 12, Batch 800, Loss: 2.3028650951385496\n",
      "Epoch 12, Batch 900, Loss: 2.3029820203781126\n",
      "Epoch 13, Batch 100, Loss: 2.3024656677246096\n",
      "Epoch 13, Batch 200, Loss: 2.302534215450287\n",
      "Epoch 13, Batch 300, Loss: 2.3026867961883544\n",
      "Epoch 13, Batch 400, Loss: 2.302780771255493\n",
      "Epoch 13, Batch 500, Loss: 2.302748136520386\n",
      "Epoch 13, Batch 600, Loss: 2.303379945755005\n",
      "Epoch 13, Batch 700, Loss: 2.302892875671387\n",
      "Epoch 13, Batch 800, Loss: 2.3029807353019716\n",
      "Epoch 13, Batch 900, Loss: 2.3032629466056824\n",
      "Epoch 14, Batch 100, Loss: 2.302767162322998\n",
      "Epoch 14, Batch 200, Loss: 2.303195116519928\n",
      "Epoch 14, Batch 300, Loss: 2.3031615447998046\n",
      "Epoch 14, Batch 400, Loss: 2.3027695870399474\n",
      "Epoch 14, Batch 500, Loss: 2.3026504707336426\n",
      "Epoch 14, Batch 600, Loss: 2.3029277396202086\n",
      "Epoch 14, Batch 700, Loss: 2.3031472873687746\n",
      "Epoch 14, Batch 800, Loss: 2.302706422805786\n",
      "Epoch 14, Batch 900, Loss: 2.302605998516083\n",
      "Epoch 15, Batch 100, Loss: 2.303111186027527\n",
      "Epoch 15, Batch 200, Loss: 2.3027959394454958\n",
      "Epoch 15, Batch 300, Loss: 2.30263215303421\n",
      "Epoch 15, Batch 400, Loss: 2.302561740875244\n",
      "Epoch 15, Batch 500, Loss: 2.3024658608436583\n",
      "Epoch 15, Batch 600, Loss: 2.303267822265625\n",
      "Epoch 15, Batch 700, Loss: 2.3030245876312256\n",
      "Epoch 15, Batch 800, Loss: 2.3030724573135375\n",
      "Epoch 15, Batch 900, Loss: 2.303204047679901\n",
      "Epoch 16, Batch 100, Loss: 2.3028900527954104\n",
      "Epoch 16, Batch 200, Loss: 2.3025353527069092\n",
      "Epoch 16, Batch 300, Loss: 2.303183193206787\n",
      "Epoch 16, Batch 400, Loss: 2.302586853504181\n",
      "Epoch 16, Batch 500, Loss: 2.3028852915763856\n",
      "Epoch 16, Batch 600, Loss: 2.3029921102523803\n",
      "Epoch 16, Batch 700, Loss: 2.302527754306793\n",
      "Epoch 16, Batch 800, Loss: 2.302758219242096\n",
      "Epoch 16, Batch 900, Loss: 2.302638075351715\n",
      "Epoch 17, Batch 100, Loss: 2.3028410124778746\n",
      "Epoch 17, Batch 200, Loss: 2.3030441212654114\n",
      "Epoch 17, Batch 300, Loss: 2.302793126106262\n",
      "Epoch 17, Batch 400, Loss: 2.302705464363098\n",
      "Epoch 17, Batch 500, Loss: 2.3029109501838683\n",
      "Epoch 17, Batch 600, Loss: 2.3021771001815794\n",
      "Epoch 17, Batch 700, Loss: 2.3028675222396853\n",
      "Epoch 17, Batch 800, Loss: 2.3034413647651673\n",
      "Epoch 17, Batch 900, Loss: 2.3029212188720702\n",
      "Epoch 18, Batch 100, Loss: 2.3028474068641662\n",
      "Epoch 18, Batch 200, Loss: 2.303172912597656\n",
      "Epoch 18, Batch 300, Loss: 2.30316862821579\n",
      "Epoch 18, Batch 400, Loss: 2.3031687426567076\n",
      "Epoch 18, Batch 500, Loss: 2.3030870008468627\n",
      "Epoch 18, Batch 600, Loss: 2.302902252674103\n",
      "Epoch 18, Batch 700, Loss: 2.3025616908073427\n",
      "Epoch 18, Batch 800, Loss: 2.3026670145988466\n",
      "Epoch 18, Batch 900, Loss: 2.303093194961548\n",
      "Epoch 19, Batch 100, Loss: 2.3027794623374938\n",
      "Epoch 19, Batch 200, Loss: 2.3027275943756105\n",
      "Epoch 19, Batch 300, Loss: 2.3029016709327697\n",
      "Epoch 19, Batch 400, Loss: 2.302920255661011\n",
      "Epoch 19, Batch 500, Loss: 2.3027272176742555\n",
      "Epoch 19, Batch 600, Loss: 2.303053290843964\n",
      "Epoch 19, Batch 700, Loss: 2.302641792297363\n",
      "Epoch 19, Batch 800, Loss: 2.302555253505707\n",
      "Epoch 19, Batch 900, Loss: 2.3028332138061525\n",
      "Epoch 20, Batch 100, Loss: 2.3027971363067627\n",
      "Epoch 20, Batch 200, Loss: 2.302503628730774\n",
      "Epoch 20, Batch 300, Loss: 2.303029224872589\n",
      "Epoch 20, Batch 400, Loss: 2.3028502345085142\n",
      "Epoch 20, Batch 500, Loss: 2.303114573955536\n",
      "Epoch 20, Batch 600, Loss: 2.302739462852478\n",
      "Epoch 20, Batch 700, Loss: 2.302985887527466\n",
      "Epoch 20, Batch 800, Loss: 2.3025105690956114\n",
      "Epoch 20, Batch 900, Loss: 2.303321738243103\n",
      "Epoch 21, Batch 100, Loss: 2.3031029510498047\n",
      "Epoch 21, Batch 200, Loss: 2.302784035205841\n",
      "Epoch 21, Batch 300, Loss: 2.303408362865448\n",
      "Epoch 21, Batch 400, Loss: 2.3027833557128905\n",
      "Epoch 21, Batch 500, Loss: 2.3033073568344116\n",
      "Epoch 21, Batch 600, Loss: 2.3022762799263\n",
      "Epoch 21, Batch 700, Loss: 2.3030174779891968\n",
      "Epoch 21, Batch 800, Loss: 2.302792229652405\n",
      "Epoch 21, Batch 900, Loss: 2.3029956603050232\n",
      "Epoch 22, Batch 100, Loss: 2.302692003250122\n",
      "Epoch 22, Batch 200, Loss: 2.3029190373420714\n",
      "Epoch 22, Batch 300, Loss: 2.3031825542449953\n",
      "Epoch 22, Batch 400, Loss: 2.302364513874054\n",
      "Epoch 22, Batch 500, Loss: 2.303089234828949\n",
      "Epoch 22, Batch 600, Loss: 2.3028458189964294\n",
      "Epoch 22, Batch 700, Loss: 2.303144648075104\n",
      "Epoch 22, Batch 800, Loss: 2.3033020210266115\n",
      "Epoch 22, Batch 900, Loss: 2.3030194735527036\n",
      "Epoch 23, Batch 100, Loss: 2.3030577397346494\n",
      "Epoch 23, Batch 200, Loss: 2.3024777698516847\n",
      "Epoch 23, Batch 300, Loss: 2.303081576824188\n",
      "Epoch 23, Batch 400, Loss: 2.302747657299042\n",
      "Epoch 23, Batch 500, Loss: 2.3026155281066893\n",
      "Epoch 23, Batch 600, Loss: 2.302649917602539\n",
      "Epoch 23, Batch 700, Loss: 2.302817885875702\n",
      "Epoch 23, Batch 800, Loss: 2.3031974816322327\n",
      "Epoch 23, Batch 900, Loss: 2.303408269882202\n",
      "Epoch 24, Batch 100, Loss: 2.3028426599502563\n",
      "Epoch 24, Batch 200, Loss: 2.3032238936424254\n",
      "Epoch 24, Batch 300, Loss: 2.3030607318878173\n",
      "Epoch 24, Batch 400, Loss: 2.3030265593528747\n",
      "Epoch 24, Batch 500, Loss: 2.3027451062202453\n",
      "Epoch 24, Batch 600, Loss: 2.3031627321243286\n",
      "Epoch 24, Batch 700, Loss: 2.302935252189636\n",
      "Epoch 24, Batch 800, Loss: 2.3023963809013366\n",
      "Epoch 24, Batch 900, Loss: 2.3027903246879577\n",
      "Epoch 25, Batch 100, Loss: 2.3029818534851074\n",
      "Epoch 25, Batch 200, Loss: 2.3029675030708314\n",
      "Epoch 25, Batch 300, Loss: 2.302792568206787\n",
      "Epoch 25, Batch 400, Loss: 2.3030498313903807\n",
      "Epoch 25, Batch 500, Loss: 2.302783894538879\n",
      "Epoch 25, Batch 600, Loss: 2.3030477142333985\n",
      "Epoch 25, Batch 700, Loss: 2.303180367946625\n",
      "Epoch 25, Batch 800, Loss: 2.3031831622123717\n",
      "Epoch 25, Batch 900, Loss: 2.302446584701538\n",
      "Accuracy on test set: 0.1%\n",
      "Fitting for combination 52\n",
      "784\n",
      "3\n",
      "10\n",
      "[40, 10, 40, 10]\n",
      "False\n",
      "['relu', 'sigmoid', 'tanh']\n",
      "Adam\n",
      "0.3\n",
      "0.01\n",
      "CrossEntropyLoss\n",
      "200\n",
      "25\n",
      "0\n",
      "Epoch 1, Batch 200, Loss: 6.611633433103561\n",
      "Epoch 1, Batch 400, Loss: 7.436080930233001\n",
      "Epoch 1, Batch 600, Loss: 4.813166203498841\n",
      "Epoch 1, Batch 800, Loss: 4.738075776100159\n",
      "Epoch 2, Batch 200, Loss: 10.389312448501586\n",
      "Epoch 2, Batch 400, Loss: 4.83339802980423\n",
      "Epoch 2, Batch 600, Loss: 4.715104207992554\n",
      "Epoch 2, Batch 800, Loss: 9.465784595012664\n",
      "Epoch 3, Batch 200, Loss: 4.652580904960632\n",
      "Epoch 3, Batch 400, Loss: 4.734567605257034\n",
      "Epoch 3, Batch 600, Loss: 5.066623296737671\n",
      "Epoch 3, Batch 800, Loss: 5.04900703907013\n",
      "Epoch 4, Batch 200, Loss: 14.208988122940063\n",
      "Epoch 4, Batch 400, Loss: 6.104293315410614\n",
      "Epoch 4, Batch 600, Loss: 5.0240593743324276\n",
      "Epoch 4, Batch 800, Loss: 5.036082963943482\n",
      "Epoch 5, Batch 200, Loss: 4.848658187389374\n",
      "Epoch 5, Batch 400, Loss: 4.877676508426666\n",
      "Epoch 5, Batch 600, Loss: 9.48827588558197\n",
      "Epoch 5, Batch 800, Loss: 9.221600127220153\n",
      "Epoch 6, Batch 200, Loss: 4.758763840198517\n",
      "Epoch 6, Batch 400, Loss: 4.947399154901505\n",
      "Epoch 6, Batch 600, Loss: 4.768065221309662\n",
      "Epoch 6, Batch 800, Loss: 4.8545191287994385\n",
      "Epoch 7, Batch 200, Loss: 10.18853675842285\n",
      "Epoch 7, Batch 400, Loss: 4.873136434555054\n",
      "Epoch 7, Batch 600, Loss: 4.974214160442353\n",
      "Epoch 7, Batch 800, Loss: 4.856374197006225\n",
      "Epoch 8, Batch 200, Loss: 13.898057277202605\n",
      "Epoch 8, Batch 400, Loss: 4.98405641913414\n",
      "Epoch 8, Batch 600, Loss: 4.7952302706241605\n",
      "Epoch 8, Batch 800, Loss: 4.706596412658691\n",
      "Epoch 9, Batch 200, Loss: 4.825716233253479\n",
      "Epoch 9, Batch 400, Loss: 4.966337251663208\n",
      "Epoch 9, Batch 600, Loss: 4.955303370952606\n",
      "Epoch 9, Batch 800, Loss: 17.272334897518157\n",
      "Epoch 10, Batch 200, Loss: 4.7309697663784025\n",
      "Epoch 10, Batch 400, Loss: 4.668056428432465\n",
      "Epoch 10, Batch 600, Loss: 10.50814178943634\n",
      "Epoch 10, Batch 800, Loss: 7.979707679748535\n",
      "Epoch 11, Batch 200, Loss: 4.709968430995941\n",
      "Epoch 11, Batch 400, Loss: 4.751513392925262\n",
      "Epoch 11, Batch 600, Loss: 4.839066405296325\n",
      "Epoch 11, Batch 800, Loss: 4.860811653137207\n",
      "Epoch 12, Batch 200, Loss: 10.637236585617066\n",
      "Epoch 12, Batch 400, Loss: 10.963187553882598\n",
      "Epoch 12, Batch 600, Loss: 4.744394017457962\n",
      "Epoch 12, Batch 800, Loss: 4.814863482713699\n",
      "Epoch 13, Batch 200, Loss: 5.269756479263306\n",
      "Epoch 13, Batch 400, Loss: 5.02027507185936\n",
      "Epoch 13, Batch 600, Loss: 4.908169374465943\n",
      "Epoch 13, Batch 800, Loss: 6.830265843868256\n",
      "Epoch 14, Batch 200, Loss: 5.733539817333221\n",
      "Epoch 14, Batch 400, Loss: 4.9563380408287045\n",
      "Epoch 14, Batch 600, Loss: 4.7614705276489255\n",
      "Epoch 14, Batch 800, Loss: 4.848644387722016\n",
      "Epoch 15, Batch 200, Loss: 15.61672490119934\n",
      "Epoch 15, Batch 400, Loss: 5.109794096946716\n",
      "Epoch 15, Batch 600, Loss: 4.6940680027008055\n",
      "Epoch 15, Batch 800, Loss: 4.863901044130325\n",
      "Epoch 16, Batch 200, Loss: 8.261201949119569\n",
      "Epoch 16, Batch 400, Loss: 9.989658391475677\n",
      "Epoch 16, Batch 600, Loss: 4.804884328842163\n",
      "Epoch 16, Batch 800, Loss: 4.712280428409576\n",
      "Epoch 17, Batch 200, Loss: 4.827594093084335\n",
      "Epoch 17, Batch 400, Loss: 4.949450423717499\n",
      "Epoch 17, Batch 600, Loss: 16.26338432073593\n",
      "Epoch 17, Batch 800, Loss: 5.921249866485596\n",
      "Epoch 18, Batch 200, Loss: 4.705777218341828\n",
      "Epoch 18, Batch 400, Loss: 4.901965198516845\n",
      "Epoch 18, Batch 600, Loss: 4.891170597076416\n",
      "Epoch 18, Batch 800, Loss: 6.89646337389946\n",
      "Epoch 19, Batch 200, Loss: 4.753969969749451\n",
      "Epoch 19, Batch 400, Loss: 4.677124499082566\n",
      "Epoch 19, Batch 600, Loss: 5.000514178276062\n",
      "Epoch 19, Batch 800, Loss: 4.805475914478302\n",
      "Epoch 20, Batch 200, Loss: 8.423029782772064\n",
      "Epoch 20, Batch 400, Loss: 14.211065864562988\n",
      "Epoch 20, Batch 600, Loss: 4.748700529336929\n",
      "Epoch 20, Batch 800, Loss: 4.672246563434601\n",
      "Epoch 21, Batch 200, Loss: 4.761173720359802\n",
      "Epoch 21, Batch 400, Loss: 12.669022862911225\n",
      "Epoch 21, Batch 600, Loss: 7.202822525501251\n",
      "Epoch 21, Batch 800, Loss: 4.688172523975372\n",
      "Epoch 22, Batch 200, Loss: 4.71836676120758\n",
      "Epoch 22, Batch 400, Loss: 4.859987542629242\n",
      "Epoch 22, Batch 600, Loss: 5.604153769016266\n",
      "Epoch 22, Batch 800, Loss: 4.894558463096619\n",
      "Epoch 23, Batch 200, Loss: 8.97952879667282\n",
      "Epoch 23, Batch 400, Loss: 4.796760859489441\n",
      "Epoch 23, Batch 600, Loss: 4.7308284950256345\n",
      "Epoch 23, Batch 800, Loss: 4.813401393890381\n",
      "Epoch 24, Batch 200, Loss: 4.868458950519562\n",
      "Epoch 24, Batch 400, Loss: 16.916492393016814\n",
      "Epoch 24, Batch 600, Loss: 8.342452385425567\n",
      "Epoch 24, Batch 800, Loss: 4.641869535446167\n",
      "Epoch 25, Batch 200, Loss: 4.771077959537506\n",
      "Epoch 25, Batch 400, Loss: 4.849899582862854\n",
      "Epoch 25, Batch 600, Loss: 4.788150881528854\n",
      "Epoch 25, Batch 800, Loss: 4.902520918846131\n",
      "Accuracy on test set: 0.1%\n",
      "Fitting for combination 53\n",
      "784\n",
      "3\n",
      "10\n",
      "[40, 10, 40, 10]\n",
      "True\n",
      "['relu', 'sigmoid', 'relu']\n",
      "SGD\n",
      "0.3\n",
      "0.03\n",
      "CrossEntropyLoss\n",
      "100\n",
      "25\n",
      "0\n",
      "Epoch 1, Batch 100, Loss: 1.9720889580249787\n",
      "Epoch 1, Batch 200, Loss: 1.5089071953296662\n",
      "Epoch 1, Batch 300, Loss: 1.3977614963054656\n",
      "Epoch 1, Batch 400, Loss: 1.322078937292099\n",
      "Epoch 1, Batch 500, Loss: 1.2530937337875365\n",
      "Epoch 1, Batch 600, Loss: 1.2642113614082335\n",
      "Epoch 1, Batch 700, Loss: 1.2085892379283905\n",
      "Epoch 1, Batch 800, Loss: 1.2036519873142242\n",
      "Epoch 1, Batch 900, Loss: 1.1794035392999649\n",
      "Epoch 2, Batch 100, Loss: 1.156637054681778\n",
      "Epoch 2, Batch 200, Loss: 1.1296039140224456\n",
      "Epoch 2, Batch 300, Loss: 1.1133532375097275\n",
      "Epoch 2, Batch 400, Loss: 1.0971057790517806\n",
      "Epoch 2, Batch 500, Loss: 1.091670114994049\n",
      "Epoch 2, Batch 600, Loss: 1.0906818681955337\n",
      "Epoch 2, Batch 700, Loss: 1.1090940147638322\n",
      "Epoch 2, Batch 800, Loss: 1.088765387535095\n",
      "Epoch 2, Batch 900, Loss: 1.0860103303194046\n",
      "Epoch 3, Batch 100, Loss: 1.0914309161901474\n",
      "Epoch 3, Batch 200, Loss: 1.0787694317102432\n",
      "Epoch 3, Batch 300, Loss: 1.0825485825538634\n",
      "Epoch 3, Batch 400, Loss: 1.0533477848768233\n",
      "Epoch 3, Batch 500, Loss: 1.0314083755016328\n",
      "Epoch 3, Batch 600, Loss: 1.0668824321031571\n",
      "Epoch 3, Batch 700, Loss: 1.0516591256856918\n",
      "Epoch 3, Batch 800, Loss: 1.060330507159233\n",
      "Epoch 3, Batch 900, Loss: 1.0531452453136445\n",
      "Epoch 4, Batch 100, Loss: 1.04431795835495\n",
      "Epoch 4, Batch 200, Loss: 1.0285661977529525\n",
      "Epoch 4, Batch 300, Loss: 1.0512238770723343\n",
      "Epoch 4, Batch 400, Loss: 1.0470284670591354\n",
      "Epoch 4, Batch 500, Loss: 1.0590322095155715\n",
      "Epoch 4, Batch 600, Loss: 1.0474133437871933\n",
      "Epoch 4, Batch 700, Loss: 1.0428367424011231\n",
      "Epoch 4, Batch 800, Loss: 1.0565100991725922\n",
      "Epoch 4, Batch 900, Loss: 1.0572962176799774\n",
      "Epoch 5, Batch 100, Loss: 1.0411127287149429\n",
      "Epoch 5, Batch 200, Loss: 1.051126895546913\n",
      "Epoch 5, Batch 300, Loss: 1.0595518845319747\n",
      "Epoch 5, Batch 400, Loss: 1.0478280889987945\n",
      "Epoch 5, Batch 500, Loss: 1.0432249897718429\n",
      "Epoch 5, Batch 600, Loss: 1.0126653683185578\n",
      "Epoch 5, Batch 700, Loss: 1.049608244895935\n",
      "Epoch 5, Batch 800, Loss: 1.04544791162014\n",
      "Epoch 5, Batch 900, Loss: 1.0457836943864822\n",
      "Epoch 6, Batch 100, Loss: 1.0457670658826828\n",
      "Epoch 6, Batch 200, Loss: 1.0553308409452438\n",
      "Epoch 6, Batch 300, Loss: 1.0327672851085663\n",
      "Epoch 6, Batch 400, Loss: 1.0098367738723755\n",
      "Epoch 6, Batch 500, Loss: 1.037743622660637\n",
      "Epoch 6, Batch 600, Loss: 1.0391692268848418\n",
      "Epoch 6, Batch 700, Loss: 1.023700206875801\n",
      "Epoch 6, Batch 800, Loss: 1.0253600788116455\n",
      "Epoch 6, Batch 900, Loss: 1.0333611851930617\n",
      "Epoch 7, Batch 100, Loss: 1.0491509115695954\n",
      "Epoch 7, Batch 200, Loss: 1.0595041859149932\n",
      "Epoch 7, Batch 300, Loss: 1.037119256258011\n",
      "Epoch 7, Batch 400, Loss: 1.0697012788057327\n",
      "Epoch 7, Batch 500, Loss: 1.0552876091003418\n",
      "Epoch 7, Batch 600, Loss: 1.0323085176944733\n",
      "Epoch 7, Batch 700, Loss: 1.046129748225212\n",
      "Epoch 7, Batch 800, Loss: 1.0486055064201354\n",
      "Epoch 7, Batch 900, Loss: 1.0363232809305192\n",
      "Epoch 8, Batch 100, Loss: 1.0498252522945404\n",
      "Epoch 8, Batch 200, Loss: 1.0350958693027497\n",
      "Epoch 8, Batch 300, Loss: 1.0481684547662735\n",
      "Epoch 8, Batch 400, Loss: 1.0293003809452057\n",
      "Epoch 8, Batch 500, Loss: 1.0330706185102463\n",
      "Epoch 8, Batch 600, Loss: 1.0405239695310593\n",
      "Epoch 8, Batch 700, Loss: 1.0504674458503722\n",
      "Epoch 8, Batch 800, Loss: 1.0546073591709137\n",
      "Epoch 8, Batch 900, Loss: 1.0438498705625534\n",
      "Epoch 9, Batch 100, Loss: 1.0495509749650955\n",
      "Epoch 9, Batch 200, Loss: 1.062475826740265\n",
      "Epoch 9, Batch 300, Loss: 1.0307260847091675\n",
      "Epoch 9, Batch 400, Loss: 1.0126768016815186\n",
      "Epoch 9, Batch 500, Loss: 1.0558757710456848\n",
      "Epoch 9, Batch 600, Loss: 1.0710924983024597\n",
      "Epoch 9, Batch 700, Loss: 1.0516766291856765\n",
      "Epoch 9, Batch 800, Loss: 1.0555451548099517\n",
      "Epoch 9, Batch 900, Loss: 1.0477306938171387\n",
      "Epoch 10, Batch 100, Loss: 1.0520308643579483\n",
      "Epoch 10, Batch 200, Loss: 1.006669989824295\n",
      "Epoch 10, Batch 300, Loss: 1.062822088599205\n",
      "Epoch 10, Batch 400, Loss: 1.0581232905387878\n",
      "Epoch 10, Batch 500, Loss: 1.0526775985956192\n",
      "Epoch 10, Batch 600, Loss: 1.0233300083875656\n",
      "Epoch 10, Batch 700, Loss: 1.044491342306137\n",
      "Epoch 10, Batch 800, Loss: 1.0526598751544953\n",
      "Epoch 10, Batch 900, Loss: 1.0426178967952728\n",
      "Epoch 11, Batch 100, Loss: 1.031766232252121\n",
      "Epoch 11, Batch 200, Loss: 1.0327823233604432\n",
      "Epoch 11, Batch 300, Loss: 1.040388588309288\n",
      "Epoch 11, Batch 400, Loss: 1.0547163498401642\n",
      "Epoch 11, Batch 500, Loss: 1.031444194316864\n",
      "Epoch 11, Batch 600, Loss: 1.0625366389751434\n",
      "Epoch 11, Batch 700, Loss: 1.0253989642858505\n",
      "Epoch 11, Batch 800, Loss: 1.0480633926391603\n",
      "Epoch 11, Batch 900, Loss: 1.0192953479290008\n",
      "Epoch 12, Batch 100, Loss: 1.0383834570646286\n",
      "Epoch 12, Batch 200, Loss: 1.0430101639032363\n",
      "Epoch 12, Batch 300, Loss: 1.052561687231064\n",
      "Epoch 12, Batch 400, Loss: 1.026984622478485\n",
      "Epoch 12, Batch 500, Loss: 1.0443088257312774\n",
      "Epoch 12, Batch 600, Loss: 1.034634377360344\n",
      "Epoch 12, Batch 700, Loss: 1.0439807242155075\n",
      "Epoch 12, Batch 800, Loss: 1.0490004444122314\n",
      "Epoch 12, Batch 900, Loss: 1.050476679801941\n",
      "Epoch 13, Batch 100, Loss: 1.0343098098039627\n",
      "Epoch 13, Batch 200, Loss: 1.0337219190597535\n",
      "Epoch 13, Batch 300, Loss: 1.0463853341341018\n",
      "Epoch 13, Batch 400, Loss: 1.0452820181846618\n",
      "Epoch 13, Batch 500, Loss: 1.0112059563398361\n",
      "Epoch 13, Batch 600, Loss: 1.0486301517486571\n",
      "Epoch 13, Batch 700, Loss: 1.0245551496744156\n",
      "Epoch 13, Batch 800, Loss: 1.0434004533290864\n",
      "Epoch 13, Batch 900, Loss: 1.029599843621254\n",
      "Epoch 14, Batch 100, Loss: 1.0345495492219925\n",
      "Epoch 14, Batch 200, Loss: 1.0309228157997132\n",
      "Epoch 14, Batch 300, Loss: 1.0359898102283478\n",
      "Epoch 14, Batch 400, Loss: 1.027118474841118\n",
      "Epoch 14, Batch 500, Loss: 1.0322120887041093\n",
      "Epoch 14, Batch 600, Loss: 1.0361337339878083\n",
      "Epoch 14, Batch 700, Loss: 1.029257174730301\n",
      "Epoch 14, Batch 800, Loss: 1.0201667940616608\n",
      "Epoch 14, Batch 900, Loss: 1.0383876460790633\n",
      "Epoch 15, Batch 100, Loss: 1.0388336825370788\n",
      "Epoch 15, Batch 200, Loss: 1.0346176367998123\n",
      "Epoch 15, Batch 300, Loss: 1.020952073931694\n",
      "Epoch 15, Batch 400, Loss: 1.0517778480052948\n",
      "Epoch 15, Batch 500, Loss: 1.0121277570724487\n",
      "Epoch 15, Batch 600, Loss: 1.0546882128715516\n",
      "Epoch 15, Batch 700, Loss: 1.0281577956676484\n",
      "Epoch 15, Batch 800, Loss: 1.024441134929657\n",
      "Epoch 15, Batch 900, Loss: 1.0350838905572892\n",
      "Epoch 16, Batch 100, Loss: 1.035732671022415\n",
      "Epoch 16, Batch 200, Loss: 1.0205775284767151\n",
      "Epoch 16, Batch 300, Loss: 1.04889333486557\n",
      "Epoch 16, Batch 400, Loss: 1.03700465798378\n",
      "Epoch 16, Batch 500, Loss: 1.0300371450185777\n",
      "Epoch 16, Batch 600, Loss: 1.0076709991693498\n",
      "Epoch 16, Batch 700, Loss: 1.0382988840341567\n",
      "Epoch 16, Batch 800, Loss: 1.0347121876478196\n",
      "Epoch 16, Batch 900, Loss: 1.0359732204675673\n",
      "Epoch 17, Batch 100, Loss: 1.0270321303606034\n",
      "Epoch 17, Batch 200, Loss: 1.0310406255722047\n",
      "Epoch 17, Batch 300, Loss: 1.036526581645012\n",
      "Epoch 17, Batch 400, Loss: 1.0342228192090988\n",
      "Epoch 17, Batch 500, Loss: 1.016352059841156\n",
      "Epoch 17, Batch 600, Loss: 1.0370119053125382\n",
      "Epoch 17, Batch 700, Loss: 1.0300541979074478\n",
      "Epoch 17, Batch 800, Loss: 1.0276151728630065\n",
      "Epoch 17, Batch 900, Loss: 1.0273371881246567\n",
      "Epoch 18, Batch 100, Loss: 1.0192063838243484\n",
      "Epoch 18, Batch 200, Loss: 1.0307010161876677\n",
      "Epoch 18, Batch 300, Loss: 1.0312293767929077\n",
      "Epoch 18, Batch 400, Loss: 1.025868859887123\n",
      "Epoch 18, Batch 500, Loss: 1.0328223979473115\n",
      "Epoch 18, Batch 600, Loss: 1.0252457582950592\n",
      "Epoch 18, Batch 700, Loss: 1.0242871302366257\n",
      "Epoch 18, Batch 800, Loss: 1.0627228999137879\n",
      "Epoch 18, Batch 900, Loss: 1.0082939124107362\n",
      "Epoch 19, Batch 100, Loss: 1.0284864449501037\n",
      "Epoch 19, Batch 200, Loss: 1.0291089683771133\n",
      "Epoch 19, Batch 300, Loss: 1.0249629402160645\n",
      "Epoch 19, Batch 400, Loss: 1.0382142508029937\n",
      "Epoch 19, Batch 500, Loss: 1.0321782034635545\n",
      "Epoch 19, Batch 600, Loss: 0.9928711020946502\n",
      "Epoch 19, Batch 700, Loss: 1.038599238395691\n",
      "Epoch 19, Batch 800, Loss: 1.0326334142684936\n",
      "Epoch 19, Batch 900, Loss: 1.0335602009296416\n",
      "Epoch 20, Batch 100, Loss: 1.0048223513364791\n",
      "Epoch 20, Batch 200, Loss: 1.035241219997406\n",
      "Epoch 20, Batch 300, Loss: 1.0260117644071578\n",
      "Epoch 20, Batch 400, Loss: 1.046714282631874\n",
      "Epoch 20, Batch 500, Loss: 1.0331939381361008\n",
      "Epoch 20, Batch 600, Loss: 1.014477921128273\n",
      "Epoch 20, Batch 700, Loss: 1.014092618227005\n",
      "Epoch 20, Batch 800, Loss: 1.0398850870132446\n",
      "Epoch 20, Batch 900, Loss: 1.0447707617282866\n",
      "Epoch 21, Batch 100, Loss: 1.0309401535987854\n",
      "Epoch 21, Batch 200, Loss: 1.0248152738809586\n",
      "Epoch 21, Batch 300, Loss: 1.0222951829433442\n",
      "Epoch 21, Batch 400, Loss: 1.0396361815929414\n",
      "Epoch 21, Batch 500, Loss: 1.0168882817029954\n",
      "Epoch 21, Batch 600, Loss: 1.034307827949524\n",
      "Epoch 21, Batch 700, Loss: 1.0238957715034485\n",
      "Epoch 21, Batch 800, Loss: 1.0158757662773132\n",
      "Epoch 21, Batch 900, Loss: 1.033084352016449\n",
      "Epoch 22, Batch 100, Loss: 1.034478851556778\n",
      "Epoch 22, Batch 200, Loss: 1.0346266943216325\n",
      "Epoch 22, Batch 300, Loss: 1.0200275099277496\n",
      "Epoch 22, Batch 400, Loss: 1.0207722723484038\n",
      "Epoch 22, Batch 500, Loss: 1.0118565714359284\n",
      "Epoch 22, Batch 600, Loss: 1.0234105479717255\n",
      "Epoch 22, Batch 700, Loss: 1.023683984875679\n",
      "Epoch 22, Batch 800, Loss: 1.0154262763261794\n",
      "Epoch 22, Batch 900, Loss: 1.0368436175584792\n",
      "Epoch 23, Batch 100, Loss: 1.0348938131332397\n",
      "Epoch 23, Batch 200, Loss: 1.0477225255966187\n",
      "Epoch 23, Batch 300, Loss: 1.0460653233528137\n",
      "Epoch 23, Batch 400, Loss: 1.0427634757757187\n",
      "Epoch 23, Batch 500, Loss: 1.0347874134778976\n",
      "Epoch 23, Batch 600, Loss: 1.013036264181137\n",
      "Epoch 23, Batch 700, Loss: 1.0513407921791076\n",
      "Epoch 23, Batch 800, Loss: 0.9968697142601013\n",
      "Epoch 23, Batch 900, Loss: 1.0222694808244706\n",
      "Epoch 24, Batch 100, Loss: 1.03072456240654\n",
      "Epoch 24, Batch 200, Loss: 1.0106340211629867\n",
      "Epoch 24, Batch 300, Loss: 1.0470433980226517\n",
      "Epoch 24, Batch 400, Loss: 1.0449846762418746\n",
      "Epoch 24, Batch 500, Loss: 1.0191070359945298\n",
      "Epoch 24, Batch 600, Loss: 1.0389625358581542\n",
      "Epoch 24, Batch 700, Loss: 1.0414022707939148\n",
      "Epoch 24, Batch 800, Loss: 1.0173069262504577\n",
      "Epoch 24, Batch 900, Loss: 1.0183992052078248\n",
      "Epoch 25, Batch 100, Loss: 1.0137199479341508\n",
      "Epoch 25, Batch 200, Loss: 1.041091485619545\n",
      "Epoch 25, Batch 300, Loss: 1.0332173490524292\n",
      "Epoch 25, Batch 400, Loss: 1.0335565257072448\n",
      "Epoch 25, Batch 500, Loss: 1.0263716471195221\n",
      "Epoch 25, Batch 600, Loss: 1.049853224158287\n",
      "Epoch 25, Batch 700, Loss: 1.034305571913719\n",
      "Epoch 25, Batch 800, Loss: 1.0361631298065186\n",
      "Epoch 25, Batch 900, Loss: 1.0116837829351426\n",
      "Accuracy on test set: 0.6263%\n",
      "Fitting for combination 54\n",
      "784\n",
      "3\n",
      "10\n",
      "[40, 10, 40, 10]\n",
      "True\n",
      "['relu', 'sigmoid', 'tanh']\n",
      "Adam\n",
      "0.1\n",
      "0.03\n",
      "CrossEntropyLoss\n",
      "200\n",
      "25\n",
      "0\n",
      "Epoch 1, Batch 200, Loss: 4.44144976735115\n",
      "Epoch 1, Batch 400, Loss: 4.108488667011261\n",
      "Epoch 1, Batch 600, Loss: 4.3861361336708065\n",
      "Epoch 1, Batch 800, Loss: 4.164844228029251\n",
      "Epoch 2, Batch 200, Loss: 4.104612052440643\n",
      "Epoch 2, Batch 400, Loss: 4.461457082033157\n",
      "Epoch 2, Batch 600, Loss: 4.433876321315766\n",
      "Epoch 2, Batch 800, Loss: 4.365729308128357\n",
      "Epoch 3, Batch 200, Loss: 4.433097532987595\n",
      "Epoch 3, Batch 400, Loss: 4.352272223234177\n",
      "Epoch 3, Batch 600, Loss: 4.528065857887268\n",
      "Epoch 3, Batch 800, Loss: 4.509379258155823\n",
      "Epoch 4, Batch 200, Loss: 4.562210066318512\n",
      "Epoch 4, Batch 400, Loss: 4.3164767801761625\n",
      "Epoch 4, Batch 600, Loss: 4.431227538585663\n",
      "Epoch 4, Batch 800, Loss: 4.2973573422431945\n",
      "Epoch 5, Batch 200, Loss: 4.36387939453125\n",
      "Epoch 5, Batch 400, Loss: 4.297929813861847\n",
      "Epoch 5, Batch 600, Loss: 4.436606924533844\n",
      "Epoch 5, Batch 800, Loss: 4.424448063373566\n",
      "Epoch 6, Batch 200, Loss: 4.502127389907837\n",
      "Epoch 6, Batch 400, Loss: 4.440794450044632\n",
      "Epoch 6, Batch 600, Loss: 4.496058515310287\n",
      "Epoch 6, Batch 800, Loss: 4.38880158662796\n",
      "Epoch 7, Batch 200, Loss: 4.504095033407212\n",
      "Epoch 7, Batch 400, Loss: 4.490174247026443\n",
      "Epoch 7, Batch 600, Loss: 4.3934460186958315\n",
      "Epoch 7, Batch 800, Loss: 4.496102019548416\n",
      "Epoch 8, Batch 200, Loss: 4.310782781839371\n",
      "Epoch 8, Batch 400, Loss: 4.489069125652313\n",
      "Epoch 8, Batch 600, Loss: 4.4590681564807895\n",
      "Epoch 8, Batch 800, Loss: 4.494129078388214\n",
      "Epoch 9, Batch 200, Loss: 4.441117036342621\n",
      "Epoch 9, Batch 400, Loss: 4.495784187316895\n",
      "Epoch 9, Batch 600, Loss: 4.356031301021576\n",
      "Epoch 9, Batch 800, Loss: 4.455514328479767\n",
      "Epoch 10, Batch 200, Loss: 4.47805206656456\n",
      "Epoch 10, Batch 400, Loss: 4.43864058971405\n",
      "Epoch 10, Batch 600, Loss: 4.4146783316135405\n",
      "Epoch 10, Batch 800, Loss: 4.521427366733551\n",
      "Epoch 11, Batch 200, Loss: 4.351036165952682\n",
      "Epoch 11, Batch 400, Loss: 4.355580905675888\n",
      "Epoch 11, Batch 600, Loss: 4.397181636095047\n",
      "Epoch 11, Batch 800, Loss: 4.391012871265412\n",
      "Epoch 12, Batch 200, Loss: 4.457767559289932\n",
      "Epoch 12, Batch 400, Loss: 4.234238319396972\n",
      "Epoch 12, Batch 600, Loss: 4.388568131923676\n",
      "Epoch 12, Batch 800, Loss: 4.279714592695236\n",
      "Epoch 13, Batch 200, Loss: 4.602068103551865\n",
      "Epoch 13, Batch 400, Loss: 4.50321763753891\n",
      "Epoch 13, Batch 600, Loss: 4.481015737056732\n",
      "Epoch 13, Batch 800, Loss: 4.393727985620498\n",
      "Epoch 14, Batch 200, Loss: 4.372727291584015\n",
      "Epoch 14, Batch 400, Loss: 4.443821280002594\n",
      "Epoch 14, Batch 600, Loss: 4.316323388814926\n",
      "Epoch 14, Batch 800, Loss: 4.299604150056839\n",
      "Epoch 15, Batch 200, Loss: 4.39698156118393\n",
      "Epoch 15, Batch 400, Loss: 4.390186297893524\n",
      "Epoch 15, Batch 600, Loss: 4.450449020862579\n",
      "Epoch 15, Batch 800, Loss: 4.510626488924027\n",
      "Epoch 16, Batch 200, Loss: 4.493758054971695\n",
      "Epoch 16, Batch 400, Loss: 4.462241146564484\n",
      "Epoch 16, Batch 600, Loss: 4.467734175920486\n",
      "Epoch 16, Batch 800, Loss: 4.388956145048142\n",
      "Epoch 17, Batch 200, Loss: 4.3278403568267825\n",
      "Epoch 17, Batch 400, Loss: 4.455428278446197\n",
      "Epoch 17, Batch 600, Loss: 4.440985760688782\n",
      "Epoch 17, Batch 800, Loss: 4.568884299993515\n",
      "Epoch 18, Batch 200, Loss: 4.537444498538971\n",
      "Epoch 18, Batch 400, Loss: 4.503988734483719\n",
      "Epoch 18, Batch 600, Loss: 4.4866868019104\n",
      "Epoch 18, Batch 800, Loss: 4.420862333774567\n",
      "Epoch 19, Batch 200, Loss: 4.392644845247268\n",
      "Epoch 19, Batch 400, Loss: 4.3635705935955045\n",
      "Epoch 19, Batch 600, Loss: 4.245447982549667\n",
      "Epoch 19, Batch 800, Loss: 4.464846574068069\n",
      "Epoch 20, Batch 200, Loss: 4.315745304822922\n",
      "Epoch 20, Batch 400, Loss: 4.383800150156021\n",
      "Epoch 20, Batch 600, Loss: 4.484382874965668\n",
      "Epoch 20, Batch 800, Loss: 4.27855570435524\n",
      "Epoch 21, Batch 200, Loss: 4.449389654397964\n",
      "Epoch 21, Batch 400, Loss: 4.330100655555725\n",
      "Epoch 21, Batch 600, Loss: 4.431770927906037\n",
      "Epoch 21, Batch 800, Loss: 4.450420072078705\n",
      "Epoch 22, Batch 200, Loss: 4.248521724939346\n",
      "Epoch 22, Batch 400, Loss: 4.352062208652496\n",
      "Epoch 22, Batch 600, Loss: 4.3149465072155\n",
      "Epoch 22, Batch 800, Loss: 4.194475847482681\n",
      "Epoch 23, Batch 200, Loss: 4.428058084249496\n",
      "Epoch 23, Batch 400, Loss: 4.426574103832245\n",
      "Epoch 23, Batch 600, Loss: 4.169242005348206\n",
      "Epoch 23, Batch 800, Loss: 4.186170755624771\n",
      "Epoch 24, Batch 200, Loss: 4.343509798049927\n",
      "Epoch 24, Batch 400, Loss: 4.510933051109314\n",
      "Epoch 24, Batch 600, Loss: 4.435759902000427\n",
      "Epoch 24, Batch 800, Loss: 4.458377804756164\n",
      "Epoch 25, Batch 200, Loss: 4.4639557838439945\n",
      "Epoch 25, Batch 400, Loss: 4.51785129904747\n",
      "Epoch 25, Batch 600, Loss: 4.4107588768005375\n",
      "Epoch 25, Batch 800, Loss: 4.429988337755203\n",
      "Accuracy on test set: 0.2088%\n",
      "Fitting for combination 55\n",
      "784\n",
      "3\n",
      "10\n",
      "[40, 10, 40, 10]\n",
      "False\n",
      "['relu', 'sigmoid', 'relu']\n",
      "SGD\n",
      "0.03\n",
      "0.1\n",
      "CrossEntropyLoss\n",
      "200\n",
      "25\n",
      "0\n",
      "Epoch 1, Batch 200, Loss: 4.599472787380218\n",
      "Epoch 1, Batch 400, Loss: 4.601699206829071\n",
      "Epoch 1, Batch 600, Loss: 4.605020108222962\n",
      "Epoch 1, Batch 800, Loss: 4.6053829002380375\n",
      "Epoch 2, Batch 200, Loss: 4.605297298431396\n",
      "Epoch 2, Batch 400, Loss: 4.605406947135926\n",
      "Epoch 2, Batch 600, Loss: 4.605442590713501\n",
      "Epoch 2, Batch 800, Loss: 4.605421059131622\n",
      "Epoch 3, Batch 200, Loss: 4.605363504886627\n",
      "Epoch 3, Batch 400, Loss: 4.605379297733307\n",
      "Epoch 3, Batch 600, Loss: 4.605432019233704\n",
      "Epoch 3, Batch 800, Loss: 4.605282340049744\n",
      "Epoch 4, Batch 200, Loss: 4.605311400890351\n",
      "Epoch 4, Batch 400, Loss: 4.605408275127411\n",
      "Epoch 4, Batch 600, Loss: 4.605418891906738\n",
      "Epoch 4, Batch 800, Loss: 4.605284440517425\n",
      "Epoch 5, Batch 200, Loss: 4.605301506519318\n",
      "Epoch 5, Batch 400, Loss: 4.60549504995346\n",
      "Epoch 5, Batch 600, Loss: 4.605245945453643\n",
      "Epoch 5, Batch 800, Loss: 4.605195109844208\n",
      "Epoch 6, Batch 200, Loss: 4.605283102989197\n",
      "Epoch 6, Batch 400, Loss: 4.605487725734711\n",
      "Epoch 6, Batch 600, Loss: 4.605281186103821\n",
      "Epoch 6, Batch 800, Loss: 4.605512425899506\n",
      "Epoch 7, Batch 200, Loss: 4.605457937717437\n",
      "Epoch 7, Batch 400, Loss: 4.605379121303558\n",
      "Epoch 7, Batch 600, Loss: 4.605318298339844\n",
      "Epoch 7, Batch 800, Loss: 4.605511562824249\n",
      "Epoch 8, Batch 200, Loss: 4.605441100597382\n",
      "Epoch 8, Batch 400, Loss: 4.605409531593323\n",
      "Epoch 8, Batch 600, Loss: 4.605348703861236\n",
      "Epoch 8, Batch 800, Loss: 4.605501708984375\n",
      "Epoch 9, Batch 200, Loss: 4.605401835441589\n",
      "Epoch 9, Batch 400, Loss: 4.605460231304169\n",
      "Epoch 9, Batch 600, Loss: 4.605451304912567\n",
      "Epoch 9, Batch 800, Loss: 4.605317878723144\n",
      "Epoch 10, Batch 200, Loss: 4.6052324891090395\n",
      "Epoch 10, Batch 400, Loss: 4.605383594036102\n",
      "Epoch 10, Batch 600, Loss: 4.605315346717834\n",
      "Epoch 10, Batch 800, Loss: 4.605315961837769\n",
      "Epoch 11, Batch 200, Loss: 4.605248553752899\n",
      "Epoch 11, Batch 400, Loss: 4.605458602905274\n",
      "Epoch 11, Batch 600, Loss: 4.605354192256928\n",
      "Epoch 11, Batch 800, Loss: 4.60551926612854\n",
      "Epoch 12, Batch 200, Loss: 4.605334486961365\n",
      "Epoch 12, Batch 400, Loss: 4.6054197335243225\n",
      "Epoch 12, Batch 600, Loss: 4.605310621261597\n",
      "Epoch 12, Batch 800, Loss: 4.60545902967453\n",
      "Epoch 13, Batch 200, Loss: 4.605247025489807\n",
      "Epoch 13, Batch 400, Loss: 4.605372741222381\n",
      "Epoch 13, Batch 600, Loss: 4.605456430912017\n",
      "Epoch 13, Batch 800, Loss: 4.605253529548645\n",
      "Epoch 14, Batch 200, Loss: 4.605336027145386\n",
      "Epoch 14, Batch 400, Loss: 4.60544333934784\n",
      "Epoch 14, Batch 600, Loss: 4.605415484905243\n",
      "Epoch 14, Batch 800, Loss: 4.605431666374207\n",
      "Epoch 15, Batch 200, Loss: 4.60534363746643\n",
      "Epoch 15, Batch 400, Loss: 4.605436260700226\n",
      "Epoch 15, Batch 600, Loss: 4.605188295841217\n",
      "Epoch 15, Batch 800, Loss: 4.6055634450912475\n",
      "Epoch 16, Batch 200, Loss: 4.605391657352447\n",
      "Epoch 16, Batch 400, Loss: 4.605354661941528\n",
      "Epoch 16, Batch 600, Loss: 4.605454862117767\n",
      "Epoch 16, Batch 800, Loss: 4.605282726287842\n",
      "Epoch 17, Batch 200, Loss: 4.605282940864563\n",
      "Epoch 17, Batch 400, Loss: 4.605415985584259\n",
      "Epoch 17, Batch 600, Loss: 4.605295710563659\n",
      "Epoch 17, Batch 800, Loss: 4.605420715808869\n",
      "Epoch 18, Batch 200, Loss: 4.605436096191406\n",
      "Epoch 18, Batch 400, Loss: 4.605397462844849\n",
      "Epoch 18, Batch 600, Loss: 4.605412650108337\n",
      "Epoch 18, Batch 800, Loss: 4.605357363224029\n",
      "Epoch 19, Batch 200, Loss: 4.60529111623764\n",
      "Epoch 19, Batch 400, Loss: 4.605330209732056\n",
      "Epoch 19, Batch 600, Loss: 4.60549544095993\n",
      "Epoch 19, Batch 800, Loss: 4.605343799591065\n",
      "Epoch 20, Batch 200, Loss: 4.60540031671524\n",
      "Epoch 20, Batch 400, Loss: 4.605251607894897\n",
      "Epoch 20, Batch 600, Loss: 4.605557117462158\n",
      "Epoch 20, Batch 800, Loss: 4.605361733436585\n",
      "Epoch 21, Batch 200, Loss: 4.605049042701721\n",
      "Epoch 21, Batch 400, Loss: 4.6054874968528745\n",
      "Epoch 21, Batch 600, Loss: 4.605533034801483\n",
      "Epoch 21, Batch 800, Loss: 4.605421087741852\n",
      "Epoch 22, Batch 200, Loss: 4.605204803943634\n",
      "Epoch 22, Batch 400, Loss: 4.605547666549683\n",
      "Epoch 22, Batch 600, Loss: 4.605462827682495\n",
      "Epoch 22, Batch 800, Loss: 4.60535608291626\n",
      "Epoch 23, Batch 200, Loss: 4.605305459499359\n",
      "Epoch 23, Batch 400, Loss: 4.60535567522049\n",
      "Epoch 23, Batch 600, Loss: 4.605414493083954\n",
      "Epoch 23, Batch 800, Loss: 4.605254855155945\n",
      "Epoch 24, Batch 200, Loss: 4.605227057933807\n",
      "Epoch 24, Batch 400, Loss: 4.605351436138153\n",
      "Epoch 24, Batch 600, Loss: 4.60534631729126\n",
      "Epoch 24, Batch 800, Loss: 4.605347888469696\n",
      "Epoch 25, Batch 200, Loss: 4.605355463027954\n",
      "Epoch 25, Batch 400, Loss: 4.605541257858277\n",
      "Epoch 25, Batch 600, Loss: 4.605439832210541\n",
      "Epoch 25, Batch 800, Loss: 4.605239522457123\n",
      "Accuracy on test set: 0.1%\n",
      "Fitting for combination 56\n",
      "784\n",
      "3\n",
      "10\n",
      "[40, 10, 50, 10]\n",
      "True\n",
      "['relu', 'sigmoid', 'sigmoid']\n",
      "Adam\n",
      "0.3\n",
      "0.1\n",
      "CrossEntropyLoss\n",
      "50\n",
      "25\n",
      "0\n",
      "Epoch 1, Batch 50, Loss: 1.3763955426216126\n",
      "Epoch 1, Batch 100, Loss: 1.2930027508735658\n",
      "Epoch 1, Batch 150, Loss: 1.271512336730957\n",
      "Epoch 1, Batch 200, Loss: 1.2889031124114991\n",
      "Epoch 1, Batch 250, Loss: 1.3012412261962891\n",
      "Epoch 1, Batch 300, Loss: 1.2638956451416015\n",
      "Epoch 1, Batch 350, Loss: 1.2676204943656921\n",
      "Epoch 1, Batch 400, Loss: 1.2685629296302796\n",
      "Epoch 1, Batch 450, Loss: 1.2789482426643373\n",
      "Epoch 1, Batch 500, Loss: 1.2578167581558228\n",
      "Epoch 1, Batch 550, Loss: 1.2687588047981262\n",
      "Epoch 1, Batch 600, Loss: 1.2762909197807313\n",
      "Epoch 1, Batch 650, Loss: 1.3029307103157044\n",
      "Epoch 1, Batch 700, Loss: 1.2634657573699952\n",
      "Epoch 1, Batch 750, Loss: 1.2678133320808411\n",
      "Epoch 1, Batch 800, Loss: 1.2721708035469055\n",
      "Epoch 1, Batch 850, Loss: 1.2748431730270386\n",
      "Epoch 1, Batch 900, Loss: 1.267529742717743\n",
      "Epoch 2, Batch 50, Loss: 1.2733800601959229\n",
      "Epoch 2, Batch 100, Loss: 1.2613031601905822\n",
      "Epoch 2, Batch 150, Loss: 1.2837628746032714\n",
      "Epoch 2, Batch 200, Loss: 1.2776022338867188\n",
      "Epoch 2, Batch 250, Loss: 1.260425386428833\n",
      "Epoch 2, Batch 300, Loss: 1.272115364074707\n",
      "Epoch 2, Batch 350, Loss: 1.2926119303703307\n",
      "Epoch 2, Batch 400, Loss: 1.295117268562317\n",
      "Epoch 2, Batch 450, Loss: 1.2872519779205323\n",
      "Epoch 2, Batch 500, Loss: 1.2764852213859559\n",
      "Epoch 2, Batch 550, Loss: 1.2598852515220642\n",
      "Epoch 2, Batch 600, Loss: 1.2532515740394592\n",
      "Epoch 2, Batch 650, Loss: 1.2561344933509826\n",
      "Epoch 2, Batch 700, Loss: 1.2664210844039916\n",
      "Epoch 2, Batch 750, Loss: 1.3428019976615906\n",
      "Epoch 2, Batch 800, Loss: 1.3027552461624146\n",
      "Epoch 2, Batch 850, Loss: 1.2596420860290527\n",
      "Epoch 2, Batch 900, Loss: 1.265642342567444\n",
      "Epoch 3, Batch 50, Loss: 1.3176870846748352\n",
      "Epoch 3, Batch 100, Loss: 1.2796594524383544\n",
      "Epoch 3, Batch 150, Loss: 1.2720015096664428\n",
      "Epoch 3, Batch 200, Loss: 1.2513499212265016\n",
      "Epoch 3, Batch 250, Loss: 1.2684252548217774\n",
      "Epoch 3, Batch 300, Loss: 1.2934891724586486\n",
      "Epoch 3, Batch 350, Loss: 1.2811952900886536\n",
      "Epoch 3, Batch 400, Loss: 1.2605835556983949\n",
      "Epoch 3, Batch 450, Loss: 1.2846115136146545\n",
      "Epoch 3, Batch 500, Loss: 1.3119157147407532\n",
      "Epoch 3, Batch 550, Loss: 1.2985428023338317\n",
      "Epoch 3, Batch 600, Loss: 1.2948943853378296\n",
      "Epoch 3, Batch 650, Loss: 1.2760350465774537\n",
      "Epoch 3, Batch 700, Loss: 1.2946651244163514\n",
      "Epoch 3, Batch 750, Loss: 1.2671874022483827\n",
      "Epoch 3, Batch 800, Loss: 1.2829562640190124\n",
      "Epoch 3, Batch 850, Loss: 1.2605346894264222\n",
      "Epoch 3, Batch 900, Loss: 1.2873074913024902\n",
      "Epoch 4, Batch 50, Loss: 1.2879457402229308\n",
      "Epoch 4, Batch 100, Loss: 1.3017632794380187\n",
      "Epoch 4, Batch 150, Loss: 1.2896887612342836\n",
      "Epoch 4, Batch 200, Loss: 1.266079933643341\n",
      "Epoch 4, Batch 250, Loss: 1.2494184803962707\n",
      "Epoch 4, Batch 300, Loss: 1.2835611486434937\n",
      "Epoch 4, Batch 350, Loss: 1.2758716917037964\n",
      "Epoch 4, Batch 400, Loss: 1.2550137066841125\n",
      "Epoch 4, Batch 450, Loss: 1.256978690624237\n",
      "Epoch 4, Batch 500, Loss: 1.2788641715049744\n",
      "Epoch 4, Batch 550, Loss: 1.2678806018829345\n",
      "Epoch 4, Batch 600, Loss: 1.2766798663139343\n",
      "Epoch 4, Batch 650, Loss: 1.294132215976715\n",
      "Epoch 4, Batch 700, Loss: 1.280495855808258\n",
      "Epoch 4, Batch 750, Loss: 1.252562291622162\n",
      "Epoch 4, Batch 800, Loss: 1.2795729303359986\n",
      "Epoch 4, Batch 850, Loss: 1.2858396720886232\n",
      "Epoch 4, Batch 900, Loss: 1.2770406365394593\n",
      "Epoch 5, Batch 50, Loss: 1.2614253067970276\n",
      "Epoch 5, Batch 100, Loss: 1.287215712070465\n",
      "Epoch 5, Batch 150, Loss: 1.3152188992500304\n",
      "Epoch 5, Batch 200, Loss: 1.252159674167633\n",
      "Epoch 5, Batch 250, Loss: 1.2621205854415893\n",
      "Epoch 5, Batch 300, Loss: 1.2735400247573851\n",
      "Epoch 5, Batch 350, Loss: 1.283039562702179\n",
      "Epoch 5, Batch 400, Loss: 1.2556570553779602\n",
      "Epoch 5, Batch 450, Loss: 1.2702581667900086\n",
      "Epoch 5, Batch 500, Loss: 1.2476917147636413\n",
      "Epoch 5, Batch 550, Loss: 1.2601466608047485\n",
      "Epoch 5, Batch 600, Loss: 1.295438630580902\n",
      "Epoch 5, Batch 650, Loss: 1.2720083236694335\n",
      "Epoch 5, Batch 700, Loss: 1.2687952995300293\n",
      "Epoch 5, Batch 750, Loss: 1.2666847538948058\n",
      "Epoch 5, Batch 800, Loss: 1.261209053993225\n",
      "Epoch 5, Batch 850, Loss: 1.2862744164466857\n",
      "Epoch 5, Batch 900, Loss: 1.2616851949691772\n",
      "Epoch 6, Batch 50, Loss: 1.304133379459381\n",
      "Epoch 6, Batch 100, Loss: 1.2522025561332704\n",
      "Epoch 6, Batch 150, Loss: 1.2679389119148254\n",
      "Epoch 6, Batch 200, Loss: 1.2630946254730224\n",
      "Epoch 6, Batch 250, Loss: 1.2960442996025086\n",
      "Epoch 6, Batch 300, Loss: 1.33572758436203\n",
      "Epoch 6, Batch 350, Loss: 1.2786533689498902\n",
      "Epoch 6, Batch 400, Loss: 1.2553679513931275\n",
      "Epoch 6, Batch 450, Loss: 1.2527179193496705\n",
      "Epoch 6, Batch 500, Loss: 1.2588424038887025\n",
      "Epoch 6, Batch 550, Loss: 1.2986744594573976\n",
      "Epoch 6, Batch 600, Loss: 1.2831615710258484\n",
      "Epoch 6, Batch 650, Loss: 1.2913418698310852\n",
      "Epoch 6, Batch 700, Loss: 1.279381275177002\n",
      "Epoch 6, Batch 750, Loss: 1.292585334777832\n",
      "Epoch 6, Batch 800, Loss: 1.2662239050865174\n",
      "Epoch 6, Batch 850, Loss: 1.322596926689148\n",
      "Epoch 6, Batch 900, Loss: 1.2562198281288146\n",
      "Epoch 7, Batch 50, Loss: 1.2579445266723632\n",
      "Epoch 7, Batch 100, Loss: 1.2556269836425782\n",
      "Epoch 7, Batch 150, Loss: 1.2714471983909608\n",
      "Epoch 7, Batch 200, Loss: 1.2606410527229308\n",
      "Epoch 7, Batch 250, Loss: 1.281620237827301\n",
      "Epoch 7, Batch 300, Loss: 1.2546714186668395\n",
      "Epoch 7, Batch 350, Loss: 1.3474045419692993\n",
      "Epoch 7, Batch 400, Loss: 1.252762405872345\n",
      "Epoch 7, Batch 450, Loss: 1.274025239944458\n",
      "Epoch 7, Batch 500, Loss: 1.273422555923462\n",
      "Epoch 7, Batch 550, Loss: 1.2768685936927795\n",
      "Epoch 7, Batch 600, Loss: 1.2703903818130493\n",
      "Epoch 7, Batch 650, Loss: 1.252996425628662\n",
      "Epoch 7, Batch 700, Loss: 1.2890318489074708\n",
      "Epoch 7, Batch 750, Loss: 1.265789258480072\n",
      "Epoch 7, Batch 800, Loss: 1.2575818443298339\n",
      "Epoch 7, Batch 850, Loss: 1.245791494846344\n",
      "Epoch 7, Batch 900, Loss: 1.283938045501709\n",
      "Epoch 8, Batch 50, Loss: 1.283799924850464\n",
      "Epoch 8, Batch 100, Loss: 1.2733006811141967\n",
      "Epoch 8, Batch 150, Loss: 1.2824917483329772\n",
      "Epoch 8, Batch 200, Loss: 1.2810524606704712\n",
      "Epoch 8, Batch 250, Loss: 1.2696459126472472\n",
      "Epoch 8, Batch 300, Loss: 1.2614935612678528\n",
      "Epoch 8, Batch 350, Loss: 1.2822915744781493\n",
      "Epoch 8, Batch 400, Loss: 1.2860947632789612\n",
      "Epoch 8, Batch 450, Loss: 1.3020126795768738\n",
      "Epoch 8, Batch 500, Loss: 1.2757646226882935\n",
      "Epoch 8, Batch 550, Loss: 1.242163519859314\n",
      "Epoch 8, Batch 600, Loss: 1.2650596952438355\n",
      "Epoch 8, Batch 650, Loss: 1.247060434818268\n",
      "Epoch 8, Batch 700, Loss: 1.302585208415985\n",
      "Epoch 8, Batch 750, Loss: 1.2693762254714966\n",
      "Epoch 8, Batch 800, Loss: 1.2604287576675415\n",
      "Epoch 8, Batch 850, Loss: 1.2917649388313293\n",
      "Epoch 8, Batch 900, Loss: 1.262881543636322\n",
      "Epoch 9, Batch 50, Loss: 1.2729390096664428\n",
      "Epoch 9, Batch 100, Loss: 1.290814094543457\n",
      "Epoch 9, Batch 150, Loss: 1.262901315689087\n",
      "Epoch 9, Batch 200, Loss: 1.2428192543983458\n",
      "Epoch 9, Batch 250, Loss: 1.2682150220870971\n",
      "Epoch 9, Batch 300, Loss: 1.2810765886306763\n",
      "Epoch 9, Batch 350, Loss: 1.3065136790275573\n",
      "Epoch 9, Batch 400, Loss: 1.2717004919052124\n",
      "Epoch 9, Batch 450, Loss: 1.288577344417572\n",
      "Epoch 9, Batch 500, Loss: 1.2569520163536072\n",
      "Epoch 9, Batch 550, Loss: 1.2736396813392639\n",
      "Epoch 9, Batch 600, Loss: 1.2767212057113648\n",
      "Epoch 9, Batch 650, Loss: 1.2544715571403504\n",
      "Epoch 9, Batch 700, Loss: 1.3032230734825134\n",
      "Epoch 9, Batch 750, Loss: 1.269663782119751\n",
      "Epoch 9, Batch 800, Loss: 1.2783701586723328\n",
      "Epoch 9, Batch 850, Loss: 1.2854288291931153\n",
      "Epoch 9, Batch 900, Loss: 1.2898840737342834\n",
      "Epoch 10, Batch 50, Loss: 1.2988040447235107\n",
      "Epoch 10, Batch 100, Loss: 1.3037110352516175\n",
      "Epoch 10, Batch 150, Loss: 1.2668925976753236\n",
      "Epoch 10, Batch 200, Loss: 1.2524876928329467\n",
      "Epoch 10, Batch 250, Loss: 1.3138941860198974\n",
      "Epoch 10, Batch 300, Loss: 1.2653376412391664\n",
      "Epoch 10, Batch 350, Loss: 1.2512105894088745\n",
      "Epoch 10, Batch 400, Loss: 1.2719736790657044\n",
      "Epoch 10, Batch 450, Loss: 1.2726767110824584\n",
      "Epoch 10, Batch 500, Loss: 1.2630118775367736\n",
      "Epoch 10, Batch 550, Loss: 1.2725322699546815\n",
      "Epoch 10, Batch 600, Loss: 1.2835465693473815\n",
      "Epoch 10, Batch 650, Loss: 1.3045421504974366\n",
      "Epoch 10, Batch 700, Loss: 1.2880419349670411\n",
      "Epoch 10, Batch 750, Loss: 1.2892364835739136\n",
      "Epoch 10, Batch 800, Loss: 1.2594498443603515\n",
      "Epoch 10, Batch 850, Loss: 1.2982716488838195\n",
      "Epoch 10, Batch 900, Loss: 1.2712914061546325\n",
      "Epoch 11, Batch 50, Loss: 1.2674265575408936\n",
      "Epoch 11, Batch 100, Loss: 1.2381181263923644\n",
      "Epoch 11, Batch 150, Loss: 1.2820039248466493\n",
      "Epoch 11, Batch 200, Loss: 1.286675968170166\n",
      "Epoch 11, Batch 250, Loss: 1.2825391030311584\n",
      "Epoch 11, Batch 300, Loss: 1.2808710885047914\n",
      "Epoch 11, Batch 350, Loss: 1.2727433347702026\n",
      "Epoch 11, Batch 400, Loss: 1.2928718161582946\n",
      "Epoch 11, Batch 450, Loss: 1.2808552265167237\n",
      "Epoch 11, Batch 500, Loss: 1.262075674533844\n",
      "Epoch 11, Batch 550, Loss: 1.2465326833724975\n",
      "Epoch 11, Batch 600, Loss: 1.266489462852478\n",
      "Epoch 11, Batch 650, Loss: 1.3106361937522888\n",
      "Epoch 11, Batch 700, Loss: 1.3103207707405091\n",
      "Epoch 11, Batch 750, Loss: 1.2596683478355408\n",
      "Epoch 11, Batch 800, Loss: 1.2672042083740234\n",
      "Epoch 11, Batch 850, Loss: 1.270694522857666\n",
      "Epoch 11, Batch 900, Loss: 1.2702259802818299\n",
      "Epoch 12, Batch 50, Loss: 1.2920229315757752\n",
      "Epoch 12, Batch 100, Loss: 1.2700117230415344\n",
      "Epoch 12, Batch 150, Loss: 1.2716915106773377\n",
      "Epoch 12, Batch 200, Loss: 1.2878876280784608\n",
      "Epoch 12, Batch 250, Loss: 1.317629623413086\n",
      "Epoch 12, Batch 300, Loss: 1.2506691789627076\n",
      "Epoch 12, Batch 350, Loss: 1.2789706921577453\n",
      "Epoch 12, Batch 400, Loss: 1.2553955078125\n",
      "Epoch 12, Batch 450, Loss: 1.260774052143097\n",
      "Epoch 12, Batch 500, Loss: 1.2518020391464233\n",
      "Epoch 12, Batch 550, Loss: 1.2810191893577576\n",
      "Epoch 12, Batch 600, Loss: 1.2747104859352112\n",
      "Epoch 12, Batch 650, Loss: 1.3192994165420533\n",
      "Epoch 12, Batch 700, Loss: 1.2934355211257935\n",
      "Epoch 12, Batch 750, Loss: 1.2562050890922547\n",
      "Epoch 12, Batch 800, Loss: 1.2477790212631226\n",
      "Epoch 12, Batch 850, Loss: 1.2638467001914977\n",
      "Epoch 12, Batch 900, Loss: 1.2599277663230897\n",
      "Epoch 13, Batch 50, Loss: 1.2700572752952575\n",
      "Epoch 13, Batch 100, Loss: 1.2443816018104554\n",
      "Epoch 13, Batch 150, Loss: 1.2584551763534546\n",
      "Epoch 13, Batch 200, Loss: 1.282474913597107\n",
      "Epoch 13, Batch 250, Loss: 1.2893204641342164\n",
      "Epoch 13, Batch 300, Loss: 1.3164968490600586\n",
      "Epoch 13, Batch 350, Loss: 1.2631568288803101\n",
      "Epoch 13, Batch 400, Loss: 1.2655556774139405\n",
      "Epoch 13, Batch 450, Loss: 1.2765438437461853\n",
      "Epoch 13, Batch 500, Loss: 1.2927833580970765\n",
      "Epoch 13, Batch 550, Loss: 1.253683774471283\n",
      "Epoch 13, Batch 600, Loss: 1.3426491117477417\n",
      "Epoch 13, Batch 650, Loss: 1.2735966324806214\n",
      "Epoch 13, Batch 700, Loss: 1.2684924006462097\n",
      "Epoch 13, Batch 750, Loss: 1.2647543382644653\n",
      "Epoch 13, Batch 800, Loss: 1.2789102029800414\n",
      "Epoch 13, Batch 850, Loss: 1.2610570645332337\n",
      "Epoch 13, Batch 900, Loss: 1.2683467507362365\n",
      "Epoch 14, Batch 50, Loss: 1.2751553845405579\n",
      "Epoch 14, Batch 100, Loss: 1.261873836517334\n",
      "Epoch 14, Batch 150, Loss: 1.2603058624267578\n",
      "Epoch 14, Batch 200, Loss: 1.2936571145057678\n",
      "Epoch 14, Batch 250, Loss: 1.293757872581482\n",
      "Epoch 14, Batch 300, Loss: 1.262948338985443\n",
      "Epoch 14, Batch 350, Loss: 1.2836566710472106\n",
      "Epoch 14, Batch 400, Loss: 1.2660586285591124\n",
      "Epoch 14, Batch 450, Loss: 1.2790837955474854\n",
      "Epoch 14, Batch 500, Loss: 1.2552031922340392\n",
      "Epoch 14, Batch 550, Loss: 1.273010380268097\n",
      "Epoch 14, Batch 600, Loss: 1.2814782309532164\n",
      "Epoch 14, Batch 650, Loss: 1.2858886885643006\n",
      "Epoch 14, Batch 700, Loss: 1.274945945739746\n",
      "Epoch 14, Batch 750, Loss: 1.2725506377220155\n",
      "Epoch 14, Batch 800, Loss: 1.2606715941429139\n",
      "Epoch 14, Batch 850, Loss: 1.2797271227836609\n",
      "Epoch 14, Batch 900, Loss: 1.26895587682724\n",
      "Epoch 15, Batch 50, Loss: 1.2742370939254761\n",
      "Epoch 15, Batch 100, Loss: 1.2850207424163818\n",
      "Epoch 15, Batch 150, Loss: 1.2759833073616027\n",
      "Epoch 15, Batch 200, Loss: 1.2651090693473817\n",
      "Epoch 15, Batch 250, Loss: 1.2810789775848388\n",
      "Epoch 15, Batch 300, Loss: 1.2828843998908996\n",
      "Epoch 15, Batch 350, Loss: 1.2789987564086913\n",
      "Epoch 15, Batch 400, Loss: 1.266013617515564\n",
      "Epoch 15, Batch 450, Loss: 1.2761537766456603\n",
      "Epoch 15, Batch 500, Loss: 1.2642711782455445\n",
      "Epoch 15, Batch 550, Loss: 1.2708494567871094\n",
      "Epoch 15, Batch 600, Loss: 1.266601505279541\n",
      "Epoch 15, Batch 650, Loss: 1.2719209504127502\n",
      "Epoch 15, Batch 700, Loss: 1.2980845308303832\n",
      "Epoch 15, Batch 750, Loss: 1.267605061531067\n",
      "Epoch 15, Batch 800, Loss: 1.2863371777534485\n",
      "Epoch 15, Batch 850, Loss: 1.2549944400787354\n",
      "Epoch 15, Batch 900, Loss: 1.2615160584449767\n",
      "Epoch 16, Batch 50, Loss: 1.2575228857994079\n",
      "Epoch 16, Batch 100, Loss: 1.2628488588333129\n",
      "Epoch 16, Batch 150, Loss: 1.2536815881729126\n",
      "Epoch 16, Batch 200, Loss: 1.3077222394943238\n",
      "Epoch 16, Batch 250, Loss: 1.2664031767845154\n",
      "Epoch 16, Batch 300, Loss: 1.2521324181556701\n",
      "Epoch 16, Batch 350, Loss: 1.2569161438941956\n",
      "Epoch 16, Batch 400, Loss: 1.2724989604949952\n",
      "Epoch 16, Batch 450, Loss: 1.2578489995002746\n",
      "Epoch 16, Batch 500, Loss: 1.309047200679779\n",
      "Epoch 16, Batch 550, Loss: 1.2515216231346131\n",
      "Epoch 16, Batch 600, Loss: 1.2888582849502563\n",
      "Epoch 16, Batch 650, Loss: 1.2711459112167358\n",
      "Epoch 16, Batch 700, Loss: 1.2703915786743165\n",
      "Epoch 16, Batch 750, Loss: 1.2798853278160096\n",
      "Epoch 16, Batch 800, Loss: 1.2648911380767822\n",
      "Epoch 16, Batch 850, Loss: 1.315554883480072\n",
      "Epoch 16, Batch 900, Loss: 1.3318656659126282\n",
      "Epoch 17, Batch 50, Loss: 1.2646977400779724\n",
      "Epoch 17, Batch 100, Loss: 1.2460574173927308\n",
      "Epoch 17, Batch 150, Loss: 1.32510915517807\n",
      "Epoch 17, Batch 200, Loss: 1.2748604393005372\n",
      "Epoch 17, Batch 250, Loss: 1.3564047694206238\n",
      "Epoch 17, Batch 300, Loss: 1.2742153596878052\n",
      "Epoch 17, Batch 350, Loss: 1.2729483485221862\n",
      "Epoch 17, Batch 400, Loss: 1.2706223630905151\n",
      "Epoch 17, Batch 450, Loss: 1.2888320422172546\n",
      "Epoch 17, Batch 500, Loss: 1.286924958229065\n",
      "Epoch 17, Batch 550, Loss: 1.287658588886261\n",
      "Epoch 17, Batch 600, Loss: 1.2543579149246216\n",
      "Epoch 17, Batch 650, Loss: 1.2661459851264953\n",
      "Epoch 17, Batch 700, Loss: 1.2531866478919982\n",
      "Epoch 17, Batch 750, Loss: 1.2773638081550598\n",
      "Epoch 17, Batch 800, Loss: 1.2751468801498413\n",
      "Epoch 17, Batch 850, Loss: 1.2725172090530394\n",
      "Epoch 17, Batch 900, Loss: 1.2586373782157898\n",
      "Epoch 18, Batch 50, Loss: 1.2779710292816162\n",
      "Epoch 18, Batch 100, Loss: 1.272816517353058\n",
      "Epoch 18, Batch 150, Loss: 1.3074989461898803\n",
      "Epoch 18, Batch 200, Loss: 1.2729560375213622\n",
      "Epoch 18, Batch 250, Loss: 1.2913862943649292\n",
      "Epoch 18, Batch 300, Loss: 1.254617702960968\n",
      "Epoch 18, Batch 350, Loss: 1.3004513931274415\n",
      "Epoch 18, Batch 400, Loss: 1.2710218977928163\n",
      "Epoch 18, Batch 450, Loss: 1.2540704607963562\n",
      "Epoch 18, Batch 500, Loss: 1.2762428498268128\n",
      "Epoch 18, Batch 550, Loss: 1.267320566177368\n",
      "Epoch 18, Batch 600, Loss: 1.2762615156173707\n",
      "Epoch 18, Batch 650, Loss: 1.2855319499969482\n",
      "Epoch 18, Batch 700, Loss: 1.2728369307518006\n",
      "Epoch 18, Batch 750, Loss: 1.2946490335464478\n",
      "Epoch 18, Batch 800, Loss: 1.2708052158355714\n",
      "Epoch 18, Batch 850, Loss: 1.2554925203323364\n",
      "Epoch 18, Batch 900, Loss: 1.2777243399620055\n",
      "Epoch 19, Batch 50, Loss: 1.2819803595542907\n",
      "Epoch 19, Batch 100, Loss: 1.2776569199562073\n",
      "Epoch 19, Batch 150, Loss: 1.2576372885704041\n",
      "Epoch 19, Batch 200, Loss: 1.270639033317566\n",
      "Epoch 19, Batch 250, Loss: 1.267486970424652\n",
      "Epoch 19, Batch 300, Loss: 1.3111672973632813\n",
      "Epoch 19, Batch 350, Loss: 1.2844220781326294\n",
      "Epoch 19, Batch 400, Loss: 1.293102560043335\n",
      "Epoch 19, Batch 450, Loss: 1.2636007714271544\n",
      "Epoch 19, Batch 500, Loss: 1.264374532699585\n",
      "Epoch 19, Batch 550, Loss: 1.2476361298561096\n",
      "Epoch 19, Batch 600, Loss: 1.2736468410491943\n",
      "Epoch 19, Batch 650, Loss: 1.2450659632682801\n",
      "Epoch 19, Batch 700, Loss: 1.2845602488517762\n",
      "Epoch 19, Batch 750, Loss: 1.2570411324501038\n",
      "Epoch 19, Batch 800, Loss: 1.2645542454719543\n",
      "Epoch 19, Batch 850, Loss: 1.294099531173706\n",
      "Epoch 19, Batch 900, Loss: 1.324285147190094\n",
      "Epoch 20, Batch 50, Loss: 1.2554199194908142\n",
      "Epoch 20, Batch 100, Loss: 1.2718805193901062\n",
      "Epoch 20, Batch 150, Loss: 1.2738283848762513\n",
      "Epoch 20, Batch 200, Loss: 1.298556261062622\n",
      "Epoch 20, Batch 250, Loss: 1.2533817315101623\n",
      "Epoch 20, Batch 300, Loss: 1.259039809703827\n",
      "Epoch 20, Batch 350, Loss: 1.2561607837677002\n",
      "Epoch 20, Batch 400, Loss: 1.2664666962623596\n",
      "Epoch 20, Batch 450, Loss: 1.2788040256500244\n",
      "Epoch 20, Batch 500, Loss: 1.29381516456604\n",
      "Epoch 20, Batch 550, Loss: 1.2823958325386047\n",
      "Epoch 20, Batch 600, Loss: 1.2763035106658935\n",
      "Epoch 20, Batch 650, Loss: 1.2719355940818786\n",
      "Epoch 20, Batch 700, Loss: 1.2633219599723815\n",
      "Epoch 20, Batch 750, Loss: 1.2742414736747742\n",
      "Epoch 20, Batch 800, Loss: 1.293172357082367\n",
      "Epoch 20, Batch 850, Loss: 1.2910119223594665\n",
      "Epoch 20, Batch 900, Loss: 1.254397575855255\n",
      "Epoch 21, Batch 50, Loss: 1.2983631634712218\n",
      "Epoch 21, Batch 100, Loss: 1.3003731536865235\n",
      "Epoch 21, Batch 150, Loss: 1.2908075451850891\n",
      "Epoch 21, Batch 200, Loss: 1.279985592365265\n",
      "Epoch 21, Batch 250, Loss: 1.2601588153839112\n",
      "Epoch 21, Batch 300, Loss: 1.280141580104828\n",
      "Epoch 21, Batch 350, Loss: 1.2902942633628844\n",
      "Epoch 21, Batch 400, Loss: 1.3148055052757264\n",
      "Epoch 21, Batch 450, Loss: 1.3341948294639587\n",
      "Epoch 21, Batch 500, Loss: 1.2893433117866515\n",
      "Epoch 21, Batch 550, Loss: 1.297614781856537\n",
      "Epoch 21, Batch 600, Loss: 1.2706694650650023\n",
      "Epoch 21, Batch 650, Loss: 1.2712640237808228\n",
      "Epoch 21, Batch 700, Loss: 1.254768626689911\n",
      "Epoch 21, Batch 750, Loss: 1.272099759578705\n",
      "Epoch 21, Batch 800, Loss: 1.2764309883117675\n",
      "Epoch 21, Batch 850, Loss: 1.2525023674964906\n",
      "Epoch 21, Batch 900, Loss: 1.3034190893173219\n",
      "Epoch 22, Batch 50, Loss: 1.2526094365119933\n",
      "Epoch 22, Batch 100, Loss: 1.2738243222236634\n",
      "Epoch 22, Batch 150, Loss: 1.3028647303581238\n",
      "Epoch 22, Batch 200, Loss: 1.2713292837142944\n",
      "Epoch 22, Batch 250, Loss: 1.2953454732894898\n",
      "Epoch 22, Batch 300, Loss: 1.271647560596466\n",
      "Epoch 22, Batch 350, Loss: 1.3152641344070435\n",
      "Epoch 22, Batch 400, Loss: 1.271939790248871\n",
      "Epoch 22, Batch 450, Loss: 1.2643000507354736\n",
      "Epoch 22, Batch 500, Loss: 1.2621991896629334\n",
      "Epoch 22, Batch 550, Loss: 1.2765510416030883\n",
      "Epoch 22, Batch 600, Loss: 1.2775534105300903\n",
      "Epoch 22, Batch 650, Loss: 1.25947589635849\n",
      "Epoch 22, Batch 700, Loss: 1.24822003364563\n",
      "Epoch 22, Batch 750, Loss: 1.2982523655891418\n",
      "Epoch 22, Batch 800, Loss: 1.2870902824401855\n",
      "Epoch 22, Batch 850, Loss: 1.2704856991767883\n",
      "Epoch 22, Batch 900, Loss: 1.256131887435913\n",
      "Epoch 23, Batch 50, Loss: 1.2589384722709656\n",
      "Epoch 23, Batch 100, Loss: 1.269193868637085\n",
      "Epoch 23, Batch 150, Loss: 1.2758872270584107\n",
      "Epoch 23, Batch 200, Loss: 1.2626988530158996\n",
      "Epoch 23, Batch 250, Loss: 1.2800090408325195\n",
      "Epoch 23, Batch 300, Loss: 1.2595361614227294\n",
      "Epoch 23, Batch 350, Loss: 1.258175528049469\n",
      "Epoch 23, Batch 400, Loss: 1.274336097240448\n",
      "Epoch 23, Batch 450, Loss: 1.2615173363685608\n",
      "Epoch 23, Batch 500, Loss: 1.2866316652297973\n",
      "Epoch 23, Batch 550, Loss: 1.2830277872085571\n",
      "Epoch 23, Batch 600, Loss: 1.2711693358421325\n",
      "Epoch 23, Batch 650, Loss: 1.2611829471588134\n",
      "Epoch 23, Batch 700, Loss: 1.2602448558807373\n",
      "Epoch 23, Batch 750, Loss: 1.2800822186470031\n",
      "Epoch 23, Batch 800, Loss: 1.2902865862846375\n",
      "Epoch 23, Batch 850, Loss: 1.2632139611244202\n",
      "Epoch 23, Batch 900, Loss: 1.2761741304397582\n",
      "Epoch 24, Batch 50, Loss: 1.266661822795868\n",
      "Epoch 24, Batch 100, Loss: 1.2902137207984925\n",
      "Epoch 24, Batch 150, Loss: 1.2460383915901183\n",
      "Epoch 24, Batch 200, Loss: 1.2640052247047424\n",
      "Epoch 24, Batch 250, Loss: 1.2667857217788696\n",
      "Epoch 24, Batch 300, Loss: 1.2826287460327148\n",
      "Epoch 24, Batch 350, Loss: 1.2842016077041627\n",
      "Epoch 24, Batch 400, Loss: 1.2889795351028441\n",
      "Epoch 24, Batch 450, Loss: 1.2556809735298158\n",
      "Epoch 24, Batch 500, Loss: 1.2567859148979188\n",
      "Epoch 24, Batch 550, Loss: 1.3378887462615967\n",
      "Epoch 24, Batch 600, Loss: 1.2677786374092102\n",
      "Epoch 24, Batch 650, Loss: 1.2680551481246949\n",
      "Epoch 24, Batch 700, Loss: 1.2627622818946838\n",
      "Epoch 24, Batch 750, Loss: 1.2852196741104125\n",
      "Epoch 24, Batch 800, Loss: 1.3221672296524047\n",
      "Epoch 24, Batch 850, Loss: 1.2855200266838074\n",
      "Epoch 24, Batch 900, Loss: 1.2505290913581848\n",
      "Epoch 25, Batch 50, Loss: 1.2835721731185914\n",
      "Epoch 25, Batch 100, Loss: 1.26935409784317\n",
      "Epoch 25, Batch 150, Loss: 1.2675748729705811\n",
      "Epoch 25, Batch 200, Loss: 1.2688280582427978\n",
      "Epoch 25, Batch 250, Loss: 1.2821497488021851\n",
      "Epoch 25, Batch 300, Loss: 1.2626136946678161\n",
      "Epoch 25, Batch 350, Loss: 1.2840542483329773\n",
      "Epoch 25, Batch 400, Loss: 1.2742233157157898\n",
      "Epoch 25, Batch 450, Loss: 1.2540855455398559\n",
      "Epoch 25, Batch 500, Loss: 1.253429024219513\n",
      "Epoch 25, Batch 550, Loss: 1.254754581451416\n",
      "Epoch 25, Batch 600, Loss: 1.2860305213928223\n",
      "Epoch 25, Batch 650, Loss: 1.2689786911010743\n",
      "Epoch 25, Batch 700, Loss: 1.2537663912773132\n",
      "Epoch 25, Batch 750, Loss: 1.261781198978424\n",
      "Epoch 25, Batch 800, Loss: 1.3131421184539795\n",
      "Epoch 25, Batch 850, Loss: 1.2653005051612853\n",
      "Epoch 25, Batch 900, Loss: 1.2701173162460326\n",
      "Accuracy on test set: 0.1%\n",
      "Fitting for combination 57\n",
      "784\n",
      "3\n",
      "10\n",
      "[40, 10, 50, 10]\n",
      "False\n",
      "['relu', 'sigmoid', 'tanh']\n",
      "SGD\n",
      "0.03\n",
      "0.03\n",
      "CrossEntropyLoss\n",
      "100\n",
      "25\n",
      "0\n",
      "Epoch 1, Batch 100, Loss: 2.2872485589981078\n",
      "Epoch 1, Batch 200, Loss: 2.2231436014175414\n",
      "Epoch 1, Batch 300, Loss: 2.0445010638237\n",
      "Epoch 1, Batch 400, Loss: 1.8308094441890717\n",
      "Epoch 1, Batch 500, Loss: 1.7303937757015229\n",
      "Epoch 1, Batch 600, Loss: 1.6525110518932342\n",
      "Epoch 1, Batch 700, Loss: 1.5896562659740447\n",
      "Epoch 1, Batch 800, Loss: 1.530784250497818\n",
      "Epoch 1, Batch 900, Loss: 1.4834086120128631\n",
      "Epoch 2, Batch 100, Loss: 1.4379952502250672\n",
      "Epoch 2, Batch 200, Loss: 1.4112669897079468\n",
      "Epoch 2, Batch 300, Loss: 1.3917199170589447\n",
      "Epoch 2, Batch 400, Loss: 1.3735246026515961\n",
      "Epoch 2, Batch 500, Loss: 1.3559698927402497\n",
      "Epoch 2, Batch 600, Loss: 1.3350195944309236\n",
      "Epoch 2, Batch 700, Loss: 1.310716210603714\n",
      "Epoch 2, Batch 800, Loss: 1.2890511131286622\n",
      "Epoch 2, Batch 900, Loss: 1.258263863325119\n",
      "Epoch 3, Batch 100, Loss: 1.237389565706253\n",
      "Epoch 3, Batch 200, Loss: 1.2137796652317048\n",
      "Epoch 3, Batch 300, Loss: 1.187407715320587\n",
      "Epoch 3, Batch 400, Loss: 1.171721612215042\n",
      "Epoch 3, Batch 500, Loss: 1.168471617102623\n",
      "Epoch 3, Batch 600, Loss: 1.1649710786342622\n",
      "Epoch 3, Batch 700, Loss: 1.1364659178256988\n",
      "Epoch 3, Batch 800, Loss: 1.1463307988643647\n",
      "Epoch 3, Batch 900, Loss: 1.1194400757551193\n",
      "Epoch 4, Batch 100, Loss: 1.130527330636978\n",
      "Epoch 4, Batch 200, Loss: 1.1235589635372163\n",
      "Epoch 4, Batch 300, Loss: 1.123769520521164\n",
      "Epoch 4, Batch 400, Loss: 1.1174578177928924\n",
      "Epoch 4, Batch 500, Loss: 1.116252155303955\n",
      "Epoch 4, Batch 600, Loss: 1.1043613183498382\n",
      "Epoch 4, Batch 700, Loss: 1.1019229608774186\n",
      "Epoch 4, Batch 800, Loss: 1.0964609754085541\n",
      "Epoch 4, Batch 900, Loss: 1.1017092990875244\n",
      "Epoch 5, Batch 100, Loss: 1.0826880323886872\n",
      "Epoch 5, Batch 200, Loss: 1.0929478412866593\n",
      "Epoch 5, Batch 300, Loss: 1.0954026740789413\n",
      "Epoch 5, Batch 400, Loss: 1.0856952738761902\n",
      "Epoch 5, Batch 500, Loss: 1.0805411213636398\n",
      "Epoch 5, Batch 600, Loss: 1.0891904240846635\n",
      "Epoch 5, Batch 700, Loss: 1.0908825969696045\n",
      "Epoch 5, Batch 800, Loss: 1.073178671002388\n",
      "Epoch 5, Batch 900, Loss: 1.079689250588417\n",
      "Epoch 6, Batch 100, Loss: 1.0683747053146362\n",
      "Epoch 6, Batch 200, Loss: 1.0627015674114226\n",
      "Epoch 6, Batch 300, Loss: 1.0619156140089034\n",
      "Epoch 6, Batch 400, Loss: 1.0648054081201552\n",
      "Epoch 6, Batch 500, Loss: 1.0696917283535003\n",
      "Epoch 6, Batch 600, Loss: 1.056446904540062\n",
      "Epoch 6, Batch 700, Loss: 1.045875875353813\n",
      "Epoch 6, Batch 800, Loss: 1.0546188235282898\n",
      "Epoch 6, Batch 900, Loss: 1.0456269711256028\n",
      "Epoch 7, Batch 100, Loss: 1.038631681203842\n",
      "Epoch 7, Batch 200, Loss: 1.0254599326848983\n",
      "Epoch 7, Batch 300, Loss: 1.022924292087555\n",
      "Epoch 7, Batch 400, Loss: 1.0099157178401947\n",
      "Epoch 7, Batch 500, Loss: 1.0250240236520767\n",
      "Epoch 7, Batch 600, Loss: 1.032932974100113\n",
      "Epoch 7, Batch 700, Loss: 1.022511022090912\n",
      "Epoch 7, Batch 800, Loss: 1.02506150662899\n",
      "Epoch 7, Batch 900, Loss: 1.0179448252916337\n",
      "Epoch 8, Batch 100, Loss: 0.9992180168628693\n",
      "Epoch 8, Batch 200, Loss: 1.0087096524238586\n",
      "Epoch 8, Batch 300, Loss: 0.9988121342658997\n",
      "Epoch 8, Batch 400, Loss: 0.987359670996666\n",
      "Epoch 8, Batch 500, Loss: 0.995005202293396\n",
      "Epoch 8, Batch 600, Loss: 0.9917978823184967\n",
      "Epoch 8, Batch 700, Loss: 0.9874584573507309\n",
      "Epoch 8, Batch 800, Loss: 0.9982632261514663\n",
      "Epoch 8, Batch 900, Loss: 0.9625147598981857\n",
      "Epoch 9, Batch 100, Loss: 0.9767214405536652\n",
      "Epoch 9, Batch 200, Loss: 0.9632471138238907\n",
      "Epoch 9, Batch 300, Loss: 0.9623052674531937\n",
      "Epoch 9, Batch 400, Loss: 0.9531602847576142\n",
      "Epoch 9, Batch 500, Loss: 0.957248501777649\n",
      "Epoch 9, Batch 600, Loss: 0.9462967073917389\n",
      "Epoch 9, Batch 700, Loss: 0.9503859800100326\n",
      "Epoch 9, Batch 800, Loss: 0.9403869730234146\n",
      "Epoch 9, Batch 900, Loss: 0.9394881647825241\n",
      "Epoch 10, Batch 100, Loss: 0.9428881418704986\n",
      "Epoch 10, Batch 200, Loss: 0.9369456475973129\n",
      "Epoch 10, Batch 300, Loss: 0.9205724257230758\n",
      "Epoch 10, Batch 400, Loss: 0.9179498231410981\n",
      "Epoch 10, Batch 500, Loss: 0.9252188533544541\n",
      "Epoch 10, Batch 600, Loss: 0.9226685726642608\n",
      "Epoch 10, Batch 700, Loss: 0.9231521105766296\n",
      "Epoch 10, Batch 800, Loss: 0.9152286785840988\n",
      "Epoch 10, Batch 900, Loss: 0.9127201628684998\n",
      "Epoch 11, Batch 100, Loss: 0.8987418127059936\n",
      "Epoch 11, Batch 200, Loss: 0.8969784188270569\n",
      "Epoch 11, Batch 300, Loss: 0.9081373661756516\n",
      "Epoch 11, Batch 400, Loss: 0.9003126764297485\n",
      "Epoch 11, Batch 500, Loss: 0.9016563248634338\n",
      "Epoch 11, Batch 600, Loss: 0.8803493750095367\n",
      "Epoch 11, Batch 700, Loss: 0.8848924386501312\n",
      "Epoch 11, Batch 800, Loss: 0.9041706895828248\n",
      "Epoch 11, Batch 900, Loss: 0.8995672780275344\n",
      "Epoch 12, Batch 100, Loss: 0.8878484678268432\n",
      "Epoch 12, Batch 200, Loss: 0.8828056937456131\n",
      "Epoch 12, Batch 300, Loss: 0.8837511742115021\n",
      "Epoch 12, Batch 400, Loss: 0.8806181919574737\n",
      "Epoch 12, Batch 500, Loss: 0.8660388624668122\n",
      "Epoch 12, Batch 600, Loss: 0.874089059829712\n",
      "Epoch 12, Batch 700, Loss: 0.8825148230791092\n",
      "Epoch 12, Batch 800, Loss: 0.8850190752744674\n",
      "Epoch 12, Batch 900, Loss: 0.8694782572984695\n",
      "Epoch 13, Batch 100, Loss: 0.8549326997995377\n",
      "Epoch 13, Batch 200, Loss: 0.864093924164772\n",
      "Epoch 13, Batch 300, Loss: 0.8742701053619385\n",
      "Epoch 13, Batch 400, Loss: 0.8603302961587906\n",
      "Epoch 13, Batch 500, Loss: 0.8868016242980957\n",
      "Epoch 13, Batch 600, Loss: 0.8581052601337433\n",
      "Epoch 13, Batch 700, Loss: 0.8552156949043274\n",
      "Epoch 13, Batch 800, Loss: 0.8774991697072982\n",
      "Epoch 13, Batch 900, Loss: 0.8676837158203125\n",
      "Epoch 14, Batch 100, Loss: 0.865787113904953\n",
      "Epoch 14, Batch 200, Loss: 0.862785981297493\n",
      "Epoch 14, Batch 300, Loss: 0.8608459347486496\n",
      "Epoch 14, Batch 400, Loss: 0.8749313813447952\n",
      "Epoch 14, Batch 500, Loss: 0.8504816740751266\n",
      "Epoch 14, Batch 600, Loss: 0.8626272797584533\n",
      "Epoch 14, Batch 700, Loss: 0.8728053522109985\n",
      "Epoch 14, Batch 800, Loss: 0.8485143971443176\n",
      "Epoch 14, Batch 900, Loss: 0.8599946790933609\n",
      "Epoch 15, Batch 100, Loss: 0.8702618849277496\n",
      "Epoch 15, Batch 200, Loss: 0.8742115008831024\n",
      "Epoch 15, Batch 300, Loss: 0.8585901689529419\n",
      "Epoch 15, Batch 400, Loss: 0.8592306005954743\n",
      "Epoch 15, Batch 500, Loss: 0.8528308856487274\n",
      "Epoch 15, Batch 600, Loss: 0.8692257660627365\n",
      "Epoch 15, Batch 700, Loss: 0.8596283876895905\n",
      "Epoch 15, Batch 800, Loss: 0.8545398634672164\n",
      "Epoch 15, Batch 900, Loss: 0.8443831831216813\n",
      "Epoch 16, Batch 100, Loss: 0.8597296714782715\n",
      "Epoch 16, Batch 200, Loss: 0.8494805306196213\n",
      "Epoch 16, Batch 300, Loss: 0.8630386060476303\n",
      "Epoch 16, Batch 400, Loss: 0.8594302886724472\n",
      "Epoch 16, Batch 500, Loss: 0.8518415087461472\n",
      "Epoch 16, Batch 600, Loss: 0.8582027608156204\n",
      "Epoch 16, Batch 700, Loss: 0.8526454025506973\n",
      "Epoch 16, Batch 800, Loss: 0.8478861039876938\n",
      "Epoch 16, Batch 900, Loss: 0.8506055104732514\n",
      "Epoch 17, Batch 100, Loss: 0.8480491638183594\n",
      "Epoch 17, Batch 200, Loss: 0.8598685222864151\n",
      "Epoch 17, Batch 300, Loss: 0.8566431039571762\n",
      "Epoch 17, Batch 400, Loss: 0.8479482275247574\n",
      "Epoch 17, Batch 500, Loss: 0.8452684539556503\n",
      "Epoch 17, Batch 600, Loss: 0.8669811922311783\n",
      "Epoch 17, Batch 700, Loss: 0.858704405426979\n",
      "Epoch 17, Batch 800, Loss: 0.8510268729925156\n",
      "Epoch 17, Batch 900, Loss: 0.8619724142551423\n",
      "Epoch 18, Batch 100, Loss: 0.8602371299266816\n",
      "Epoch 18, Batch 200, Loss: 0.8566828817129135\n",
      "Epoch 18, Batch 300, Loss: 0.843650768995285\n",
      "Epoch 18, Batch 400, Loss: 0.8567876946926117\n",
      "Epoch 18, Batch 500, Loss: 0.8629061901569366\n",
      "Epoch 18, Batch 600, Loss: 0.8576365315914154\n",
      "Epoch 18, Batch 700, Loss: 0.8550619757175446\n",
      "Epoch 18, Batch 800, Loss: 0.8649300104379654\n",
      "Epoch 18, Batch 900, Loss: 0.8324859791994095\n",
      "Epoch 19, Batch 100, Loss: 0.8494956827163697\n",
      "Epoch 19, Batch 200, Loss: 0.8515904927253723\n",
      "Epoch 19, Batch 300, Loss: 0.840400859117508\n",
      "Epoch 19, Batch 400, Loss: 0.8466739749908447\n",
      "Epoch 19, Batch 500, Loss: 0.849397479891777\n",
      "Epoch 19, Batch 600, Loss: 0.8544725960493088\n",
      "Epoch 19, Batch 700, Loss: 0.8526519405841827\n",
      "Epoch 19, Batch 800, Loss: 0.8602482950687409\n",
      "Epoch 19, Batch 900, Loss: 0.8522373867034913\n",
      "Epoch 20, Batch 100, Loss: 0.8605266147851944\n",
      "Epoch 20, Batch 200, Loss: 0.8554468250274658\n",
      "Epoch 20, Batch 300, Loss: 0.8603452873229981\n",
      "Epoch 20, Batch 400, Loss: 0.8427842569351196\n",
      "Epoch 20, Batch 500, Loss: 0.8555573290586471\n",
      "Epoch 20, Batch 600, Loss: 0.844846540093422\n",
      "Epoch 20, Batch 700, Loss: 0.840578453540802\n",
      "Epoch 20, Batch 800, Loss: 0.8415345287322998\n",
      "Epoch 20, Batch 900, Loss: 0.8400995594263077\n",
      "Epoch 21, Batch 100, Loss: 0.8557993686199188\n",
      "Epoch 21, Batch 200, Loss: 0.8566056668758393\n",
      "Epoch 21, Batch 300, Loss: 0.8362909597158432\n",
      "Epoch 21, Batch 400, Loss: 0.8545947837829589\n",
      "Epoch 21, Batch 500, Loss: 0.8318225634098053\n",
      "Epoch 21, Batch 600, Loss: 0.8493373787403107\n",
      "Epoch 21, Batch 700, Loss: 0.8634980267286301\n",
      "Epoch 21, Batch 800, Loss: 0.850733363032341\n",
      "Epoch 21, Batch 900, Loss: 0.8438158845901489\n",
      "Epoch 22, Batch 100, Loss: 0.8395622456073761\n",
      "Epoch 22, Batch 200, Loss: 0.8434962511062623\n",
      "Epoch 22, Batch 300, Loss: 0.8516229951381683\n",
      "Epoch 22, Batch 400, Loss: 0.8426741659641266\n",
      "Epoch 22, Batch 500, Loss: 0.86022352039814\n",
      "Epoch 22, Batch 600, Loss: 0.8574674689769745\n",
      "Epoch 22, Batch 700, Loss: 0.8477086240053177\n",
      "Epoch 22, Batch 800, Loss: 0.8457391959428787\n",
      "Epoch 22, Batch 900, Loss: 0.8445234102010727\n",
      "Epoch 23, Batch 100, Loss: 0.8506916898488999\n",
      "Epoch 23, Batch 200, Loss: 0.8380241519212723\n",
      "Epoch 23, Batch 300, Loss: 0.8528959774971008\n",
      "Epoch 23, Batch 400, Loss: 0.8434497648477555\n",
      "Epoch 23, Batch 500, Loss: 0.8426528877019882\n",
      "Epoch 23, Batch 600, Loss: 0.8480353331565857\n",
      "Epoch 23, Batch 700, Loss: 0.8347366672754287\n",
      "Epoch 23, Batch 800, Loss: 0.8655453956127167\n",
      "Epoch 23, Batch 900, Loss: 0.8499132525920868\n",
      "Epoch 24, Batch 100, Loss: 0.8408638018369675\n",
      "Epoch 24, Batch 200, Loss: 0.857147889137268\n",
      "Epoch 24, Batch 300, Loss: 0.8456173920631409\n",
      "Epoch 24, Batch 400, Loss: 0.8402954214811325\n",
      "Epoch 24, Batch 500, Loss: 0.854002149105072\n",
      "Epoch 24, Batch 600, Loss: 0.8407097554206848\n",
      "Epoch 24, Batch 700, Loss: 0.8493043828010559\n",
      "Epoch 24, Batch 800, Loss: 0.8354170078039169\n",
      "Epoch 24, Batch 900, Loss: 0.8463147330284119\n",
      "Epoch 25, Batch 100, Loss: 0.8355380409955978\n",
      "Epoch 25, Batch 200, Loss: 0.8480367088317871\n",
      "Epoch 25, Batch 300, Loss: 0.8399227350950241\n",
      "Epoch 25, Batch 400, Loss: 0.8359712094068528\n",
      "Epoch 25, Batch 500, Loss: 0.841769403219223\n",
      "Epoch 25, Batch 600, Loss: 0.83442309319973\n",
      "Epoch 25, Batch 700, Loss: 0.8552228337526322\n",
      "Epoch 25, Batch 800, Loss: 0.8584572494029998\n",
      "Epoch 25, Batch 900, Loss: 0.8549332118034363\n",
      "Accuracy on test set: 0.7267%\n",
      "Fitting for combination 58\n",
      "784\n",
      "3\n",
      "10\n",
      "[40, 10, 50, 10]\n",
      "True\n",
      "['relu', 'sigmoid', 'sigmoid']\n",
      "Adam\n",
      "0.1\n",
      "0.03\n",
      "CrossEntropyLoss\n",
      "100\n",
      "25\n",
      "0\n",
      "Epoch 1, Batch 100, Loss: 2.427404158115387\n",
      "Epoch 1, Batch 200, Loss: 2.4006155729293823\n",
      "Epoch 1, Batch 300, Loss: 2.4096872758865358\n",
      "Epoch 1, Batch 400, Loss: 2.398026292324066\n",
      "Epoch 1, Batch 500, Loss: 2.3970411157608034\n",
      "Epoch 1, Batch 600, Loss: 2.4051185727119444\n",
      "Epoch 1, Batch 700, Loss: 2.39634982585907\n",
      "Epoch 1, Batch 800, Loss: 2.392658915519714\n",
      "Epoch 1, Batch 900, Loss: 2.3924311327934267\n",
      "Epoch 2, Batch 100, Loss: 2.423012833595276\n",
      "Epoch 2, Batch 200, Loss: 2.40101366519928\n",
      "Epoch 2, Batch 300, Loss: 2.4000967812538145\n",
      "Epoch 2, Batch 400, Loss: 2.4056212949752807\n",
      "Epoch 2, Batch 500, Loss: 2.4087512350082396\n",
      "Epoch 2, Batch 600, Loss: 2.4019181394577025\n",
      "Epoch 2, Batch 700, Loss: 2.398022518157959\n",
      "Epoch 2, Batch 800, Loss: 2.4035333800315857\n",
      "Epoch 2, Batch 900, Loss: 2.392439589500427\n",
      "Epoch 3, Batch 100, Loss: 2.4038533902168275\n",
      "Epoch 3, Batch 200, Loss: 2.3958019018173218\n",
      "Epoch 3, Batch 300, Loss: 2.3818235492706297\n",
      "Epoch 3, Batch 400, Loss: 2.398423099517822\n",
      "Epoch 3, Batch 500, Loss: 2.399379994869232\n",
      "Epoch 3, Batch 600, Loss: 2.395091769695282\n",
      "Epoch 3, Batch 700, Loss: 2.403749732971191\n",
      "Epoch 3, Batch 800, Loss: 2.4067945408821108\n",
      "Epoch 3, Batch 900, Loss: 2.3941743421554564\n",
      "Epoch 4, Batch 100, Loss: 2.4139303827285765\n",
      "Epoch 4, Batch 200, Loss: 2.4025739765167238\n",
      "Epoch 4, Batch 300, Loss: 2.3923483729362487\n",
      "Epoch 4, Batch 400, Loss: 2.3968314957618713\n",
      "Epoch 4, Batch 500, Loss: 2.406582758426666\n",
      "Epoch 4, Batch 600, Loss: 2.399984369277954\n",
      "Epoch 4, Batch 700, Loss: 2.3884105920791625\n",
      "Epoch 4, Batch 800, Loss: 2.407510118484497\n",
      "Epoch 4, Batch 900, Loss: 2.409846079349518\n",
      "Epoch 5, Batch 100, Loss: 2.4147321128845216\n",
      "Epoch 5, Batch 200, Loss: 2.3848065090179444\n",
      "Epoch 5, Batch 300, Loss: 2.4152624082565306\n",
      "Epoch 5, Batch 400, Loss: 2.4076647090911867\n",
      "Epoch 5, Batch 500, Loss: 2.395536119937897\n",
      "Epoch 5, Batch 600, Loss: 2.397793703079224\n",
      "Epoch 5, Batch 700, Loss: 2.4077050423622133\n",
      "Epoch 5, Batch 800, Loss: 2.4093636727333068\n",
      "Epoch 5, Batch 900, Loss: 2.39762494802475\n",
      "Epoch 6, Batch 100, Loss: 2.400186474323273\n",
      "Epoch 6, Batch 200, Loss: 2.39989773273468\n",
      "Epoch 6, Batch 300, Loss: 2.397329559326172\n",
      "Epoch 6, Batch 400, Loss: 2.4104746317863466\n",
      "Epoch 6, Batch 500, Loss: 2.4049128556251524\n",
      "Epoch 6, Batch 600, Loss: 2.394712738990784\n",
      "Epoch 6, Batch 700, Loss: 2.397989585399628\n",
      "Epoch 6, Batch 800, Loss: 2.3947901010513304\n",
      "Epoch 6, Batch 900, Loss: 2.409850742816925\n",
      "Epoch 7, Batch 100, Loss: 2.390843479633331\n",
      "Epoch 7, Batch 200, Loss: 2.4035733461380007\n",
      "Epoch 7, Batch 300, Loss: 2.4108404541015624\n",
      "Epoch 7, Batch 400, Loss: 2.390488302707672\n",
      "Epoch 7, Batch 500, Loss: 2.3891323828697204\n",
      "Epoch 7, Batch 600, Loss: 2.405875346660614\n",
      "Epoch 7, Batch 700, Loss: 2.403700246810913\n",
      "Epoch 7, Batch 800, Loss: 2.4034468650817873\n",
      "Epoch 7, Batch 900, Loss: 2.3924103021621703\n",
      "Epoch 8, Batch 100, Loss: 2.398218245506287\n",
      "Epoch 8, Batch 200, Loss: 2.4024237155914308\n",
      "Epoch 8, Batch 300, Loss: 2.414140341281891\n",
      "Epoch 8, Batch 400, Loss: 2.395693438053131\n",
      "Epoch 8, Batch 500, Loss: 2.380777509212494\n",
      "Epoch 8, Batch 600, Loss: 2.3906371068954466\n",
      "Epoch 8, Batch 700, Loss: 2.3959785676002503\n",
      "Epoch 8, Batch 800, Loss: 2.401479632854462\n",
      "Epoch 8, Batch 900, Loss: 2.4045468854904173\n",
      "Epoch 9, Batch 100, Loss: 2.392138786315918\n",
      "Epoch 9, Batch 200, Loss: 2.4110461831092835\n",
      "Epoch 9, Batch 300, Loss: 2.409969403743744\n",
      "Epoch 9, Batch 400, Loss: 2.3968228363990782\n",
      "Epoch 9, Batch 500, Loss: 2.4019390630722044\n",
      "Epoch 9, Batch 600, Loss: 2.3974080204963686\n",
      "Epoch 9, Batch 700, Loss: 2.3922055053710936\n",
      "Epoch 9, Batch 800, Loss: 2.39579213142395\n",
      "Epoch 9, Batch 900, Loss: 2.404741868972778\n",
      "Epoch 10, Batch 100, Loss: 2.4137052631378175\n",
      "Epoch 10, Batch 200, Loss: 2.4092660450935366\n",
      "Epoch 10, Batch 300, Loss: 2.400719048976898\n",
      "Epoch 10, Batch 400, Loss: 2.42411447763443\n",
      "Epoch 10, Batch 500, Loss: 2.4079274678230287\n",
      "Epoch 10, Batch 600, Loss: 2.3961138558387756\n",
      "Epoch 10, Batch 700, Loss: 2.4071726298332212\n",
      "Epoch 10, Batch 800, Loss: 2.3961204838752748\n",
      "Epoch 10, Batch 900, Loss: 2.417145080566406\n",
      "Epoch 11, Batch 100, Loss: 2.4021767258644102\n",
      "Epoch 11, Batch 200, Loss: 2.3944429111480714\n",
      "Epoch 11, Batch 300, Loss: 2.4083281445503233\n",
      "Epoch 11, Batch 400, Loss: 2.409504566192627\n",
      "Epoch 11, Batch 500, Loss: 2.388437604904175\n",
      "Epoch 11, Batch 600, Loss: 2.404980306625366\n",
      "Epoch 11, Batch 700, Loss: 2.406202828884125\n",
      "Epoch 11, Batch 800, Loss: 2.3914218759536743\n",
      "Epoch 11, Batch 900, Loss: 2.3921051597595215\n",
      "Epoch 12, Batch 100, Loss: 2.406049835681915\n",
      "Epoch 12, Batch 200, Loss: 2.4035310983657836\n",
      "Epoch 12, Batch 300, Loss: 2.3866026735305788\n",
      "Epoch 12, Batch 400, Loss: 2.397885148525238\n",
      "Epoch 12, Batch 500, Loss: 2.4181247639656065\n",
      "Epoch 12, Batch 600, Loss: 2.3965451169013976\n",
      "Epoch 12, Batch 700, Loss: 2.4040171837806703\n",
      "Epoch 12, Batch 800, Loss: 2.3970682525634768\n",
      "Epoch 12, Batch 900, Loss: 2.397834770679474\n",
      "Epoch 13, Batch 100, Loss: 2.3921126437187197\n",
      "Epoch 13, Batch 200, Loss: 2.4100680804252623\n",
      "Epoch 13, Batch 300, Loss: 2.41946161031723\n",
      "Epoch 13, Batch 400, Loss: 2.4095707154273986\n",
      "Epoch 13, Batch 500, Loss: 2.3951605272293093\n",
      "Epoch 13, Batch 600, Loss: 2.397565035820007\n",
      "Epoch 13, Batch 700, Loss: 2.419841241836548\n",
      "Epoch 13, Batch 800, Loss: 2.390299563407898\n",
      "Epoch 13, Batch 900, Loss: 2.3922992515563966\n",
      "Epoch 14, Batch 100, Loss: 2.413089270591736\n",
      "Epoch 14, Batch 200, Loss: 2.3906542658805847\n",
      "Epoch 14, Batch 300, Loss: 2.405199990272522\n",
      "Epoch 14, Batch 400, Loss: 2.393069078922272\n",
      "Epoch 14, Batch 500, Loss: 2.4055488991737364\n",
      "Epoch 14, Batch 600, Loss: 2.3976309299468994\n",
      "Epoch 14, Batch 700, Loss: 2.395115876197815\n",
      "Epoch 14, Batch 800, Loss: 2.3948852610588074\n",
      "Epoch 14, Batch 900, Loss: 2.3907890033721926\n",
      "Epoch 15, Batch 100, Loss: 2.4011334919929506\n",
      "Epoch 15, Batch 200, Loss: 2.39530118227005\n",
      "Epoch 15, Batch 300, Loss: 2.4058710956573486\n",
      "Epoch 15, Batch 400, Loss: 2.4077532863616944\n",
      "Epoch 15, Batch 500, Loss: 2.4054660272598265\n",
      "Epoch 15, Batch 600, Loss: 2.393211350440979\n",
      "Epoch 15, Batch 700, Loss: 2.401767408847809\n",
      "Epoch 15, Batch 800, Loss: 2.390236248970032\n",
      "Epoch 15, Batch 900, Loss: 2.3884285402297976\n",
      "Epoch 16, Batch 100, Loss: 2.3962071633338926\n",
      "Epoch 16, Batch 200, Loss: 2.4104740047454833\n",
      "Epoch 16, Batch 300, Loss: 2.4036745619773865\n",
      "Epoch 16, Batch 400, Loss: 2.4081048154830933\n",
      "Epoch 16, Batch 500, Loss: 2.3905018949508667\n",
      "Epoch 16, Batch 600, Loss: 2.38593585729599\n",
      "Epoch 16, Batch 700, Loss: 2.397361888885498\n",
      "Epoch 16, Batch 800, Loss: 2.3995972204208376\n",
      "Epoch 16, Batch 900, Loss: 2.403070182800293\n",
      "Epoch 17, Batch 100, Loss: 2.413916690349579\n",
      "Epoch 17, Batch 200, Loss: 2.402013020515442\n",
      "Epoch 17, Batch 300, Loss: 2.4016774106025696\n",
      "Epoch 17, Batch 400, Loss: 2.418475105762482\n",
      "Epoch 17, Batch 500, Loss: 2.394277400970459\n",
      "Epoch 17, Batch 600, Loss: 2.3934381556510926\n",
      "Epoch 17, Batch 700, Loss: 2.397274491786957\n",
      "Epoch 17, Batch 800, Loss: 2.4277718234062196\n",
      "Epoch 17, Batch 900, Loss: 2.390448200702667\n",
      "Epoch 18, Batch 100, Loss: 2.398252191543579\n",
      "Epoch 18, Batch 200, Loss: 2.3971156787872316\n",
      "Epoch 18, Batch 300, Loss: 2.399518623352051\n",
      "Epoch 18, Batch 400, Loss: 2.4069217324256895\n",
      "Epoch 18, Batch 500, Loss: 2.4128467655181884\n",
      "Epoch 18, Batch 600, Loss: 2.4051484417915345\n",
      "Epoch 18, Batch 700, Loss: 2.404196763038635\n",
      "Epoch 18, Batch 800, Loss: 2.400860300064087\n",
      "Epoch 18, Batch 900, Loss: 2.3949532198905943\n",
      "Epoch 19, Batch 100, Loss: 2.3996744656562807\n",
      "Epoch 19, Batch 200, Loss: 2.4180118894577025\n",
      "Epoch 19, Batch 300, Loss: 2.3995781445503237\n",
      "Epoch 19, Batch 400, Loss: 2.3983203077316286\n",
      "Epoch 19, Batch 500, Loss: 2.402768211364746\n",
      "Epoch 19, Batch 600, Loss: 2.405077288150787\n",
      "Epoch 19, Batch 700, Loss: 2.386672382354736\n",
      "Epoch 19, Batch 800, Loss: 2.4066170954704287\n",
      "Epoch 19, Batch 900, Loss: 2.3970580291748047\n",
      "Epoch 20, Batch 100, Loss: 2.393195080757141\n",
      "Epoch 20, Batch 200, Loss: 2.402019789218903\n",
      "Epoch 20, Batch 300, Loss: 2.3980411314964294\n",
      "Epoch 20, Batch 400, Loss: 2.393665602207184\n",
      "Epoch 20, Batch 500, Loss: 2.4070283460617063\n",
      "Epoch 20, Batch 600, Loss: 2.406275358200073\n",
      "Epoch 20, Batch 700, Loss: 2.416419599056244\n",
      "Epoch 20, Batch 800, Loss: 2.421544277667999\n",
      "Epoch 20, Batch 900, Loss: 2.409939851760864\n",
      "Epoch 21, Batch 100, Loss: 2.3919346284866334\n",
      "Epoch 21, Batch 200, Loss: 2.3972367358207705\n",
      "Epoch 21, Batch 300, Loss: 2.4058902525901793\n",
      "Epoch 21, Batch 400, Loss: 2.40221088886261\n",
      "Epoch 21, Batch 500, Loss: 2.4032668757438658\n",
      "Epoch 21, Batch 600, Loss: 2.399328761100769\n",
      "Epoch 21, Batch 700, Loss: 2.4028422737121584\n",
      "Epoch 21, Batch 800, Loss: 2.4047702169418335\n",
      "Epoch 21, Batch 900, Loss: 2.3996466279029844\n",
      "Epoch 22, Batch 100, Loss: 2.402434833049774\n",
      "Epoch 22, Batch 200, Loss: 2.3921517276763917\n",
      "Epoch 22, Batch 300, Loss: 2.4081191420555115\n",
      "Epoch 22, Batch 400, Loss: 2.402792558670044\n",
      "Epoch 22, Batch 500, Loss: 2.391736772060394\n",
      "Epoch 22, Batch 600, Loss: 2.407316083908081\n",
      "Epoch 22, Batch 700, Loss: 2.410278217792511\n",
      "Epoch 22, Batch 800, Loss: 2.3977104163169862\n",
      "Epoch 22, Batch 900, Loss: 2.4086704325675963\n",
      "Epoch 23, Batch 100, Loss: 2.3947072863578795\n",
      "Epoch 23, Batch 200, Loss: 2.4062181091308594\n",
      "Epoch 23, Batch 300, Loss: 2.392365062236786\n",
      "Epoch 23, Batch 400, Loss: 2.4216638946533204\n",
      "Epoch 23, Batch 500, Loss: 2.4069065928459166\n",
      "Epoch 23, Batch 600, Loss: 2.409946722984314\n",
      "Epoch 23, Batch 700, Loss: 2.3969580578804015\n",
      "Epoch 23, Batch 800, Loss: 2.402876615524292\n",
      "Epoch 23, Batch 900, Loss: 2.4017490983009337\n",
      "Epoch 24, Batch 100, Loss: 2.392503514289856\n",
      "Epoch 24, Batch 200, Loss: 2.399760627746582\n",
      "Epoch 24, Batch 300, Loss: 2.4129520726203917\n",
      "Epoch 24, Batch 400, Loss: 2.4017881894111635\n",
      "Epoch 24, Batch 500, Loss: 2.3978956580162047\n",
      "Epoch 24, Batch 600, Loss: 2.4062447118759156\n",
      "Epoch 24, Batch 700, Loss: 2.4074780774116515\n",
      "Epoch 24, Batch 800, Loss: 2.391991810798645\n",
      "Epoch 24, Batch 900, Loss: 2.389662706851959\n",
      "Epoch 25, Batch 100, Loss: 2.415352833271027\n",
      "Epoch 25, Batch 200, Loss: 2.4018414044380187\n",
      "Epoch 25, Batch 300, Loss: 2.392873902320862\n",
      "Epoch 25, Batch 400, Loss: 2.4024589824676514\n",
      "Epoch 25, Batch 500, Loss: 2.403785128593445\n",
      "Epoch 25, Batch 600, Loss: 2.4196780514717102\n",
      "Epoch 25, Batch 700, Loss: 2.392687120437622\n",
      "Epoch 25, Batch 800, Loss: 2.396357283592224\n",
      "Epoch 25, Batch 900, Loss: 2.4138881874084475\n",
      "Accuracy on test set: 0.1%\n",
      "Fitting for combination 59\n",
      "784\n",
      "3\n",
      "10\n",
      "[40, 10, 50, 10]\n",
      "True\n",
      "['relu', 'sigmoid', 'relu']\n",
      "SGD\n",
      "0.01\n",
      "0.01\n",
      "CrossEntropyLoss\n",
      "50\n",
      "25\n",
      "0\n",
      "Epoch 1, Batch 50, Loss: 1.1555996346473694\n",
      "Epoch 1, Batch 100, Loss: 1.1508820962905884\n",
      "Epoch 1, Batch 150, Loss: 1.1459453153610228\n",
      "Epoch 1, Batch 200, Loss: 1.1435247993469237\n",
      "Epoch 1, Batch 250, Loss: 1.1391895627975464\n",
      "Epoch 1, Batch 300, Loss: 1.1371048855781556\n",
      "Epoch 1, Batch 350, Loss: 1.1326262068748474\n",
      "Epoch 1, Batch 400, Loss: 1.128066020011902\n",
      "Epoch 1, Batch 450, Loss: 1.1226663327217101\n",
      "Epoch 1, Batch 500, Loss: 1.1156940388679504\n",
      "Epoch 1, Batch 550, Loss: 1.1070401096343994\n",
      "Epoch 1, Batch 600, Loss: 1.0971229195594787\n",
      "Epoch 1, Batch 650, Loss: 1.0845047950744628\n",
      "Epoch 1, Batch 700, Loss: 1.0695804786682128\n",
      "Epoch 1, Batch 750, Loss: 1.0558475494384765\n",
      "Epoch 1, Batch 800, Loss: 1.0361455440521241\n",
      "Epoch 1, Batch 850, Loss: 1.0181280851364136\n",
      "Epoch 1, Batch 900, Loss: 0.9935759484767914\n",
      "Epoch 2, Batch 50, Loss: 0.9533281183242798\n",
      "Epoch 2, Batch 100, Loss: 0.9368386018276215\n",
      "Epoch 2, Batch 150, Loss: 0.908759617805481\n",
      "Epoch 2, Batch 200, Loss: 0.891931961774826\n",
      "Epoch 2, Batch 250, Loss: 0.869854930639267\n",
      "Epoch 2, Batch 300, Loss: 0.849219605922699\n",
      "Epoch 2, Batch 350, Loss: 0.8284481596946717\n",
      "Epoch 2, Batch 400, Loss: 0.813708735704422\n",
      "Epoch 2, Batch 450, Loss: 0.7925477159023285\n",
      "Epoch 2, Batch 500, Loss: 0.7847296166419983\n",
      "Epoch 2, Batch 550, Loss: 0.7614763724803925\n",
      "Epoch 2, Batch 600, Loss: 0.7541927862167358\n",
      "Epoch 2, Batch 650, Loss: 0.7404049801826477\n",
      "Epoch 2, Batch 700, Loss: 0.7254414272308349\n",
      "Epoch 2, Batch 750, Loss: 0.7140039443969727\n",
      "Epoch 2, Batch 800, Loss: 0.7041346371173859\n",
      "Epoch 2, Batch 850, Loss: 0.69429536819458\n",
      "Epoch 2, Batch 900, Loss: 0.683582353591919\n",
      "Epoch 3, Batch 50, Loss: 0.6696986651420593\n",
      "Epoch 3, Batch 100, Loss: 0.6638801419734954\n",
      "Epoch 3, Batch 150, Loss: 0.6597854471206666\n",
      "Epoch 3, Batch 200, Loss: 0.6459383237361908\n",
      "Epoch 3, Batch 250, Loss: 0.6354930186271668\n",
      "Epoch 3, Batch 300, Loss: 0.6318136215209961\n",
      "Epoch 3, Batch 350, Loss: 0.6279848182201385\n",
      "Epoch 3, Batch 400, Loss: 0.6244403910636902\n",
      "Epoch 3, Batch 450, Loss: 0.6170302319526673\n",
      "Epoch 3, Batch 500, Loss: 0.6056726574897766\n",
      "Epoch 3, Batch 550, Loss: 0.6076696300506592\n",
      "Epoch 3, Batch 600, Loss: 0.5929482126235962\n",
      "Epoch 3, Batch 650, Loss: 0.597802404165268\n",
      "Epoch 3, Batch 700, Loss: 0.58954216837883\n",
      "Epoch 3, Batch 750, Loss: 0.5865139698982239\n",
      "Epoch 3, Batch 800, Loss: 0.5773097217082978\n",
      "Epoch 3, Batch 850, Loss: 0.5759069943428039\n",
      "Epoch 3, Batch 900, Loss: 0.5672017312049866\n",
      "Epoch 4, Batch 50, Loss: 0.5620121377706527\n",
      "Epoch 4, Batch 100, Loss: 0.5706988823413849\n",
      "Epoch 4, Batch 150, Loss: 0.5588492423295974\n",
      "Epoch 4, Batch 200, Loss: 0.5416295742988586\n",
      "Epoch 4, Batch 250, Loss: 0.541452671289444\n",
      "Epoch 4, Batch 300, Loss: 0.5424950790405273\n",
      "Epoch 4, Batch 350, Loss: 0.5475561970472336\n",
      "Epoch 4, Batch 400, Loss: 0.5388420021533966\n",
      "Epoch 4, Batch 450, Loss: 0.5506181585788726\n",
      "Epoch 4, Batch 500, Loss: 0.5309244120121002\n",
      "Epoch 4, Batch 550, Loss: 0.532931860089302\n",
      "Epoch 4, Batch 600, Loss: 0.5244499778747559\n",
      "Epoch 4, Batch 650, Loss: 0.52558074593544\n",
      "Epoch 4, Batch 700, Loss: 0.5149708014726638\n",
      "Epoch 4, Batch 750, Loss: 0.5150436246395111\n",
      "Epoch 4, Batch 800, Loss: 0.5147924500703812\n",
      "Epoch 4, Batch 850, Loss: 0.5205809772014618\n",
      "Epoch 4, Batch 900, Loss: 0.5065281891822815\n",
      "Epoch 5, Batch 50, Loss: 0.5095396822690964\n",
      "Epoch 5, Batch 100, Loss: 0.4977377587556839\n",
      "Epoch 5, Batch 150, Loss: 0.49742156863212583\n",
      "Epoch 5, Batch 200, Loss: 0.5032682687044143\n",
      "Epoch 5, Batch 250, Loss: 0.48996431946754454\n",
      "Epoch 5, Batch 300, Loss: 0.4831984674930572\n",
      "Epoch 5, Batch 350, Loss: 0.48025794088840484\n",
      "Epoch 5, Batch 400, Loss: 0.4838794696331024\n",
      "Epoch 5, Batch 450, Loss: 0.4976415324211121\n",
      "Epoch 5, Batch 500, Loss: 0.48339500367641447\n",
      "Epoch 5, Batch 550, Loss: 0.47458784699440004\n",
      "Epoch 5, Batch 600, Loss: 0.47986279547214505\n",
      "Epoch 5, Batch 650, Loss: 0.47470894634723665\n",
      "Epoch 5, Batch 700, Loss: 0.4757509112358093\n",
      "Epoch 5, Batch 750, Loss: 0.4652558308839798\n",
      "Epoch 5, Batch 800, Loss: 0.47107636332511904\n",
      "Epoch 5, Batch 850, Loss: 0.4597049742937088\n",
      "Epoch 5, Batch 900, Loss: 0.4728695142269135\n",
      "Epoch 6, Batch 50, Loss: 0.4541909176111221\n",
      "Epoch 6, Batch 100, Loss: 0.45348203778266905\n",
      "Epoch 6, Batch 150, Loss: 0.4470139229297638\n",
      "Epoch 6, Batch 200, Loss: 0.4464362704753876\n",
      "Epoch 6, Batch 250, Loss: 0.45185363948345186\n",
      "Epoch 6, Batch 300, Loss: 0.4512697261571884\n",
      "Epoch 6, Batch 350, Loss: 0.44961769104003907\n",
      "Epoch 6, Batch 400, Loss: 0.4381129741668701\n",
      "Epoch 6, Batch 450, Loss: 0.45436640560626984\n",
      "Epoch 6, Batch 500, Loss: 0.449318071603775\n",
      "Epoch 6, Batch 550, Loss: 0.43509609341621397\n",
      "Epoch 6, Batch 600, Loss: 0.44352902948856354\n",
      "Epoch 6, Batch 650, Loss: 0.4298135274648666\n",
      "Epoch 6, Batch 700, Loss: 0.43639071702957155\n",
      "Epoch 6, Batch 750, Loss: 0.4316754615306854\n",
      "Epoch 6, Batch 800, Loss: 0.4311503088474274\n",
      "Epoch 6, Batch 850, Loss: 0.42758518517017363\n",
      "Epoch 6, Batch 900, Loss: 0.4297207921743393\n",
      "Epoch 7, Batch 50, Loss: 0.43451681315898893\n",
      "Epoch 7, Batch 100, Loss: 0.42405938446521757\n",
      "Epoch 7, Batch 150, Loss: 0.4284180772304535\n",
      "Epoch 7, Batch 200, Loss: 0.4147734421491623\n",
      "Epoch 7, Batch 250, Loss: 0.41145381033420564\n",
      "Epoch 7, Batch 300, Loss: 0.4114623910188675\n",
      "Epoch 7, Batch 350, Loss: 0.4291314619779587\n",
      "Epoch 7, Batch 400, Loss: 0.4088375985622406\n",
      "Epoch 7, Batch 450, Loss: 0.40912037253379824\n",
      "Epoch 7, Batch 500, Loss: 0.4213033866882324\n",
      "Epoch 7, Batch 550, Loss: 0.40149816155433654\n",
      "Epoch 7, Batch 600, Loss: 0.40415149688720703\n",
      "Epoch 7, Batch 650, Loss: 0.41080732822418214\n",
      "Epoch 7, Batch 700, Loss: 0.4076309686899185\n",
      "Epoch 7, Batch 750, Loss: 0.3997680252790451\n",
      "Epoch 7, Batch 800, Loss: 0.4058030378818512\n",
      "Epoch 7, Batch 850, Loss: 0.4103399354219437\n",
      "Epoch 7, Batch 900, Loss: 0.4145448064804077\n",
      "Epoch 8, Batch 50, Loss: 0.395833734869957\n",
      "Epoch 8, Batch 100, Loss: 0.3988374352455139\n",
      "Epoch 8, Batch 150, Loss: 0.41052808821201325\n",
      "Epoch 8, Batch 200, Loss: 0.39846399426460266\n",
      "Epoch 8, Batch 250, Loss: 0.40085922956466674\n",
      "Epoch 8, Batch 300, Loss: 0.40483713388442993\n",
      "Epoch 8, Batch 350, Loss: 0.4003610008955002\n",
      "Epoch 8, Batch 400, Loss: 0.386812841296196\n",
      "Epoch 8, Batch 450, Loss: 0.38428158819675445\n",
      "Epoch 8, Batch 500, Loss: 0.4003563505411148\n",
      "Epoch 8, Batch 550, Loss: 0.3781533873081207\n",
      "Epoch 8, Batch 600, Loss: 0.3832292354106903\n",
      "Epoch 8, Batch 650, Loss: 0.39765883386135104\n",
      "Epoch 8, Batch 700, Loss: 0.3810287630558014\n",
      "Epoch 8, Batch 750, Loss: 0.39300366818904875\n",
      "Epoch 8, Batch 800, Loss: 0.38044669687747956\n",
      "Epoch 8, Batch 850, Loss: 0.39032814383506775\n",
      "Epoch 8, Batch 900, Loss: 0.3873581314086914\n",
      "Epoch 9, Batch 50, Loss: 0.3958569151163101\n",
      "Epoch 9, Batch 100, Loss: 0.3985477888584137\n",
      "Epoch 9, Batch 150, Loss: 0.3827847450971603\n",
      "Epoch 9, Batch 200, Loss: 0.38086195826530456\n",
      "Epoch 9, Batch 250, Loss: 0.36766966819763186\n",
      "Epoch 9, Batch 300, Loss: 0.39159020185470583\n",
      "Epoch 9, Batch 350, Loss: 0.3748793351650238\n",
      "Epoch 9, Batch 400, Loss: 0.38180635690689085\n",
      "Epoch 9, Batch 450, Loss: 0.368207688331604\n",
      "Epoch 9, Batch 500, Loss: 0.3762597447633743\n",
      "Epoch 9, Batch 550, Loss: 0.3644293600320816\n",
      "Epoch 9, Batch 600, Loss: 0.3781790798902512\n",
      "Epoch 9, Batch 650, Loss: 0.36643696188926694\n",
      "Epoch 9, Batch 700, Loss: 0.37392580330371855\n",
      "Epoch 9, Batch 750, Loss: 0.3762303459644318\n",
      "Epoch 9, Batch 800, Loss: 0.38478785037994384\n",
      "Epoch 9, Batch 850, Loss: 0.36317446947097776\n",
      "Epoch 9, Batch 900, Loss: 0.3702158671617508\n",
      "Epoch 10, Batch 50, Loss: 0.3665940374135971\n",
      "Epoch 10, Batch 100, Loss: 0.36649850189685823\n",
      "Epoch 10, Batch 150, Loss: 0.3767834609746933\n",
      "Epoch 10, Batch 200, Loss: 0.3696268284320831\n",
      "Epoch 10, Batch 250, Loss: 0.3701958250999451\n",
      "Epoch 10, Batch 300, Loss: 0.35212456703186035\n",
      "Epoch 10, Batch 350, Loss: 0.3657374078035355\n",
      "Epoch 10, Batch 400, Loss: 0.37132982730865477\n",
      "Epoch 10, Batch 450, Loss: 0.3625812929868698\n",
      "Epoch 10, Batch 500, Loss: 0.3586863076686859\n",
      "Epoch 10, Batch 550, Loss: 0.3692960506677628\n",
      "Epoch 10, Batch 600, Loss: 0.37278842270374296\n",
      "Epoch 10, Batch 650, Loss: 0.3599856221675873\n",
      "Epoch 10, Batch 700, Loss: 0.3653892213106155\n",
      "Epoch 10, Batch 750, Loss: 0.36283463180065156\n",
      "Epoch 10, Batch 800, Loss: 0.35840111553668974\n",
      "Epoch 10, Batch 850, Loss: 0.3566989350318909\n",
      "Epoch 10, Batch 900, Loss: 0.36158413589000704\n",
      "Epoch 11, Batch 50, Loss: 0.35644466400146485\n",
      "Epoch 11, Batch 100, Loss: 0.36225854635238647\n",
      "Epoch 11, Batch 150, Loss: 0.3515217578411102\n",
      "Epoch 11, Batch 200, Loss: 0.3563962012529373\n",
      "Epoch 11, Batch 250, Loss: 0.3508554220199585\n",
      "Epoch 11, Batch 300, Loss: 0.3553705829381943\n",
      "Epoch 11, Batch 350, Loss: 0.35423828423023224\n",
      "Epoch 11, Batch 400, Loss: 0.35925660640001295\n",
      "Epoch 11, Batch 450, Loss: 0.3543816637992859\n",
      "Epoch 11, Batch 500, Loss: 0.35212181210517884\n",
      "Epoch 11, Batch 550, Loss: 0.35477829158306123\n",
      "Epoch 11, Batch 600, Loss: 0.35090837478637693\n",
      "Epoch 11, Batch 650, Loss: 0.3565810883045197\n",
      "Epoch 11, Batch 700, Loss: 0.36019337236881255\n",
      "Epoch 11, Batch 750, Loss: 0.3632647031545639\n",
      "Epoch 11, Batch 800, Loss: 0.34055874705314637\n",
      "Epoch 11, Batch 850, Loss: 0.3470629185438156\n",
      "Epoch 11, Batch 900, Loss: 0.3394444763660431\n",
      "Epoch 12, Batch 50, Loss: 0.3553688097000122\n",
      "Epoch 12, Batch 100, Loss: 0.3399557328224182\n",
      "Epoch 12, Batch 150, Loss: 0.3352478086948395\n",
      "Epoch 12, Batch 200, Loss: 0.33256315410137177\n",
      "Epoch 12, Batch 250, Loss: 0.34398275017738345\n",
      "Epoch 12, Batch 300, Loss: 0.35702878832817075\n",
      "Epoch 12, Batch 350, Loss: 0.3558915776014328\n",
      "Epoch 12, Batch 400, Loss: 0.35197871148586274\n",
      "Epoch 12, Batch 450, Loss: 0.34584176003932954\n",
      "Epoch 12, Batch 500, Loss: 0.34580801248550413\n",
      "Epoch 12, Batch 550, Loss: 0.3458236277103424\n",
      "Epoch 12, Batch 600, Loss: 0.3509454110264778\n",
      "Epoch 12, Batch 650, Loss: 0.3407767462730408\n",
      "Epoch 12, Batch 700, Loss: 0.33543028235435485\n",
      "Epoch 12, Batch 750, Loss: 0.34254263043403627\n",
      "Epoch 12, Batch 800, Loss: 0.3350034636259079\n",
      "Epoch 12, Batch 850, Loss: 0.33353952288627625\n",
      "Epoch 12, Batch 900, Loss: 0.3384409737586975\n",
      "Epoch 13, Batch 50, Loss: 0.33902971059083936\n",
      "Epoch 13, Batch 100, Loss: 0.3356165090203285\n",
      "Epoch 13, Batch 150, Loss: 0.333068368434906\n",
      "Epoch 13, Batch 200, Loss: 0.327602356672287\n",
      "Epoch 13, Batch 250, Loss: 0.3329521095752716\n",
      "Epoch 13, Batch 300, Loss: 0.3328568553924561\n",
      "Epoch 13, Batch 350, Loss: 0.33293398082256315\n",
      "Epoch 13, Batch 400, Loss: 0.3425274729728699\n",
      "Epoch 13, Batch 450, Loss: 0.3358171963691711\n",
      "Epoch 13, Batch 500, Loss: 0.3325025391578674\n",
      "Epoch 13, Batch 550, Loss: 0.32812597453594206\n",
      "Epoch 13, Batch 600, Loss: 0.3287918049097061\n",
      "Epoch 13, Batch 650, Loss: 0.33458526283502577\n",
      "Epoch 13, Batch 700, Loss: 0.3295292258262634\n",
      "Epoch 13, Batch 750, Loss: 0.33073754131793975\n",
      "Epoch 13, Batch 800, Loss: 0.3394540050625801\n",
      "Epoch 13, Batch 850, Loss: 0.3445362249016762\n",
      "Epoch 13, Batch 900, Loss: 0.3249372577667236\n",
      "Epoch 14, Batch 50, Loss: 0.33016431361436843\n",
      "Epoch 14, Batch 100, Loss: 0.330674432516098\n",
      "Epoch 14, Batch 150, Loss: 0.336970174908638\n",
      "Epoch 14, Batch 200, Loss: 0.3398213416337967\n",
      "Epoch 14, Batch 250, Loss: 0.316281740963459\n",
      "Epoch 14, Batch 300, Loss: 0.3195229798555374\n",
      "Epoch 14, Batch 350, Loss: 0.3315923312306404\n",
      "Epoch 14, Batch 400, Loss: 0.3264495939016342\n",
      "Epoch 14, Batch 450, Loss: 0.32432606518268586\n",
      "Epoch 14, Batch 500, Loss: 0.3116885468363762\n",
      "Epoch 14, Batch 550, Loss: 0.3229103180766106\n",
      "Epoch 14, Batch 600, Loss: 0.3354211747646332\n",
      "Epoch 14, Batch 650, Loss: 0.3244092929363251\n",
      "Epoch 14, Batch 700, Loss: 0.3183179667592049\n",
      "Epoch 14, Batch 750, Loss: 0.3287131899595261\n",
      "Epoch 14, Batch 800, Loss: 0.31128407299518585\n",
      "Epoch 14, Batch 850, Loss: 0.33385824620723725\n",
      "Epoch 14, Batch 900, Loss: 0.32162663847208023\n",
      "Epoch 15, Batch 50, Loss: 0.3256756493449211\n",
      "Epoch 15, Batch 100, Loss: 0.31702192008495333\n",
      "Epoch 15, Batch 150, Loss: 0.3212844377756119\n",
      "Epoch 15, Batch 200, Loss: 0.32077574789524077\n",
      "Epoch 15, Batch 250, Loss: 0.32096603244543076\n",
      "Epoch 15, Batch 300, Loss: 0.3228416308760643\n",
      "Epoch 15, Batch 350, Loss: 0.33159972846508023\n",
      "Epoch 15, Batch 400, Loss: 0.326729898750782\n",
      "Epoch 15, Batch 450, Loss: 0.31487282931804655\n",
      "Epoch 15, Batch 500, Loss: 0.3131028002500534\n",
      "Epoch 15, Batch 550, Loss: 0.3155558729171753\n",
      "Epoch 15, Batch 600, Loss: 0.31879953742027284\n",
      "Epoch 15, Batch 650, Loss: 0.3158480581641197\n",
      "Epoch 15, Batch 700, Loss: 0.3013221806287765\n",
      "Epoch 15, Batch 750, Loss: 0.31692995309829713\n",
      "Epoch 15, Batch 800, Loss: 0.31458528727293017\n",
      "Epoch 15, Batch 850, Loss: 0.3149981307983398\n",
      "Epoch 15, Batch 900, Loss: 0.3140950506925583\n",
      "Epoch 16, Batch 50, Loss: 0.3180344867706299\n",
      "Epoch 16, Batch 100, Loss: 0.3162319815158844\n",
      "Epoch 16, Batch 150, Loss: 0.3200633230805397\n",
      "Epoch 16, Batch 200, Loss: 0.31297278791666033\n",
      "Epoch 16, Batch 250, Loss: 0.30645095229148867\n",
      "Epoch 16, Batch 300, Loss: 0.31046760886907576\n",
      "Epoch 16, Batch 350, Loss: 0.3151892104744911\n",
      "Epoch 16, Batch 400, Loss: 0.3145980489253998\n",
      "Epoch 16, Batch 450, Loss: 0.31773453056812284\n",
      "Epoch 16, Batch 500, Loss: 0.30486380726099016\n",
      "Epoch 16, Batch 550, Loss: 0.3118389466404915\n",
      "Epoch 16, Batch 600, Loss: 0.31112958133220675\n",
      "Epoch 16, Batch 650, Loss: 0.303322437107563\n",
      "Epoch 16, Batch 700, Loss: 0.3064101576805115\n",
      "Epoch 16, Batch 750, Loss: 0.3098591923713684\n",
      "Epoch 16, Batch 800, Loss: 0.30402436465024946\n",
      "Epoch 16, Batch 850, Loss: 0.3204588779807091\n",
      "Epoch 16, Batch 900, Loss: 0.2974506917595863\n",
      "Epoch 17, Batch 50, Loss: 0.32061540573835373\n",
      "Epoch 17, Batch 100, Loss: 0.3011083287000656\n",
      "Epoch 17, Batch 150, Loss: 0.3080657970905304\n",
      "Epoch 17, Batch 200, Loss: 0.30718217253685\n",
      "Epoch 17, Batch 250, Loss: 0.3071736085414887\n",
      "Epoch 17, Batch 300, Loss: 0.3076662614941597\n",
      "Epoch 17, Batch 350, Loss: 0.2998675790429115\n",
      "Epoch 17, Batch 400, Loss: 0.30914842516183855\n",
      "Epoch 17, Batch 450, Loss: 0.29362825244665147\n",
      "Epoch 17, Batch 500, Loss: 0.2993635758757591\n",
      "Epoch 17, Batch 550, Loss: 0.30116364270448687\n",
      "Epoch 17, Batch 600, Loss: 0.31098693907260894\n",
      "Epoch 17, Batch 650, Loss: 0.30246309041976926\n",
      "Epoch 17, Batch 700, Loss: 0.30257513344287873\n",
      "Epoch 17, Batch 750, Loss: 0.3059537515044212\n",
      "Epoch 17, Batch 800, Loss: 0.29458199739456176\n",
      "Epoch 17, Batch 850, Loss: 0.30303615629673003\n",
      "Epoch 17, Batch 900, Loss: 0.3021987020969391\n",
      "Epoch 18, Batch 50, Loss: 0.294203497171402\n",
      "Epoch 18, Batch 100, Loss: 0.29854927778244017\n",
      "Epoch 18, Batch 150, Loss: 0.2984924241900444\n",
      "Epoch 18, Batch 200, Loss: 0.2975871044397354\n",
      "Epoch 18, Batch 250, Loss: 0.29277182430028914\n",
      "Epoch 18, Batch 300, Loss: 0.2832884898781776\n",
      "Epoch 18, Batch 350, Loss: 0.3058210694789886\n",
      "Epoch 18, Batch 400, Loss: 0.3178567153215408\n",
      "Epoch 18, Batch 450, Loss: 0.2975850519537926\n",
      "Epoch 18, Batch 500, Loss: 0.3114395698904991\n",
      "Epoch 18, Batch 550, Loss: 0.28966142028570174\n",
      "Epoch 18, Batch 600, Loss: 0.29694978922605514\n",
      "Epoch 18, Batch 650, Loss: 0.29107610642910003\n",
      "Epoch 18, Batch 700, Loss: 0.3059631082415581\n",
      "Epoch 18, Batch 750, Loss: 0.2880781418085098\n",
      "Epoch 18, Batch 800, Loss: 0.3031179517507553\n",
      "Epoch 18, Batch 850, Loss: 0.30587214946746827\n",
      "Epoch 18, Batch 900, Loss: 0.29999535739421845\n",
      "Epoch 19, Batch 50, Loss: 0.2985404539108276\n",
      "Epoch 19, Batch 100, Loss: 0.29463319331407545\n",
      "Epoch 19, Batch 150, Loss: 0.29247551649808884\n",
      "Epoch 19, Batch 200, Loss: 0.2906409928202629\n",
      "Epoch 19, Batch 250, Loss: 0.30123247891664506\n",
      "Epoch 19, Batch 300, Loss: 0.29851824522018433\n",
      "Epoch 19, Batch 350, Loss: 0.3004016077518463\n",
      "Epoch 19, Batch 400, Loss: 0.29911560893058775\n",
      "Epoch 19, Batch 450, Loss: 0.292019707262516\n",
      "Epoch 19, Batch 500, Loss: 0.2957852199673653\n",
      "Epoch 19, Batch 550, Loss: 0.2950934225320816\n",
      "Epoch 19, Batch 600, Loss: 0.2856980636715889\n",
      "Epoch 19, Batch 650, Loss: 0.30153454035520555\n",
      "Epoch 19, Batch 700, Loss: 0.2815239578485489\n",
      "Epoch 19, Batch 750, Loss: 0.2909905031323433\n",
      "Epoch 19, Batch 800, Loss: 0.30195954769849775\n",
      "Epoch 19, Batch 850, Loss: 0.29328874707221986\n",
      "Epoch 19, Batch 900, Loss: 0.3025516384840012\n",
      "Epoch 20, Batch 50, Loss: 0.2917816963791847\n",
      "Epoch 20, Batch 100, Loss: 0.28907042562961577\n",
      "Epoch 20, Batch 150, Loss: 0.28975663632154464\n",
      "Epoch 20, Batch 200, Loss: 0.28896124333143236\n",
      "Epoch 20, Batch 250, Loss: 0.2877942559123039\n",
      "Epoch 20, Batch 300, Loss: 0.3002605891227722\n",
      "Epoch 20, Batch 350, Loss: 0.2899862977862358\n",
      "Epoch 20, Batch 400, Loss: 0.293686683177948\n",
      "Epoch 20, Batch 450, Loss: 0.2911539030075073\n",
      "Epoch 20, Batch 500, Loss: 0.2822564959526062\n",
      "Epoch 20, Batch 550, Loss: 0.28290763080120085\n",
      "Epoch 20, Batch 600, Loss: 0.2867341080307961\n",
      "Epoch 20, Batch 650, Loss: 0.28888474017381666\n",
      "Epoch 20, Batch 700, Loss: 0.3043585082888603\n",
      "Epoch 20, Batch 750, Loss: 0.28909162610769273\n",
      "Epoch 20, Batch 800, Loss: 0.29334163188934326\n",
      "Epoch 20, Batch 850, Loss: 0.2845111674070358\n",
      "Epoch 20, Batch 900, Loss: 0.29711435765028\n",
      "Epoch 21, Batch 50, Loss: 0.28896801501512526\n",
      "Epoch 21, Batch 100, Loss: 0.2817458218336105\n",
      "Epoch 21, Batch 150, Loss: 0.2741350868344307\n",
      "Epoch 21, Batch 200, Loss: 0.2775949651002884\n",
      "Epoch 21, Batch 250, Loss: 0.29290490329265595\n",
      "Epoch 21, Batch 300, Loss: 0.28686969965696335\n",
      "Epoch 21, Batch 350, Loss: 0.294050812125206\n",
      "Epoch 21, Batch 400, Loss: 0.2906343474984169\n",
      "Epoch 21, Batch 450, Loss: 0.29233420461416243\n",
      "Epoch 21, Batch 500, Loss: 0.29163770377635956\n",
      "Epoch 21, Batch 550, Loss: 0.28393790155649185\n",
      "Epoch 21, Batch 600, Loss: 0.2895906960964203\n",
      "Epoch 21, Batch 650, Loss: 0.2943336695432663\n",
      "Epoch 21, Batch 700, Loss: 0.28545511305332183\n",
      "Epoch 21, Batch 750, Loss: 0.28129135102033614\n",
      "Epoch 21, Batch 800, Loss: 0.28179533004760743\n",
      "Epoch 21, Batch 850, Loss: 0.28580849438905714\n",
      "Epoch 21, Batch 900, Loss: 0.28257630437612535\n",
      "Epoch 22, Batch 50, Loss: 0.2814639863371849\n",
      "Epoch 22, Batch 100, Loss: 0.2844219642877579\n",
      "Epoch 22, Batch 150, Loss: 0.28657642990350723\n",
      "Epoch 22, Batch 200, Loss: 0.2935123860836029\n",
      "Epoch 22, Batch 250, Loss: 0.28073568373918534\n",
      "Epoch 22, Batch 300, Loss: 0.2877257859706879\n",
      "Epoch 22, Batch 350, Loss: 0.28267097622156145\n",
      "Epoch 22, Batch 400, Loss: 0.2896866962313652\n",
      "Epoch 22, Batch 450, Loss: 0.28464711099863055\n",
      "Epoch 22, Batch 500, Loss: 0.28373168259859083\n",
      "Epoch 22, Batch 550, Loss: 0.27581307768821717\n",
      "Epoch 22, Batch 600, Loss: 0.2830571338534355\n",
      "Epoch 22, Batch 650, Loss: 0.28421453803777696\n",
      "Epoch 22, Batch 700, Loss: 0.283478257060051\n",
      "Epoch 22, Batch 750, Loss: 0.2872154861688614\n",
      "Epoch 22, Batch 800, Loss: 0.28148157864809037\n",
      "Epoch 22, Batch 850, Loss: 0.2805462783575058\n",
      "Epoch 22, Batch 900, Loss: 0.27631363868713377\n",
      "Epoch 23, Batch 50, Loss: 0.2891409116983414\n",
      "Epoch 23, Batch 100, Loss: 0.27037312895059584\n",
      "Epoch 23, Batch 150, Loss: 0.2869022858142853\n",
      "Epoch 23, Batch 200, Loss: 0.2912852045893669\n",
      "Epoch 23, Batch 250, Loss: 0.2831357443332672\n",
      "Epoch 23, Batch 300, Loss: 0.28349778294563294\n",
      "Epoch 23, Batch 350, Loss: 0.2771918800473213\n",
      "Epoch 23, Batch 400, Loss: 0.2846690404415131\n",
      "Epoch 23, Batch 450, Loss: 0.28109861493110655\n",
      "Epoch 23, Batch 500, Loss: 0.2793621602654457\n",
      "Epoch 23, Batch 550, Loss: 0.2732982686161995\n",
      "Epoch 23, Batch 600, Loss: 0.2780203151702881\n",
      "Epoch 23, Batch 650, Loss: 0.29128663152456286\n",
      "Epoch 23, Batch 700, Loss: 0.27490329593420026\n",
      "Epoch 23, Batch 750, Loss: 0.26552435278892517\n",
      "Epoch 23, Batch 800, Loss: 0.2736448061466217\n",
      "Epoch 23, Batch 850, Loss: 0.2876461777091026\n",
      "Epoch 23, Batch 900, Loss: 0.2750931257009506\n",
      "Epoch 24, Batch 50, Loss: 0.2806175509095192\n",
      "Epoch 24, Batch 100, Loss: 0.28485352754592896\n",
      "Epoch 24, Batch 150, Loss: 0.27299768596887586\n",
      "Epoch 24, Batch 200, Loss: 0.2784476998448372\n",
      "Epoch 24, Batch 250, Loss: 0.27725092947483065\n",
      "Epoch 24, Batch 300, Loss: 0.2841653561592102\n",
      "Epoch 24, Batch 350, Loss: 0.2786214706301689\n",
      "Epoch 24, Batch 400, Loss: 0.2754442659020424\n",
      "Epoch 24, Batch 450, Loss: 0.2705176270008087\n",
      "Epoch 24, Batch 500, Loss: 0.27395314335823057\n",
      "Epoch 24, Batch 550, Loss: 0.28135660946369173\n",
      "Epoch 24, Batch 600, Loss: 0.2667438867688179\n",
      "Epoch 24, Batch 650, Loss: 0.2732529491186142\n",
      "Epoch 24, Batch 700, Loss: 0.2708329376578331\n",
      "Epoch 24, Batch 750, Loss: 0.2820094546675682\n",
      "Epoch 24, Batch 800, Loss: 0.2777883750200272\n",
      "Epoch 24, Batch 850, Loss: 0.2757183998823166\n",
      "Epoch 24, Batch 900, Loss: 0.28000038921833037\n",
      "Epoch 25, Batch 50, Loss: 0.27334037601947786\n",
      "Epoch 25, Batch 100, Loss: 0.27720265328884125\n",
      "Epoch 25, Batch 150, Loss: 0.28175113290548326\n",
      "Epoch 25, Batch 200, Loss: 0.2621149015426636\n",
      "Epoch 25, Batch 250, Loss: 0.2751253163814545\n",
      "Epoch 25, Batch 300, Loss: 0.27092332661151886\n",
      "Epoch 25, Batch 350, Loss: 0.2747024494409561\n",
      "Epoch 25, Batch 400, Loss: 0.275458804666996\n",
      "Epoch 25, Batch 450, Loss: 0.2725782382488251\n",
      "Epoch 25, Batch 500, Loss: 0.2708108651638031\n",
      "Epoch 25, Batch 550, Loss: 0.26582208305597305\n",
      "Epoch 25, Batch 600, Loss: 0.26614622205495836\n",
      "Epoch 25, Batch 650, Loss: 0.28125850290060045\n",
      "Epoch 25, Batch 700, Loss: 0.2794500049948692\n",
      "Epoch 25, Batch 750, Loss: 0.27083670169115065\n",
      "Epoch 25, Batch 800, Loss: 0.2800090205669403\n",
      "Epoch 25, Batch 850, Loss: 0.28140264183282854\n",
      "Epoch 25, Batch 900, Loss: 0.2776992237567902\n",
      "Accuracy on test set: 0.8164%\n",
      "Fitting for combination 60\n",
      "784\n",
      "4\n",
      "10\n",
      "[40, 10, 10, 10, 10]\n",
      "False\n",
      "['relu', 'sigmoid', 'relu', 'relu']\n",
      "Adam\n",
      "0.3\n",
      "0.03\n",
      "CrossEntropyLoss\n",
      "50\n",
      "25\n",
      "0\n",
      "Epoch 1, Batch 50, Loss: 1.1623137664794922\n",
      "Epoch 1, Batch 100, Loss: 1.1615645956993104\n",
      "Epoch 1, Batch 150, Loss: 1.1599171614646913\n",
      "Epoch 1, Batch 200, Loss: 1.1606610250473022\n",
      "Epoch 1, Batch 250, Loss: 1.1589066553115845\n",
      "Epoch 1, Batch 300, Loss: 1.1691158843040466\n",
      "Epoch 1, Batch 350, Loss: 1.177496829032898\n",
      "Epoch 1, Batch 400, Loss: 1.162686676979065\n",
      "Epoch 1, Batch 450, Loss: 1.1576800775527953\n",
      "Epoch 1, Batch 500, Loss: 1.1586295461654663\n",
      "Epoch 1, Batch 550, Loss: 1.1586176323890687\n",
      "Epoch 1, Batch 600, Loss: 1.1584218096733094\n",
      "Epoch 1, Batch 650, Loss: 1.1592096185684204\n",
      "Epoch 1, Batch 700, Loss: 1.159826202392578\n",
      "Epoch 1, Batch 750, Loss: 1.1597922515869141\n",
      "Epoch 1, Batch 800, Loss: 1.1593367433547974\n",
      "Epoch 1, Batch 850, Loss: 1.1587252283096314\n",
      "Epoch 1, Batch 900, Loss: 1.1564336657524108\n",
      "Epoch 2, Batch 50, Loss: 1.1610926604270935\n",
      "Epoch 2, Batch 100, Loss: 1.1582913947105409\n",
      "Epoch 2, Batch 150, Loss: 1.1621526265144348\n",
      "Epoch 2, Batch 200, Loss: 1.1615824317932129\n",
      "Epoch 2, Batch 250, Loss: 1.1581809878349305\n",
      "Epoch 2, Batch 300, Loss: 1.1629711723327636\n",
      "Epoch 2, Batch 350, Loss: 1.157501413822174\n",
      "Epoch 2, Batch 400, Loss: 1.1576590418815613\n",
      "Epoch 2, Batch 450, Loss: 1.158320825099945\n",
      "Epoch 2, Batch 500, Loss: 1.1596568155288696\n",
      "Epoch 2, Batch 550, Loss: 1.1570774865150453\n",
      "Epoch 2, Batch 600, Loss: 1.1574536561965942\n",
      "Epoch 2, Batch 650, Loss: 1.161056489944458\n",
      "Epoch 2, Batch 700, Loss: 1.158188407421112\n",
      "Epoch 2, Batch 750, Loss: 1.157896807193756\n",
      "Epoch 2, Batch 800, Loss: 1.1584574460983277\n",
      "Epoch 2, Batch 850, Loss: 1.1592049050331115\n",
      "Epoch 2, Batch 900, Loss: 1.162666335105896\n",
      "Epoch 3, Batch 50, Loss: 1.1736998200416564\n",
      "Epoch 3, Batch 100, Loss: 1.1586780261993408\n",
      "Epoch 3, Batch 150, Loss: 1.1585606527328491\n",
      "Epoch 3, Batch 200, Loss: 1.1611705613136292\n",
      "Epoch 3, Batch 250, Loss: 1.1604714369773865\n",
      "Epoch 3, Batch 300, Loss: 1.1604635429382324\n",
      "Epoch 3, Batch 350, Loss: 1.1637364101409913\n",
      "Epoch 3, Batch 400, Loss: 1.159384925365448\n",
      "Epoch 3, Batch 450, Loss: 1.1606426596641541\n",
      "Epoch 3, Batch 500, Loss: 1.156615025997162\n",
      "Epoch 3, Batch 550, Loss: 1.1604600596427916\n",
      "Epoch 3, Batch 600, Loss: 1.1594298529624938\n",
      "Epoch 3, Batch 650, Loss: 1.1582652235031128\n",
      "Epoch 3, Batch 700, Loss: 1.1566763377189637\n",
      "Epoch 3, Batch 750, Loss: 1.1585380458831787\n",
      "Epoch 3, Batch 800, Loss: 1.156860373020172\n",
      "Epoch 3, Batch 850, Loss: 1.1615635919570924\n",
      "Epoch 3, Batch 900, Loss: 1.1599774479866027\n",
      "Epoch 4, Batch 50, Loss: 1.1600854182243348\n",
      "Epoch 4, Batch 100, Loss: 1.1567659187316894\n",
      "Epoch 4, Batch 150, Loss: 1.1600510811805724\n",
      "Epoch 4, Batch 200, Loss: 1.1580386424064637\n",
      "Epoch 4, Batch 250, Loss: 1.15878511428833\n",
      "Epoch 4, Batch 300, Loss: 1.1578498196601867\n",
      "Epoch 4, Batch 350, Loss: 1.1599795126914978\n",
      "Epoch 4, Batch 400, Loss: 1.158500955104828\n",
      "Epoch 4, Batch 450, Loss: 1.1571410131454467\n",
      "Epoch 4, Batch 500, Loss: 1.1600625491142273\n",
      "Epoch 4, Batch 550, Loss: 1.1598955488204956\n",
      "Epoch 4, Batch 600, Loss: 1.158125524520874\n",
      "Epoch 4, Batch 650, Loss: 1.1588547873497008\n",
      "Epoch 4, Batch 700, Loss: 1.159935348033905\n",
      "Epoch 4, Batch 750, Loss: 1.1595212769508363\n",
      "Epoch 4, Batch 800, Loss: 1.1646165108680726\n",
      "Epoch 4, Batch 850, Loss: 1.277512400150299\n",
      "Epoch 4, Batch 900, Loss: 1.175864701271057\n",
      "Epoch 5, Batch 50, Loss: 1.1604637694358826\n",
      "Epoch 5, Batch 100, Loss: 1.1597689938545228\n",
      "Epoch 5, Batch 150, Loss: 1.1590979170799256\n",
      "Epoch 5, Batch 200, Loss: 1.1594744849205016\n",
      "Epoch 5, Batch 250, Loss: 1.1592306804656982\n",
      "Epoch 5, Batch 300, Loss: 1.1582388806343078\n",
      "Epoch 5, Batch 350, Loss: 1.1606766843795777\n",
      "Epoch 5, Batch 400, Loss: 1.164531967639923\n",
      "Epoch 5, Batch 450, Loss: 1.1591803884506227\n",
      "Epoch 5, Batch 500, Loss: 1.1595392870903014\n",
      "Epoch 5, Batch 550, Loss: 1.1589282441139221\n",
      "Epoch 5, Batch 600, Loss: 1.158797116279602\n",
      "Epoch 5, Batch 650, Loss: 1.1590929484367372\n",
      "Epoch 5, Batch 700, Loss: 1.1591673350334168\n",
      "Epoch 5, Batch 750, Loss: 1.158494734764099\n",
      "Epoch 5, Batch 800, Loss: 1.1585745310783386\n",
      "Epoch 5, Batch 850, Loss: 1.1576485586166383\n",
      "Epoch 5, Batch 900, Loss: 1.158123471736908\n",
      "Epoch 6, Batch 50, Loss: 1.1577612137794495\n",
      "Epoch 6, Batch 100, Loss: 1.1572951078414917\n",
      "Epoch 6, Batch 150, Loss: 1.1579280304908752\n",
      "Epoch 6, Batch 200, Loss: 1.1589812874794005\n",
      "Epoch 6, Batch 250, Loss: 1.1599287033081054\n",
      "Epoch 6, Batch 300, Loss: 1.1609093856811523\n",
      "Epoch 6, Batch 350, Loss: 1.1587216186523437\n",
      "Epoch 6, Batch 400, Loss: 1.1607695746421813\n",
      "Epoch 6, Batch 450, Loss: 1.1575108456611634\n",
      "Epoch 6, Batch 500, Loss: 1.1597813034057618\n",
      "Epoch 6, Batch 550, Loss: 1.160138680934906\n",
      "Epoch 6, Batch 600, Loss: 1.1602614641189575\n",
      "Epoch 6, Batch 650, Loss: 1.1604596495628356\n",
      "Epoch 6, Batch 700, Loss: 1.1579340267181397\n",
      "Epoch 6, Batch 750, Loss: 1.1574799585342408\n",
      "Epoch 6, Batch 800, Loss: 1.1567465329170228\n",
      "Epoch 6, Batch 850, Loss: 1.1606019163131713\n",
      "Epoch 6, Batch 900, Loss: 1.1585100078582764\n",
      "Epoch 7, Batch 50, Loss: 1.1602160120010376\n",
      "Epoch 7, Batch 100, Loss: 1.1595562505722046\n",
      "Epoch 7, Batch 150, Loss: 1.1585447692871094\n",
      "Epoch 7, Batch 200, Loss: 1.1594172930717468\n",
      "Epoch 7, Batch 250, Loss: 1.1572851181030273\n",
      "Epoch 7, Batch 300, Loss: 1.1608433771133422\n",
      "Epoch 7, Batch 350, Loss: 1.1594129872322083\n",
      "Epoch 7, Batch 400, Loss: 1.1572648501396179\n",
      "Epoch 7, Batch 450, Loss: 1.1623017287254334\n",
      "Epoch 7, Batch 500, Loss: 1.1611840844154357\n",
      "Epoch 7, Batch 550, Loss: 1.261606981754303\n",
      "Epoch 7, Batch 600, Loss: 1.225972328186035\n",
      "Epoch 7, Batch 650, Loss: 1.170405683517456\n",
      "Epoch 7, Batch 700, Loss: 1.1631391382217406\n",
      "Epoch 7, Batch 750, Loss: 1.1546892571449279\n",
      "Epoch 7, Batch 800, Loss: 1.1597593569755553\n",
      "Epoch 7, Batch 850, Loss: 1.1640975189208984\n",
      "Epoch 7, Batch 900, Loss: 1.1600193929672242\n",
      "Epoch 8, Batch 50, Loss: 1.1592068481445312\n",
      "Epoch 8, Batch 100, Loss: 1.1586838555335999\n",
      "Epoch 8, Batch 150, Loss: 1.1579112386703492\n",
      "Epoch 8, Batch 200, Loss: 1.161150209903717\n",
      "Epoch 8, Batch 250, Loss: 1.162784094810486\n",
      "Epoch 8, Batch 300, Loss: 1.158830771446228\n",
      "Epoch 8, Batch 350, Loss: 1.1606824040412902\n",
      "Epoch 8, Batch 400, Loss: 1.1597815084457397\n",
      "Epoch 8, Batch 450, Loss: 1.1616762042045594\n",
      "Epoch 8, Batch 500, Loss: 1.161870126724243\n",
      "Epoch 8, Batch 550, Loss: 1.1585312938690187\n",
      "Epoch 8, Batch 600, Loss: 1.1579412722587585\n",
      "Epoch 8, Batch 650, Loss: 1.1589629030227662\n",
      "Epoch 8, Batch 700, Loss: 1.1590728950500488\n",
      "Epoch 8, Batch 750, Loss: 1.1636096501350404\n",
      "Epoch 8, Batch 800, Loss: 1.1654748630523681\n",
      "Epoch 8, Batch 850, Loss: 1.1627714729309082\n",
      "Epoch 8, Batch 900, Loss: 1.1581481623649597\n",
      "Epoch 9, Batch 50, Loss: 1.1631490278244019\n",
      "Epoch 9, Batch 100, Loss: 1.1622190928459168\n",
      "Epoch 9, Batch 150, Loss: 1.157661635875702\n",
      "Epoch 9, Batch 200, Loss: 1.1573168277740478\n",
      "Epoch 9, Batch 250, Loss: 1.1585744976997376\n",
      "Epoch 9, Batch 300, Loss: 1.1622286033630371\n",
      "Epoch 9, Batch 350, Loss: 1.1578691601753235\n",
      "Epoch 9, Batch 400, Loss: 1.1565032410621643\n",
      "Epoch 9, Batch 450, Loss: 1.1585696840286255\n",
      "Epoch 9, Batch 500, Loss: 1.160193703174591\n",
      "Epoch 9, Batch 550, Loss: 1.1596267890930176\n",
      "Epoch 9, Batch 600, Loss: 1.159731993675232\n",
      "Epoch 9, Batch 650, Loss: 1.1583676767349242\n",
      "Epoch 9, Batch 700, Loss: 1.161456835269928\n",
      "Epoch 9, Batch 750, Loss: 1.1595356845855713\n",
      "Epoch 9, Batch 800, Loss: 1.1606462907791137\n",
      "Epoch 9, Batch 850, Loss: 1.1597154688835145\n",
      "Epoch 9, Batch 900, Loss: 1.1606553602218628\n",
      "Epoch 10, Batch 50, Loss: 1.158972339630127\n",
      "Epoch 10, Batch 100, Loss: 1.1585787725448609\n",
      "Epoch 10, Batch 150, Loss: 1.1565106701850891\n",
      "Epoch 10, Batch 200, Loss: 1.1623518562316895\n",
      "Epoch 10, Batch 250, Loss: 1.161297857761383\n",
      "Epoch 10, Batch 300, Loss: 1.160949010848999\n",
      "Epoch 10, Batch 350, Loss: 1.1591757321357727\n",
      "Epoch 10, Batch 400, Loss: 1.1609072494506836\n",
      "Epoch 10, Batch 450, Loss: 1.1636679005622863\n",
      "Epoch 10, Batch 500, Loss: 1.1607175326347352\n",
      "Epoch 10, Batch 550, Loss: 1.1577046704292298\n",
      "Epoch 10, Batch 600, Loss: 1.1615268540382386\n",
      "Epoch 10, Batch 650, Loss: 1.1599870777130128\n",
      "Epoch 10, Batch 700, Loss: 1.158641014099121\n",
      "Epoch 10, Batch 750, Loss: 1.1578872942924499\n",
      "Epoch 10, Batch 800, Loss: 1.1578890204429626\n",
      "Epoch 10, Batch 850, Loss: 1.1606996846199036\n",
      "Epoch 10, Batch 900, Loss: 1.1597571063041687\n",
      "Epoch 11, Batch 50, Loss: 1.159245581626892\n",
      "Epoch 11, Batch 100, Loss: 1.1587709188461304\n",
      "Epoch 11, Batch 150, Loss: 1.1585727834701538\n",
      "Epoch 11, Batch 200, Loss: 1.1586366534233092\n",
      "Epoch 11, Batch 250, Loss: 1.1593686389923095\n",
      "Epoch 11, Batch 300, Loss: 1.1594419264793396\n",
      "Epoch 11, Batch 350, Loss: 1.1574021768569946\n",
      "Epoch 11, Batch 400, Loss: 1.1588447213172912\n",
      "Epoch 11, Batch 450, Loss: 1.1565429854393006\n",
      "Epoch 11, Batch 500, Loss: 1.1607572865486144\n",
      "Epoch 11, Batch 550, Loss: 1.1578134083747864\n",
      "Epoch 11, Batch 600, Loss: 1.1616685700416565\n",
      "Epoch 11, Batch 650, Loss: 1.1588988733291625\n",
      "Epoch 11, Batch 700, Loss: 1.1592251944541931\n",
      "Epoch 11, Batch 750, Loss: 1.1581330037117004\n",
      "Epoch 11, Batch 800, Loss: 1.1581939148902893\n",
      "Epoch 11, Batch 850, Loss: 1.1584808945655822\n",
      "Epoch 11, Batch 900, Loss: 1.1578649997711181\n",
      "Epoch 12, Batch 50, Loss: 1.158575358390808\n",
      "Epoch 12, Batch 100, Loss: 1.1589739108085633\n",
      "Epoch 12, Batch 150, Loss: 1.1615535163879394\n",
      "Epoch 12, Batch 200, Loss: 1.156870617866516\n",
      "Epoch 12, Batch 250, Loss: 1.157741129398346\n",
      "Epoch 12, Batch 300, Loss: 1.1579998874664306\n",
      "Epoch 12, Batch 350, Loss: 1.157219934463501\n",
      "Epoch 12, Batch 400, Loss: 1.1593875312805175\n",
      "Epoch 12, Batch 450, Loss: 1.1592258644104003\n",
      "Epoch 12, Batch 500, Loss: 1.157287380695343\n",
      "Epoch 12, Batch 550, Loss: 1.1590825057029723\n",
      "Epoch 12, Batch 600, Loss: 1.1592561101913452\n",
      "Epoch 12, Batch 650, Loss: 1.1606191301345825\n",
      "Epoch 12, Batch 700, Loss: 1.1592234563827515\n",
      "Epoch 12, Batch 750, Loss: 1.1568578863143921\n",
      "Epoch 12, Batch 800, Loss: 1.1579814553260803\n",
      "Epoch 12, Batch 850, Loss: 1.1579656219482422\n",
      "Epoch 12, Batch 900, Loss: 1.1604950523376465\n",
      "Epoch 13, Batch 50, Loss: 1.1580053853988648\n",
      "Epoch 13, Batch 100, Loss: 1.1574950981140137\n",
      "Epoch 13, Batch 150, Loss: 1.1618386220932007\n",
      "Epoch 13, Batch 200, Loss: 1.1559017205238342\n",
      "Epoch 13, Batch 250, Loss: 1.1584678435325622\n",
      "Epoch 13, Batch 300, Loss: 1.1580629777908324\n",
      "Epoch 13, Batch 350, Loss: 1.1587109088897705\n",
      "Epoch 13, Batch 400, Loss: 1.1592951941490173\n",
      "Epoch 13, Batch 450, Loss: 1.1580082201957702\n",
      "Epoch 13, Batch 500, Loss: 1.1594498348236084\n",
      "Epoch 13, Batch 550, Loss: 1.158644871711731\n",
      "Epoch 13, Batch 600, Loss: 1.1609391212463378\n",
      "Epoch 13, Batch 650, Loss: 1.1581131744384765\n",
      "Epoch 13, Batch 700, Loss: 1.1587983894348144\n",
      "Epoch 13, Batch 750, Loss: 1.1600928235054015\n",
      "Epoch 13, Batch 800, Loss: 1.1585432767868042\n",
      "Epoch 13, Batch 850, Loss: 1.165730471611023\n",
      "Epoch 13, Batch 900, Loss: 1.224951901435852\n",
      "Epoch 14, Batch 50, Loss: 1.1733936190605163\n",
      "Epoch 14, Batch 100, Loss: 1.1595045042037964\n",
      "Epoch 14, Batch 150, Loss: 1.1658421182632446\n",
      "Epoch 14, Batch 200, Loss: 1.160986053943634\n",
      "Epoch 14, Batch 250, Loss: 1.16075786113739\n",
      "Epoch 14, Batch 300, Loss: 1.1619864225387573\n",
      "Epoch 14, Batch 350, Loss: 1.159299635887146\n",
      "Epoch 14, Batch 400, Loss: 1.1608990168571471\n",
      "Epoch 14, Batch 450, Loss: 1.1588588166236877\n",
      "Epoch 14, Batch 500, Loss: 1.1615165996551513\n",
      "Epoch 14, Batch 550, Loss: 1.1589019656181336\n",
      "Epoch 14, Batch 600, Loss: 1.1633834528923035\n",
      "Epoch 14, Batch 650, Loss: 1.1589504504203796\n",
      "Epoch 14, Batch 700, Loss: 1.1594591856002807\n",
      "Epoch 14, Batch 750, Loss: 1.1579281449317933\n",
      "Epoch 14, Batch 800, Loss: 1.1611535787582397\n",
      "Epoch 14, Batch 850, Loss: 1.1611929941177368\n",
      "Epoch 14, Batch 900, Loss: 1.1600554871559143\n",
      "Epoch 15, Batch 50, Loss: 1.162724006175995\n",
      "Epoch 15, Batch 100, Loss: 1.1573629903793334\n",
      "Epoch 15, Batch 150, Loss: 1.1581177067756654\n",
      "Epoch 15, Batch 200, Loss: 1.1585305500030518\n",
      "Epoch 15, Batch 250, Loss: 1.1603401589393616\n",
      "Epoch 15, Batch 300, Loss: 1.162312877178192\n",
      "Epoch 15, Batch 350, Loss: 1.160533549785614\n",
      "Epoch 15, Batch 400, Loss: 1.1612345528602601\n",
      "Epoch 15, Batch 450, Loss: 1.1611551833152771\n",
      "Epoch 15, Batch 500, Loss: 1.1603643703460693\n",
      "Epoch 15, Batch 550, Loss: 1.158492214679718\n",
      "Epoch 15, Batch 600, Loss: 1.1610147190093993\n",
      "Epoch 15, Batch 650, Loss: 1.1591597867012025\n",
      "Epoch 15, Batch 700, Loss: 1.1589743661880494\n",
      "Epoch 15, Batch 750, Loss: 1.1632183861732484\n",
      "Epoch 15, Batch 800, Loss: 1.1846228909492493\n",
      "Epoch 15, Batch 850, Loss: 1.1783179354667663\n",
      "Epoch 15, Batch 900, Loss: 1.1609492921829223\n",
      "Epoch 16, Batch 50, Loss: 1.157209303379059\n",
      "Epoch 16, Batch 100, Loss: 1.160126051902771\n",
      "Epoch 16, Batch 150, Loss: 1.1598349046707153\n",
      "Epoch 16, Batch 200, Loss: 1.1610484719276428\n",
      "Epoch 16, Batch 250, Loss: 1.1610475587844848\n",
      "Epoch 16, Batch 300, Loss: 1.1643710494041444\n",
      "Epoch 16, Batch 350, Loss: 1.157634527683258\n",
      "Epoch 16, Batch 400, Loss: 1.1618456077575683\n",
      "Epoch 16, Batch 450, Loss: 1.1579565715789795\n",
      "Epoch 16, Batch 500, Loss: 1.157315709590912\n",
      "Epoch 16, Batch 550, Loss: 1.1579006576538087\n",
      "Epoch 16, Batch 600, Loss: 1.1620577812194823\n",
      "Epoch 16, Batch 650, Loss: 1.160926353931427\n",
      "Epoch 16, Batch 700, Loss: 1.1582050180435182\n",
      "Epoch 16, Batch 750, Loss: 1.1586581110954284\n",
      "Epoch 16, Batch 800, Loss: 1.1575521874427794\n",
      "Epoch 16, Batch 850, Loss: 1.156403706073761\n",
      "Epoch 16, Batch 900, Loss: 1.1613591313362122\n",
      "Epoch 17, Batch 50, Loss: 1.1575346827507018\n",
      "Epoch 17, Batch 100, Loss: 1.1591231942176818\n",
      "Epoch 17, Batch 150, Loss: 1.1621136379241943\n",
      "Epoch 17, Batch 200, Loss: 1.1657512497901916\n",
      "Epoch 17, Batch 250, Loss: 1.1602238631248474\n",
      "Epoch 17, Batch 300, Loss: 1.1607009100914\n",
      "Epoch 17, Batch 350, Loss: 1.1598697519302368\n",
      "Epoch 17, Batch 400, Loss: 1.1592602610588074\n",
      "Epoch 17, Batch 450, Loss: 1.160769953727722\n",
      "Epoch 17, Batch 500, Loss: 1.1591525053977967\n",
      "Epoch 17, Batch 550, Loss: 1.1583107018470764\n",
      "Epoch 17, Batch 600, Loss: 1.15921954870224\n",
      "Epoch 17, Batch 650, Loss: 1.1596296310424805\n",
      "Epoch 17, Batch 700, Loss: 1.158395013809204\n",
      "Epoch 17, Batch 750, Loss: 1.1608774328231812\n",
      "Epoch 17, Batch 800, Loss: 1.1586743783950806\n",
      "Epoch 17, Batch 850, Loss: 1.1597604489326476\n",
      "Epoch 17, Batch 900, Loss: 1.158840355873108\n",
      "Epoch 18, Batch 50, Loss: 1.1613756895065308\n",
      "Epoch 18, Batch 100, Loss: 1.1600701785087586\n",
      "Epoch 18, Batch 150, Loss: 1.1626438093185425\n",
      "Epoch 18, Batch 200, Loss: 1.1579273653030395\n",
      "Epoch 18, Batch 250, Loss: 1.1589611458778382\n",
      "Epoch 18, Batch 300, Loss: 1.1591763234138488\n",
      "Epoch 18, Batch 350, Loss: 1.1602101302146912\n",
      "Epoch 18, Batch 400, Loss: 1.1604191374778747\n",
      "Epoch 18, Batch 450, Loss: 1.1599598288536073\n",
      "Epoch 18, Batch 500, Loss: 1.1600694966316223\n",
      "Epoch 18, Batch 550, Loss: 1.1586699533462523\n",
      "Epoch 18, Batch 600, Loss: 1.1572238063812257\n",
      "Epoch 18, Batch 650, Loss: 1.1559582781791686\n",
      "Epoch 18, Batch 700, Loss: 1.1590944695472718\n",
      "Epoch 18, Batch 750, Loss: 1.1561319780349733\n",
      "Epoch 18, Batch 800, Loss: 1.1575375342369079\n",
      "Epoch 18, Batch 850, Loss: 1.1578011393547059\n",
      "Epoch 18, Batch 900, Loss: 1.1607276606559753\n",
      "Epoch 19, Batch 50, Loss: 1.1606468677520752\n",
      "Epoch 19, Batch 100, Loss: 1.216996862888336\n",
      "Epoch 19, Batch 150, Loss: 1.1630509734153747\n",
      "Epoch 19, Batch 200, Loss: 1.160123815536499\n",
      "Epoch 19, Batch 250, Loss: 1.1576404094696044\n",
      "Epoch 19, Batch 300, Loss: 1.1593304133415223\n",
      "Epoch 19, Batch 350, Loss: 1.1583991265296936\n",
      "Epoch 19, Batch 400, Loss: 1.158992862701416\n",
      "Epoch 19, Batch 450, Loss: 1.1601566219329833\n",
      "Epoch 19, Batch 500, Loss: 1.1607006216049194\n",
      "Epoch 19, Batch 550, Loss: 1.1591873502731322\n",
      "Epoch 19, Batch 600, Loss: 1.1613034367561341\n",
      "Epoch 19, Batch 650, Loss: 1.1594990944862367\n",
      "Epoch 19, Batch 700, Loss: 1.1588539314270019\n",
      "Epoch 19, Batch 750, Loss: 1.160872073173523\n",
      "Epoch 19, Batch 800, Loss: 1.157104353904724\n",
      "Epoch 19, Batch 850, Loss: 1.158574571609497\n",
      "Epoch 19, Batch 900, Loss: 1.1587635803222656\n",
      "Epoch 20, Batch 50, Loss: 1.1569376802444458\n",
      "Epoch 20, Batch 100, Loss: 1.1615676546096803\n",
      "Epoch 20, Batch 150, Loss: 1.1603378462791443\n",
      "Epoch 20, Batch 200, Loss: 1.1636664009094237\n",
      "Epoch 20, Batch 250, Loss: 1.1572924423217774\n",
      "Epoch 20, Batch 300, Loss: 1.1590436506271362\n",
      "Epoch 20, Batch 350, Loss: 1.1596581578254699\n",
      "Epoch 20, Batch 400, Loss: 1.1615023493766785\n",
      "Epoch 20, Batch 450, Loss: 1.15978417634964\n",
      "Epoch 20, Batch 500, Loss: 1.1635559272766114\n",
      "Epoch 20, Batch 550, Loss: 1.1567947173118591\n",
      "Epoch 20, Batch 600, Loss: 1.1607446479797363\n",
      "Epoch 20, Batch 650, Loss: 1.1591748261451722\n",
      "Epoch 20, Batch 700, Loss: 1.160941982269287\n",
      "Epoch 20, Batch 750, Loss: 1.1603324484825135\n",
      "Epoch 20, Batch 800, Loss: 1.1605819869041443\n",
      "Epoch 20, Batch 850, Loss: 1.1567874956130981\n",
      "Epoch 20, Batch 900, Loss: 1.1601560068130494\n",
      "Epoch 21, Batch 50, Loss: 1.1594981241226197\n",
      "Epoch 21, Batch 100, Loss: 1.1609206104278564\n",
      "Epoch 21, Batch 150, Loss: 1.1598187518119811\n",
      "Epoch 21, Batch 200, Loss: 1.1552809262275696\n",
      "Epoch 21, Batch 250, Loss: 1.161980881690979\n",
      "Epoch 21, Batch 300, Loss: 1.1602164554595946\n",
      "Epoch 21, Batch 350, Loss: 1.1583038473129272\n",
      "Epoch 21, Batch 400, Loss: 1.1583362889289857\n",
      "Epoch 21, Batch 450, Loss: 1.1565715813636779\n",
      "Epoch 21, Batch 500, Loss: 1.1578224730491637\n",
      "Epoch 21, Batch 550, Loss: 1.15900093793869\n",
      "Epoch 21, Batch 600, Loss: 1.1589245653152467\n",
      "Epoch 21, Batch 650, Loss: 1.1574301218986511\n",
      "Epoch 21, Batch 700, Loss: 1.1650982546806334\n",
      "Epoch 21, Batch 750, Loss: 1.1690285873413087\n",
      "Epoch 21, Batch 800, Loss: 1.1625630235671998\n",
      "Epoch 21, Batch 850, Loss: 1.160295763015747\n",
      "Epoch 21, Batch 900, Loss: 1.1578257846832276\n",
      "Epoch 22, Batch 50, Loss: 1.157292742729187\n",
      "Epoch 22, Batch 100, Loss: 1.1581518054008484\n",
      "Epoch 22, Batch 150, Loss: 1.1593413639068604\n",
      "Epoch 22, Batch 200, Loss: 1.1603619718551637\n",
      "Epoch 22, Batch 250, Loss: 1.156693410873413\n",
      "Epoch 22, Batch 300, Loss: 1.160701138973236\n",
      "Epoch 22, Batch 350, Loss: 1.1599826526641845\n",
      "Epoch 22, Batch 400, Loss: 1.1587780475616456\n",
      "Epoch 22, Batch 450, Loss: 1.1586452794075013\n",
      "Epoch 22, Batch 500, Loss: 1.15964097738266\n",
      "Epoch 22, Batch 550, Loss: 1.16026269197464\n",
      "Epoch 22, Batch 600, Loss: 1.1574994492530823\n",
      "Epoch 22, Batch 650, Loss: 1.159169943332672\n",
      "Epoch 22, Batch 700, Loss: 1.159886465072632\n",
      "Epoch 22, Batch 750, Loss: 1.161461465358734\n",
      "Epoch 22, Batch 800, Loss: 1.1581736516952514\n",
      "Epoch 22, Batch 850, Loss: 1.157306056022644\n",
      "Epoch 22, Batch 900, Loss: 1.1580987620353698\n",
      "Epoch 23, Batch 50, Loss: 1.1589200735092162\n",
      "Epoch 23, Batch 100, Loss: 1.1585138297080995\n",
      "Epoch 23, Batch 150, Loss: 1.1599835705757142\n",
      "Epoch 23, Batch 200, Loss: 1.1580221676826477\n",
      "Epoch 23, Batch 250, Loss: 1.1603644919395446\n",
      "Epoch 23, Batch 300, Loss: 1.158980405330658\n",
      "Epoch 23, Batch 350, Loss: 1.1558883786201477\n",
      "Epoch 23, Batch 400, Loss: 1.1606366753578186\n",
      "Epoch 23, Batch 450, Loss: 1.1599382281303405\n",
      "Epoch 23, Batch 500, Loss: 1.1597624182701112\n",
      "Epoch 23, Batch 550, Loss: 1.1585585498809814\n",
      "Epoch 23, Batch 600, Loss: 1.1603228235244751\n",
      "Epoch 23, Batch 650, Loss: 1.1582669019699097\n",
      "Epoch 23, Batch 700, Loss: 1.1586023902893066\n",
      "Epoch 23, Batch 750, Loss: 1.1591486167907714\n",
      "Epoch 23, Batch 800, Loss: 1.1600346040725709\n",
      "Epoch 23, Batch 850, Loss: 1.1616064524650573\n",
      "Epoch 23, Batch 900, Loss: 1.1593257641792298\n",
      "Epoch 24, Batch 50, Loss: 1.1587924790382385\n",
      "Epoch 24, Batch 100, Loss: 1.159136085510254\n",
      "Epoch 24, Batch 150, Loss: 1.1631027030944825\n",
      "Epoch 24, Batch 200, Loss: 1.1591067624092102\n",
      "Epoch 24, Batch 250, Loss: 1.1602740144729615\n",
      "Epoch 24, Batch 300, Loss: 1.1601782584190368\n",
      "Epoch 24, Batch 350, Loss: 1.1571199560165406\n",
      "Epoch 24, Batch 400, Loss: 1.1610734105110168\n",
      "Epoch 24, Batch 450, Loss: 1.1562757086753845\n",
      "Epoch 24, Batch 500, Loss: 1.1562782406806946\n",
      "Epoch 24, Batch 550, Loss: 1.159048569202423\n",
      "Epoch 24, Batch 600, Loss: 1.1593702793121339\n",
      "Epoch 24, Batch 650, Loss: 1.1606267595291138\n",
      "Epoch 24, Batch 700, Loss: 1.1635759472846985\n",
      "Epoch 24, Batch 750, Loss: 1.1704069375991821\n",
      "Epoch 24, Batch 800, Loss: 1.1581447291374207\n",
      "Epoch 24, Batch 850, Loss: 1.1586994433403015\n",
      "Epoch 24, Batch 900, Loss: 1.160748541355133\n",
      "Epoch 25, Batch 50, Loss: 1.1574189782142639\n",
      "Epoch 25, Batch 100, Loss: 1.1619545316696167\n",
      "Epoch 25, Batch 150, Loss: 1.1594103002548217\n",
      "Epoch 25, Batch 200, Loss: 1.1617958760261535\n",
      "Epoch 25, Batch 250, Loss: 1.1590348982810974\n",
      "Epoch 25, Batch 300, Loss: 1.1580380773544312\n",
      "Epoch 25, Batch 350, Loss: 1.1593062376976013\n",
      "Epoch 25, Batch 400, Loss: 1.1611809396743775\n",
      "Epoch 25, Batch 450, Loss: 1.1610313320159913\n",
      "Epoch 25, Batch 500, Loss: 1.1597284722328185\n",
      "Epoch 25, Batch 550, Loss: 1.1588609719276428\n",
      "Epoch 25, Batch 600, Loss: 1.158252010345459\n",
      "Epoch 25, Batch 650, Loss: 1.1605430603027345\n",
      "Epoch 25, Batch 700, Loss: 1.1593305611610412\n",
      "Epoch 25, Batch 750, Loss: 1.1571167135238647\n",
      "Epoch 25, Batch 800, Loss: 1.1623962831497192\n",
      "Epoch 25, Batch 850, Loss: 1.1591218495368958\n",
      "Epoch 25, Batch 900, Loss: 1.1591517305374146\n",
      "Accuracy on test set: 0.1%\n",
      "Fitting for combination 61\n",
      "784\n",
      "4\n",
      "10\n",
      "[40, 10, 10, 10, 10]\n",
      "True\n",
      "['relu', 'sigmoid', 'relu', 'relu']\n",
      "SGD\n",
      "0.1\n",
      "0.01\n",
      "CrossEntropyLoss\n",
      "50\n",
      "25\n",
      "0\n",
      "Epoch 1, Batch 50, Loss: 1.1517735242843627\n",
      "Epoch 1, Batch 100, Loss: 1.1492051124572753\n",
      "Epoch 1, Batch 150, Loss: 1.144099507331848\n",
      "Epoch 1, Batch 200, Loss: 1.1307671928405763\n",
      "Epoch 1, Batch 250, Loss: 1.0936848187446595\n",
      "Epoch 1, Batch 300, Loss: 1.0280855083465577\n",
      "Epoch 1, Batch 350, Loss: 0.9644400465488434\n",
      "Epoch 1, Batch 400, Loss: 0.9370407795906067\n",
      "Epoch 1, Batch 450, Loss: 0.9145343673229217\n",
      "Epoch 1, Batch 500, Loss: 0.8792924189567566\n",
      "Epoch 1, Batch 550, Loss: 0.8340431272983551\n",
      "Epoch 1, Batch 600, Loss: 0.8395256161689758\n",
      "Epoch 1, Batch 650, Loss: 0.7697832655906677\n",
      "Epoch 1, Batch 700, Loss: 0.7649101150035859\n",
      "Epoch 1, Batch 750, Loss: 0.7326153683662414\n",
      "Epoch 1, Batch 800, Loss: 0.7346976208686828\n",
      "Epoch 1, Batch 850, Loss: 0.7144825720787048\n",
      "Epoch 1, Batch 900, Loss: 0.719948695898056\n",
      "Epoch 2, Batch 50, Loss: 0.6771066820621491\n",
      "Epoch 2, Batch 100, Loss: 0.6663510715961456\n",
      "Epoch 2, Batch 150, Loss: 0.6285165929794312\n",
      "Epoch 2, Batch 200, Loss: 0.6142161715030671\n",
      "Epoch 2, Batch 250, Loss: 0.650376136302948\n",
      "Epoch 2, Batch 300, Loss: 0.5980076348781586\n",
      "Epoch 2, Batch 350, Loss: 0.582873802781105\n",
      "Epoch 2, Batch 400, Loss: 0.5887203234434127\n",
      "Epoch 2, Batch 450, Loss: 0.5657077449560165\n",
      "Epoch 2, Batch 500, Loss: 0.5529218363761902\n",
      "Epoch 2, Batch 550, Loss: 0.5118502271175385\n",
      "Epoch 2, Batch 600, Loss: 0.5319813668727875\n",
      "Epoch 2, Batch 650, Loss: 0.492936310172081\n",
      "Epoch 2, Batch 700, Loss: 0.46722170770168303\n",
      "Epoch 2, Batch 750, Loss: 0.474122987985611\n",
      "Epoch 2, Batch 800, Loss: 0.48176256477832796\n",
      "Epoch 2, Batch 850, Loss: 0.4555665111541748\n",
      "Epoch 2, Batch 900, Loss: 0.4477621966600418\n",
      "Epoch 3, Batch 50, Loss: 0.44003383696079257\n",
      "Epoch 3, Batch 100, Loss: 0.41916178226470946\n",
      "Epoch 3, Batch 150, Loss: 0.4127733963727951\n",
      "Epoch 3, Batch 200, Loss: 0.4077777481079102\n",
      "Epoch 3, Batch 250, Loss: 0.4137680840492248\n",
      "Epoch 3, Batch 300, Loss: 0.37852986991405485\n",
      "Epoch 3, Batch 350, Loss: 0.3692633450031281\n",
      "Epoch 3, Batch 400, Loss: 0.3711000031232834\n",
      "Epoch 3, Batch 450, Loss: 0.3742739117145538\n",
      "Epoch 3, Batch 500, Loss: 0.3799923720955849\n",
      "Epoch 3, Batch 550, Loss: 0.3785524570941925\n",
      "Epoch 3, Batch 600, Loss: 0.3525238996744156\n",
      "Epoch 3, Batch 650, Loss: 0.3782637751102447\n",
      "Epoch 3, Batch 700, Loss: 0.3662609142065048\n",
      "Epoch 3, Batch 750, Loss: 0.3645917469263077\n",
      "Epoch 3, Batch 800, Loss: 0.37539349555969237\n",
      "Epoch 3, Batch 850, Loss: 0.36684623539447786\n",
      "Epoch 3, Batch 900, Loss: 0.3710387825965881\n",
      "Epoch 4, Batch 50, Loss: 0.364086035490036\n",
      "Epoch 4, Batch 100, Loss: 0.36385767489671705\n",
      "Epoch 4, Batch 150, Loss: 0.3797873204946518\n",
      "Epoch 4, Batch 200, Loss: 0.342777786552906\n",
      "Epoch 4, Batch 250, Loss: 0.348043298125267\n",
      "Epoch 4, Batch 300, Loss: 0.352395683825016\n",
      "Epoch 4, Batch 350, Loss: 0.35083418726921084\n",
      "Epoch 4, Batch 400, Loss: 0.3366289854049683\n",
      "Epoch 4, Batch 450, Loss: 0.3365009963512421\n",
      "Epoch 4, Batch 500, Loss: 0.3508391171693802\n",
      "Epoch 4, Batch 550, Loss: 0.33413426220417025\n",
      "Epoch 4, Batch 600, Loss: 0.34273142635822296\n",
      "Epoch 4, Batch 650, Loss: 0.35790663480758667\n",
      "Epoch 4, Batch 700, Loss: 0.30296487927436827\n",
      "Epoch 4, Batch 750, Loss: 0.3749560704827309\n",
      "Epoch 4, Batch 800, Loss: 0.32630331814289093\n",
      "Epoch 4, Batch 850, Loss: 0.3178302338719368\n",
      "Epoch 4, Batch 900, Loss: 0.33861089825630186\n",
      "Epoch 5, Batch 50, Loss: 0.3228155994415283\n",
      "Epoch 5, Batch 100, Loss: 0.31584499567747115\n",
      "Epoch 5, Batch 150, Loss: 0.3129712015390396\n",
      "Epoch 5, Batch 200, Loss: 0.32754427194595337\n",
      "Epoch 5, Batch 250, Loss: 0.32449680507183076\n",
      "Epoch 5, Batch 300, Loss: 0.32062156885862353\n",
      "Epoch 5, Batch 350, Loss: 0.30811561048030855\n",
      "Epoch 5, Batch 400, Loss: 0.32744299471378324\n",
      "Epoch 5, Batch 450, Loss: 0.33081468760967253\n",
      "Epoch 5, Batch 500, Loss: 0.3343577092885971\n",
      "Epoch 5, Batch 550, Loss: 0.3377760177850723\n",
      "Epoch 5, Batch 600, Loss: 0.33472279489040374\n",
      "Epoch 5, Batch 650, Loss: 0.32303745120763777\n",
      "Epoch 5, Batch 700, Loss: 0.31027728736400606\n",
      "Epoch 5, Batch 750, Loss: 0.3258006906509399\n",
      "Epoch 5, Batch 800, Loss: 0.34175422668457034\n",
      "Epoch 5, Batch 850, Loss: 0.3251375690102577\n",
      "Epoch 5, Batch 900, Loss: 0.3142440918087959\n",
      "Epoch 6, Batch 50, Loss: 0.29789276242256163\n",
      "Epoch 6, Batch 100, Loss: 0.32364254117012026\n",
      "Epoch 6, Batch 150, Loss: 0.312714649438858\n",
      "Epoch 6, Batch 200, Loss: 0.3166021305322647\n",
      "Epoch 6, Batch 250, Loss: 0.3246523398160934\n",
      "Epoch 6, Batch 300, Loss: 0.3107640278339386\n",
      "Epoch 6, Batch 350, Loss: 0.30131085753440856\n",
      "Epoch 6, Batch 400, Loss: 0.312046075463295\n",
      "Epoch 6, Batch 450, Loss: 0.3417727753520012\n",
      "Epoch 6, Batch 500, Loss: 0.32255523949861525\n",
      "Epoch 6, Batch 550, Loss: 0.30898534029722213\n",
      "Epoch 6, Batch 600, Loss: 0.3054062885046005\n",
      "Epoch 6, Batch 650, Loss: 0.3172787719964981\n",
      "Epoch 6, Batch 700, Loss: 0.2978223967552185\n",
      "Epoch 6, Batch 750, Loss: 0.3195166638493538\n",
      "Epoch 6, Batch 800, Loss: 0.31271401792764664\n",
      "Epoch 6, Batch 850, Loss: 0.3194914734363556\n",
      "Epoch 6, Batch 900, Loss: 0.30700579166412356\n",
      "Epoch 7, Batch 50, Loss: 0.3104041761159897\n",
      "Epoch 7, Batch 100, Loss: 0.3176245087385178\n",
      "Epoch 7, Batch 150, Loss: 0.2979411354660988\n",
      "Epoch 7, Batch 200, Loss: 0.3073474645614624\n",
      "Epoch 7, Batch 250, Loss: 0.3169435480237007\n",
      "Epoch 7, Batch 300, Loss: 0.31571741074323656\n",
      "Epoch 7, Batch 350, Loss: 0.29843785256147387\n",
      "Epoch 7, Batch 400, Loss: 0.3034377521276474\n",
      "Epoch 7, Batch 450, Loss: 0.2897285574674606\n",
      "Epoch 7, Batch 500, Loss: 0.30742887943983077\n",
      "Epoch 7, Batch 550, Loss: 0.3168599742650986\n",
      "Epoch 7, Batch 600, Loss: 0.3088515809178352\n",
      "Epoch 7, Batch 650, Loss: 0.3197560167312622\n",
      "Epoch 7, Batch 700, Loss: 0.3134874373674393\n",
      "Epoch 7, Batch 750, Loss: 0.31810030341148376\n",
      "Epoch 7, Batch 800, Loss: 0.3193775382637978\n",
      "Epoch 7, Batch 850, Loss: 0.2989116370677948\n",
      "Epoch 7, Batch 900, Loss: 0.2878696593642235\n",
      "Epoch 8, Batch 50, Loss: 0.2970861837267876\n",
      "Epoch 8, Batch 100, Loss: 0.30489574432373046\n",
      "Epoch 8, Batch 150, Loss: 0.30511903584003447\n",
      "Epoch 8, Batch 200, Loss: 0.3117436620593071\n",
      "Epoch 8, Batch 250, Loss: 0.3082559397816658\n",
      "Epoch 8, Batch 300, Loss: 0.2873199519515037\n",
      "Epoch 8, Batch 350, Loss: 0.2912789690494537\n",
      "Epoch 8, Batch 400, Loss: 0.2984649309515953\n",
      "Epoch 8, Batch 450, Loss: 0.29465314984321594\n",
      "Epoch 8, Batch 500, Loss: 0.30267039090394976\n",
      "Epoch 8, Batch 550, Loss: 0.2960253232717514\n",
      "Epoch 8, Batch 600, Loss: 0.31224963188171384\n",
      "Epoch 8, Batch 650, Loss: 0.3063643455505371\n",
      "Epoch 8, Batch 700, Loss: 0.30199551045894624\n",
      "Epoch 8, Batch 750, Loss: 0.28726983338594436\n",
      "Epoch 8, Batch 800, Loss: 0.3086168485879898\n",
      "Epoch 8, Batch 850, Loss: 0.32169936209917066\n",
      "Epoch 8, Batch 900, Loss: 0.2926920372247696\n",
      "Epoch 9, Batch 50, Loss: 0.30344567716121673\n",
      "Epoch 9, Batch 100, Loss: 0.31465329706668854\n",
      "Epoch 9, Batch 150, Loss: 0.29758822470903395\n",
      "Epoch 9, Batch 200, Loss: 0.2940039837360382\n",
      "Epoch 9, Batch 250, Loss: 0.307077938914299\n",
      "Epoch 9, Batch 300, Loss: 0.30551846235990526\n",
      "Epoch 9, Batch 350, Loss: 0.30378115504980086\n",
      "Epoch 9, Batch 400, Loss: 0.3103965124487877\n",
      "Epoch 9, Batch 450, Loss: 0.2861833894252777\n",
      "Epoch 9, Batch 500, Loss: 0.2964239063858986\n",
      "Epoch 9, Batch 550, Loss: 0.30591563761234286\n",
      "Epoch 9, Batch 600, Loss: 0.28393451660871505\n",
      "Epoch 9, Batch 650, Loss: 0.2882129830121994\n",
      "Epoch 9, Batch 700, Loss: 0.29925844609737395\n",
      "Epoch 9, Batch 750, Loss: 0.2861444905400276\n",
      "Epoch 9, Batch 800, Loss: 0.29832695305347445\n",
      "Epoch 9, Batch 850, Loss: 0.3014699611067772\n",
      "Epoch 9, Batch 900, Loss: 0.30439083456993105\n",
      "Epoch 10, Batch 50, Loss: 0.2950830602645874\n",
      "Epoch 10, Batch 100, Loss: 0.2836926528811455\n",
      "Epoch 10, Batch 150, Loss: 0.3075464436411858\n",
      "Epoch 10, Batch 200, Loss: 0.294999968111515\n",
      "Epoch 10, Batch 250, Loss: 0.27879187285900114\n",
      "Epoch 10, Batch 300, Loss: 0.29720287054777145\n",
      "Epoch 10, Batch 350, Loss: 0.3181924456357956\n",
      "Epoch 10, Batch 400, Loss: 0.2920020416378975\n",
      "Epoch 10, Batch 450, Loss: 0.29691035687923434\n",
      "Epoch 10, Batch 500, Loss: 0.2920344683527947\n",
      "Epoch 10, Batch 550, Loss: 0.28047133058309553\n",
      "Epoch 10, Batch 600, Loss: 0.30265973299741744\n",
      "Epoch 10, Batch 650, Loss: 0.2964246854186058\n",
      "Epoch 10, Batch 700, Loss: 0.29614039450883867\n",
      "Epoch 10, Batch 750, Loss: 0.30878973960876466\n",
      "Epoch 10, Batch 800, Loss: 0.30047665804624557\n",
      "Epoch 10, Batch 850, Loss: 0.2784166151285172\n",
      "Epoch 10, Batch 900, Loss: 0.29262452363967895\n",
      "Epoch 11, Batch 50, Loss: 0.29735864788293837\n",
      "Epoch 11, Batch 100, Loss: 0.28330553740262987\n",
      "Epoch 11, Batch 150, Loss: 0.30351463675498963\n",
      "Epoch 11, Batch 200, Loss: 0.2959946748614311\n",
      "Epoch 11, Batch 250, Loss: 0.2900381037592888\n",
      "Epoch 11, Batch 300, Loss: 0.3133241701126099\n",
      "Epoch 11, Batch 350, Loss: 0.2788833862543106\n",
      "Epoch 11, Batch 400, Loss: 0.2868912836909294\n",
      "Epoch 11, Batch 450, Loss: 0.2950437048077583\n",
      "Epoch 11, Batch 500, Loss: 0.2886636006832123\n",
      "Epoch 11, Batch 550, Loss: 0.3098267957568169\n",
      "Epoch 11, Batch 600, Loss: 0.2949493744969368\n",
      "Epoch 11, Batch 650, Loss: 0.2741323381662369\n",
      "Epoch 11, Batch 700, Loss: 0.3017739850282669\n",
      "Epoch 11, Batch 750, Loss: 0.2931324237585068\n",
      "Epoch 11, Batch 800, Loss: 0.29205924153327945\n",
      "Epoch 11, Batch 850, Loss: 0.28569151610136034\n",
      "Epoch 11, Batch 900, Loss: 0.2934143111109734\n",
      "Epoch 12, Batch 50, Loss: 0.2953257572650909\n",
      "Epoch 12, Batch 100, Loss: 0.3030765572190285\n",
      "Epoch 12, Batch 150, Loss: 0.2835680943727493\n",
      "Epoch 12, Batch 200, Loss: 0.2797095522284508\n",
      "Epoch 12, Batch 250, Loss: 0.27154593899846075\n",
      "Epoch 12, Batch 300, Loss: 0.29343770146369935\n",
      "Epoch 12, Batch 350, Loss: 0.29291065901517865\n",
      "Epoch 12, Batch 400, Loss: 0.2890219154953957\n",
      "Epoch 12, Batch 450, Loss: 0.31891941130161283\n",
      "Epoch 12, Batch 500, Loss: 0.3042227131128311\n",
      "Epoch 12, Batch 550, Loss: 0.2804777616262436\n",
      "Epoch 12, Batch 600, Loss: 0.2812364900112152\n",
      "Epoch 12, Batch 650, Loss: 0.2717540681362152\n",
      "Epoch 12, Batch 700, Loss: 0.2945388001203537\n",
      "Epoch 12, Batch 750, Loss: 0.28269322603940966\n",
      "Epoch 12, Batch 800, Loss: 0.2872735205292702\n",
      "Epoch 12, Batch 850, Loss: 0.3179522883892059\n",
      "Epoch 12, Batch 900, Loss: 0.289738122522831\n",
      "Epoch 13, Batch 50, Loss: 0.3038851550221443\n",
      "Epoch 13, Batch 100, Loss: 0.2874237325787544\n",
      "Epoch 13, Batch 150, Loss: 0.2872033444046974\n",
      "Epoch 13, Batch 200, Loss: 0.2884402847290039\n",
      "Epoch 13, Batch 250, Loss: 0.31428355067968367\n",
      "Epoch 13, Batch 300, Loss: 0.2700779637694359\n",
      "Epoch 13, Batch 350, Loss: 0.28429745852947236\n",
      "Epoch 13, Batch 400, Loss: 0.30475611925125123\n",
      "Epoch 13, Batch 450, Loss: 0.2770696696639061\n",
      "Epoch 13, Batch 500, Loss: 0.3037333673238754\n",
      "Epoch 13, Batch 550, Loss: 0.28108376502990723\n",
      "Epoch 13, Batch 600, Loss: 0.2911788767576218\n",
      "Epoch 13, Batch 650, Loss: 0.2924248406291008\n",
      "Epoch 13, Batch 700, Loss: 0.2752985176444054\n",
      "Epoch 13, Batch 750, Loss: 0.31233165472745894\n",
      "Epoch 13, Batch 800, Loss: 0.2842758798599243\n",
      "Epoch 13, Batch 850, Loss: 0.2835943588614464\n",
      "Epoch 13, Batch 900, Loss: 0.2805263340473175\n",
      "Epoch 14, Batch 50, Loss: 0.2890030762553215\n",
      "Epoch 14, Batch 100, Loss: 0.3096120738983154\n",
      "Epoch 14, Batch 150, Loss: 0.2880187976360321\n",
      "Epoch 14, Batch 200, Loss: 0.282840630710125\n",
      "Epoch 14, Batch 250, Loss: 0.28762080281972885\n",
      "Epoch 14, Batch 300, Loss: 0.2859442266821861\n",
      "Epoch 14, Batch 350, Loss: 0.3025691777467728\n",
      "Epoch 14, Batch 400, Loss: 0.27642022967338564\n",
      "Epoch 14, Batch 450, Loss: 0.30066190630197526\n",
      "Epoch 14, Batch 500, Loss: 0.2799547576904297\n",
      "Epoch 14, Batch 550, Loss: 0.275708224773407\n",
      "Epoch 14, Batch 600, Loss: 0.32445430755615234\n",
      "Epoch 14, Batch 650, Loss: 0.2917670276761055\n",
      "Epoch 14, Batch 700, Loss: 0.28429213613271714\n",
      "Epoch 14, Batch 750, Loss: 0.28232652217149734\n",
      "Epoch 14, Batch 800, Loss: 0.28970799744129183\n",
      "Epoch 14, Batch 850, Loss: 0.2923505267500877\n",
      "Epoch 14, Batch 900, Loss: 0.29257321447134016\n",
      "Epoch 15, Batch 50, Loss: 0.284799183011055\n",
      "Epoch 15, Batch 100, Loss: 0.29199387043714525\n",
      "Epoch 15, Batch 150, Loss: 0.3042358186841011\n",
      "Epoch 15, Batch 200, Loss: 0.28444138169288635\n",
      "Epoch 15, Batch 250, Loss: 0.28976353764533996\n",
      "Epoch 15, Batch 300, Loss: 0.28146017730236056\n",
      "Epoch 15, Batch 350, Loss: 0.2958166807889938\n",
      "Epoch 15, Batch 400, Loss: 0.28343112468719484\n",
      "Epoch 15, Batch 450, Loss: 0.3020727661252022\n",
      "Epoch 15, Batch 500, Loss: 0.310126451253891\n",
      "Epoch 15, Batch 550, Loss: 0.2728788748383522\n",
      "Epoch 15, Batch 600, Loss: 0.28855462819337846\n",
      "Epoch 15, Batch 650, Loss: 0.28194445610046387\n",
      "Epoch 15, Batch 700, Loss: 0.27752605468034747\n",
      "Epoch 15, Batch 750, Loss: 0.28281940072774886\n",
      "Epoch 15, Batch 800, Loss: 0.2938417759537697\n",
      "Epoch 15, Batch 850, Loss: 0.28911564022302627\n",
      "Epoch 15, Batch 900, Loss: 0.2761952781677246\n",
      "Epoch 16, Batch 50, Loss: 0.2900257796049118\n",
      "Epoch 16, Batch 100, Loss: 0.26342370092868805\n",
      "Epoch 16, Batch 150, Loss: 0.25524864315986634\n",
      "Epoch 16, Batch 200, Loss: 0.3045177063345909\n",
      "Epoch 16, Batch 250, Loss: 0.3079839962720871\n",
      "Epoch 16, Batch 300, Loss: 0.2890742188692093\n",
      "Epoch 16, Batch 350, Loss: 0.28065969079732894\n",
      "Epoch 16, Batch 400, Loss: 0.2724392530322075\n",
      "Epoch 16, Batch 450, Loss: 0.2820615631341934\n",
      "Epoch 16, Batch 500, Loss: 0.2799342194199562\n",
      "Epoch 16, Batch 550, Loss: 0.28661806851625443\n",
      "Epoch 16, Batch 600, Loss: 0.29530952125787735\n",
      "Epoch 16, Batch 650, Loss: 0.2767690777778625\n",
      "Epoch 16, Batch 700, Loss: 0.28431012213230134\n",
      "Epoch 16, Batch 750, Loss: 0.28070190370082854\n",
      "Epoch 16, Batch 800, Loss: 0.2709454140067101\n",
      "Epoch 16, Batch 850, Loss: 0.28997938126325606\n",
      "Epoch 16, Batch 900, Loss: 0.27210679560899736\n",
      "Epoch 17, Batch 50, Loss: 0.28530968219041825\n",
      "Epoch 17, Batch 100, Loss: 0.27298974931240083\n",
      "Epoch 17, Batch 150, Loss: 0.2781022509932518\n",
      "Epoch 17, Batch 200, Loss: 0.2865640735626221\n",
      "Epoch 17, Batch 250, Loss: 0.27747769773006437\n",
      "Epoch 17, Batch 300, Loss: 0.2797581124305725\n",
      "Epoch 17, Batch 350, Loss: 0.2707909768819809\n",
      "Epoch 17, Batch 400, Loss: 0.2648375573754311\n",
      "Epoch 17, Batch 450, Loss: 0.2849841007590294\n",
      "Epoch 17, Batch 500, Loss: 0.2838845333456993\n",
      "Epoch 17, Batch 550, Loss: 0.3143297976255417\n",
      "Epoch 17, Batch 600, Loss: 0.27647750109434127\n",
      "Epoch 17, Batch 650, Loss: 0.3017397505044937\n",
      "Epoch 17, Batch 700, Loss: 0.2820274052023888\n",
      "Epoch 17, Batch 750, Loss: 0.2761526611447334\n",
      "Epoch 17, Batch 800, Loss: 0.2720870840549469\n",
      "Epoch 17, Batch 850, Loss: 0.28525887578725817\n",
      "Epoch 17, Batch 900, Loss: 0.2905890879034996\n",
      "Epoch 18, Batch 50, Loss: 0.2833048775792122\n",
      "Epoch 18, Batch 100, Loss: 0.30231667786836625\n",
      "Epoch 18, Batch 150, Loss: 0.27686674416065216\n",
      "Epoch 18, Batch 200, Loss: 0.28081977397203445\n",
      "Epoch 18, Batch 250, Loss: 0.27094009071588515\n",
      "Epoch 18, Batch 300, Loss: 0.28532041162252425\n",
      "Epoch 18, Batch 350, Loss: 0.2832063999772072\n",
      "Epoch 18, Batch 400, Loss: 0.26392020255327225\n",
      "Epoch 18, Batch 450, Loss: 0.30109808772802354\n",
      "Epoch 18, Batch 500, Loss: 0.2756197080016136\n",
      "Epoch 18, Batch 550, Loss: 0.28252038955688474\n",
      "Epoch 18, Batch 600, Loss: 0.29155916631221773\n",
      "Epoch 18, Batch 650, Loss: 0.2871082881093025\n",
      "Epoch 18, Batch 700, Loss: 0.2747126764059067\n",
      "Epoch 18, Batch 750, Loss: 0.2667070591449738\n",
      "Epoch 18, Batch 800, Loss: 0.26671337395906447\n",
      "Epoch 18, Batch 850, Loss: 0.2777239829301834\n",
      "Epoch 18, Batch 900, Loss: 0.2954872158169746\n",
      "Epoch 19, Batch 50, Loss: 0.2750766274333\n",
      "Epoch 19, Batch 100, Loss: 0.27996702417731284\n",
      "Epoch 19, Batch 150, Loss: 0.2888081181049347\n",
      "Epoch 19, Batch 200, Loss: 0.26594292432069777\n",
      "Epoch 19, Batch 250, Loss: 0.28117692828178403\n",
      "Epoch 19, Batch 300, Loss: 0.27152490556240083\n",
      "Epoch 19, Batch 350, Loss: 0.28363960087299345\n",
      "Epoch 19, Batch 400, Loss: 0.27133882939815523\n",
      "Epoch 19, Batch 450, Loss: 0.27296722292900083\n",
      "Epoch 19, Batch 500, Loss: 0.28485196828842163\n",
      "Epoch 19, Batch 550, Loss: 0.2762436720728874\n",
      "Epoch 19, Batch 600, Loss: 0.26655037492513656\n",
      "Epoch 19, Batch 650, Loss: 0.28140508741140363\n",
      "Epoch 19, Batch 700, Loss: 0.2759403344988823\n",
      "Epoch 19, Batch 750, Loss: 0.2656816238164902\n",
      "Epoch 19, Batch 800, Loss: 0.27360640436410905\n",
      "Epoch 19, Batch 850, Loss: 0.26440310657024385\n",
      "Epoch 19, Batch 900, Loss: 0.2789289075136185\n",
      "Epoch 20, Batch 50, Loss: 0.26044520825147627\n",
      "Epoch 20, Batch 100, Loss: 0.26956792175769806\n",
      "Epoch 20, Batch 150, Loss: 0.2678191816806793\n",
      "Epoch 20, Batch 200, Loss: 0.2576397022604942\n",
      "Epoch 20, Batch 250, Loss: 0.29256444960832595\n",
      "Epoch 20, Batch 300, Loss: 0.2609301459789276\n",
      "Epoch 20, Batch 350, Loss: 0.2721948605775833\n",
      "Epoch 20, Batch 400, Loss: 0.2772046473622322\n",
      "Epoch 20, Batch 450, Loss: 0.2583495408296585\n",
      "Epoch 20, Batch 500, Loss: 0.28822218596935273\n",
      "Epoch 20, Batch 550, Loss: 0.2841817477345467\n",
      "Epoch 20, Batch 600, Loss: 0.29400497615337373\n",
      "Epoch 20, Batch 650, Loss: 0.28138356149196625\n",
      "Epoch 20, Batch 700, Loss: 0.28200056821107866\n",
      "Epoch 20, Batch 750, Loss: 0.27886721551418303\n",
      "Epoch 20, Batch 800, Loss: 0.28319460809230806\n",
      "Epoch 20, Batch 850, Loss: 0.26866026401519777\n",
      "Epoch 20, Batch 900, Loss: 0.274362615942955\n",
      "Epoch 21, Batch 50, Loss: 0.2760061383247375\n",
      "Epoch 21, Batch 100, Loss: 0.26672594279050826\n",
      "Epoch 21, Batch 150, Loss: 0.2690945103764534\n",
      "Epoch 21, Batch 200, Loss: 0.24819368466734887\n",
      "Epoch 21, Batch 250, Loss: 0.28574891477823255\n",
      "Epoch 21, Batch 300, Loss: 0.2708398303389549\n",
      "Epoch 21, Batch 350, Loss: 0.28011902391910554\n",
      "Epoch 21, Batch 400, Loss: 0.2834633758664131\n",
      "Epoch 21, Batch 450, Loss: 0.27704004138708116\n",
      "Epoch 21, Batch 500, Loss: 0.2607670924067497\n",
      "Epoch 21, Batch 550, Loss: 0.27858222275972366\n",
      "Epoch 21, Batch 600, Loss: 0.2555262830853462\n",
      "Epoch 21, Batch 650, Loss: 0.27945280104875564\n",
      "Epoch 21, Batch 700, Loss: 0.2838153964281082\n",
      "Epoch 21, Batch 750, Loss: 0.2694841820001602\n",
      "Epoch 21, Batch 800, Loss: 0.2896841236948967\n",
      "Epoch 21, Batch 850, Loss: 0.27851365000009537\n",
      "Epoch 21, Batch 900, Loss: 0.2669474232196808\n",
      "Epoch 22, Batch 50, Loss: 0.2629663521051407\n",
      "Epoch 22, Batch 100, Loss: 0.2714698639512062\n",
      "Epoch 22, Batch 150, Loss: 0.28212429851293563\n",
      "Epoch 22, Batch 200, Loss: 0.2562639981508255\n",
      "Epoch 22, Batch 250, Loss: 0.27562625557184217\n",
      "Epoch 22, Batch 300, Loss: 0.2727764865756035\n",
      "Epoch 22, Batch 350, Loss: 0.26129992127418517\n",
      "Epoch 22, Batch 400, Loss: 0.26334548950195313\n",
      "Epoch 22, Batch 450, Loss: 0.27416061013937\n",
      "Epoch 22, Batch 500, Loss: 0.30066331326961515\n",
      "Epoch 22, Batch 550, Loss: 0.2728336930274963\n",
      "Epoch 22, Batch 600, Loss: 0.2567886170744896\n",
      "Epoch 22, Batch 650, Loss: 0.27993087232112884\n",
      "Epoch 22, Batch 700, Loss: 0.26907178610563276\n",
      "Epoch 22, Batch 750, Loss: 0.26414793491363525\n",
      "Epoch 22, Batch 800, Loss: 0.2614882290363312\n",
      "Epoch 22, Batch 850, Loss: 0.2540364146232605\n",
      "Epoch 22, Batch 900, Loss: 0.26976276010274886\n",
      "Epoch 23, Batch 50, Loss: 0.28462913393974304\n",
      "Epoch 23, Batch 100, Loss: 0.2711827298998833\n",
      "Epoch 23, Batch 150, Loss: 0.25388271242380145\n",
      "Epoch 23, Batch 200, Loss: 0.2618200835585594\n",
      "Epoch 23, Batch 250, Loss: 0.2849719354510307\n",
      "Epoch 23, Batch 300, Loss: 0.26997950553894046\n",
      "Epoch 23, Batch 350, Loss: 0.28070003658533094\n",
      "Epoch 23, Batch 400, Loss: 0.26247361063957214\n",
      "Epoch 23, Batch 450, Loss: 0.2674701789021492\n",
      "Epoch 23, Batch 500, Loss: 0.2611349007487297\n",
      "Epoch 23, Batch 550, Loss: 0.2781570267677307\n",
      "Epoch 23, Batch 600, Loss: 0.26282223403453825\n",
      "Epoch 23, Batch 650, Loss: 0.27089938402175906\n",
      "Epoch 23, Batch 700, Loss: 0.2505546897649765\n",
      "Epoch 23, Batch 750, Loss: 0.25730767995119097\n",
      "Epoch 23, Batch 800, Loss: 0.2777785187959671\n",
      "Epoch 23, Batch 850, Loss: 0.26195321917533876\n",
      "Epoch 23, Batch 900, Loss: 0.2714633905887604\n",
      "Epoch 24, Batch 50, Loss: 0.28465366750955584\n",
      "Epoch 24, Batch 100, Loss: 0.2729086446762085\n",
      "Epoch 24, Batch 150, Loss: 0.25919772624969484\n",
      "Epoch 24, Batch 200, Loss: 0.2817953708767891\n",
      "Epoch 24, Batch 250, Loss: 0.26040794074535367\n",
      "Epoch 24, Batch 300, Loss: 0.25649229168891907\n",
      "Epoch 24, Batch 350, Loss: 0.2743648998439312\n",
      "Epoch 24, Batch 400, Loss: 0.2548649498820305\n",
      "Epoch 24, Batch 450, Loss: 0.2524273517727852\n",
      "Epoch 24, Batch 500, Loss: 0.27215484231710435\n",
      "Epoch 24, Batch 550, Loss: 0.2668814831972122\n",
      "Epoch 24, Batch 600, Loss: 0.27303791403770444\n",
      "Epoch 24, Batch 650, Loss: 0.25845827519893644\n",
      "Epoch 24, Batch 700, Loss: 0.26257289230823516\n",
      "Epoch 24, Batch 750, Loss: 0.27667067140340806\n",
      "Epoch 24, Batch 800, Loss: 0.2834824311733246\n",
      "Epoch 24, Batch 850, Loss: 0.2646723210811615\n",
      "Epoch 24, Batch 900, Loss: 0.2693678730726242\n",
      "Epoch 25, Batch 50, Loss: 0.2647970339655876\n",
      "Epoch 25, Batch 100, Loss: 0.26322591125965117\n",
      "Epoch 25, Batch 150, Loss: 0.27159045457839964\n",
      "Epoch 25, Batch 200, Loss: 0.2478869765996933\n",
      "Epoch 25, Batch 250, Loss: 0.2655560654401779\n",
      "Epoch 25, Batch 300, Loss: 0.2614888766407967\n",
      "Epoch 25, Batch 350, Loss: 0.26774069786071775\n",
      "Epoch 25, Batch 400, Loss: 0.26496684074401855\n",
      "Epoch 25, Batch 450, Loss: 0.2751835483312607\n",
      "Epoch 25, Batch 500, Loss: 0.26619317531585696\n",
      "Epoch 25, Batch 550, Loss: 0.26106772243976595\n",
      "Epoch 25, Batch 600, Loss: 0.2686537054181099\n",
      "Epoch 25, Batch 650, Loss: 0.2642572715878487\n",
      "Epoch 25, Batch 700, Loss: 0.2670558133721352\n",
      "Epoch 25, Batch 750, Loss: 0.26091563552618025\n",
      "Epoch 25, Batch 800, Loss: 0.29025721818208694\n",
      "Epoch 25, Batch 850, Loss: 0.2750013789534569\n",
      "Epoch 25, Batch 900, Loss: 0.265023033618927\n",
      "Accuracy on test set: 0.7129%\n",
      "Fitting for combination 62\n",
      "784\n",
      "4\n",
      "10\n",
      "[40, 10, 10, 10, 10]\n",
      "True\n",
      "['relu', 'sigmoid', 'relu', 'relu']\n",
      "Adam\n",
      "0.01\n",
      "0.3\n",
      "CrossEntropyLoss\n",
      "200\n",
      "25\n",
      "0\n",
      "Epoch 1, Batch 200, Loss: 4.607247993946076\n",
      "Epoch 1, Batch 400, Loss: 4.605562932491303\n",
      "Epoch 1, Batch 600, Loss: 4.60515899181366\n",
      "Epoch 1, Batch 800, Loss: 4.605652933120727\n",
      "Epoch 2, Batch 200, Loss: 4.606352047920227\n",
      "Epoch 2, Batch 400, Loss: 4.605316212177277\n",
      "Epoch 2, Batch 600, Loss: 4.605889420509339\n",
      "Epoch 2, Batch 800, Loss: 4.605133335590363\n",
      "Epoch 3, Batch 200, Loss: 4.606159710884095\n",
      "Epoch 3, Batch 400, Loss: 4.605897612571717\n",
      "Epoch 3, Batch 600, Loss: 4.6054986119270325\n",
      "Epoch 3, Batch 800, Loss: 4.606032040119171\n",
      "Epoch 4, Batch 200, Loss: 4.60625321149826\n",
      "Epoch 4, Batch 400, Loss: 4.605727291107177\n",
      "Epoch 4, Batch 600, Loss: 4.605803623199463\n",
      "Epoch 4, Batch 800, Loss: 4.6054431605339055\n",
      "Epoch 5, Batch 200, Loss: 4.606269857883453\n",
      "Epoch 5, Batch 400, Loss: 4.605172817707062\n",
      "Epoch 5, Batch 600, Loss: 4.605885446071625\n",
      "Epoch 5, Batch 800, Loss: 4.605127758979798\n",
      "Epoch 6, Batch 200, Loss: 4.605379796028137\n",
      "Epoch 6, Batch 400, Loss: 4.605977876186371\n",
      "Epoch 6, Batch 600, Loss: 4.606134369373321\n",
      "Epoch 6, Batch 800, Loss: 4.605898776054382\n",
      "Epoch 7, Batch 200, Loss: 4.604947741031647\n",
      "Epoch 7, Batch 400, Loss: 4.606026129722595\n",
      "Epoch 7, Batch 600, Loss: 4.605459711551666\n",
      "Epoch 7, Batch 800, Loss: 4.606301734447479\n",
      "Epoch 8, Batch 200, Loss: 4.605422463417053\n",
      "Epoch 8, Batch 400, Loss: 4.6062852573394775\n",
      "Epoch 8, Batch 600, Loss: 4.605440721511841\n",
      "Epoch 8, Batch 800, Loss: 4.605754361152649\n",
      "Epoch 9, Batch 200, Loss: 4.605473992824554\n",
      "Epoch 9, Batch 400, Loss: 4.6053111743927\n",
      "Epoch 9, Batch 600, Loss: 4.605793027877808\n",
      "Epoch 9, Batch 800, Loss: 4.606075038909912\n",
      "Epoch 10, Batch 200, Loss: 4.605678238868713\n",
      "Epoch 10, Batch 400, Loss: 4.605511381626129\n",
      "Epoch 10, Batch 600, Loss: 4.6058124876022335\n",
      "Epoch 10, Batch 800, Loss: 4.605374608039856\n",
      "Epoch 11, Batch 200, Loss: 4.605427212715149\n",
      "Epoch 11, Batch 400, Loss: 4.6055665206909175\n",
      "Epoch 11, Batch 600, Loss: 4.605918841362\n",
      "Epoch 11, Batch 800, Loss: 4.605436406135559\n",
      "Epoch 12, Batch 200, Loss: 4.605172164440155\n",
      "Epoch 12, Batch 400, Loss: 4.606092691421509\n",
      "Epoch 12, Batch 600, Loss: 4.605452227592468\n",
      "Epoch 12, Batch 800, Loss: 4.605654733181\n",
      "Epoch 13, Batch 200, Loss: 4.6055515766143795\n",
      "Epoch 13, Batch 400, Loss: 4.605954561233521\n",
      "Epoch 13, Batch 600, Loss: 4.606115334033966\n",
      "Epoch 13, Batch 800, Loss: 4.60663688659668\n",
      "Epoch 14, Batch 200, Loss: 4.605856792926788\n",
      "Epoch 14, Batch 400, Loss: 4.605429105758667\n",
      "Epoch 14, Batch 600, Loss: 4.606182525157928\n",
      "Epoch 14, Batch 800, Loss: 4.60551144361496\n",
      "Epoch 15, Batch 200, Loss: 4.605342974662781\n",
      "Epoch 15, Batch 400, Loss: 4.605803055763245\n",
      "Epoch 15, Batch 600, Loss: 4.605784113407135\n",
      "Epoch 15, Batch 800, Loss: 4.606004543304444\n",
      "Epoch 16, Batch 200, Loss: 4.605804343223571\n",
      "Epoch 16, Batch 400, Loss: 4.6059752011299135\n",
      "Epoch 16, Batch 600, Loss: 4.605890476703644\n",
      "Epoch 16, Batch 800, Loss: 4.605841672420501\n",
      "Epoch 17, Batch 200, Loss: 4.6054708290100095\n",
      "Epoch 17, Batch 400, Loss: 4.605900247097015\n",
      "Epoch 17, Batch 600, Loss: 4.605655488967895\n",
      "Epoch 17, Batch 800, Loss: 4.605505840778351\n",
      "Epoch 18, Batch 200, Loss: 4.605393629074097\n",
      "Epoch 18, Batch 400, Loss: 4.605880632400512\n",
      "Epoch 18, Batch 600, Loss: 4.605573680400848\n",
      "Epoch 18, Batch 800, Loss: 4.605677371025085\n",
      "Epoch 19, Batch 200, Loss: 4.605297017097473\n",
      "Epoch 19, Batch 400, Loss: 4.605845818519592\n",
      "Epoch 19, Batch 600, Loss: 4.605421805381775\n",
      "Epoch 19, Batch 800, Loss: 4.605816156864166\n",
      "Epoch 20, Batch 200, Loss: 4.604973595142365\n",
      "Epoch 20, Batch 400, Loss: 4.605461399555207\n",
      "Epoch 20, Batch 600, Loss: 4.605921676158905\n",
      "Epoch 20, Batch 800, Loss: 4.605119917392731\n",
      "Epoch 21, Batch 200, Loss: 4.605461385250091\n",
      "Epoch 21, Batch 400, Loss: 4.606431515216827\n",
      "Epoch 21, Batch 600, Loss: 4.605499444007873\n",
      "Epoch 21, Batch 800, Loss: 4.6052882599830625\n",
      "Epoch 22, Batch 200, Loss: 4.60555771112442\n",
      "Epoch 22, Batch 400, Loss: 4.604974610805511\n",
      "Epoch 22, Batch 600, Loss: 4.605566926002503\n",
      "Epoch 22, Batch 800, Loss: 4.605303447246552\n",
      "Epoch 23, Batch 200, Loss: 4.6063423299789426\n",
      "Epoch 23, Batch 400, Loss: 4.606050112247467\n",
      "Epoch 23, Batch 600, Loss: 4.605761461257934\n",
      "Epoch 23, Batch 800, Loss: 4.605679216384888\n",
      "Epoch 24, Batch 200, Loss: 4.605547475814819\n",
      "Epoch 24, Batch 400, Loss: 4.6058864402771\n",
      "Epoch 24, Batch 600, Loss: 4.605892143249512\n",
      "Epoch 24, Batch 800, Loss: 4.605433113574982\n",
      "Epoch 25, Batch 200, Loss: 4.606306958198547\n",
      "Epoch 25, Batch 400, Loss: 4.605891008377075\n",
      "Epoch 25, Batch 600, Loss: 4.605584993362426\n",
      "Epoch 25, Batch 800, Loss: 4.6059404826164245\n",
      "Accuracy on test set: 0.1%\n",
      "Fitting for combination 63\n",
      "784\n",
      "4\n",
      "10\n",
      "[40, 10, 10, 10, 10]\n",
      "True\n",
      "['relu', 'sigmoid', 'relu', 'relu']\n",
      "SGD\n",
      "0.01\n",
      "0.3\n",
      "CrossEntropyLoss\n",
      "100\n",
      "25\n",
      "0\n",
      "Epoch 1, Batch 100, Loss: 2.318822376728058\n",
      "Epoch 1, Batch 200, Loss: 2.3049795794487\n",
      "Epoch 1, Batch 300, Loss: 2.3037649416923522\n",
      "Epoch 1, Batch 400, Loss: 2.302964825630188\n",
      "Epoch 1, Batch 500, Loss: 2.302900187969208\n",
      "Epoch 1, Batch 600, Loss: 2.3025700402259828\n",
      "Epoch 1, Batch 700, Loss: 2.302868077754974\n",
      "Epoch 1, Batch 800, Loss: 2.3026715183258055\n",
      "Epoch 1, Batch 900, Loss: 2.3026078367233276\n",
      "Epoch 2, Batch 100, Loss: 2.302504062652588\n",
      "Epoch 2, Batch 200, Loss: 2.302675998210907\n",
      "Epoch 2, Batch 300, Loss: 2.3026124310493468\n",
      "Epoch 2, Batch 400, Loss: 2.3025917840003967\n",
      "Epoch 2, Batch 500, Loss: 2.3026556754112244\n",
      "Epoch 2, Batch 600, Loss: 2.3026181769371035\n",
      "Epoch 2, Batch 700, Loss: 2.302630705833435\n",
      "Epoch 2, Batch 800, Loss: 2.302641122341156\n",
      "Epoch 2, Batch 900, Loss: 2.302622549533844\n",
      "Epoch 3, Batch 100, Loss: 2.302609498500824\n",
      "Epoch 3, Batch 200, Loss: 2.302622847557068\n",
      "Epoch 3, Batch 300, Loss: 2.302627487182617\n",
      "Epoch 3, Batch 400, Loss: 2.3026056909561157\n",
      "Epoch 3, Batch 500, Loss: 2.3026271724700926\n",
      "Epoch 3, Batch 600, Loss: 2.302620849609375\n",
      "Epoch 3, Batch 700, Loss: 2.3026494860649107\n",
      "Epoch 3, Batch 800, Loss: 2.3026181626319886\n",
      "Epoch 3, Batch 900, Loss: 2.3026411938667297\n",
      "Epoch 4, Batch 100, Loss: 2.302600393295288\n",
      "Epoch 4, Batch 200, Loss: 2.3025669932365416\n",
      "Epoch 4, Batch 300, Loss: 2.30265855550766\n",
      "Epoch 4, Batch 400, Loss: 2.302596063613892\n",
      "Epoch 4, Batch 500, Loss: 2.302605223655701\n",
      "Epoch 4, Batch 600, Loss: 2.302604868412018\n",
      "Epoch 4, Batch 700, Loss: 2.3026603484153747\n",
      "Epoch 4, Batch 800, Loss: 2.3026216983795167\n",
      "Epoch 4, Batch 900, Loss: 2.3026401734352113\n",
      "Epoch 5, Batch 100, Loss: 2.302620060443878\n",
      "Epoch 5, Batch 200, Loss: 2.3025560641288756\n",
      "Epoch 5, Batch 300, Loss: 2.3025236344337463\n",
      "Epoch 5, Batch 400, Loss: 2.302647125720978\n",
      "Epoch 5, Batch 500, Loss: 2.3026580953598024\n",
      "Epoch 5, Batch 600, Loss: 2.3026342248916625\n",
      "Epoch 5, Batch 700, Loss: 2.3026524066925047\n",
      "Epoch 5, Batch 800, Loss: 2.3025921273231504\n",
      "Epoch 5, Batch 900, Loss: 2.3026249742507936\n",
      "Epoch 6, Batch 100, Loss: 2.3025851345062254\n",
      "Epoch 6, Batch 200, Loss: 2.302580106258392\n",
      "Epoch 6, Batch 300, Loss: 2.3025919604301452\n",
      "Epoch 6, Batch 400, Loss: 2.302666571140289\n",
      "Epoch 6, Batch 500, Loss: 2.302627203464508\n",
      "Epoch 6, Batch 600, Loss: 2.302611927986145\n",
      "Epoch 6, Batch 700, Loss: 2.30265780210495\n",
      "Epoch 6, Batch 800, Loss: 2.302589433193207\n",
      "Epoch 6, Batch 900, Loss: 2.302606794834137\n",
      "Epoch 7, Batch 100, Loss: 2.3026351046562197\n",
      "Epoch 7, Batch 200, Loss: 2.302601912021637\n",
      "Epoch 7, Batch 300, Loss: 2.3025149440765382\n",
      "Epoch 7, Batch 400, Loss: 2.3026588225364684\n",
      "Epoch 7, Batch 500, Loss: 2.3026376581192016\n",
      "Epoch 7, Batch 600, Loss: 2.302576892375946\n",
      "Epoch 7, Batch 700, Loss: 2.302638370990753\n",
      "Epoch 7, Batch 800, Loss: 2.3025615358352662\n",
      "Epoch 7, Batch 900, Loss: 2.3026156973838807\n",
      "Epoch 8, Batch 100, Loss: 2.3025974798202515\n",
      "Epoch 8, Batch 200, Loss: 2.3026623368263244\n",
      "Epoch 8, Batch 300, Loss: 2.3025779628753664\n",
      "Epoch 8, Batch 400, Loss: 2.3026060581207277\n",
      "Epoch 8, Batch 500, Loss: 2.302650287151337\n",
      "Epoch 8, Batch 600, Loss: 2.3025899720191956\n",
      "Epoch 8, Batch 700, Loss: 2.3026571750640867\n",
      "Epoch 8, Batch 800, Loss: 2.302629337310791\n",
      "Epoch 8, Batch 900, Loss: 2.3025943350791933\n",
      "Epoch 9, Batch 100, Loss: 2.3026319932937622\n",
      "Epoch 9, Batch 200, Loss: 2.302599325180054\n",
      "Epoch 9, Batch 300, Loss: 2.302412760257721\n",
      "Epoch 9, Batch 400, Loss: 2.302667565345764\n",
      "Epoch 9, Batch 500, Loss: 2.3026127076148986\n",
      "Epoch 9, Batch 600, Loss: 2.3026398706436155\n",
      "Epoch 9, Batch 700, Loss: 2.3026532173156737\n",
      "Epoch 9, Batch 800, Loss: 2.302569465637207\n",
      "Epoch 9, Batch 900, Loss: 2.3026232957839965\n",
      "Epoch 10, Batch 100, Loss: 2.3026028299331665\n",
      "Epoch 10, Batch 200, Loss: 2.302611701488495\n",
      "Epoch 10, Batch 300, Loss: 2.302652208805084\n",
      "Epoch 10, Batch 400, Loss: 2.302538905143738\n",
      "Epoch 10, Batch 500, Loss: 2.302591781616211\n",
      "Epoch 10, Batch 600, Loss: 2.302651445865631\n",
      "Epoch 10, Batch 700, Loss: 2.302623724937439\n",
      "Epoch 10, Batch 800, Loss: 2.3026056623458864\n",
      "Epoch 10, Batch 900, Loss: 2.3025522327423094\n",
      "Epoch 11, Batch 100, Loss: 2.3025923776626587\n",
      "Epoch 11, Batch 200, Loss: 2.3026384043693544\n",
      "Epoch 11, Batch 300, Loss: 2.3026333093643188\n",
      "Epoch 11, Batch 400, Loss: 2.302622203826904\n",
      "Epoch 11, Batch 500, Loss: 2.3025985503196718\n",
      "Epoch 11, Batch 600, Loss: 2.302648046016693\n",
      "Epoch 11, Batch 700, Loss: 2.302633514404297\n",
      "Epoch 11, Batch 800, Loss: 2.3026105737686158\n",
      "Epoch 11, Batch 900, Loss: 2.3026589727401734\n",
      "Epoch 12, Batch 100, Loss: 2.3026571607589723\n",
      "Epoch 12, Batch 200, Loss: 2.3025719904899598\n",
      "Epoch 12, Batch 300, Loss: 2.3026580238342285\n",
      "Epoch 12, Batch 400, Loss: 2.302585141658783\n",
      "Epoch 12, Batch 500, Loss: 2.3025307774543764\n",
      "Epoch 12, Batch 600, Loss: 2.302628755569458\n",
      "Epoch 12, Batch 700, Loss: 2.302664997577667\n",
      "Epoch 12, Batch 800, Loss: 2.302628562450409\n",
      "Epoch 12, Batch 900, Loss: 2.3026008439064025\n",
      "Epoch 13, Batch 100, Loss: 2.302620596885681\n",
      "Epoch 13, Batch 200, Loss: 2.3025623321533204\n",
      "Epoch 13, Batch 300, Loss: 2.302566103935242\n",
      "Epoch 13, Batch 400, Loss: 2.3026379561424255\n",
      "Epoch 13, Batch 500, Loss: 2.302626166343689\n",
      "Epoch 13, Batch 600, Loss: 2.3026338267326354\n",
      "Epoch 13, Batch 700, Loss: 2.3026136565208435\n",
      "Epoch 13, Batch 800, Loss: 2.3026364469528198\n",
      "Epoch 13, Batch 900, Loss: 2.302629082202911\n",
      "Epoch 14, Batch 100, Loss: 2.302651808261871\n",
      "Epoch 14, Batch 200, Loss: 2.3026367163658144\n",
      "Epoch 14, Batch 300, Loss: 2.3026126170158387\n",
      "Epoch 14, Batch 400, Loss: 2.3026211929321287\n",
      "Epoch 14, Batch 500, Loss: 2.3026473188400267\n",
      "Epoch 14, Batch 600, Loss: 2.3026337099075316\n",
      "Epoch 14, Batch 700, Loss: 2.3026026916503906\n",
      "Epoch 14, Batch 800, Loss: 2.3026127552986146\n",
      "Epoch 14, Batch 900, Loss: 2.302658824920654\n",
      "Epoch 15, Batch 100, Loss: 2.302605836391449\n",
      "Epoch 15, Batch 200, Loss: 2.3026384139060974\n",
      "Epoch 15, Batch 300, Loss: 2.302638373374939\n",
      "Epoch 15, Batch 400, Loss: 2.302639648914337\n",
      "Epoch 15, Batch 500, Loss: 2.302585620880127\n",
      "Epoch 15, Batch 600, Loss: 2.302643611431122\n",
      "Epoch 15, Batch 700, Loss: 2.302638792991638\n",
      "Epoch 15, Batch 800, Loss: 2.302633068561554\n",
      "Epoch 15, Batch 900, Loss: 2.3026332330703734\n",
      "Epoch 16, Batch 100, Loss: 2.3025770473480223\n",
      "Epoch 16, Batch 200, Loss: 2.3026449155807494\n",
      "Epoch 16, Batch 300, Loss: 2.302579786777496\n",
      "Epoch 16, Batch 400, Loss: 2.302592158317566\n",
      "Epoch 16, Batch 500, Loss: 2.302660484313965\n",
      "Epoch 16, Batch 600, Loss: 2.302596528530121\n",
      "Epoch 16, Batch 700, Loss: 2.302641966342926\n",
      "Epoch 16, Batch 800, Loss: 2.302643187046051\n",
      "Epoch 16, Batch 900, Loss: 2.302634103298187\n",
      "Epoch 17, Batch 100, Loss: 2.3026165890693666\n",
      "Epoch 17, Batch 200, Loss: 2.302578694820404\n",
      "Epoch 17, Batch 300, Loss: 2.302571938037872\n",
      "Epoch 17, Batch 400, Loss: 2.3026672482490538\n",
      "Epoch 17, Batch 500, Loss: 2.302612566947937\n",
      "Epoch 17, Batch 600, Loss: 2.3025756955146788\n",
      "Epoch 17, Batch 700, Loss: 2.3026882719993593\n",
      "Epoch 17, Batch 800, Loss: 2.3026389908790588\n",
      "Epoch 17, Batch 900, Loss: 2.3026451873779297\n",
      "Epoch 18, Batch 100, Loss: 2.302622051239014\n",
      "Epoch 18, Batch 200, Loss: 2.3026519227027893\n",
      "Epoch 18, Batch 300, Loss: 2.302607572078705\n",
      "Epoch 18, Batch 400, Loss: 2.3025968170166013\n",
      "Epoch 18, Batch 500, Loss: 2.302644271850586\n",
      "Epoch 18, Batch 600, Loss: 2.302618477344513\n",
      "Epoch 18, Batch 700, Loss: 2.302618534564972\n",
      "Epoch 18, Batch 800, Loss: 2.302633373737335\n",
      "Epoch 18, Batch 900, Loss: 2.3026134395599365\n",
      "Epoch 19, Batch 100, Loss: 2.3026728057861328\n",
      "Epoch 19, Batch 200, Loss: 2.3025960159301757\n",
      "Epoch 19, Batch 300, Loss: 2.3026655864715577\n",
      "Epoch 19, Batch 400, Loss: 2.3026551175117493\n",
      "Epoch 19, Batch 500, Loss: 2.3026368188858033\n",
      "Epoch 19, Batch 600, Loss: 2.302622559070587\n",
      "Epoch 19, Batch 700, Loss: 2.302663071155548\n",
      "Epoch 19, Batch 800, Loss: 2.3026265025138857\n",
      "Epoch 19, Batch 900, Loss: 2.3026312518119814\n",
      "Epoch 20, Batch 100, Loss: 2.3025698971748354\n",
      "Epoch 20, Batch 200, Loss: 2.3026140904426575\n",
      "Epoch 20, Batch 300, Loss: 2.3026370882987974\n",
      "Epoch 20, Batch 400, Loss: 2.302648169994354\n",
      "Epoch 20, Batch 500, Loss: 2.3026099586486817\n",
      "Epoch 20, Batch 600, Loss: 2.3026259899139405\n",
      "Epoch 20, Batch 700, Loss: 2.3026438426971434\n",
      "Epoch 20, Batch 800, Loss: 2.302622504234314\n",
      "Epoch 20, Batch 900, Loss: 2.302662250995636\n",
      "Epoch 21, Batch 100, Loss: 2.302598621845245\n",
      "Epoch 21, Batch 200, Loss: 2.302619659900665\n",
      "Epoch 21, Batch 300, Loss: 2.3026352882385255\n",
      "Epoch 21, Batch 400, Loss: 2.302572410106659\n",
      "Epoch 21, Batch 500, Loss: 2.302666485309601\n",
      "Epoch 21, Batch 600, Loss: 2.302544405460358\n",
      "Epoch 21, Batch 700, Loss: 2.3026257157325745\n",
      "Epoch 21, Batch 800, Loss: 2.302659134864807\n",
      "Epoch 21, Batch 900, Loss: 2.302577483654022\n",
      "Epoch 22, Batch 100, Loss: 2.3026285409927367\n",
      "Epoch 22, Batch 200, Loss: 2.302619152069092\n",
      "Epoch 22, Batch 300, Loss: 2.3026404428482055\n",
      "Epoch 22, Batch 400, Loss: 2.302590672969818\n",
      "Epoch 22, Batch 500, Loss: 2.302601616382599\n",
      "Epoch 22, Batch 600, Loss: 2.302604904174805\n",
      "Epoch 22, Batch 700, Loss: 2.302603695392609\n",
      "Epoch 22, Batch 800, Loss: 2.3026419115066528\n",
      "Epoch 22, Batch 900, Loss: 2.3026305508613585\n",
      "Epoch 23, Batch 100, Loss: 2.302623300552368\n",
      "Epoch 23, Batch 200, Loss: 2.3025970244407654\n",
      "Epoch 23, Batch 300, Loss: 2.302591288089752\n",
      "Epoch 23, Batch 400, Loss: 2.3026869964599608\n",
      "Epoch 23, Batch 500, Loss: 2.3026220870018004\n",
      "Epoch 23, Batch 600, Loss: 2.302633671760559\n",
      "Epoch 23, Batch 700, Loss: 2.302630031108856\n",
      "Epoch 23, Batch 800, Loss: 2.302586925029755\n",
      "Epoch 23, Batch 900, Loss: 2.302635681629181\n",
      "Epoch 24, Batch 100, Loss: 2.302588791847229\n",
      "Epoch 24, Batch 200, Loss: 2.302654662132263\n",
      "Epoch 24, Batch 300, Loss: 2.3026204442977907\n",
      "Epoch 24, Batch 400, Loss: 2.3026321601867674\n",
      "Epoch 24, Batch 500, Loss: 2.3025904536247253\n",
      "Epoch 24, Batch 600, Loss: 2.3026240634918214\n",
      "Epoch 24, Batch 700, Loss: 2.3026664686203\n",
      "Epoch 24, Batch 800, Loss: 2.3026206421852113\n",
      "Epoch 24, Batch 900, Loss: 2.302616763114929\n",
      "Epoch 25, Batch 100, Loss: 2.3024935626983645\n",
      "Epoch 25, Batch 200, Loss: 2.302694456577301\n",
      "Epoch 25, Batch 300, Loss: 2.3026545214653016\n",
      "Epoch 25, Batch 400, Loss: 2.3026221656799315\n",
      "Epoch 25, Batch 500, Loss: 2.3026417899131775\n",
      "Epoch 25, Batch 600, Loss: 2.3026144123077392\n",
      "Epoch 25, Batch 700, Loss: 2.3026502776145934\n",
      "Epoch 25, Batch 800, Loss: 2.3026275777816774\n",
      "Epoch 25, Batch 900, Loss: 2.3026426815986634\n",
      "Accuracy on test set: 0.1%\n",
      "Fitting for combination 64\n",
      "784\n",
      "4\n",
      "10\n",
      "[40, 10, 10, 20, 10]\n",
      "True\n",
      "['relu', 'sigmoid', 'relu', 'tanh']\n",
      "Adam\n",
      "0.03\n",
      "0\n",
      "CrossEntropyLoss\n",
      "100\n",
      "25\n",
      "0\n",
      "Epoch 1, Batch 100, Loss: 1.6033887445926667\n",
      "Epoch 1, Batch 200, Loss: 1.3312491047382355\n",
      "Epoch 1, Batch 300, Loss: 1.420898085832596\n",
      "Epoch 1, Batch 400, Loss: 1.4023616695404053\n",
      "Epoch 1, Batch 500, Loss: 1.310702419281006\n",
      "Epoch 1, Batch 600, Loss: 1.238680717945099\n",
      "Epoch 1, Batch 700, Loss: 1.3242303979396821\n",
      "Epoch 1, Batch 800, Loss: 1.3391696047782897\n",
      "Epoch 1, Batch 900, Loss: 1.4164669251441955\n",
      "Epoch 2, Batch 100, Loss: 1.4839294648170471\n",
      "Epoch 2, Batch 200, Loss: 1.5201243448257447\n",
      "Epoch 2, Batch 300, Loss: 1.614078254699707\n",
      "Epoch 2, Batch 400, Loss: 1.60055703997612\n",
      "Epoch 2, Batch 500, Loss: 1.5869308423995971\n",
      "Epoch 2, Batch 600, Loss: 1.531561335325241\n",
      "Epoch 2, Batch 700, Loss: 1.3652410340309142\n",
      "Epoch 2, Batch 800, Loss: 1.3515230059623717\n",
      "Epoch 2, Batch 900, Loss: 1.486377967596054\n",
      "Epoch 3, Batch 100, Loss: 1.695530319213867\n",
      "Epoch 3, Batch 200, Loss: 1.4997429263591766\n",
      "Epoch 3, Batch 300, Loss: 1.4343847715854645\n",
      "Epoch 3, Batch 400, Loss: 1.424264553785324\n",
      "Epoch 3, Batch 500, Loss: 1.405478059053421\n",
      "Epoch 3, Batch 600, Loss: 1.3179150807857514\n",
      "Epoch 3, Batch 700, Loss: 1.3242863094806672\n",
      "Epoch 3, Batch 800, Loss: 1.4238136851787566\n",
      "Epoch 3, Batch 900, Loss: 1.3571879303455352\n",
      "Epoch 4, Batch 100, Loss: 1.323987958431244\n",
      "Epoch 4, Batch 200, Loss: 1.3617782270908356\n",
      "Epoch 4, Batch 300, Loss: 1.5581382167339326\n",
      "Epoch 4, Batch 400, Loss: 1.4354920899868011\n",
      "Epoch 4, Batch 500, Loss: 1.3535438096523285\n",
      "Epoch 4, Batch 600, Loss: 1.312966731786728\n",
      "Epoch 4, Batch 700, Loss: 1.2678341937065125\n",
      "Epoch 4, Batch 800, Loss: 1.270264585018158\n",
      "Epoch 4, Batch 900, Loss: 1.3197125685214997\n",
      "Epoch 5, Batch 100, Loss: 1.38189981341362\n",
      "Epoch 5, Batch 200, Loss: 1.4381517887115478\n",
      "Epoch 5, Batch 300, Loss: 1.3995992183685302\n",
      "Epoch 5, Batch 400, Loss: 1.2894428968429565\n",
      "Epoch 5, Batch 500, Loss: 1.389784151315689\n",
      "Epoch 5, Batch 600, Loss: 1.4647602915763855\n",
      "Epoch 5, Batch 700, Loss: 1.3446010541915894\n",
      "Epoch 5, Batch 800, Loss: 1.4126805365085602\n",
      "Epoch 5, Batch 900, Loss: 1.4242470288276672\n",
      "Epoch 6, Batch 100, Loss: 1.3762357771396636\n",
      "Epoch 6, Batch 200, Loss: 1.4516015326976777\n",
      "Epoch 6, Batch 300, Loss: 1.4393670451641083\n",
      "Epoch 6, Batch 400, Loss: 1.4975982701778412\n",
      "Epoch 6, Batch 500, Loss: 1.4793576729297637\n",
      "Epoch 6, Batch 600, Loss: 1.384397532939911\n",
      "Epoch 6, Batch 700, Loss: 1.3454151141643524\n",
      "Epoch 6, Batch 800, Loss: 1.3704949641227722\n",
      "Epoch 6, Batch 900, Loss: 1.361131101846695\n",
      "Epoch 7, Batch 100, Loss: 1.2683755511045456\n",
      "Epoch 7, Batch 200, Loss: 1.2816162198781966\n",
      "Epoch 7, Batch 300, Loss: 1.6077774500846862\n",
      "Epoch 7, Batch 400, Loss: 1.4747435450553894\n",
      "Epoch 7, Batch 500, Loss: 1.4045522594451905\n",
      "Epoch 7, Batch 600, Loss: 1.5681568431854247\n",
      "Epoch 7, Batch 700, Loss: 1.72463969707489\n",
      "Epoch 7, Batch 800, Loss: 1.7389050102233887\n",
      "Epoch 7, Batch 900, Loss: 1.6890942072868347\n",
      "Epoch 8, Batch 100, Loss: 1.67515340924263\n",
      "Epoch 8, Batch 200, Loss: 1.6679859161376953\n",
      "Epoch 8, Batch 300, Loss: 1.7801367366313934\n",
      "Epoch 8, Batch 400, Loss: 1.7648144960403442\n",
      "Epoch 8, Batch 500, Loss: 1.7361636531352997\n",
      "Epoch 8, Batch 600, Loss: 1.7052765738964082\n",
      "Epoch 8, Batch 700, Loss: 1.7796906328201294\n",
      "Epoch 8, Batch 800, Loss: 1.8172799050807953\n",
      "Epoch 8, Batch 900, Loss: 1.818359889984131\n",
      "Epoch 9, Batch 100, Loss: 1.778335567712784\n",
      "Epoch 9, Batch 200, Loss: 1.7184834730625154\n",
      "Epoch 9, Batch 300, Loss: 1.5346459889411925\n",
      "Epoch 9, Batch 400, Loss: 1.5499293041229247\n",
      "Epoch 9, Batch 500, Loss: 1.4936458766460419\n",
      "Epoch 9, Batch 600, Loss: 1.4442931187152863\n",
      "Epoch 9, Batch 700, Loss: 1.4246933376789093\n",
      "Epoch 9, Batch 800, Loss: 1.5259337186813355\n",
      "Epoch 9, Batch 900, Loss: 1.6666775691509246\n",
      "Epoch 10, Batch 100, Loss: 1.6556777012348176\n",
      "Epoch 10, Batch 200, Loss: 1.6338434731960296\n",
      "Epoch 10, Batch 300, Loss: 1.6090542304515838\n",
      "Epoch 10, Batch 400, Loss: 1.6078457093238832\n",
      "Epoch 10, Batch 500, Loss: 1.6143244767189027\n",
      "Epoch 10, Batch 600, Loss: 1.6128580486774444\n",
      "Epoch 10, Batch 700, Loss: 1.6360041534900664\n",
      "Epoch 10, Batch 800, Loss: 1.7785434556007385\n",
      "Epoch 10, Batch 900, Loss: 1.7934400737285614\n",
      "Epoch 11, Batch 100, Loss: 1.7740508151054382\n",
      "Epoch 11, Batch 200, Loss: 1.786756466627121\n",
      "Epoch 11, Batch 300, Loss: 1.7988534450531006\n",
      "Epoch 11, Batch 400, Loss: 1.792904598712921\n",
      "Epoch 11, Batch 500, Loss: 1.66014444231987\n",
      "Epoch 11, Batch 600, Loss: 1.6249376082420348\n",
      "Epoch 11, Batch 700, Loss: 1.63417533993721\n",
      "Epoch 11, Batch 800, Loss: 1.5828338015079497\n",
      "Epoch 11, Batch 900, Loss: 1.5092200326919556\n",
      "Epoch 12, Batch 100, Loss: 1.5151193809509278\n",
      "Epoch 12, Batch 200, Loss: 1.5033006858825684\n",
      "Epoch 12, Batch 300, Loss: 1.5399067389965058\n",
      "Epoch 12, Batch 400, Loss: 1.5059521865844727\n",
      "Epoch 12, Batch 500, Loss: 1.5031550681591035\n",
      "Epoch 12, Batch 600, Loss: 1.5030104148387908\n",
      "Epoch 12, Batch 700, Loss: 1.4841581213474273\n",
      "Epoch 12, Batch 800, Loss: 1.4708583116531373\n",
      "Epoch 12, Batch 900, Loss: 1.483031620979309\n",
      "Epoch 13, Batch 100, Loss: 1.4893221187591552\n",
      "Epoch 13, Batch 200, Loss: 1.4727072739601135\n",
      "Epoch 13, Batch 300, Loss: 1.463672204017639\n",
      "Epoch 13, Batch 400, Loss: 1.4374354410171508\n",
      "Epoch 13, Batch 500, Loss: 1.3453828775882721\n",
      "Epoch 13, Batch 600, Loss: 1.3178584897518157\n",
      "Epoch 13, Batch 700, Loss: 1.3129624092578889\n",
      "Epoch 13, Batch 800, Loss: 1.326897304058075\n",
      "Epoch 13, Batch 900, Loss: 1.396110702753067\n",
      "Epoch 14, Batch 100, Loss: 1.357790242433548\n",
      "Epoch 14, Batch 200, Loss: 1.3530847060680389\n",
      "Epoch 14, Batch 300, Loss: 1.3877646327018738\n",
      "Epoch 14, Batch 400, Loss: 1.420004550218582\n",
      "Epoch 14, Batch 500, Loss: 1.4362076091766358\n",
      "Epoch 14, Batch 600, Loss: 1.454931116104126\n",
      "Epoch 14, Batch 700, Loss: 1.4377736711502076\n",
      "Epoch 14, Batch 800, Loss: 1.4410497212409974\n",
      "Epoch 14, Batch 900, Loss: 1.5298639309406281\n",
      "Epoch 15, Batch 100, Loss: 1.558529132604599\n",
      "Epoch 15, Batch 200, Loss: 1.5532717669010163\n",
      "Epoch 15, Batch 300, Loss: 1.537081216573715\n",
      "Epoch 15, Batch 400, Loss: 1.5300950062274934\n",
      "Epoch 15, Batch 500, Loss: 1.5142327785491942\n",
      "Epoch 15, Batch 600, Loss: 1.5512229156494142\n",
      "Epoch 15, Batch 700, Loss: 1.7134511137008668\n",
      "Epoch 15, Batch 800, Loss: 1.8275143241882323\n",
      "Epoch 15, Batch 900, Loss: 1.8217398488521577\n",
      "Epoch 16, Batch 100, Loss: 1.785374710559845\n",
      "Epoch 16, Batch 200, Loss: 1.8512729156017302\n",
      "Epoch 16, Batch 300, Loss: 1.8437019085884094\n",
      "Epoch 16, Batch 400, Loss: 1.8205351436138153\n",
      "Epoch 16, Batch 500, Loss: 1.8028616440296172\n",
      "Epoch 16, Batch 600, Loss: 1.7004353141784667\n",
      "Epoch 16, Batch 700, Loss: 1.6407577347755433\n",
      "Epoch 16, Batch 800, Loss: 1.712527620792389\n",
      "Epoch 16, Batch 900, Loss: 1.6270361280441283\n",
      "Epoch 17, Batch 100, Loss: 1.6484630274772645\n",
      "Epoch 17, Batch 200, Loss: 1.6502174520492554\n",
      "Epoch 17, Batch 300, Loss: 1.6594097924232483\n",
      "Epoch 17, Batch 400, Loss: 1.7728244853019715\n",
      "Epoch 17, Batch 500, Loss: 2.2619553816318514\n",
      "Epoch 17, Batch 600, Loss: 2.245558259487152\n",
      "Epoch 17, Batch 700, Loss: 2.2512871050834655\n",
      "Epoch 17, Batch 800, Loss: 2.2503567481040956\n",
      "Epoch 17, Batch 900, Loss: 2.2802530670166017\n",
      "Epoch 18, Batch 100, Loss: 2.2695063829421995\n",
      "Epoch 18, Batch 200, Loss: 2.2097186815738676\n",
      "Epoch 18, Batch 300, Loss: 2.12258069396019\n",
      "Epoch 18, Batch 400, Loss: 2.262838611602783\n",
      "Epoch 18, Batch 500, Loss: 2.2694293689727782\n",
      "Epoch 18, Batch 600, Loss: 2.2628552746772765\n",
      "Epoch 18, Batch 700, Loss: 2.263657577037811\n",
      "Epoch 18, Batch 800, Loss: 2.264909398555756\n",
      "Epoch 18, Batch 900, Loss: 2.2908350896835326\n",
      "Epoch 19, Batch 100, Loss: 2.2885588932037355\n",
      "Epoch 19, Batch 200, Loss: 2.2804800176620486\n",
      "Epoch 19, Batch 300, Loss: 2.2919527769088743\n",
      "Epoch 19, Batch 400, Loss: 2.2824031710624695\n",
      "Epoch 19, Batch 500, Loss: 2.280144467353821\n",
      "Epoch 19, Batch 600, Loss: 2.2806707334518435\n",
      "Epoch 19, Batch 700, Loss: 2.2810980820655824\n",
      "Epoch 19, Batch 800, Loss: 2.282464442253113\n",
      "Epoch 19, Batch 900, Loss: 2.289316780567169\n",
      "Epoch 20, Batch 100, Loss: 2.295197186470032\n",
      "Epoch 20, Batch 200, Loss: 2.2847159814834597\n",
      "Epoch 20, Batch 300, Loss: 2.29415513753891\n",
      "Epoch 20, Batch 400, Loss: 2.2894428300857546\n",
      "Epoch 20, Batch 500, Loss: 2.278915674686432\n",
      "Epoch 20, Batch 600, Loss: 2.171624104976654\n",
      "Epoch 20, Batch 700, Loss: 2.0937546253204347\n",
      "Epoch 20, Batch 800, Loss: 2.0812981963157653\n",
      "Epoch 20, Batch 900, Loss: 2.0817260539531706\n",
      "Epoch 21, Batch 100, Loss: 2.0814644944667817\n",
      "Epoch 21, Batch 200, Loss: 2.1907237541675566\n",
      "Epoch 21, Batch 300, Loss: 2.2271296977996826\n",
      "Epoch 21, Batch 400, Loss: 2.2272812008857725\n",
      "Epoch 21, Batch 500, Loss: 2.21684499502182\n",
      "Epoch 21, Batch 600, Loss: 2.2159665513038633\n",
      "Epoch 21, Batch 700, Loss: 2.261731011867523\n",
      "Epoch 21, Batch 800, Loss: 2.236343569755554\n",
      "Epoch 21, Batch 900, Loss: 2.2458773398399354\n",
      "Epoch 22, Batch 100, Loss: 2.2769343042373658\n",
      "Epoch 22, Batch 200, Loss: 2.3257006645202636\n",
      "Epoch 22, Batch 300, Loss: 2.3253273463249204\n",
      "Epoch 22, Batch 400, Loss: 2.3264482283592223\n",
      "Epoch 22, Batch 500, Loss: 2.32216272354126\n",
      "Epoch 22, Batch 600, Loss: 2.324829773902893\n",
      "Epoch 22, Batch 700, Loss: 2.3257334280014037\n",
      "Epoch 22, Batch 800, Loss: 2.323946685791016\n",
      "Epoch 22, Batch 900, Loss: 2.32704740524292\n",
      "Epoch 23, Batch 100, Loss: 2.3220425939559934\n",
      "Epoch 23, Batch 200, Loss: 2.320605204105377\n",
      "Epoch 23, Batch 300, Loss: 2.3229753518104554\n",
      "Epoch 23, Batch 400, Loss: 2.326337206363678\n",
      "Epoch 23, Batch 500, Loss: 2.3182561111450197\n",
      "Epoch 23, Batch 600, Loss: 2.325092663764954\n",
      "Epoch 23, Batch 700, Loss: 2.324264531135559\n",
      "Epoch 23, Batch 800, Loss: 2.322680187225342\n",
      "Epoch 23, Batch 900, Loss: 2.324382338523865\n",
      "Epoch 24, Batch 100, Loss: 2.3290081143379213\n",
      "Epoch 24, Batch 200, Loss: 2.3215094661712645\n",
      "Epoch 24, Batch 300, Loss: 2.319584245681763\n",
      "Epoch 24, Batch 400, Loss: 2.3200938725471496\n",
      "Epoch 24, Batch 500, Loss: 2.326928699016571\n",
      "Epoch 24, Batch 600, Loss: 2.3218116998672484\n",
      "Epoch 24, Batch 700, Loss: 2.322761745452881\n",
      "Epoch 24, Batch 800, Loss: 2.3254638648033144\n",
      "Epoch 24, Batch 900, Loss: 2.3257184863090514\n",
      "Epoch 25, Batch 100, Loss: 2.3216074991226194\n",
      "Epoch 25, Batch 200, Loss: 2.326044557094574\n",
      "Epoch 25, Batch 300, Loss: 2.317171537876129\n",
      "Epoch 25, Batch 400, Loss: 2.318349165916443\n",
      "Epoch 25, Batch 500, Loss: 2.3173225712776184\n",
      "Epoch 25, Batch 600, Loss: 2.318964262008667\n",
      "Epoch 25, Batch 700, Loss: 2.323199951648712\n",
      "Epoch 25, Batch 800, Loss: 2.317461395263672\n",
      "Epoch 25, Batch 900, Loss: 2.3171897172927856\n",
      "Accuracy on test set: 0.1001%\n",
      "Fitting for combination 65\n",
      "784\n",
      "4\n",
      "10\n",
      "[40, 10, 10, 20, 10]\n",
      "False\n",
      "['relu', 'sigmoid', 'relu', 'tanh']\n",
      "SGD\n",
      "0.3\n",
      "0\n",
      "CrossEntropyLoss\n",
      "50\n",
      "25\n",
      "0\n",
      "Epoch 1, Batch 50, Loss: 1.1290145659446715\n",
      "Epoch 1, Batch 100, Loss: 0.7985181379318237\n",
      "Epoch 1, Batch 150, Loss: 0.5957984071969986\n",
      "Epoch 1, Batch 200, Loss: 0.5070379191637039\n",
      "Epoch 1, Batch 250, Loss: 0.5289379525184631\n",
      "Epoch 1, Batch 300, Loss: 0.4761529570817947\n",
      "Epoch 1, Batch 350, Loss: 0.4279437321424484\n",
      "Epoch 1, Batch 400, Loss: 0.39711682677268983\n",
      "Epoch 1, Batch 450, Loss: 0.3815943706035614\n",
      "Epoch 1, Batch 500, Loss: 0.37318612515926364\n",
      "Epoch 1, Batch 550, Loss: 0.3633399522304535\n",
      "Epoch 1, Batch 600, Loss: 0.36049123674631117\n",
      "Epoch 1, Batch 650, Loss: 0.34310180217027664\n",
      "Epoch 1, Batch 700, Loss: 0.3265273681282997\n",
      "Epoch 1, Batch 750, Loss: 0.3374041631817818\n",
      "Epoch 1, Batch 800, Loss: 0.3355567705631256\n",
      "Epoch 1, Batch 850, Loss: 0.3141648387908936\n",
      "Epoch 1, Batch 900, Loss: 0.30659628808498385\n",
      "Epoch 2, Batch 50, Loss: 0.2923342251777649\n",
      "Epoch 2, Batch 100, Loss: 0.2999032008647919\n",
      "Epoch 2, Batch 150, Loss: 0.2906707328557968\n",
      "Epoch 2, Batch 200, Loss: 0.28893477499485015\n",
      "Epoch 2, Batch 250, Loss: 0.31457674503326416\n",
      "Epoch 2, Batch 300, Loss: 0.2911769527196884\n",
      "Epoch 2, Batch 350, Loss: 0.2728784623742104\n",
      "Epoch 2, Batch 400, Loss: 0.2754175662994385\n",
      "Epoch 2, Batch 450, Loss: 0.2794667014479637\n",
      "Epoch 2, Batch 500, Loss: 0.26951215267181394\n",
      "Epoch 2, Batch 550, Loss: 0.27474717050790787\n",
      "Epoch 2, Batch 600, Loss: 0.26178394109010694\n",
      "Epoch 2, Batch 650, Loss: 0.2687596261501312\n",
      "Epoch 2, Batch 700, Loss: 0.274722555577755\n",
      "Epoch 2, Batch 750, Loss: 0.26100473731756213\n",
      "Epoch 2, Batch 800, Loss: 0.2411310189962387\n",
      "Epoch 2, Batch 850, Loss: 0.2469632975757122\n",
      "Epoch 2, Batch 900, Loss: 0.24767415404319762\n",
      "Epoch 3, Batch 50, Loss: 0.26855533227324485\n",
      "Epoch 3, Batch 100, Loss: 0.22891731709241867\n",
      "Epoch 3, Batch 150, Loss: 0.23709996283054352\n",
      "Epoch 3, Batch 200, Loss: 0.245061204880476\n",
      "Epoch 3, Batch 250, Loss: 0.2614619392156601\n",
      "Epoch 3, Batch 300, Loss: 0.24734701812267304\n",
      "Epoch 3, Batch 350, Loss: 0.2515003600716591\n",
      "Epoch 3, Batch 400, Loss: 0.24179569453001024\n",
      "Epoch 3, Batch 450, Loss: 0.22720311731100082\n",
      "Epoch 3, Batch 500, Loss: 0.2406664091348648\n",
      "Epoch 3, Batch 550, Loss: 0.24028383061289788\n",
      "Epoch 3, Batch 600, Loss: 0.2341666278243065\n",
      "Epoch 3, Batch 650, Loss: 0.23809994116425515\n",
      "Epoch 3, Batch 700, Loss: 0.23564539939165116\n",
      "Epoch 3, Batch 750, Loss: 0.22967759132385254\n",
      "Epoch 3, Batch 800, Loss: 0.23491763472557067\n",
      "Epoch 3, Batch 850, Loss: 0.22679564133286476\n",
      "Epoch 3, Batch 900, Loss: 0.22238387554883957\n",
      "Epoch 4, Batch 50, Loss: 0.22956173598766327\n",
      "Epoch 4, Batch 100, Loss: 0.2271784019470215\n",
      "Epoch 4, Batch 150, Loss: 0.23444998115301133\n",
      "Epoch 4, Batch 200, Loss: 0.21714109659194947\n",
      "Epoch 4, Batch 250, Loss: 0.2365019887685776\n",
      "Epoch 4, Batch 300, Loss: 0.24189549997448923\n",
      "Epoch 4, Batch 350, Loss: 0.21487925156950952\n",
      "Epoch 4, Batch 400, Loss: 0.21682834267616272\n",
      "Epoch 4, Batch 450, Loss: 0.2331916069984436\n",
      "Epoch 4, Batch 500, Loss: 0.21429162189364434\n",
      "Epoch 4, Batch 550, Loss: 0.20737740904092788\n",
      "Epoch 4, Batch 600, Loss: 0.22482162445783616\n",
      "Epoch 4, Batch 650, Loss: 0.2225365187227726\n",
      "Epoch 4, Batch 700, Loss: 0.21102308303117753\n",
      "Epoch 4, Batch 750, Loss: 0.2189503537118435\n",
      "Epoch 4, Batch 800, Loss: 0.2224842266738415\n",
      "Epoch 4, Batch 850, Loss: 0.22708721488714217\n",
      "Epoch 4, Batch 900, Loss: 0.21712807834148407\n",
      "Epoch 5, Batch 50, Loss: 0.2143240174651146\n",
      "Epoch 5, Batch 100, Loss: 0.22252695366740227\n",
      "Epoch 5, Batch 150, Loss: 0.22127794191241265\n",
      "Epoch 5, Batch 200, Loss: 0.21044854030013085\n",
      "Epoch 5, Batch 250, Loss: 0.21885740578174592\n",
      "Epoch 5, Batch 300, Loss: 0.20250005647540092\n",
      "Epoch 5, Batch 350, Loss: 0.2131570027768612\n",
      "Epoch 5, Batch 400, Loss: 0.21088950648903848\n",
      "Epoch 5, Batch 450, Loss: 0.21198159500956534\n",
      "Epoch 5, Batch 500, Loss: 0.21014693453907968\n",
      "Epoch 5, Batch 550, Loss: 0.2013301108777523\n",
      "Epoch 5, Batch 600, Loss: 0.21365947932004928\n",
      "Epoch 5, Batch 650, Loss: 0.20857883736491203\n",
      "Epoch 5, Batch 700, Loss: 0.20955529779195786\n",
      "Epoch 5, Batch 750, Loss: 0.21116834849119187\n",
      "Epoch 5, Batch 800, Loss: 0.20028913527727127\n",
      "Epoch 5, Batch 850, Loss: 0.20666278913617134\n",
      "Epoch 5, Batch 900, Loss: 0.20285325422883033\n",
      "Epoch 6, Batch 50, Loss: 0.20779681250452994\n",
      "Epoch 6, Batch 100, Loss: 0.2006857493519783\n",
      "Epoch 6, Batch 150, Loss: 0.19201001569628715\n",
      "Epoch 6, Batch 200, Loss: 0.1941748797893524\n",
      "Epoch 6, Batch 250, Loss: 0.20355470925569535\n",
      "Epoch 6, Batch 300, Loss: 0.2083037081360817\n",
      "Epoch 6, Batch 350, Loss: 0.18953950762748717\n",
      "Epoch 6, Batch 400, Loss: 0.21149769201874732\n",
      "Epoch 6, Batch 450, Loss: 0.2011014148592949\n",
      "Epoch 6, Batch 500, Loss: 0.2017406314611435\n",
      "Epoch 6, Batch 550, Loss: 0.1888486884534359\n",
      "Epoch 6, Batch 600, Loss: 0.20923548817634582\n",
      "Epoch 6, Batch 650, Loss: 0.19927805438637733\n",
      "Epoch 6, Batch 700, Loss: 0.2074286960065365\n",
      "Epoch 6, Batch 750, Loss: 0.18628791347146034\n",
      "Epoch 6, Batch 800, Loss: 0.19943394437432288\n",
      "Epoch 6, Batch 850, Loss: 0.1957511429488659\n",
      "Epoch 6, Batch 900, Loss: 0.2004456588625908\n",
      "Epoch 7, Batch 50, Loss: 0.19065293908119202\n",
      "Epoch 7, Batch 100, Loss: 0.18015719145536424\n",
      "Epoch 7, Batch 150, Loss: 0.20846061125397683\n",
      "Epoch 7, Batch 200, Loss: 0.18684301972389222\n",
      "Epoch 7, Batch 250, Loss: 0.19893819063901902\n",
      "Epoch 7, Batch 300, Loss: 0.1998877127468586\n",
      "Epoch 7, Batch 350, Loss: 0.18055316731333731\n",
      "Epoch 7, Batch 400, Loss: 0.19912407636642457\n",
      "Epoch 7, Batch 450, Loss: 0.1900155359506607\n",
      "Epoch 7, Batch 500, Loss: 0.18812392726540567\n",
      "Epoch 7, Batch 550, Loss: 0.19629738226532936\n",
      "Epoch 7, Batch 600, Loss: 0.2001019884645939\n",
      "Epoch 7, Batch 650, Loss: 0.1950752218067646\n",
      "Epoch 7, Batch 700, Loss: 0.19719164833426475\n",
      "Epoch 7, Batch 750, Loss: 0.202120840549469\n",
      "Epoch 7, Batch 800, Loss: 0.19698178634047508\n",
      "Epoch 7, Batch 850, Loss: 0.19758532613515853\n",
      "Epoch 7, Batch 900, Loss: 0.1878255295753479\n",
      "Epoch 8, Batch 50, Loss: 0.1918894064426422\n",
      "Epoch 8, Batch 100, Loss: 0.1936389046907425\n",
      "Epoch 8, Batch 150, Loss: 0.1812854914367199\n",
      "Epoch 8, Batch 200, Loss: 0.18828514829277992\n",
      "Epoch 8, Batch 250, Loss: 0.1916976609826088\n",
      "Epoch 8, Batch 300, Loss: 0.18378924399614335\n",
      "Epoch 8, Batch 350, Loss: 0.1742814773321152\n",
      "Epoch 8, Batch 400, Loss: 0.19111130997538567\n",
      "Epoch 8, Batch 450, Loss: 0.1788356636464596\n",
      "Epoch 8, Batch 500, Loss: 0.18136009946465492\n",
      "Epoch 8, Batch 550, Loss: 0.19582121826708318\n",
      "Epoch 8, Batch 600, Loss: 0.1840742738544941\n",
      "Epoch 8, Batch 650, Loss: 0.19539129748940468\n",
      "Epoch 8, Batch 700, Loss: 0.1841191865503788\n",
      "Epoch 8, Batch 750, Loss: 0.1861162328720093\n",
      "Epoch 8, Batch 800, Loss: 0.19980008974671365\n",
      "Epoch 8, Batch 850, Loss: 0.1851964822411537\n",
      "Epoch 8, Batch 900, Loss: 0.19275339111685752\n",
      "Epoch 9, Batch 50, Loss: 0.17972256273031234\n",
      "Epoch 9, Batch 100, Loss: 0.17909854143857956\n",
      "Epoch 9, Batch 150, Loss: 0.17646613955497742\n",
      "Epoch 9, Batch 200, Loss: 0.19808996438980103\n",
      "Epoch 9, Batch 250, Loss: 0.1856040671467781\n",
      "Epoch 9, Batch 300, Loss: 0.1783927223086357\n",
      "Epoch 9, Batch 350, Loss: 0.19284086897969246\n",
      "Epoch 9, Batch 400, Loss: 0.18350152060389519\n",
      "Epoch 9, Batch 450, Loss: 0.1792789514362812\n",
      "Epoch 9, Batch 500, Loss: 0.19421379551291465\n",
      "Epoch 9, Batch 550, Loss: 0.19269871965050697\n",
      "Epoch 9, Batch 600, Loss: 0.19319166779518127\n",
      "Epoch 9, Batch 650, Loss: 0.1875930704176426\n",
      "Epoch 9, Batch 700, Loss: 0.17394257977604866\n",
      "Epoch 9, Batch 750, Loss: 0.18652847334742545\n",
      "Epoch 9, Batch 800, Loss: 0.18321464985609054\n",
      "Epoch 9, Batch 850, Loss: 0.17196562990546227\n",
      "Epoch 9, Batch 900, Loss: 0.1802406294643879\n",
      "Epoch 10, Batch 50, Loss: 0.17367666617035865\n",
      "Epoch 10, Batch 100, Loss: 0.1743882702291012\n",
      "Epoch 10, Batch 150, Loss: 0.18166962519288063\n",
      "Epoch 10, Batch 200, Loss: 0.189984093606472\n",
      "Epoch 10, Batch 250, Loss: 0.1825181521475315\n",
      "Epoch 10, Batch 300, Loss: 0.188114163428545\n",
      "Epoch 10, Batch 350, Loss: 0.17524164378643037\n",
      "Epoch 10, Batch 400, Loss: 0.1828077181428671\n",
      "Epoch 10, Batch 450, Loss: 0.191168160289526\n",
      "Epoch 10, Batch 500, Loss: 0.17899345308542253\n",
      "Epoch 10, Batch 550, Loss: 0.1826030607521534\n",
      "Epoch 10, Batch 600, Loss: 0.19008515298366546\n",
      "Epoch 10, Batch 650, Loss: 0.18337779372930527\n",
      "Epoch 10, Batch 700, Loss: 0.18174195498228074\n",
      "Epoch 10, Batch 750, Loss: 0.18334817990660668\n",
      "Epoch 10, Batch 800, Loss: 0.1703464749455452\n",
      "Epoch 10, Batch 850, Loss: 0.17142778605222703\n",
      "Epoch 10, Batch 900, Loss: 0.1906848120689392\n",
      "Epoch 11, Batch 50, Loss: 0.17199743568897247\n",
      "Epoch 11, Batch 100, Loss: 0.18287301063537598\n",
      "Epoch 11, Batch 150, Loss: 0.1807447138428688\n",
      "Epoch 11, Batch 200, Loss: 0.16461874827742576\n",
      "Epoch 11, Batch 250, Loss: 0.17620173811912537\n",
      "Epoch 11, Batch 300, Loss: 0.17636735633015632\n",
      "Epoch 11, Batch 350, Loss: 0.18265813127160072\n",
      "Epoch 11, Batch 400, Loss: 0.18416060015559196\n",
      "Epoch 11, Batch 450, Loss: 0.17147073060274123\n",
      "Epoch 11, Batch 500, Loss: 0.17961045801639558\n",
      "Epoch 11, Batch 550, Loss: 0.17609444811940192\n",
      "Epoch 11, Batch 600, Loss: 0.1716964988410473\n",
      "Epoch 11, Batch 650, Loss: 0.1784225708246231\n",
      "Epoch 11, Batch 700, Loss: 0.1643568444252014\n",
      "Epoch 11, Batch 750, Loss: 0.17282523989677429\n",
      "Epoch 11, Batch 800, Loss: 0.17933368109166622\n",
      "Epoch 11, Batch 850, Loss: 0.16976880349218845\n",
      "Epoch 11, Batch 900, Loss: 0.18421213656663896\n",
      "Epoch 12, Batch 50, Loss: 0.17699226558208467\n",
      "Epoch 12, Batch 100, Loss: 0.18146886855363845\n",
      "Epoch 12, Batch 150, Loss: 0.1657895950973034\n",
      "Epoch 12, Batch 200, Loss: 0.1624439226090908\n",
      "Epoch 12, Batch 250, Loss: 0.16803588658571245\n",
      "Epoch 12, Batch 300, Loss: 0.17646759808063506\n",
      "Epoch 12, Batch 350, Loss: 0.19149407878518104\n",
      "Epoch 12, Batch 400, Loss: 0.16747992783784865\n",
      "Epoch 12, Batch 450, Loss: 0.17677051141858102\n",
      "Epoch 12, Batch 500, Loss: 0.17211397752165794\n",
      "Epoch 12, Batch 550, Loss: 0.16849344000220298\n",
      "Epoch 12, Batch 600, Loss: 0.1679992298781872\n",
      "Epoch 12, Batch 650, Loss: 0.1886762249469757\n",
      "Epoch 12, Batch 700, Loss: 0.17585536062717438\n",
      "Epoch 12, Batch 750, Loss: 0.18082571536302566\n",
      "Epoch 12, Batch 800, Loss: 0.17231574699282645\n",
      "Epoch 12, Batch 850, Loss: 0.16584342688322068\n",
      "Epoch 12, Batch 900, Loss: 0.16014495775103568\n",
      "Epoch 13, Batch 50, Loss: 0.17987264171242714\n",
      "Epoch 13, Batch 100, Loss: 0.1782442508637905\n",
      "Epoch 13, Batch 150, Loss: 0.1719730180501938\n",
      "Epoch 13, Batch 200, Loss: 0.16702297523617746\n",
      "Epoch 13, Batch 250, Loss: 0.16645202338695525\n",
      "Epoch 13, Batch 300, Loss: 0.1743056656420231\n",
      "Epoch 13, Batch 350, Loss: 0.15793684482574463\n",
      "Epoch 13, Batch 400, Loss: 0.178805350959301\n",
      "Epoch 13, Batch 450, Loss: 0.17214557096362115\n",
      "Epoch 13, Batch 500, Loss: 0.169182750582695\n",
      "Epoch 13, Batch 550, Loss: 0.17426875680685044\n",
      "Epoch 13, Batch 600, Loss: 0.1693724797666073\n",
      "Epoch 13, Batch 650, Loss: 0.17420562252402305\n",
      "Epoch 13, Batch 700, Loss: 0.14946154944598675\n",
      "Epoch 13, Batch 750, Loss: 0.18605316564440727\n",
      "Epoch 13, Batch 800, Loss: 0.16632793918251992\n",
      "Epoch 13, Batch 850, Loss: 0.1714469476044178\n",
      "Epoch 13, Batch 900, Loss: 0.18003374934196473\n",
      "Epoch 14, Batch 50, Loss: 0.17042300194501878\n",
      "Epoch 14, Batch 100, Loss: 0.1715901555120945\n",
      "Epoch 14, Batch 150, Loss: 0.16181246414780617\n",
      "Epoch 14, Batch 200, Loss: 0.17569663658738136\n",
      "Epoch 14, Batch 250, Loss: 0.16162708446383475\n",
      "Epoch 14, Batch 300, Loss: 0.17302727505564688\n",
      "Epoch 14, Batch 350, Loss: 0.16374843671917916\n",
      "Epoch 14, Batch 400, Loss: 0.1749571806937456\n",
      "Epoch 14, Batch 450, Loss: 0.167278198748827\n",
      "Epoch 14, Batch 500, Loss: 0.16385909035801888\n",
      "Epoch 14, Batch 550, Loss: 0.18201261483132838\n",
      "Epoch 14, Batch 600, Loss: 0.1666339258849621\n",
      "Epoch 14, Batch 650, Loss: 0.15910520970821382\n",
      "Epoch 14, Batch 700, Loss: 0.16433712139725684\n",
      "Epoch 14, Batch 750, Loss: 0.16188190653920173\n",
      "Epoch 14, Batch 800, Loss: 0.1693958406150341\n",
      "Epoch 14, Batch 850, Loss: 0.16065178498625754\n",
      "Epoch 14, Batch 900, Loss: 0.18244224146008492\n",
      "Epoch 15, Batch 50, Loss: 0.16458736583590508\n",
      "Epoch 15, Batch 100, Loss: 0.15474196933209897\n",
      "Epoch 15, Batch 150, Loss: 0.16924417555332183\n",
      "Epoch 15, Batch 200, Loss: 0.1680450864136219\n",
      "Epoch 15, Batch 250, Loss: 0.1626131984591484\n",
      "Epoch 15, Batch 300, Loss: 0.16185547068715095\n",
      "Epoch 15, Batch 350, Loss: 0.17610560610890388\n",
      "Epoch 15, Batch 400, Loss: 0.15718941405415535\n",
      "Epoch 15, Batch 450, Loss: 0.15216736555099486\n",
      "Epoch 15, Batch 500, Loss: 0.17460328787565232\n",
      "Epoch 15, Batch 550, Loss: 0.16158045575022698\n",
      "Epoch 15, Batch 600, Loss: 0.16575970999896528\n",
      "Epoch 15, Batch 650, Loss: 0.1624952994287014\n",
      "Epoch 15, Batch 700, Loss: 0.1771635164320469\n",
      "Epoch 15, Batch 750, Loss: 0.16386079519987107\n",
      "Epoch 15, Batch 800, Loss: 0.16683966740965844\n",
      "Epoch 15, Batch 850, Loss: 0.17095955446362496\n",
      "Epoch 15, Batch 900, Loss: 0.17277675807476045\n",
      "Epoch 16, Batch 50, Loss: 0.17220807030797006\n",
      "Epoch 16, Batch 100, Loss: 0.16532727360725402\n",
      "Epoch 16, Batch 150, Loss: 0.1677245231717825\n",
      "Epoch 16, Batch 200, Loss: 0.1520457298308611\n",
      "Epoch 16, Batch 250, Loss: 0.14684811182320118\n",
      "Epoch 16, Batch 300, Loss: 0.16401153042912484\n",
      "Epoch 16, Batch 350, Loss: 0.15528569996356964\n",
      "Epoch 16, Batch 400, Loss: 0.17769999161362648\n",
      "Epoch 16, Batch 450, Loss: 0.17710285231471062\n",
      "Epoch 16, Batch 500, Loss: 0.16194677904248236\n",
      "Epoch 16, Batch 550, Loss: 0.160617266446352\n",
      "Epoch 16, Batch 600, Loss: 0.1675630532205105\n",
      "Epoch 16, Batch 650, Loss: 0.15276564240455628\n",
      "Epoch 16, Batch 700, Loss: 0.1667775656282902\n",
      "Epoch 16, Batch 750, Loss: 0.1545008398592472\n",
      "Epoch 16, Batch 800, Loss: 0.16588815420866013\n",
      "Epoch 16, Batch 850, Loss: 0.15901818633079529\n",
      "Epoch 16, Batch 900, Loss: 0.17679486483335494\n",
      "Epoch 17, Batch 50, Loss: 0.1751128053665161\n",
      "Epoch 17, Batch 100, Loss: 0.16118050664663314\n",
      "Epoch 17, Batch 150, Loss: 0.15881724640727043\n",
      "Epoch 17, Batch 200, Loss: 0.15940559759736062\n",
      "Epoch 17, Batch 250, Loss: 0.16357491180300712\n",
      "Epoch 17, Batch 300, Loss: 0.16208605706691742\n",
      "Epoch 17, Batch 350, Loss: 0.15371118605136871\n",
      "Epoch 17, Batch 400, Loss: 0.16589413419365884\n",
      "Epoch 17, Batch 450, Loss: 0.15587330020964146\n",
      "Epoch 17, Batch 500, Loss: 0.17240753650665283\n",
      "Epoch 17, Batch 550, Loss: 0.15393571093678474\n",
      "Epoch 17, Batch 600, Loss: 0.17717640951275826\n",
      "Epoch 17, Batch 650, Loss: 0.1467410884052515\n",
      "Epoch 17, Batch 700, Loss: 0.1641872677206993\n",
      "Epoch 17, Batch 750, Loss: 0.16721511378884316\n",
      "Epoch 17, Batch 800, Loss: 0.16193131640553474\n",
      "Epoch 17, Batch 850, Loss: 0.1498838136345148\n",
      "Epoch 17, Batch 900, Loss: 0.16793682642281055\n",
      "Epoch 18, Batch 50, Loss: 0.15630449987947942\n",
      "Epoch 18, Batch 100, Loss: 0.16251679241657258\n",
      "Epoch 18, Batch 150, Loss: 0.14716632537543772\n",
      "Epoch 18, Batch 200, Loss: 0.14817228525877\n",
      "Epoch 18, Batch 250, Loss: 0.14548970244824885\n",
      "Epoch 18, Batch 300, Loss: 0.1632898809015751\n",
      "Epoch 18, Batch 350, Loss: 0.15862516686320305\n",
      "Epoch 18, Batch 400, Loss: 0.158354974091053\n",
      "Epoch 18, Batch 450, Loss: 0.15896598622202873\n",
      "Epoch 18, Batch 500, Loss: 0.15597501993179322\n",
      "Epoch 18, Batch 550, Loss: 0.1674075071513653\n",
      "Epoch 18, Batch 600, Loss: 0.1541095432639122\n",
      "Epoch 18, Batch 650, Loss: 0.16628965392708778\n",
      "Epoch 18, Batch 700, Loss: 0.16424646884202956\n",
      "Epoch 18, Batch 750, Loss: 0.17875340417027474\n",
      "Epoch 18, Batch 800, Loss: 0.1693187126517296\n",
      "Epoch 18, Batch 850, Loss: 0.15702945739030838\n",
      "Epoch 18, Batch 900, Loss: 0.1537136687338352\n",
      "Epoch 19, Batch 50, Loss: 0.15085771277546883\n",
      "Epoch 19, Batch 100, Loss: 0.15583415240049361\n",
      "Epoch 19, Batch 150, Loss: 0.16064972773194314\n",
      "Epoch 19, Batch 200, Loss: 0.1615259975194931\n",
      "Epoch 19, Batch 250, Loss: 0.14915418192744256\n",
      "Epoch 19, Batch 300, Loss: 0.15823178201913835\n",
      "Epoch 19, Batch 350, Loss: 0.15943077057600022\n",
      "Epoch 19, Batch 400, Loss: 0.16254364654421807\n",
      "Epoch 19, Batch 450, Loss: 0.15625349074602127\n",
      "Epoch 19, Batch 500, Loss: 0.13297180697321892\n",
      "Epoch 19, Batch 550, Loss: 0.16678233601152898\n",
      "Epoch 19, Batch 600, Loss: 0.15286353036761283\n",
      "Epoch 19, Batch 650, Loss: 0.17000650733709335\n",
      "Epoch 19, Batch 700, Loss: 0.16328411296010018\n",
      "Epoch 19, Batch 750, Loss: 0.1521769642829895\n",
      "Epoch 19, Batch 800, Loss: 0.16196860201656818\n",
      "Epoch 19, Batch 850, Loss: 0.16253886818885804\n",
      "Epoch 19, Batch 900, Loss: 0.15540568351745607\n",
      "Epoch 20, Batch 50, Loss: 0.15258897542953492\n",
      "Epoch 20, Batch 100, Loss: 0.15290598839521408\n",
      "Epoch 20, Batch 150, Loss: 0.16236163809895515\n",
      "Epoch 20, Batch 200, Loss: 0.14619772970676423\n",
      "Epoch 20, Batch 250, Loss: 0.1424006348103285\n",
      "Epoch 20, Batch 300, Loss: 0.15310006387531758\n",
      "Epoch 20, Batch 350, Loss: 0.14255852043628692\n",
      "Epoch 20, Batch 400, Loss: 0.16417285889387132\n",
      "Epoch 20, Batch 450, Loss: 0.14716029033064842\n",
      "Epoch 20, Batch 500, Loss: 0.1576281614601612\n",
      "Epoch 20, Batch 550, Loss: 0.1549891473352909\n",
      "Epoch 20, Batch 600, Loss: 0.15885421209037304\n",
      "Epoch 20, Batch 650, Loss: 0.15161121875047684\n",
      "Epoch 20, Batch 700, Loss: 0.14422348208725452\n",
      "Epoch 20, Batch 750, Loss: 0.15246958732604982\n",
      "Epoch 20, Batch 800, Loss: 0.1721543151140213\n",
      "Epoch 20, Batch 850, Loss: 0.15167060136795044\n",
      "Epoch 20, Batch 900, Loss: 0.1481806942820549\n",
      "Epoch 21, Batch 50, Loss: 0.15911711670458317\n",
      "Epoch 21, Batch 100, Loss: 0.1408792108297348\n",
      "Epoch 21, Batch 150, Loss: 0.14488608941435813\n",
      "Epoch 21, Batch 200, Loss: 0.15569936364889145\n",
      "Epoch 21, Batch 250, Loss: 0.15043975211679936\n",
      "Epoch 21, Batch 300, Loss: 0.1545790834724903\n",
      "Epoch 21, Batch 350, Loss: 0.14593938663601874\n",
      "Epoch 21, Batch 400, Loss: 0.15696682661771774\n",
      "Epoch 21, Batch 450, Loss: 0.16363224670290946\n",
      "Epoch 21, Batch 500, Loss: 0.15173227652907373\n",
      "Epoch 21, Batch 550, Loss: 0.16171206161379814\n",
      "Epoch 21, Batch 600, Loss: 0.1466925410926342\n",
      "Epoch 21, Batch 650, Loss: 0.1534853007644415\n",
      "Epoch 21, Batch 700, Loss: 0.15013862580060958\n",
      "Epoch 21, Batch 750, Loss: 0.15291070565581322\n",
      "Epoch 21, Batch 800, Loss: 0.1649588079750538\n",
      "Epoch 21, Batch 850, Loss: 0.14872581087052822\n",
      "Epoch 21, Batch 900, Loss: 0.15285149715840818\n",
      "Epoch 22, Batch 50, Loss: 0.1522246091067791\n",
      "Epoch 22, Batch 100, Loss: 0.1352838545292616\n",
      "Epoch 22, Batch 150, Loss: 0.15637397527694702\n",
      "Epoch 22, Batch 200, Loss: 0.15476734682917595\n",
      "Epoch 22, Batch 250, Loss: 0.14183569207787514\n",
      "Epoch 22, Batch 300, Loss: 0.1517917314171791\n",
      "Epoch 22, Batch 350, Loss: 0.16013433277606964\n",
      "Epoch 22, Batch 400, Loss: 0.1529631143063307\n",
      "Epoch 22, Batch 450, Loss: 0.15218692734837533\n",
      "Epoch 22, Batch 500, Loss: 0.1559814417362213\n",
      "Epoch 22, Batch 550, Loss: 0.15366426184773446\n",
      "Epoch 22, Batch 600, Loss: 0.15340215489268302\n",
      "Epoch 22, Batch 650, Loss: 0.15324263915419578\n",
      "Epoch 22, Batch 700, Loss: 0.16820226922631265\n",
      "Epoch 22, Batch 750, Loss: 0.14744232907891275\n",
      "Epoch 22, Batch 800, Loss: 0.1501796855032444\n",
      "Epoch 22, Batch 850, Loss: 0.15064842119812966\n",
      "Epoch 22, Batch 900, Loss: 0.15928937777876853\n",
      "Epoch 23, Batch 50, Loss: 0.16223255977034567\n",
      "Epoch 23, Batch 100, Loss: 0.13847921282052994\n",
      "Epoch 23, Batch 150, Loss: 0.14966075383126737\n",
      "Epoch 23, Batch 200, Loss: 0.14849056951701642\n",
      "Epoch 23, Batch 250, Loss: 0.14588540501892566\n",
      "Epoch 23, Batch 300, Loss: 0.15280378922820093\n",
      "Epoch 23, Batch 350, Loss: 0.15406421646475793\n",
      "Epoch 23, Batch 400, Loss: 0.14550743937492372\n",
      "Epoch 23, Batch 450, Loss: 0.15584829568862915\n",
      "Epoch 23, Batch 500, Loss: 0.1412829916924238\n",
      "Epoch 23, Batch 550, Loss: 0.14364766091108322\n",
      "Epoch 23, Batch 600, Loss: 0.15192619241774083\n",
      "Epoch 23, Batch 650, Loss: 0.14908616565167904\n",
      "Epoch 23, Batch 700, Loss: 0.15380986988544465\n",
      "Epoch 23, Batch 750, Loss: 0.16252563059329986\n",
      "Epoch 23, Batch 800, Loss: 0.15820358276367188\n",
      "Epoch 23, Batch 850, Loss: 0.15359980136156082\n",
      "Epoch 23, Batch 900, Loss: 0.16192600548267364\n",
      "Epoch 24, Batch 50, Loss: 0.1433443483710289\n",
      "Epoch 24, Batch 100, Loss: 0.14685963429510593\n",
      "Epoch 24, Batch 150, Loss: 0.14799339473247528\n",
      "Epoch 24, Batch 200, Loss: 0.14595809407532215\n",
      "Epoch 24, Batch 250, Loss: 0.1478826493024826\n",
      "Epoch 24, Batch 300, Loss: 0.1431328219920397\n",
      "Epoch 24, Batch 350, Loss: 0.15471203066408634\n",
      "Epoch 24, Batch 400, Loss: 0.14772314786911012\n",
      "Epoch 24, Batch 450, Loss: 0.14637315161526204\n",
      "Epoch 24, Batch 500, Loss: 0.17032791614532472\n",
      "Epoch 24, Batch 550, Loss: 0.15023948095738887\n",
      "Epoch 24, Batch 600, Loss: 0.16373507395386697\n",
      "Epoch 24, Batch 650, Loss: 0.1470657080411911\n",
      "Epoch 24, Batch 700, Loss: 0.13721914030611515\n",
      "Epoch 24, Batch 750, Loss: 0.1608527585864067\n",
      "Epoch 24, Batch 800, Loss: 0.14654257752001285\n",
      "Epoch 24, Batch 850, Loss: 0.15983872547745703\n",
      "Epoch 24, Batch 900, Loss: 0.1580412434786558\n",
      "Epoch 25, Batch 50, Loss: 0.14617141127586364\n",
      "Epoch 25, Batch 100, Loss: 0.14205384321510792\n",
      "Epoch 25, Batch 150, Loss: 0.1609362095594406\n",
      "Epoch 25, Batch 200, Loss: 0.14074584394693374\n",
      "Epoch 25, Batch 250, Loss: 0.15298738218843938\n",
      "Epoch 25, Batch 300, Loss: 0.1429179422557354\n",
      "Epoch 25, Batch 350, Loss: 0.14950109206140041\n",
      "Epoch 25, Batch 400, Loss: 0.16207807749509812\n",
      "Epoch 25, Batch 450, Loss: 0.1535078565776348\n",
      "Epoch 25, Batch 500, Loss: 0.16106253586709499\n",
      "Epoch 25, Batch 550, Loss: 0.15278007246553899\n",
      "Epoch 25, Batch 600, Loss: 0.1452743172645569\n",
      "Epoch 25, Batch 650, Loss: 0.14958031758666038\n",
      "Epoch 25, Batch 700, Loss: 0.1556965185701847\n",
      "Epoch 25, Batch 750, Loss: 0.15062384068965912\n",
      "Epoch 25, Batch 800, Loss: 0.15592315375804902\n",
      "Epoch 25, Batch 850, Loss: 0.14947375282645226\n",
      "Epoch 25, Batch 900, Loss: 0.15745064601302147\n",
      "Accuracy on test set: 0.8621%\n",
      "Fitting for combination 66\n",
      "784\n",
      "4\n",
      "10\n",
      "[40, 10, 10, 20, 10]\n",
      "True\n",
      "['relu', 'sigmoid', 'relu', 'relu']\n",
      "Adam\n",
      "0.03\n",
      "0\n",
      "CrossEntropyLoss\n",
      "200\n",
      "25\n",
      "0\n",
      "Epoch 1, Batch 200, Loss: 2.8781627476215363\n",
      "Epoch 1, Batch 400, Loss: 2.8524062275886535\n",
      "Epoch 1, Batch 600, Loss: 2.5808509194850924\n",
      "Epoch 1, Batch 800, Loss: 2.665065397620201\n",
      "Epoch 2, Batch 200, Loss: 2.6034070956707\n",
      "Epoch 2, Batch 400, Loss: 2.7831405901908877\n",
      "Epoch 2, Batch 600, Loss: 2.625427392721176\n",
      "Epoch 2, Batch 800, Loss: 2.5920616763830187\n",
      "Epoch 3, Batch 200, Loss: 2.4756748592853546\n",
      "Epoch 3, Batch 400, Loss: 2.5696863245964052\n",
      "Epoch 3, Batch 600, Loss: 2.2724857103824614\n",
      "Epoch 3, Batch 800, Loss: 2.547080778479576\n",
      "Epoch 4, Batch 200, Loss: 3.1068908488750457\n",
      "Epoch 4, Batch 400, Loss: 2.7607932651042937\n",
      "Epoch 4, Batch 600, Loss: 2.6497055172920225\n",
      "Epoch 4, Batch 800, Loss: 2.933812322616577\n",
      "Epoch 5, Batch 200, Loss: 2.8454152047634125\n",
      "Epoch 5, Batch 400, Loss: 2.7055774116516114\n",
      "Epoch 5, Batch 600, Loss: 2.8287613081932066\n",
      "Epoch 5, Batch 800, Loss: 2.7883376026153566\n",
      "Epoch 6, Batch 200, Loss: 3.0128775107860566\n",
      "Epoch 6, Batch 400, Loss: 3.191058351993561\n",
      "Epoch 6, Batch 600, Loss: 3.051150704622269\n",
      "Epoch 6, Batch 800, Loss: 2.729524818658829\n",
      "Epoch 7, Batch 200, Loss: 2.8959709763526917\n",
      "Epoch 7, Batch 400, Loss: 3.1423795473575593\n",
      "Epoch 7, Batch 600, Loss: 3.0790328359603882\n",
      "Epoch 7, Batch 800, Loss: 2.8451201128959656\n",
      "Epoch 8, Batch 200, Loss: 2.7001395070552827\n",
      "Epoch 8, Batch 400, Loss: 2.8015562391281126\n",
      "Epoch 8, Batch 600, Loss: 2.929654588699341\n",
      "Epoch 8, Batch 800, Loss: 3.117032759189606\n",
      "Epoch 9, Batch 200, Loss: 3.0999173402786253\n",
      "Epoch 9, Batch 400, Loss: 3.0552731835842133\n",
      "Epoch 9, Batch 600, Loss: 3.1880064821243286\n",
      "Epoch 9, Batch 800, Loss: 3.0815250301361083\n",
      "Epoch 10, Batch 200, Loss: 2.8877506971359255\n",
      "Epoch 10, Batch 400, Loss: 2.740392200946808\n",
      "Epoch 10, Batch 600, Loss: 2.780356355905533\n",
      "Epoch 10, Batch 800, Loss: 3.5604435384273527\n",
      "Epoch 11, Batch 200, Loss: 3.35238604426384\n",
      "Epoch 11, Batch 400, Loss: 3.1449959647655485\n",
      "Epoch 11, Batch 600, Loss: 2.604010031223297\n",
      "Epoch 11, Batch 800, Loss: 2.633429846763611\n",
      "Epoch 12, Batch 200, Loss: 2.8654852390289305\n",
      "Epoch 12, Batch 400, Loss: 2.9973320925235747\n",
      "Epoch 12, Batch 600, Loss: 3.134667054414749\n",
      "Epoch 12, Batch 800, Loss: 3.1270859825611113\n",
      "Epoch 13, Batch 200, Loss: 3.274599087238312\n",
      "Epoch 13, Batch 400, Loss: 3.2718471586704254\n",
      "Epoch 13, Batch 600, Loss: 3.2154310607910155\n",
      "Epoch 13, Batch 800, Loss: 3.1346377193927766\n",
      "Epoch 14, Batch 200, Loss: 3.07020734667778\n",
      "Epoch 14, Batch 400, Loss: 3.135242247581482\n",
      "Epoch 14, Batch 600, Loss: 3.329910634756088\n",
      "Epoch 14, Batch 800, Loss: 3.1981928300857545\n",
      "Epoch 15, Batch 200, Loss: 2.9959136736392975\n",
      "Epoch 15, Batch 400, Loss: 2.9128102934360505\n",
      "Epoch 15, Batch 600, Loss: 2.711362348794937\n",
      "Epoch 15, Batch 800, Loss: 2.7577998447418213\n",
      "Epoch 16, Batch 200, Loss: 2.771689258813858\n",
      "Epoch 16, Batch 400, Loss: 2.829600304365158\n",
      "Epoch 16, Batch 600, Loss: 2.6544862043857576\n",
      "Epoch 16, Batch 800, Loss: 2.6598735523223875\n",
      "Epoch 17, Batch 200, Loss: 2.6423151326179504\n",
      "Epoch 17, Batch 400, Loss: 2.656514791250229\n",
      "Epoch 17, Batch 600, Loss: 2.59231389939785\n",
      "Epoch 17, Batch 800, Loss: 2.576124554872513\n",
      "Epoch 18, Batch 200, Loss: 2.574207052588463\n",
      "Epoch 18, Batch 400, Loss: 2.5075135338306427\n",
      "Epoch 18, Batch 600, Loss: 2.4881491553783417\n",
      "Epoch 18, Batch 800, Loss: 2.60063013792038\n",
      "Epoch 19, Batch 200, Loss: 3.560794676542282\n",
      "Epoch 19, Batch 400, Loss: 3.038747898340225\n",
      "Epoch 19, Batch 600, Loss: 2.995351651906967\n",
      "Epoch 19, Batch 800, Loss: 2.971614931821823\n",
      "Epoch 20, Batch 200, Loss: 2.805092977285385\n",
      "Epoch 20, Batch 400, Loss: 2.7369748187065124\n",
      "Epoch 20, Batch 600, Loss: 2.7737012815475466\n",
      "Epoch 20, Batch 800, Loss: 3.0146027410030367\n",
      "Epoch 21, Batch 200, Loss: 3.298127144575119\n",
      "Epoch 21, Batch 400, Loss: 3.237964596748352\n",
      "Epoch 21, Batch 600, Loss: 3.1503825795650484\n",
      "Epoch 21, Batch 800, Loss: 3.4143217957019805\n",
      "Epoch 22, Batch 200, Loss: 2.850524067878723\n",
      "Epoch 22, Batch 400, Loss: 3.0761596536636353\n",
      "Epoch 22, Batch 600, Loss: 2.9903564846515653\n",
      "Epoch 22, Batch 800, Loss: 3.0271799218654634\n",
      "Epoch 23, Batch 200, Loss: 2.8870758712291718\n",
      "Epoch 23, Batch 400, Loss: 2.836223818063736\n",
      "Epoch 23, Batch 600, Loss: 2.8248390114307402\n",
      "Epoch 23, Batch 800, Loss: 2.745503385066986\n",
      "Epoch 24, Batch 200, Loss: 3.132229700088501\n",
      "Epoch 24, Batch 400, Loss: 2.972116235494614\n",
      "Epoch 24, Batch 600, Loss: 2.9582197666168213\n",
      "Epoch 24, Batch 800, Loss: 2.9468737518787385\n",
      "Epoch 25, Batch 200, Loss: 3.017579175233841\n",
      "Epoch 25, Batch 400, Loss: 2.958560963869095\n",
      "Epoch 25, Batch 600, Loss: 2.9231728506088257\n",
      "Epoch 25, Batch 800, Loss: 2.962233809232712\n",
      "Accuracy on test set: 0.33%\n",
      "Fitting for combination 67\n",
      "784\n",
      "4\n",
      "10\n",
      "[40, 10, 10, 20, 10]\n",
      "True\n",
      "['relu', 'sigmoid', 'relu', 'tanh']\n",
      "SGD\n",
      "0.3\n",
      "0.03\n",
      "CrossEntropyLoss\n",
      "100\n",
      "25\n",
      "0\n",
      "Epoch 1, Batch 100, Loss: 2.3044714188575743\n",
      "Epoch 1, Batch 200, Loss: 2.3033289766311644\n",
      "Epoch 1, Batch 300, Loss: 2.3035427355766296\n",
      "Epoch 1, Batch 400, Loss: 2.303429698944092\n",
      "Epoch 1, Batch 500, Loss: 2.3034775829315186\n",
      "Epoch 1, Batch 600, Loss: 2.303221728801727\n",
      "Epoch 1, Batch 700, Loss: 2.3037250804901124\n",
      "Epoch 1, Batch 800, Loss: 2.3034647274017335\n",
      "Epoch 1, Batch 900, Loss: 2.3034810972213746\n",
      "Epoch 2, Batch 100, Loss: 2.303479826450348\n",
      "Epoch 2, Batch 200, Loss: 2.3038588976860046\n",
      "Epoch 2, Batch 300, Loss: 2.3030473828315734\n",
      "Epoch 2, Batch 400, Loss: 2.3038907718658446\n",
      "Epoch 2, Batch 500, Loss: 2.3034166884422302\n",
      "Epoch 2, Batch 600, Loss: 2.3034286284446717\n",
      "Epoch 2, Batch 700, Loss: 2.303525094985962\n",
      "Epoch 2, Batch 800, Loss: 2.3034545230865477\n",
      "Epoch 2, Batch 900, Loss: 2.303345181941986\n",
      "Epoch 3, Batch 100, Loss: 2.3025378251075743\n",
      "Epoch 3, Batch 200, Loss: 2.303295352458954\n",
      "Epoch 3, Batch 300, Loss: 2.303219401836395\n",
      "Epoch 3, Batch 400, Loss: 2.3036767578125\n",
      "Epoch 3, Batch 500, Loss: 2.3033594417572023\n",
      "Epoch 3, Batch 600, Loss: 2.3033737468719484\n",
      "Epoch 3, Batch 700, Loss: 2.303851132392883\n",
      "Epoch 3, Batch 800, Loss: 2.3036485862731935\n",
      "Epoch 3, Batch 900, Loss: 2.303811104297638\n",
      "Epoch 4, Batch 100, Loss: 2.303529679775238\n",
      "Epoch 4, Batch 200, Loss: 2.3028607964515686\n",
      "Epoch 4, Batch 300, Loss: 2.3036207962036133\n",
      "Epoch 4, Batch 400, Loss: 2.3031912875175475\n",
      "Epoch 4, Batch 500, Loss: 2.3039297318458556\n",
      "Epoch 4, Batch 600, Loss: 2.3034856009483335\n",
      "Epoch 4, Batch 700, Loss: 2.3037907481193542\n",
      "Epoch 4, Batch 800, Loss: 2.3035936164855957\n",
      "Epoch 4, Batch 900, Loss: 2.3039035558700562\n",
      "Epoch 5, Batch 100, Loss: 2.303763029575348\n",
      "Epoch 5, Batch 200, Loss: 2.3035410046577454\n",
      "Epoch 5, Batch 300, Loss: 2.303071882724762\n",
      "Epoch 5, Batch 400, Loss: 2.3036704683303832\n",
      "Epoch 5, Batch 500, Loss: 2.3036740136146547\n",
      "Epoch 5, Batch 600, Loss: 2.3033272218704224\n",
      "Epoch 5, Batch 700, Loss: 2.3038864612579344\n",
      "Epoch 5, Batch 800, Loss: 2.3032993340492247\n",
      "Epoch 5, Batch 900, Loss: 2.3036467695236205\n",
      "Epoch 6, Batch 100, Loss: 2.303540711402893\n",
      "Epoch 6, Batch 200, Loss: 2.303429112434387\n",
      "Epoch 6, Batch 300, Loss: 2.3030002331733703\n",
      "Epoch 6, Batch 400, Loss: 2.3034452772140503\n",
      "Epoch 6, Batch 500, Loss: 2.303708851337433\n",
      "Epoch 6, Batch 600, Loss: 2.303764705657959\n",
      "Epoch 6, Batch 700, Loss: 2.3039015316963196\n",
      "Epoch 6, Batch 800, Loss: 2.3033886981010436\n",
      "Epoch 6, Batch 900, Loss: 2.3029179620742797\n",
      "Epoch 7, Batch 100, Loss: 2.303105065822601\n",
      "Epoch 7, Batch 200, Loss: 2.30405383348465\n",
      "Epoch 7, Batch 300, Loss: 2.303318853378296\n",
      "Epoch 7, Batch 400, Loss: 2.3035662364959717\n",
      "Epoch 7, Batch 500, Loss: 2.3039598536491392\n",
      "Epoch 7, Batch 600, Loss: 2.303648917675018\n",
      "Epoch 7, Batch 700, Loss: 2.30289039850235\n",
      "Epoch 7, Batch 800, Loss: 2.3034816670417784\n",
      "Epoch 7, Batch 900, Loss: 2.3034800839424134\n",
      "Epoch 8, Batch 100, Loss: 2.3038009428977966\n",
      "Epoch 8, Batch 200, Loss: 2.3036041975021364\n",
      "Epoch 8, Batch 300, Loss: 2.3036169171333314\n",
      "Epoch 8, Batch 400, Loss: 2.303552269935608\n",
      "Epoch 8, Batch 500, Loss: 2.3037766122817995\n",
      "Epoch 8, Batch 600, Loss: 2.3037154293060302\n",
      "Epoch 8, Batch 700, Loss: 2.3033249258995054\n",
      "Epoch 8, Batch 800, Loss: 2.30357985496521\n",
      "Epoch 8, Batch 900, Loss: 2.3030888271331786\n",
      "Epoch 9, Batch 100, Loss: 2.3032666563987734\n",
      "Epoch 9, Batch 200, Loss: 2.3035888624191285\n",
      "Epoch 9, Batch 300, Loss: 2.303043341636658\n",
      "Epoch 9, Batch 400, Loss: 2.3035276889801026\n",
      "Epoch 9, Batch 500, Loss: 2.303809700012207\n",
      "Epoch 9, Batch 600, Loss: 2.3035439109802245\n",
      "Epoch 9, Batch 700, Loss: 2.303162362575531\n",
      "Epoch 9, Batch 800, Loss: 2.3030027103424073\n",
      "Epoch 9, Batch 900, Loss: 2.303860373497009\n",
      "Epoch 10, Batch 100, Loss: 2.303241515159607\n",
      "Epoch 10, Batch 200, Loss: 2.3034480571746827\n",
      "Epoch 10, Batch 300, Loss: 2.303469488620758\n",
      "Epoch 10, Batch 400, Loss: 2.303504915237427\n",
      "Epoch 10, Batch 500, Loss: 2.303207314014435\n",
      "Epoch 10, Batch 600, Loss: 2.3034766364097594\n",
      "Epoch 10, Batch 700, Loss: 2.3033011198043822\n",
      "Epoch 10, Batch 800, Loss: 2.3037761211395265\n",
      "Epoch 10, Batch 900, Loss: 2.3041083145141603\n",
      "Epoch 11, Batch 100, Loss: 2.303830633163452\n",
      "Epoch 11, Batch 200, Loss: 2.303273551464081\n",
      "Epoch 11, Batch 300, Loss: 2.303490798473358\n",
      "Epoch 11, Batch 400, Loss: 2.303602738380432\n",
      "Epoch 11, Batch 500, Loss: 2.3035513758659363\n",
      "Epoch 11, Batch 600, Loss: 2.303257029056549\n",
      "Epoch 11, Batch 700, Loss: 2.302943694591522\n",
      "Epoch 11, Batch 800, Loss: 2.304014298915863\n",
      "Epoch 11, Batch 900, Loss: 2.3037171840667723\n",
      "Epoch 12, Batch 100, Loss: 2.302765963077545\n",
      "Epoch 12, Batch 200, Loss: 2.303543782234192\n",
      "Epoch 12, Batch 300, Loss: 2.3037041306495665\n",
      "Epoch 12, Batch 400, Loss: 2.303763110637665\n",
      "Epoch 12, Batch 500, Loss: 2.303179223537445\n",
      "Epoch 12, Batch 600, Loss: 2.3038526010513305\n",
      "Epoch 12, Batch 700, Loss: 2.3039993119239806\n",
      "Epoch 12, Batch 800, Loss: 2.3036298298835756\n",
      "Epoch 12, Batch 900, Loss: 2.303550057411194\n",
      "Epoch 13, Batch 100, Loss: 2.3034132075309754\n",
      "Epoch 13, Batch 200, Loss: 2.3037575936317443\n",
      "Epoch 13, Batch 300, Loss: 2.303494381904602\n",
      "Epoch 13, Batch 400, Loss: 2.3032419228553773\n",
      "Epoch 13, Batch 500, Loss: 2.30316819190979\n",
      "Epoch 13, Batch 600, Loss: 2.3028956484794616\n",
      "Epoch 13, Batch 700, Loss: 2.30407897233963\n",
      "Epoch 13, Batch 800, Loss: 2.3026601886749267\n",
      "Epoch 13, Batch 900, Loss: 2.3030688118934632\n",
      "Epoch 14, Batch 100, Loss: 2.3037082147598267\n",
      "Epoch 14, Batch 200, Loss: 2.303473038673401\n",
      "Epoch 14, Batch 300, Loss: 2.3031844234466554\n",
      "Epoch 14, Batch 400, Loss: 2.3030495166778566\n",
      "Epoch 14, Batch 500, Loss: 2.303002345561981\n",
      "Epoch 14, Batch 600, Loss: 2.3037581706047057\n",
      "Epoch 14, Batch 700, Loss: 2.303521282672882\n",
      "Epoch 14, Batch 800, Loss: 2.303716778755188\n",
      "Epoch 14, Batch 900, Loss: 2.303299343585968\n",
      "Epoch 15, Batch 100, Loss: 2.3033419322967528\n",
      "Epoch 15, Batch 200, Loss: 2.3033446860313416\n",
      "Epoch 15, Batch 300, Loss: 2.3031600975990294\n",
      "Epoch 15, Batch 400, Loss: 2.3036374855041504\n",
      "Epoch 15, Batch 500, Loss: 2.3033532023429872\n",
      "Epoch 15, Batch 600, Loss: 2.303210618495941\n",
      "Epoch 15, Batch 700, Loss: 2.303307580947876\n",
      "Epoch 15, Batch 800, Loss: 2.3029991292953493\n",
      "Epoch 15, Batch 900, Loss: 2.3039534735679625\n",
      "Epoch 16, Batch 100, Loss: 2.3037730765342714\n",
      "Epoch 16, Batch 200, Loss: 2.30355929851532\n",
      "Epoch 16, Batch 300, Loss: 2.3032679891586305\n",
      "Epoch 16, Batch 400, Loss: 2.303838028907776\n",
      "Epoch 16, Batch 500, Loss: 2.3025978946685792\n",
      "Epoch 16, Batch 600, Loss: 2.3031486010551454\n",
      "Epoch 16, Batch 700, Loss: 2.303724389076233\n",
      "Epoch 16, Batch 800, Loss: 2.303922762870789\n",
      "Epoch 16, Batch 900, Loss: 2.3032177114486694\n",
      "Epoch 17, Batch 100, Loss: 2.3036144185066223\n",
      "Epoch 17, Batch 200, Loss: 2.3034917736053466\n",
      "Epoch 17, Batch 300, Loss: 2.3031200480461123\n",
      "Epoch 17, Batch 400, Loss: 2.303689630031586\n",
      "Epoch 17, Batch 500, Loss: 2.303693814277649\n",
      "Epoch 17, Batch 600, Loss: 2.3034540843963622\n",
      "Epoch 17, Batch 700, Loss: 2.3033974957466126\n",
      "Epoch 17, Batch 800, Loss: 2.303700304031372\n",
      "Epoch 17, Batch 900, Loss: 2.3029665613174437\n",
      "Epoch 18, Batch 100, Loss: 2.3036043214797974\n",
      "Epoch 18, Batch 200, Loss: 2.303355939388275\n",
      "Epoch 18, Batch 300, Loss: 2.3032907319068907\n",
      "Epoch 18, Batch 400, Loss: 2.303753559589386\n",
      "Epoch 18, Batch 500, Loss: 2.303792459964752\n",
      "Epoch 18, Batch 600, Loss: 2.303160216808319\n",
      "Epoch 18, Batch 700, Loss: 2.3036786270141603\n",
      "Epoch 18, Batch 800, Loss: 2.3035402941703795\n",
      "Epoch 18, Batch 900, Loss: 2.3030238723754883\n",
      "Epoch 19, Batch 100, Loss: 2.3030215740203857\n",
      "Epoch 19, Batch 200, Loss: 2.3036980652809143\n",
      "Epoch 19, Batch 300, Loss: 2.303840343952179\n",
      "Epoch 19, Batch 400, Loss: 2.3033542895317076\n",
      "Epoch 19, Batch 500, Loss: 2.3032865214347837\n",
      "Epoch 19, Batch 600, Loss: 2.3035737562179563\n",
      "Epoch 19, Batch 700, Loss: 2.303978865146637\n",
      "Epoch 19, Batch 800, Loss: 2.3037677359580995\n",
      "Epoch 19, Batch 900, Loss: 2.3038031911849974\n",
      "Epoch 20, Batch 100, Loss: 2.303588955402374\n",
      "Epoch 20, Batch 200, Loss: 2.3033349752426147\n",
      "Epoch 20, Batch 300, Loss: 2.303748064041138\n",
      "Epoch 20, Batch 400, Loss: 2.303859510421753\n",
      "Epoch 20, Batch 500, Loss: 2.3029022312164305\n",
      "Epoch 20, Batch 600, Loss: 2.303829543590546\n",
      "Epoch 20, Batch 700, Loss: 2.3033766317367554\n",
      "Epoch 20, Batch 800, Loss: 2.3037399673461914\n",
      "Epoch 20, Batch 900, Loss: 2.30373747587204\n",
      "Epoch 21, Batch 100, Loss: 2.3032027006149294\n",
      "Epoch 21, Batch 200, Loss: 2.302998526096344\n",
      "Epoch 21, Batch 300, Loss: 2.3035054302215574\n",
      "Epoch 21, Batch 400, Loss: 2.3033882761001587\n",
      "Epoch 21, Batch 500, Loss: 2.3034634685516355\n",
      "Epoch 21, Batch 600, Loss: 2.3034543323516847\n",
      "Epoch 21, Batch 700, Loss: 2.3040110063552857\n",
      "Epoch 21, Batch 800, Loss: 2.3030495166778566\n",
      "Epoch 21, Batch 900, Loss: 2.3039094519615175\n",
      "Epoch 22, Batch 100, Loss: 2.3033748149871824\n",
      "Epoch 22, Batch 200, Loss: 2.303406710624695\n",
      "Epoch 22, Batch 300, Loss: 2.3032794284820555\n",
      "Epoch 22, Batch 400, Loss: 2.3036532902717592\n",
      "Epoch 22, Batch 500, Loss: 2.3033994889259337\n",
      "Epoch 22, Batch 600, Loss: 2.303865649700165\n",
      "Epoch 22, Batch 700, Loss: 2.303857960700989\n",
      "Epoch 22, Batch 800, Loss: 2.3031346821784973\n",
      "Epoch 22, Batch 900, Loss: 2.303631057739258\n",
      "Epoch 23, Batch 100, Loss: 2.3034176135063174\n",
      "Epoch 23, Batch 200, Loss: 2.303757646083832\n",
      "Epoch 23, Batch 300, Loss: 2.303983311653137\n",
      "Epoch 23, Batch 400, Loss: 2.30301794052124\n",
      "Epoch 23, Batch 500, Loss: 2.3035501623153687\n",
      "Epoch 23, Batch 600, Loss: 2.3036947417259217\n",
      "Epoch 23, Batch 700, Loss: 2.3034977102279663\n",
      "Epoch 23, Batch 800, Loss: 2.303322231769562\n",
      "Epoch 23, Batch 900, Loss: 2.3034163117408752\n",
      "Epoch 24, Batch 100, Loss: 2.303115630149841\n",
      "Epoch 24, Batch 200, Loss: 2.303509497642517\n",
      "Epoch 24, Batch 300, Loss: 2.303274919986725\n",
      "Epoch 24, Batch 400, Loss: 2.3037693667411805\n",
      "Epoch 24, Batch 500, Loss: 2.304029450416565\n",
      "Epoch 24, Batch 600, Loss: 2.3034630727767946\n",
      "Epoch 24, Batch 700, Loss: 2.3033664798736573\n",
      "Epoch 24, Batch 800, Loss: 2.303630518913269\n",
      "Epoch 24, Batch 900, Loss: 2.303193898200989\n",
      "Epoch 25, Batch 100, Loss: 2.3035481643676756\n",
      "Epoch 25, Batch 200, Loss: 2.3035001873970034\n",
      "Epoch 25, Batch 300, Loss: 2.303141663074493\n",
      "Epoch 25, Batch 400, Loss: 2.3038637804985047\n",
      "Epoch 25, Batch 500, Loss: 2.3036431312561034\n",
      "Epoch 25, Batch 600, Loss: 2.3037898182868957\n",
      "Epoch 25, Batch 700, Loss: 2.303093798160553\n",
      "Epoch 25, Batch 800, Loss: 2.3036713671684264\n",
      "Epoch 25, Batch 900, Loss: 2.3040351057052613\n",
      "Accuracy on test set: 0.1%\n",
      "Fitting for combination 68\n",
      "784\n",
      "4\n",
      "10\n",
      "[40, 10, 10, 30, 10]\n",
      "False\n",
      "['relu', 'sigmoid', 'relu', 'relu']\n",
      "Adam\n",
      "0.03\n",
      "0.03\n",
      "CrossEntropyLoss\n",
      "50\n",
      "25\n",
      "0\n",
      "Epoch 1, Batch 50, Loss: 1.1523142123222352\n",
      "Epoch 1, Batch 100, Loss: 1.1522884488105773\n",
      "Epoch 1, Batch 150, Loss: 1.1525759935379027\n",
      "Epoch 1, Batch 200, Loss: 1.1525592088699341\n",
      "Epoch 1, Batch 250, Loss: 1.1521885371208191\n",
      "Epoch 1, Batch 300, Loss: 1.1522965598106385\n",
      "Epoch 1, Batch 350, Loss: 1.1517561602592468\n",
      "Epoch 1, Batch 400, Loss: 1.1515528821945191\n",
      "Epoch 1, Batch 450, Loss: 1.1533828401565551\n",
      "Epoch 1, Batch 500, Loss: 1.1510368967056275\n",
      "Epoch 1, Batch 550, Loss: 1.1527642726898193\n",
      "Epoch 1, Batch 600, Loss: 1.1527907514572144\n",
      "Epoch 1, Batch 650, Loss: 1.1517572569847108\n",
      "Epoch 1, Batch 700, Loss: 1.1525966596603394\n",
      "Epoch 1, Batch 750, Loss: 1.1516302037239075\n",
      "Epoch 1, Batch 800, Loss: 1.1526189732551575\n",
      "Epoch 1, Batch 850, Loss: 1.1526236987113954\n",
      "Epoch 1, Batch 900, Loss: 1.1525334167480468\n",
      "Epoch 2, Batch 50, Loss: 1.1526217913627625\n",
      "Epoch 2, Batch 100, Loss: 1.152974123954773\n",
      "Epoch 2, Batch 150, Loss: 1.1523320198059082\n",
      "Epoch 2, Batch 200, Loss: 1.1529291224479676\n",
      "Epoch 2, Batch 250, Loss: 1.1520062947273255\n",
      "Epoch 2, Batch 300, Loss: 1.1521301698684692\n",
      "Epoch 2, Batch 350, Loss: 1.1525577187538147\n",
      "Epoch 2, Batch 400, Loss: 1.1516714119911193\n",
      "Epoch 2, Batch 450, Loss: 1.1532215642929078\n",
      "Epoch 2, Batch 500, Loss: 1.1529915380477904\n",
      "Epoch 2, Batch 550, Loss: 1.1523202991485595\n",
      "Epoch 2, Batch 600, Loss: 1.152478814125061\n",
      "Epoch 2, Batch 650, Loss: 1.1522194409370423\n",
      "Epoch 2, Batch 700, Loss: 1.1520511603355408\n",
      "Epoch 2, Batch 750, Loss: 1.1529747152328491\n",
      "Epoch 2, Batch 800, Loss: 1.1526084399223329\n",
      "Epoch 2, Batch 850, Loss: 1.1516802835464477\n",
      "Epoch 2, Batch 900, Loss: 1.1527019715309144\n",
      "Epoch 3, Batch 50, Loss: 1.152109179496765\n",
      "Epoch 3, Batch 100, Loss: 1.1522914004325866\n",
      "Epoch 3, Batch 150, Loss: 1.1526713848114014\n",
      "Epoch 3, Batch 200, Loss: 1.1520038509368897\n",
      "Epoch 3, Batch 250, Loss: 1.1528839778900146\n",
      "Epoch 3, Batch 300, Loss: 1.152103338241577\n",
      "Epoch 3, Batch 350, Loss: 1.152606363296509\n",
      "Epoch 3, Batch 400, Loss: 1.1526375603675842\n",
      "Epoch 3, Batch 450, Loss: 1.1520893931388856\n",
      "Epoch 3, Batch 500, Loss: 1.1523066210746764\n",
      "Epoch 3, Batch 550, Loss: 1.152447144985199\n",
      "Epoch 3, Batch 600, Loss: 1.1518561553955078\n",
      "Epoch 3, Batch 650, Loss: 1.1526603865623475\n",
      "Epoch 3, Batch 700, Loss: 1.1524500894546508\n",
      "Epoch 3, Batch 750, Loss: 1.15278972864151\n",
      "Epoch 3, Batch 800, Loss: 1.152613980770111\n",
      "Epoch 3, Batch 850, Loss: 1.1522675704956056\n",
      "Epoch 3, Batch 900, Loss: 1.1524761128425598\n",
      "Epoch 4, Batch 50, Loss: 1.1533219528198242\n",
      "Epoch 4, Batch 100, Loss: 1.152184886932373\n",
      "Epoch 4, Batch 150, Loss: 1.1518386936187743\n",
      "Epoch 4, Batch 200, Loss: 1.1516774368286133\n",
      "Epoch 4, Batch 250, Loss: 1.152968065738678\n",
      "Epoch 4, Batch 300, Loss: 1.1526242566108704\n",
      "Epoch 4, Batch 350, Loss: 1.1522863125801086\n",
      "Epoch 4, Batch 400, Loss: 1.1533246898651124\n",
      "Epoch 4, Batch 450, Loss: 1.1516087293624877\n",
      "Epoch 4, Batch 500, Loss: 1.1527204489707947\n",
      "Epoch 4, Batch 550, Loss: 1.152740602493286\n",
      "Epoch 4, Batch 600, Loss: 1.1514065718650819\n",
      "Epoch 4, Batch 650, Loss: 1.153040692806244\n",
      "Epoch 4, Batch 700, Loss: 1.1522463631629944\n",
      "Epoch 4, Batch 750, Loss: 1.152313461303711\n",
      "Epoch 4, Batch 800, Loss: 1.1519807386398315\n",
      "Epoch 4, Batch 850, Loss: 1.152876114845276\n",
      "Epoch 4, Batch 900, Loss: 1.152443549633026\n",
      "Epoch 5, Batch 50, Loss: 1.1523587322235107\n",
      "Epoch 5, Batch 100, Loss: 1.152387351989746\n",
      "Epoch 5, Batch 150, Loss: 1.1521589326858521\n",
      "Epoch 5, Batch 200, Loss: 1.1526002168655396\n",
      "Epoch 5, Batch 250, Loss: 1.15222149848938\n",
      "Epoch 5, Batch 300, Loss: 1.1516197562217712\n",
      "Epoch 5, Batch 350, Loss: 1.1525293159484864\n",
      "Epoch 5, Batch 400, Loss: 1.1525447487831115\n",
      "Epoch 5, Batch 450, Loss: 1.1529080867767334\n",
      "Epoch 5, Batch 500, Loss: 1.1523760533332825\n",
      "Epoch 5, Batch 550, Loss: 1.1527138876914977\n",
      "Epoch 5, Batch 600, Loss: 1.152531316280365\n",
      "Epoch 5, Batch 650, Loss: 1.1526028442382812\n",
      "Epoch 5, Batch 700, Loss: 1.1526203751564026\n",
      "Epoch 5, Batch 750, Loss: 1.152446219921112\n",
      "Epoch 5, Batch 800, Loss: 1.1522346425056458\n",
      "Epoch 5, Batch 850, Loss: 1.1517901253700256\n",
      "Epoch 5, Batch 900, Loss: 1.1523054242134094\n",
      "Epoch 6, Batch 50, Loss: 1.1527465796470642\n",
      "Epoch 6, Batch 100, Loss: 1.1527890825271607\n",
      "Epoch 6, Batch 150, Loss: 1.1523415923118592\n",
      "Epoch 6, Batch 200, Loss: 1.1519138932228088\n",
      "Epoch 6, Batch 250, Loss: 1.152224702835083\n",
      "Epoch 6, Batch 300, Loss: 1.1524409341812134\n",
      "Epoch 6, Batch 350, Loss: 1.1526427674293518\n",
      "Epoch 6, Batch 400, Loss: 1.1518484210968019\n",
      "Epoch 6, Batch 450, Loss: 1.1523082423210145\n",
      "Epoch 6, Batch 500, Loss: 1.152670612335205\n",
      "Epoch 6, Batch 550, Loss: 1.1519352626800536\n",
      "Epoch 6, Batch 600, Loss: 1.1528151774406432\n",
      "Epoch 6, Batch 650, Loss: 1.1518791246414184\n",
      "Epoch 6, Batch 700, Loss: 1.152010853290558\n",
      "Epoch 6, Batch 750, Loss: 1.152113869190216\n",
      "Epoch 6, Batch 800, Loss: 1.1522397899627685\n",
      "Epoch 6, Batch 850, Loss: 1.1519597959518433\n",
      "Epoch 6, Batch 900, Loss: 1.1514173865318298\n",
      "Epoch 7, Batch 50, Loss: 1.1520139861106873\n",
      "Epoch 7, Batch 100, Loss: 1.153255832195282\n",
      "Epoch 7, Batch 150, Loss: 1.1529901647567748\n",
      "Epoch 7, Batch 200, Loss: 1.152848243713379\n",
      "Epoch 7, Batch 250, Loss: 1.1524767994880676\n",
      "Epoch 7, Batch 300, Loss: 1.1520040583610536\n",
      "Epoch 7, Batch 350, Loss: 1.152635989189148\n",
      "Epoch 7, Batch 400, Loss: 1.1525502038002013\n",
      "Epoch 7, Batch 450, Loss: 1.151194908618927\n",
      "Epoch 7, Batch 500, Loss: 1.1522975063323975\n",
      "Epoch 7, Batch 550, Loss: 1.1524991798400879\n",
      "Epoch 7, Batch 600, Loss: 1.1518648433685303\n",
      "Epoch 7, Batch 650, Loss: 1.1523732161521911\n",
      "Epoch 7, Batch 700, Loss: 1.1529735183715821\n",
      "Epoch 7, Batch 750, Loss: 1.1528017830848694\n",
      "Epoch 7, Batch 800, Loss: 1.1522408056259155\n",
      "Epoch 7, Batch 850, Loss: 1.152734022140503\n",
      "Epoch 7, Batch 900, Loss: 1.1527075910568236\n",
      "Epoch 8, Batch 50, Loss: 1.1529502630233766\n",
      "Epoch 8, Batch 100, Loss: 1.1523128366470337\n",
      "Epoch 8, Batch 150, Loss: 1.1524286270141602\n",
      "Epoch 8, Batch 200, Loss: 1.1527134490013122\n",
      "Epoch 8, Batch 250, Loss: 1.152197790145874\n",
      "Epoch 8, Batch 300, Loss: 1.1522681570053102\n",
      "Epoch 8, Batch 350, Loss: 1.1521621441841126\n",
      "Epoch 8, Batch 400, Loss: 1.1522354912757873\n",
      "Epoch 8, Batch 450, Loss: 1.1526081466674805\n",
      "Epoch 8, Batch 500, Loss: 1.1523699808120726\n",
      "Epoch 8, Batch 550, Loss: 1.1527850008010865\n",
      "Epoch 8, Batch 600, Loss: 1.1532669520378114\n",
      "Epoch 8, Batch 650, Loss: 1.1521588563919067\n",
      "Epoch 8, Batch 700, Loss: 1.153030743598938\n",
      "Epoch 8, Batch 750, Loss: 1.1523129105567933\n",
      "Epoch 8, Batch 800, Loss: 1.1519771575927735\n",
      "Epoch 8, Batch 850, Loss: 1.1528357553482056\n",
      "Epoch 8, Batch 900, Loss: 1.152384717464447\n",
      "Epoch 9, Batch 50, Loss: 1.1517452907562256\n",
      "Epoch 9, Batch 100, Loss: 1.152762122154236\n",
      "Epoch 9, Batch 150, Loss: 1.1520606875419617\n",
      "Epoch 9, Batch 200, Loss: 1.1521099090576172\n",
      "Epoch 9, Batch 250, Loss: 1.1523969173431396\n",
      "Epoch 9, Batch 300, Loss: 1.15225172996521\n",
      "Epoch 9, Batch 350, Loss: 1.1521202826499939\n",
      "Epoch 9, Batch 400, Loss: 1.152478904724121\n",
      "Epoch 9, Batch 450, Loss: 1.1517569756507873\n",
      "Epoch 9, Batch 500, Loss: 1.1527947497367859\n",
      "Epoch 9, Batch 550, Loss: 1.15257586479187\n",
      "Epoch 9, Batch 600, Loss: 1.1527241277694702\n",
      "Epoch 9, Batch 650, Loss: 1.1520325756072998\n",
      "Epoch 9, Batch 700, Loss: 1.1527821016311646\n",
      "Epoch 9, Batch 750, Loss: 1.1521372509002685\n",
      "Epoch 9, Batch 800, Loss: 1.1516644763946533\n",
      "Epoch 9, Batch 850, Loss: 1.1517587065696717\n",
      "Epoch 9, Batch 900, Loss: 1.15225759267807\n",
      "Epoch 10, Batch 50, Loss: 1.1528161811828612\n",
      "Epoch 10, Batch 100, Loss: 1.1522544717788696\n",
      "Epoch 10, Batch 150, Loss: 1.1522979736328125\n",
      "Epoch 10, Batch 200, Loss: 1.151734368801117\n",
      "Epoch 10, Batch 250, Loss: 1.1523273444175721\n",
      "Epoch 10, Batch 300, Loss: 1.1531090497970582\n",
      "Epoch 10, Batch 350, Loss: 1.1518847274780273\n",
      "Epoch 10, Batch 400, Loss: 1.1526444363594055\n",
      "Epoch 10, Batch 450, Loss: 1.1524453210830687\n",
      "Epoch 10, Batch 500, Loss: 1.151966302394867\n",
      "Epoch 10, Batch 550, Loss: 1.1524098706245423\n",
      "Epoch 10, Batch 600, Loss: 1.152729833126068\n",
      "Epoch 10, Batch 650, Loss: 1.152162811756134\n",
      "Epoch 10, Batch 700, Loss: 1.1525672054290772\n",
      "Epoch 10, Batch 750, Loss: 1.1527119612693786\n",
      "Epoch 10, Batch 800, Loss: 1.1522875690460206\n",
      "Epoch 10, Batch 850, Loss: 1.1521354484558106\n",
      "Epoch 10, Batch 900, Loss: 1.1527335858345031\n",
      "Epoch 11, Batch 50, Loss: 1.1513011860847473\n",
      "Epoch 11, Batch 100, Loss: 1.1526639890670776\n",
      "Epoch 11, Batch 150, Loss: 1.152503788471222\n",
      "Epoch 11, Batch 200, Loss: 1.1529337406158446\n",
      "Epoch 11, Batch 250, Loss: 1.1518817520141602\n",
      "Epoch 11, Batch 300, Loss: 1.1524201321601868\n",
      "Epoch 11, Batch 350, Loss: 1.1526278448104859\n",
      "Epoch 11, Batch 400, Loss: 1.15217857837677\n",
      "Epoch 11, Batch 450, Loss: 1.1525114107131957\n",
      "Epoch 11, Batch 500, Loss: 1.1522774291038513\n",
      "Epoch 11, Batch 550, Loss: 1.1523373317718506\n",
      "Epoch 11, Batch 600, Loss: 1.1522118711471558\n",
      "Epoch 11, Batch 650, Loss: 1.151886715888977\n",
      "Epoch 11, Batch 700, Loss: 1.1524001145362854\n",
      "Epoch 11, Batch 750, Loss: 1.1524458193778992\n",
      "Epoch 11, Batch 800, Loss: 1.1528211164474487\n",
      "Epoch 11, Batch 850, Loss: 1.1522969961166383\n",
      "Epoch 11, Batch 900, Loss: 1.1521942901611328\n",
      "Epoch 12, Batch 50, Loss: 1.1526247358322144\n",
      "Epoch 12, Batch 100, Loss: 1.1521598029136657\n",
      "Epoch 12, Batch 150, Loss: 1.1527453327178956\n",
      "Epoch 12, Batch 200, Loss: 1.1525155472755433\n",
      "Epoch 12, Batch 250, Loss: 1.1517481517791748\n",
      "Epoch 12, Batch 300, Loss: 1.151919949054718\n",
      "Epoch 12, Batch 350, Loss: 1.1522575664520263\n",
      "Epoch 12, Batch 400, Loss: 1.1524184584617614\n",
      "Epoch 12, Batch 450, Loss: 1.151444263458252\n",
      "Epoch 12, Batch 500, Loss: 1.1519576072692872\n",
      "Epoch 12, Batch 550, Loss: 1.1523351955413819\n",
      "Epoch 12, Batch 600, Loss: 1.152137234210968\n",
      "Epoch 12, Batch 650, Loss: 1.1527319002151488\n",
      "Epoch 12, Batch 700, Loss: 1.1519304203987122\n",
      "Epoch 12, Batch 750, Loss: 1.1535393857955933\n",
      "Epoch 12, Batch 800, Loss: 1.152285852432251\n",
      "Epoch 12, Batch 850, Loss: 1.1521641683578492\n",
      "Epoch 12, Batch 900, Loss: 1.151417498588562\n",
      "Epoch 13, Batch 50, Loss: 1.1524535870552064\n",
      "Epoch 13, Batch 100, Loss: 1.1523623633384705\n",
      "Epoch 13, Batch 150, Loss: 1.1520464873313905\n",
      "Epoch 13, Batch 200, Loss: 1.1525846362113952\n",
      "Epoch 13, Batch 250, Loss: 1.1521634793281554\n",
      "Epoch 13, Batch 300, Loss: 1.1519639611244201\n",
      "Epoch 13, Batch 350, Loss: 1.152411482334137\n",
      "Epoch 13, Batch 400, Loss: 1.152025694847107\n",
      "Epoch 13, Batch 450, Loss: 1.1524450278282166\n",
      "Epoch 13, Batch 500, Loss: 1.1522449231147767\n",
      "Epoch 13, Batch 550, Loss: 1.1522278189659119\n",
      "Epoch 13, Batch 600, Loss: 1.151823375225067\n",
      "Epoch 13, Batch 650, Loss: 1.1520291399955749\n",
      "Epoch 13, Batch 700, Loss: 1.1524028658866883\n",
      "Epoch 13, Batch 750, Loss: 1.1520268177986146\n",
      "Epoch 13, Batch 800, Loss: 1.1517688894271851\n",
      "Epoch 13, Batch 850, Loss: 1.152890396118164\n",
      "Epoch 13, Batch 900, Loss: 1.1523841285705567\n",
      "Epoch 14, Batch 50, Loss: 1.1515078139305115\n",
      "Epoch 14, Batch 100, Loss: 1.152435817718506\n",
      "Epoch 14, Batch 150, Loss: 1.1520832252502442\n",
      "Epoch 14, Batch 200, Loss: 1.1529315328598022\n",
      "Epoch 14, Batch 250, Loss: 1.1520214772224426\n",
      "Epoch 14, Batch 300, Loss: 1.153230800628662\n",
      "Epoch 14, Batch 350, Loss: 1.1524584269523621\n",
      "Epoch 14, Batch 400, Loss: 1.1521392917633058\n",
      "Epoch 14, Batch 450, Loss: 1.1516053771972656\n",
      "Epoch 14, Batch 500, Loss: 1.1522248101234436\n",
      "Epoch 14, Batch 550, Loss: 1.1528392124176026\n",
      "Epoch 14, Batch 600, Loss: 1.1522335267066957\n",
      "Epoch 14, Batch 650, Loss: 1.152484383583069\n",
      "Epoch 14, Batch 700, Loss: 1.152859194278717\n",
      "Epoch 14, Batch 750, Loss: 1.1519792890548706\n",
      "Epoch 14, Batch 800, Loss: 1.1519294238090516\n",
      "Epoch 14, Batch 850, Loss: 1.1521174311637878\n",
      "Epoch 14, Batch 900, Loss: 1.1526029777526856\n",
      "Epoch 15, Batch 50, Loss: 1.1519043517112733\n",
      "Epoch 15, Batch 100, Loss: 1.1526102662086486\n",
      "Epoch 15, Batch 150, Loss: 1.1522331953048706\n",
      "Epoch 15, Batch 200, Loss: 1.152737159729004\n",
      "Epoch 15, Batch 250, Loss: 1.1520073556900023\n",
      "Epoch 15, Batch 300, Loss: 1.1516717958450318\n",
      "Epoch 15, Batch 350, Loss: 1.1531359052658081\n",
      "Epoch 15, Batch 400, Loss: 1.152430784702301\n",
      "Epoch 15, Batch 450, Loss: 1.1521332311630248\n",
      "Epoch 15, Batch 500, Loss: 1.1523782801628113\n",
      "Epoch 15, Batch 550, Loss: 1.1533254289627075\n",
      "Epoch 15, Batch 600, Loss: 1.1523209357261657\n",
      "Epoch 15, Batch 650, Loss: 1.152670373916626\n",
      "Epoch 15, Batch 700, Loss: 1.15181161403656\n",
      "Epoch 15, Batch 750, Loss: 1.1527052974700929\n",
      "Epoch 15, Batch 800, Loss: 1.1520690822601318\n",
      "Epoch 15, Batch 850, Loss: 1.1521586179733276\n",
      "Epoch 15, Batch 900, Loss: 1.152790756225586\n",
      "Epoch 16, Batch 50, Loss: 1.1528291177749634\n",
      "Epoch 16, Batch 100, Loss: 1.1524907493591308\n",
      "Epoch 16, Batch 150, Loss: 1.1527204036712646\n",
      "Epoch 16, Batch 200, Loss: 1.1522438597679139\n",
      "Epoch 16, Batch 250, Loss: 1.1524393105506896\n",
      "Epoch 16, Batch 300, Loss: 1.1524757289886474\n",
      "Epoch 16, Batch 350, Loss: 1.1520954394340515\n",
      "Epoch 16, Batch 400, Loss: 1.1521026110649109\n",
      "Epoch 16, Batch 450, Loss: 1.1517549324035645\n",
      "Epoch 16, Batch 500, Loss: 1.1531122827529907\n",
      "Epoch 16, Batch 550, Loss: 1.1525283932685852\n",
      "Epoch 16, Batch 600, Loss: 1.1526085066795348\n",
      "Epoch 16, Batch 650, Loss: 1.1530005764961242\n",
      "Epoch 16, Batch 700, Loss: 1.1517758083343506\n",
      "Epoch 16, Batch 750, Loss: 1.1516529965400695\n",
      "Epoch 16, Batch 800, Loss: 1.1536037182807923\n",
      "Epoch 16, Batch 850, Loss: 1.1521845245361328\n",
      "Epoch 16, Batch 900, Loss: 1.1524806547164916\n",
      "Epoch 17, Batch 50, Loss: 1.1524288415908814\n",
      "Epoch 17, Batch 100, Loss: 1.1522962737083435\n",
      "Epoch 17, Batch 150, Loss: 1.152710189819336\n",
      "Epoch 17, Batch 200, Loss: 1.1526333045959474\n",
      "Epoch 17, Batch 250, Loss: 1.1522536754608155\n",
      "Epoch 17, Batch 300, Loss: 1.151993124485016\n",
      "Epoch 17, Batch 350, Loss: 1.1522002482414246\n",
      "Epoch 17, Batch 400, Loss: 1.152624592781067\n",
      "Epoch 17, Batch 450, Loss: 1.1529940557479859\n",
      "Epoch 17, Batch 500, Loss: 1.1517988801002503\n",
      "Epoch 17, Batch 550, Loss: 1.1521485710144044\n",
      "Epoch 17, Batch 600, Loss: 1.1520407843589782\n",
      "Epoch 17, Batch 650, Loss: 1.1521899271011353\n",
      "Epoch 17, Batch 700, Loss: 1.1519976902008056\n",
      "Epoch 17, Batch 750, Loss: 1.153026442527771\n",
      "Epoch 17, Batch 800, Loss: 1.1525712656974791\n",
      "Epoch 17, Batch 850, Loss: 1.1519320225715637\n",
      "Epoch 17, Batch 900, Loss: 1.1517982697486877\n",
      "Epoch 18, Batch 50, Loss: 1.1515192770957947\n",
      "Epoch 18, Batch 100, Loss: 1.1528216981887818\n",
      "Epoch 18, Batch 150, Loss: 1.1522451877593993\n",
      "Epoch 18, Batch 200, Loss: 1.1516699695587158\n",
      "Epoch 18, Batch 250, Loss: 1.1527877020835877\n",
      "Epoch 18, Batch 300, Loss: 1.1523636269569397\n",
      "Epoch 18, Batch 350, Loss: 1.152832396030426\n",
      "Epoch 18, Batch 400, Loss: 1.1525463724136353\n",
      "Epoch 18, Batch 450, Loss: 1.1528216338157653\n",
      "Epoch 18, Batch 500, Loss: 1.1523270726203918\n",
      "Epoch 18, Batch 550, Loss: 1.1520444703102113\n",
      "Epoch 18, Batch 600, Loss: 1.1521791696548462\n",
      "Epoch 18, Batch 650, Loss: 1.152503664493561\n",
      "Epoch 18, Batch 700, Loss: 1.1524280095100403\n",
      "Epoch 18, Batch 750, Loss: 1.1525799965858459\n",
      "Epoch 18, Batch 800, Loss: 1.1531090569496154\n",
      "Epoch 18, Batch 850, Loss: 1.1521557450294495\n",
      "Epoch 18, Batch 900, Loss: 1.1510765528678895\n",
      "Epoch 19, Batch 50, Loss: 1.152135705947876\n",
      "Epoch 19, Batch 100, Loss: 1.1522350072860719\n",
      "Epoch 19, Batch 150, Loss: 1.1522223162651062\n",
      "Epoch 19, Batch 200, Loss: 1.1524738693237304\n",
      "Epoch 19, Batch 250, Loss: 1.1517504525184632\n",
      "Epoch 19, Batch 300, Loss: 1.1519978666305541\n",
      "Epoch 19, Batch 350, Loss: 1.1529402232170105\n",
      "Epoch 19, Batch 400, Loss: 1.152421681880951\n",
      "Epoch 19, Batch 450, Loss: 1.152128176689148\n",
      "Epoch 19, Batch 500, Loss: 1.1523373603820801\n",
      "Epoch 19, Batch 550, Loss: 1.1525215220451355\n",
      "Epoch 19, Batch 600, Loss: 1.1519473576545716\n",
      "Epoch 19, Batch 650, Loss: 1.152077910900116\n",
      "Epoch 19, Batch 700, Loss: 1.1530048441886902\n",
      "Epoch 19, Batch 750, Loss: 1.1522214365005494\n",
      "Epoch 19, Batch 800, Loss: 1.151957778930664\n",
      "Epoch 19, Batch 850, Loss: 1.1527397775650023\n",
      "Epoch 19, Batch 900, Loss: 1.1520118355751037\n",
      "Epoch 20, Batch 50, Loss: 1.1521589493751525\n",
      "Epoch 20, Batch 100, Loss: 1.1524459910392761\n",
      "Epoch 20, Batch 150, Loss: 1.1521979427337647\n",
      "Epoch 20, Batch 200, Loss: 1.1531178379058837\n",
      "Epoch 20, Batch 250, Loss: 1.1520027422904968\n",
      "Epoch 20, Batch 300, Loss: 1.1526228713989257\n",
      "Epoch 20, Batch 350, Loss: 1.1529439330101012\n",
      "Epoch 20, Batch 400, Loss: 1.1523249125480652\n",
      "Epoch 20, Batch 450, Loss: 1.1526604008674621\n",
      "Epoch 20, Batch 500, Loss: 1.1518384385108948\n",
      "Epoch 20, Batch 550, Loss: 1.1524593043327331\n",
      "Epoch 20, Batch 600, Loss: 1.1520776009559632\n",
      "Epoch 20, Batch 650, Loss: 1.1525488948822022\n",
      "Epoch 20, Batch 700, Loss: 1.1527704405784607\n",
      "Epoch 20, Batch 750, Loss: 1.1525095295906067\n",
      "Epoch 20, Batch 800, Loss: 1.1526974749565124\n",
      "Epoch 20, Batch 850, Loss: 1.1527770256996155\n",
      "Epoch 20, Batch 900, Loss: 1.1522593331336974\n",
      "Epoch 21, Batch 50, Loss: 1.1521400094032288\n",
      "Epoch 21, Batch 100, Loss: 1.15282244682312\n",
      "Epoch 21, Batch 150, Loss: 1.1519349026679992\n",
      "Epoch 21, Batch 200, Loss: 1.152832477092743\n",
      "Epoch 21, Batch 250, Loss: 1.1521878743171692\n",
      "Epoch 21, Batch 300, Loss: 1.152833752632141\n",
      "Epoch 21, Batch 350, Loss: 1.152392463684082\n",
      "Epoch 21, Batch 400, Loss: 1.152454104423523\n",
      "Epoch 21, Batch 450, Loss: 1.1509998440742493\n",
      "Epoch 21, Batch 500, Loss: 1.152625231742859\n",
      "Epoch 21, Batch 550, Loss: 1.1527562761306762\n",
      "Epoch 21, Batch 600, Loss: 1.1515101647377015\n",
      "Epoch 21, Batch 650, Loss: 1.152289435863495\n",
      "Epoch 21, Batch 700, Loss: 1.1520135879516602\n",
      "Epoch 21, Batch 750, Loss: 1.1523091745376588\n",
      "Epoch 21, Batch 800, Loss: 1.1519182300567627\n",
      "Epoch 21, Batch 850, Loss: 1.1528554916381837\n",
      "Epoch 21, Batch 900, Loss: 1.1525139808654785\n",
      "Epoch 22, Batch 50, Loss: 1.1522544884681702\n",
      "Epoch 22, Batch 100, Loss: 1.1527165913581847\n",
      "Epoch 22, Batch 150, Loss: 1.1520053005218507\n",
      "Epoch 22, Batch 200, Loss: 1.1512039971351624\n",
      "Epoch 22, Batch 250, Loss: 1.1532178163528441\n",
      "Epoch 22, Batch 300, Loss: 1.1524591064453125\n",
      "Epoch 22, Batch 350, Loss: 1.15226669549942\n",
      "Epoch 22, Batch 400, Loss: 1.152510142326355\n",
      "Epoch 22, Batch 450, Loss: 1.1519279623031615\n",
      "Epoch 22, Batch 500, Loss: 1.152553722858429\n",
      "Epoch 22, Batch 550, Loss: 1.1527424335479737\n",
      "Epoch 22, Batch 600, Loss: 1.151898910999298\n",
      "Epoch 22, Batch 650, Loss: 1.1525490117073058\n",
      "Epoch 22, Batch 700, Loss: 1.1521376442909241\n",
      "Epoch 22, Batch 750, Loss: 1.1522325372695923\n",
      "Epoch 22, Batch 800, Loss: 1.1529118061065673\n",
      "Epoch 22, Batch 850, Loss: 1.1528566813468932\n",
      "Epoch 22, Batch 900, Loss: 1.1524378323554993\n",
      "Epoch 23, Batch 50, Loss: 1.1518284225463866\n",
      "Epoch 23, Batch 100, Loss: 1.1520262026786805\n",
      "Epoch 23, Batch 150, Loss: 1.1521695494651794\n",
      "Epoch 23, Batch 200, Loss: 1.1523715090751647\n",
      "Epoch 23, Batch 250, Loss: 1.152810320854187\n",
      "Epoch 23, Batch 300, Loss: 1.1518144273757935\n",
      "Epoch 23, Batch 350, Loss: 1.1515565729141235\n",
      "Epoch 23, Batch 400, Loss: 1.1513391757011413\n",
      "Epoch 23, Batch 450, Loss: 1.1521198129653931\n",
      "Epoch 23, Batch 500, Loss: 1.152977578639984\n",
      "Epoch 23, Batch 550, Loss: 1.1520555090904236\n",
      "Epoch 23, Batch 600, Loss: 1.1533672308921814\n",
      "Epoch 23, Batch 650, Loss: 1.1512884593009949\n",
      "Epoch 23, Batch 700, Loss: 1.1523732376098632\n",
      "Epoch 23, Batch 750, Loss: 1.1516020250320436\n",
      "Epoch 23, Batch 800, Loss: 1.1523698949813843\n",
      "Epoch 23, Batch 850, Loss: 1.152243983745575\n",
      "Epoch 23, Batch 900, Loss: 1.1523318576812744\n",
      "Epoch 24, Batch 50, Loss: 1.1524393773078918\n",
      "Epoch 24, Batch 100, Loss: 1.1522783303260804\n",
      "Epoch 24, Batch 150, Loss: 1.1528034114837646\n",
      "Epoch 24, Batch 200, Loss: 1.152367923259735\n",
      "Epoch 24, Batch 250, Loss: 1.152454228401184\n",
      "Epoch 24, Batch 300, Loss: 1.1519217228889465\n",
      "Epoch 24, Batch 350, Loss: 1.152512686252594\n",
      "Epoch 24, Batch 400, Loss: 1.1522408032417297\n",
      "Epoch 24, Batch 450, Loss: 1.1530384945869445\n",
      "Epoch 24, Batch 500, Loss: 1.1518608498573304\n",
      "Epoch 24, Batch 550, Loss: 1.152715232372284\n",
      "Epoch 24, Batch 600, Loss: 1.151833198070526\n",
      "Epoch 24, Batch 650, Loss: 1.1520283579826356\n",
      "Epoch 24, Batch 700, Loss: 1.1524286103248595\n",
      "Epoch 24, Batch 750, Loss: 1.1522295904159545\n",
      "Epoch 24, Batch 800, Loss: 1.1528224158287048\n",
      "Epoch 24, Batch 850, Loss: 1.1533116388320923\n",
      "Epoch 24, Batch 900, Loss: 1.152188081741333\n",
      "Epoch 25, Batch 50, Loss: 1.1528504443168641\n",
      "Epoch 25, Batch 100, Loss: 1.1520331478118897\n",
      "Epoch 25, Batch 150, Loss: 1.1519341850280762\n",
      "Epoch 25, Batch 200, Loss: 1.1523084211349488\n",
      "Epoch 25, Batch 250, Loss: 1.1519121074676513\n",
      "Epoch 25, Batch 300, Loss: 1.152592966556549\n",
      "Epoch 25, Batch 350, Loss: 1.1519568490982055\n",
      "Epoch 25, Batch 400, Loss: 1.1520110201835632\n",
      "Epoch 25, Batch 450, Loss: 1.1523667669296265\n",
      "Epoch 25, Batch 500, Loss: 1.151921010017395\n",
      "Epoch 25, Batch 550, Loss: 1.1523251962661742\n",
      "Epoch 25, Batch 600, Loss: 1.1519489645957948\n",
      "Epoch 25, Batch 650, Loss: 1.1531416082382202\n",
      "Epoch 25, Batch 700, Loss: 1.1525876212120056\n",
      "Epoch 25, Batch 750, Loss: 1.15260023355484\n",
      "Epoch 25, Batch 800, Loss: 1.1526689720153809\n",
      "Epoch 25, Batch 850, Loss: 1.1526058673858643\n",
      "Epoch 25, Batch 900, Loss: 1.1520578694343566\n",
      "Accuracy on test set: 0.1%\n",
      "Fitting for combination 69\n",
      "784\n",
      "4\n",
      "10\n",
      "[40, 10, 10, 30, 10]\n",
      "False\n",
      "['relu', 'sigmoid', 'relu', 'sigmoid']\n",
      "SGD\n",
      "0.01\n",
      "0.01\n",
      "CrossEntropyLoss\n",
      "100\n",
      "25\n",
      "0\n",
      "Epoch 1, Batch 100, Loss: 2.3315513920783997\n",
      "Epoch 1, Batch 200, Loss: 2.3072161364555357\n",
      "Epoch 1, Batch 300, Loss: 2.3031636834144593\n",
      "Epoch 1, Batch 400, Loss: 2.3022185015678405\n",
      "Epoch 1, Batch 500, Loss: 2.3026185417175293\n",
      "Epoch 1, Batch 600, Loss: 2.3021993494033812\n",
      "Epoch 1, Batch 700, Loss: 2.301850826740265\n",
      "Epoch 1, Batch 800, Loss: 2.3015830063819886\n",
      "Epoch 1, Batch 900, Loss: 2.302191376686096\n",
      "Epoch 2, Batch 100, Loss: 2.3015654706954956\n",
      "Epoch 2, Batch 200, Loss: 2.3018046474456786\n",
      "Epoch 2, Batch 300, Loss: 2.301157124042511\n",
      "Epoch 2, Batch 400, Loss: 2.3014495086669924\n",
      "Epoch 2, Batch 500, Loss: 2.301356704235077\n",
      "Epoch 2, Batch 600, Loss: 2.301494183540344\n",
      "Epoch 2, Batch 700, Loss: 2.3011498475074768\n",
      "Epoch 2, Batch 800, Loss: 2.3008923816680906\n",
      "Epoch 2, Batch 900, Loss: 2.301235098838806\n",
      "Epoch 3, Batch 100, Loss: 2.300184485912323\n",
      "Epoch 3, Batch 200, Loss: 2.301341211795807\n",
      "Epoch 3, Batch 300, Loss: 2.3005213284492494\n",
      "Epoch 3, Batch 400, Loss: 2.301226670742035\n",
      "Epoch 3, Batch 500, Loss: 2.3007009768486024\n",
      "Epoch 3, Batch 600, Loss: 2.301031153202057\n",
      "Epoch 3, Batch 700, Loss: 2.300771119594574\n",
      "Epoch 3, Batch 800, Loss: 2.3006064677238465\n",
      "Epoch 3, Batch 900, Loss: 2.30072420835495\n",
      "Epoch 4, Batch 100, Loss: 2.3006874251365663\n",
      "Epoch 4, Batch 200, Loss: 2.3004038739204407\n",
      "Epoch 4, Batch 300, Loss: 2.2997828006744383\n",
      "Epoch 4, Batch 400, Loss: 2.3005985617637634\n",
      "Epoch 4, Batch 500, Loss: 2.3007440876960756\n",
      "Epoch 4, Batch 600, Loss: 2.3006772994995117\n",
      "Epoch 4, Batch 700, Loss: 2.300318057537079\n",
      "Epoch 4, Batch 800, Loss: 2.300416145324707\n",
      "Epoch 4, Batch 900, Loss: 2.300308997631073\n",
      "Epoch 5, Batch 100, Loss: 2.3000368690490722\n",
      "Epoch 5, Batch 200, Loss: 2.30044597864151\n",
      "Epoch 5, Batch 300, Loss: 2.3004256105422973\n",
      "Epoch 5, Batch 400, Loss: 2.299658019542694\n",
      "Epoch 5, Batch 500, Loss: 2.300424156188965\n",
      "Epoch 5, Batch 600, Loss: 2.3001375865936278\n",
      "Epoch 5, Batch 700, Loss: 2.3001145243644716\n",
      "Epoch 5, Batch 800, Loss: 2.299936816692352\n",
      "Epoch 5, Batch 900, Loss: 2.300191786289215\n",
      "Epoch 6, Batch 100, Loss: 2.2995921301841737\n",
      "Epoch 6, Batch 200, Loss: 2.2997503733634947\n",
      "Epoch 6, Batch 300, Loss: 2.2998260974884035\n",
      "Epoch 6, Batch 400, Loss: 2.299611909389496\n",
      "Epoch 6, Batch 500, Loss: 2.2996592330932617\n",
      "Epoch 6, Batch 600, Loss: 2.299619014263153\n",
      "Epoch 6, Batch 700, Loss: 2.299626233577728\n",
      "Epoch 6, Batch 800, Loss: 2.2992357850074767\n",
      "Epoch 6, Batch 900, Loss: 2.2994719982147216\n",
      "Epoch 7, Batch 100, Loss: 2.2992165064811707\n",
      "Epoch 7, Batch 200, Loss: 2.2991477727890013\n",
      "Epoch 7, Batch 300, Loss: 2.299295072555542\n",
      "Epoch 7, Batch 400, Loss: 2.2989728260040283\n",
      "Epoch 7, Batch 500, Loss: 2.2990514159202577\n",
      "Epoch 7, Batch 600, Loss: 2.2988245773315428\n",
      "Epoch 7, Batch 700, Loss: 2.2989862418174742\n",
      "Epoch 7, Batch 800, Loss: 2.2989562344551087\n",
      "Epoch 7, Batch 900, Loss: 2.2989690637588502\n",
      "Epoch 8, Batch 100, Loss: 2.298706316947937\n",
      "Epoch 8, Batch 200, Loss: 2.2983279609680176\n",
      "Epoch 8, Batch 300, Loss: 2.298328297138214\n",
      "Epoch 8, Batch 400, Loss: 2.2986346554756163\n",
      "Epoch 8, Batch 500, Loss: 2.298475720882416\n",
      "Epoch 8, Batch 600, Loss: 2.2981644082069397\n",
      "Epoch 8, Batch 700, Loss: 2.2981945633888246\n",
      "Epoch 8, Batch 800, Loss: 2.298216724395752\n",
      "Epoch 8, Batch 900, Loss: 2.297636480331421\n",
      "Epoch 9, Batch 100, Loss: 2.2977036023139954\n",
      "Epoch 9, Batch 200, Loss: 2.2976599407196043\n",
      "Epoch 9, Batch 300, Loss: 2.2975046253204345\n",
      "Epoch 9, Batch 400, Loss: 2.29743145942688\n",
      "Epoch 9, Batch 500, Loss: 2.297432367801666\n",
      "Epoch 9, Batch 600, Loss: 2.2973414850234986\n",
      "Epoch 9, Batch 700, Loss: 2.2970567107200623\n",
      "Epoch 9, Batch 800, Loss: 2.2970359325408936\n",
      "Epoch 9, Batch 900, Loss: 2.296712579727173\n",
      "Epoch 10, Batch 100, Loss: 2.2964381694793703\n",
      "Epoch 10, Batch 200, Loss: 2.2964147567749023\n",
      "Epoch 10, Batch 300, Loss: 2.296022398471832\n",
      "Epoch 10, Batch 400, Loss: 2.29611204624176\n",
      "Epoch 10, Batch 500, Loss: 2.2954819416999817\n",
      "Epoch 10, Batch 600, Loss: 2.295603301525116\n",
      "Epoch 10, Batch 700, Loss: 2.2952355694770814\n",
      "Epoch 10, Batch 800, Loss: 2.2948239827156067\n",
      "Epoch 10, Batch 900, Loss: 2.2947684741020202\n",
      "Epoch 11, Batch 100, Loss: 2.294014673233032\n",
      "Epoch 11, Batch 200, Loss: 2.293413381576538\n",
      "Epoch 11, Batch 300, Loss: 2.2936660146713255\n",
      "Epoch 11, Batch 400, Loss: 2.292883279323578\n",
      "Epoch 11, Batch 500, Loss: 2.2928961372375487\n",
      "Epoch 11, Batch 600, Loss: 2.2922721695899964\n",
      "Epoch 11, Batch 700, Loss: 2.291661169528961\n",
      "Epoch 11, Batch 800, Loss: 2.290675656795502\n",
      "Epoch 11, Batch 900, Loss: 2.2901778411865235\n",
      "Epoch 12, Batch 100, Loss: 2.2886289834976195\n",
      "Epoch 12, Batch 200, Loss: 2.288780779838562\n",
      "Epoch 12, Batch 300, Loss: 2.2873918771743775\n",
      "Epoch 12, Batch 400, Loss: 2.2871381282806396\n",
      "Epoch 12, Batch 500, Loss: 2.285134310722351\n",
      "Epoch 12, Batch 600, Loss: 2.2840409660339356\n",
      "Epoch 12, Batch 700, Loss: 2.2824614787101747\n",
      "Epoch 12, Batch 800, Loss: 2.28131178855896\n",
      "Epoch 12, Batch 900, Loss: 2.2789395332336424\n",
      "Epoch 13, Batch 100, Loss: 2.2771241664886475\n",
      "Epoch 13, Batch 200, Loss: 2.2745941829681398\n",
      "Epoch 13, Batch 300, Loss: 2.2715454792976377\n",
      "Epoch 13, Batch 400, Loss: 2.2688295650482178\n",
      "Epoch 13, Batch 500, Loss: 2.2647694301605226\n",
      "Epoch 13, Batch 600, Loss: 2.2572188854217528\n",
      "Epoch 13, Batch 700, Loss: 2.255695037841797\n",
      "Epoch 13, Batch 800, Loss: 2.2481243133544924\n",
      "Epoch 13, Batch 900, Loss: 2.2428870844841002\n",
      "Epoch 14, Batch 100, Loss: 2.2323697710037234\n",
      "Epoch 14, Batch 200, Loss: 2.2223591685295103\n",
      "Epoch 14, Batch 300, Loss: 2.212638702392578\n",
      "Epoch 14, Batch 400, Loss: 2.203658092021942\n",
      "Epoch 14, Batch 500, Loss: 2.1820610857009886\n",
      "Epoch 14, Batch 600, Loss: 2.1736736845970155\n",
      "Epoch 14, Batch 700, Loss: 2.1624259781837463\n",
      "Epoch 14, Batch 800, Loss: 2.1379382681846617\n",
      "Epoch 14, Batch 900, Loss: 2.1292968559265137\n",
      "Epoch 15, Batch 100, Loss: 2.1010642647743225\n",
      "Epoch 15, Batch 200, Loss: 2.08671085357666\n",
      "Epoch 15, Batch 300, Loss: 2.0632593095302583\n",
      "Epoch 15, Batch 400, Loss: 2.0451648008823393\n",
      "Epoch 15, Batch 500, Loss: 2.0307516193389894\n",
      "Epoch 15, Batch 600, Loss: 2.022367832660675\n",
      "Epoch 15, Batch 700, Loss: 2.004503823518753\n",
      "Epoch 15, Batch 800, Loss: 1.9811388039588929\n",
      "Epoch 15, Batch 900, Loss: 1.96989128947258\n",
      "Epoch 16, Batch 100, Loss: 1.9454306304454803\n",
      "Epoch 16, Batch 200, Loss: 1.9452092933654785\n",
      "Epoch 16, Batch 300, Loss: 1.9338726198673248\n",
      "Epoch 16, Batch 400, Loss: 1.9258807766437531\n",
      "Epoch 16, Batch 500, Loss: 1.9223971259593964\n",
      "Epoch 16, Batch 600, Loss: 1.9095051050186158\n",
      "Epoch 16, Batch 700, Loss: 1.9024891352653504\n",
      "Epoch 16, Batch 800, Loss: 1.8929886662960052\n",
      "Epoch 16, Batch 900, Loss: 1.8842718529701232\n",
      "Epoch 17, Batch 100, Loss: 1.8777993929386139\n",
      "Epoch 17, Batch 200, Loss: 1.8708765780925751\n",
      "Epoch 17, Batch 300, Loss: 1.868162486553192\n",
      "Epoch 17, Batch 400, Loss: 1.8749149358272552\n",
      "Epoch 17, Batch 500, Loss: 1.8634552872180938\n",
      "Epoch 17, Batch 600, Loss: 1.8554457652568817\n",
      "Epoch 17, Batch 700, Loss: 1.854461785554886\n",
      "Epoch 17, Batch 800, Loss: 1.8480301308631897\n",
      "Epoch 17, Batch 900, Loss: 1.8440621972084046\n",
      "Epoch 18, Batch 100, Loss: 1.8407619881629944\n",
      "Epoch 18, Batch 200, Loss: 1.8396290171146392\n",
      "Epoch 18, Batch 300, Loss: 1.8376224040985107\n",
      "Epoch 18, Batch 400, Loss: 1.8323378193378448\n",
      "Epoch 18, Batch 500, Loss: 1.827572569847107\n",
      "Epoch 18, Batch 600, Loss: 1.8322352206707\n",
      "Epoch 18, Batch 700, Loss: 1.827291612625122\n",
      "Epoch 18, Batch 800, Loss: 1.8140178334712982\n",
      "Epoch 18, Batch 900, Loss: 1.8140965187549591\n",
      "Epoch 19, Batch 100, Loss: 1.8058701837062836\n",
      "Epoch 19, Batch 200, Loss: 1.8041153395175933\n",
      "Epoch 19, Batch 300, Loss: 1.8076281023025513\n",
      "Epoch 19, Batch 400, Loss: 1.7981236565113068\n",
      "Epoch 19, Batch 500, Loss: 1.7887727689743043\n",
      "Epoch 19, Batch 600, Loss: 1.7934344184398652\n",
      "Epoch 19, Batch 700, Loss: 1.7823144268989564\n",
      "Epoch 19, Batch 800, Loss: 1.7698681795597075\n",
      "Epoch 19, Batch 900, Loss: 1.7546080183982848\n",
      "Epoch 20, Batch 100, Loss: 1.755294680595398\n",
      "Epoch 20, Batch 200, Loss: 1.734065544605255\n",
      "Epoch 20, Batch 300, Loss: 1.721177397966385\n",
      "Epoch 20, Batch 400, Loss: 1.7125264239311218\n",
      "Epoch 20, Batch 500, Loss: 1.6983044159412384\n",
      "Epoch 20, Batch 600, Loss: 1.6927091109752654\n",
      "Epoch 20, Batch 700, Loss: 1.6787620055675507\n",
      "Epoch 20, Batch 800, Loss: 1.6675863111019134\n",
      "Epoch 20, Batch 900, Loss: 1.6656073129177094\n",
      "Epoch 21, Batch 100, Loss: 1.6447267091274262\n",
      "Epoch 21, Batch 200, Loss: 1.6420942211151124\n",
      "Epoch 21, Batch 300, Loss: 1.6329190146923065\n",
      "Epoch 21, Batch 400, Loss: 1.6184110021591187\n",
      "Epoch 21, Batch 500, Loss: 1.6172521007061005\n",
      "Epoch 21, Batch 600, Loss: 1.6038885939121246\n",
      "Epoch 21, Batch 700, Loss: 1.6003661227226258\n",
      "Epoch 21, Batch 800, Loss: 1.6005270218849181\n",
      "Epoch 21, Batch 900, Loss: 1.5994459235668181\n",
      "Epoch 22, Batch 100, Loss: 1.5876769602298737\n",
      "Epoch 22, Batch 200, Loss: 1.5820438396930694\n",
      "Epoch 22, Batch 300, Loss: 1.5727626478672028\n",
      "Epoch 22, Batch 400, Loss: 1.5686196768283844\n",
      "Epoch 22, Batch 500, Loss: 1.569724645614624\n",
      "Epoch 22, Batch 600, Loss: 1.5555001544952392\n",
      "Epoch 22, Batch 700, Loss: 1.564418283700943\n",
      "Epoch 22, Batch 800, Loss: 1.5539059388637542\n",
      "Epoch 22, Batch 900, Loss: 1.5505248737335204\n",
      "Epoch 23, Batch 100, Loss: 1.5486018419265748\n",
      "Epoch 23, Batch 200, Loss: 1.5375332975387572\n",
      "Epoch 23, Batch 300, Loss: 1.5380339205265046\n",
      "Epoch 23, Batch 400, Loss: 1.5314989793300628\n",
      "Epoch 23, Batch 500, Loss: 1.5269300961494445\n",
      "Epoch 23, Batch 600, Loss: 1.5333823049068451\n",
      "Epoch 23, Batch 700, Loss: 1.5221482634544372\n",
      "Epoch 23, Batch 800, Loss: 1.518963166475296\n",
      "Epoch 23, Batch 900, Loss: 1.508805637359619\n",
      "Epoch 24, Batch 100, Loss: 1.5173852300643922\n",
      "Epoch 24, Batch 200, Loss: 1.4984157466888428\n",
      "Epoch 24, Batch 300, Loss: 1.5069973754882813\n",
      "Epoch 24, Batch 400, Loss: 1.5009113609790803\n",
      "Epoch 24, Batch 500, Loss: 1.4957979428768158\n",
      "Epoch 24, Batch 600, Loss: 1.4984411442279815\n",
      "Epoch 24, Batch 700, Loss: 1.490121965408325\n",
      "Epoch 24, Batch 800, Loss: 1.4846569943428038\n",
      "Epoch 24, Batch 900, Loss: 1.4861964559555054\n",
      "Epoch 25, Batch 100, Loss: 1.476426304578781\n",
      "Epoch 25, Batch 200, Loss: 1.4754898929595948\n",
      "Epoch 25, Batch 300, Loss: 1.4684730768203735\n",
      "Epoch 25, Batch 400, Loss: 1.4711208045482635\n",
      "Epoch 25, Batch 500, Loss: 1.4627781116962433\n",
      "Epoch 25, Batch 600, Loss: 1.4646675193309784\n",
      "Epoch 25, Batch 700, Loss: 1.461861344575882\n",
      "Epoch 25, Batch 800, Loss: 1.459878990650177\n",
      "Epoch 25, Batch 900, Loss: 1.453676679134369\n",
      "Accuracy on test set: 0.3992%\n",
      "Fitting for combination 70\n",
      "784\n",
      "4\n",
      "10\n",
      "[40, 10, 10, 30, 10]\n",
      "True\n",
      "['relu', 'sigmoid', 'relu', 'tanh']\n",
      "Adam\n",
      "0.01\n",
      "0.01\n",
      "CrossEntropyLoss\n",
      "200\n",
      "25\n",
      "0\n",
      "Epoch 1, Batch 200, Loss: 2.501770325303078\n",
      "Epoch 1, Batch 400, Loss: 1.718524255156517\n",
      "Epoch 1, Batch 600, Loss: 1.627221766114235\n",
      "Epoch 1, Batch 800, Loss: 1.5801709723472595\n",
      "Epoch 2, Batch 200, Loss: 1.5661120200157166\n",
      "Epoch 2, Batch 400, Loss: 1.4499853157997131\n",
      "Epoch 2, Batch 600, Loss: 1.4734690219163895\n",
      "Epoch 2, Batch 800, Loss: 1.5093589207530023\n",
      "Epoch 3, Batch 200, Loss: 1.4689509552717208\n",
      "Epoch 3, Batch 400, Loss: 1.4554626283049583\n",
      "Epoch 3, Batch 600, Loss: 1.4779236879944802\n",
      "Epoch 3, Batch 800, Loss: 1.450980537235737\n",
      "Epoch 4, Batch 200, Loss: 1.4496703854203223\n",
      "Epoch 4, Batch 400, Loss: 1.4566469717025756\n",
      "Epoch 4, Batch 600, Loss: 1.4961847224831581\n",
      "Epoch 4, Batch 800, Loss: 1.4520811787247658\n",
      "Epoch 5, Batch 200, Loss: 1.461992770731449\n",
      "Epoch 5, Batch 400, Loss: 1.421098764538765\n",
      "Epoch 5, Batch 600, Loss: 1.4094273135066033\n",
      "Epoch 5, Batch 800, Loss: 1.4243466356396675\n",
      "Epoch 6, Batch 200, Loss: 1.4049598520994186\n",
      "Epoch 6, Batch 400, Loss: 1.4248769196867943\n",
      "Epoch 6, Batch 600, Loss: 1.4208855664730071\n",
      "Epoch 6, Batch 800, Loss: 1.4436064076423645\n",
      "Epoch 7, Batch 200, Loss: 1.4520811155438422\n",
      "Epoch 7, Batch 400, Loss: 1.430825774371624\n",
      "Epoch 7, Batch 600, Loss: 1.4324180904030799\n",
      "Epoch 7, Batch 800, Loss: 1.398018451333046\n",
      "Epoch 8, Batch 200, Loss: 1.4145686241984368\n",
      "Epoch 8, Batch 400, Loss: 1.4149553751945496\n",
      "Epoch 8, Batch 600, Loss: 1.42589452624321\n",
      "Epoch 8, Batch 800, Loss: 1.4448075842857362\n",
      "Epoch 9, Batch 200, Loss: 1.3840262359380722\n",
      "Epoch 9, Batch 400, Loss: 1.404189024269581\n",
      "Epoch 9, Batch 600, Loss: 1.4333961236476898\n",
      "Epoch 9, Batch 800, Loss: 1.4320642086863518\n",
      "Epoch 10, Batch 200, Loss: 1.4467056503891944\n",
      "Epoch 10, Batch 400, Loss: 1.3773876211047174\n",
      "Epoch 10, Batch 600, Loss: 1.42928839802742\n",
      "Epoch 10, Batch 800, Loss: 1.4380150377750396\n",
      "Epoch 11, Batch 200, Loss: 1.4070490878820419\n",
      "Epoch 11, Batch 400, Loss: 1.434317728281021\n",
      "Epoch 11, Batch 600, Loss: 1.4161385226249694\n",
      "Epoch 11, Batch 800, Loss: 1.39794110506773\n",
      "Epoch 12, Batch 200, Loss: 1.4291336944699287\n",
      "Epoch 12, Batch 400, Loss: 1.4481921637058257\n",
      "Epoch 12, Batch 600, Loss: 1.431202735900879\n",
      "Epoch 12, Batch 800, Loss: 1.3887272402644157\n",
      "Epoch 13, Batch 200, Loss: 1.41371207177639\n",
      "Epoch 13, Batch 400, Loss: 1.4137526634335518\n",
      "Epoch 13, Batch 600, Loss: 1.4235407131910325\n",
      "Epoch 13, Batch 800, Loss: 1.4410919946432115\n",
      "Epoch 14, Batch 200, Loss: 1.4147088021039962\n",
      "Epoch 14, Batch 400, Loss: 1.41606274664402\n",
      "Epoch 14, Batch 600, Loss: 1.3919471389055251\n",
      "Epoch 14, Batch 800, Loss: 1.4260519087314605\n",
      "Epoch 15, Batch 200, Loss: 1.3987810555100442\n",
      "Epoch 15, Batch 400, Loss: 1.4094515606760978\n",
      "Epoch 15, Batch 600, Loss: 1.4311862939596176\n",
      "Epoch 15, Batch 800, Loss: 1.4519436049461365\n",
      "Epoch 16, Batch 200, Loss: 1.354624680876732\n",
      "Epoch 16, Batch 400, Loss: 1.449782606959343\n",
      "Epoch 16, Batch 600, Loss: 1.4081737837195396\n",
      "Epoch 16, Batch 800, Loss: 1.4009018948674201\n",
      "Epoch 17, Batch 200, Loss: 1.4139398178458213\n",
      "Epoch 17, Batch 400, Loss: 1.426318640112877\n",
      "Epoch 17, Batch 600, Loss: 1.4412120404839515\n",
      "Epoch 17, Batch 800, Loss: 1.4237990579009057\n",
      "Epoch 18, Batch 200, Loss: 1.4085554286837578\n",
      "Epoch 18, Batch 400, Loss: 1.4179601109027862\n",
      "Epoch 18, Batch 600, Loss: 1.4163914132118225\n",
      "Epoch 18, Batch 800, Loss: 1.4266980305314063\n",
      "Epoch 19, Batch 200, Loss: 1.4082479831576347\n",
      "Epoch 19, Batch 400, Loss: 1.4545337149500848\n",
      "Epoch 19, Batch 600, Loss: 1.4451372253894805\n",
      "Epoch 19, Batch 800, Loss: 1.4155120277404785\n",
      "Epoch 20, Batch 200, Loss: 1.4435628202557564\n",
      "Epoch 20, Batch 400, Loss: 1.439214092195034\n",
      "Epoch 20, Batch 600, Loss: 1.4155338129401207\n",
      "Epoch 20, Batch 800, Loss: 1.4369882187247276\n",
      "Epoch 21, Batch 200, Loss: 1.438696231842041\n",
      "Epoch 21, Batch 400, Loss: 1.4141563048958778\n",
      "Epoch 21, Batch 600, Loss: 1.4428535082936287\n",
      "Epoch 21, Batch 800, Loss: 1.3865313401818276\n",
      "Epoch 22, Batch 200, Loss: 1.4188316082954406\n",
      "Epoch 22, Batch 400, Loss: 1.460481425821781\n",
      "Epoch 22, Batch 600, Loss: 1.469395086467266\n",
      "Epoch 22, Batch 800, Loss: 1.442244582772255\n",
      "Epoch 23, Batch 200, Loss: 1.436667086482048\n",
      "Epoch 23, Batch 400, Loss: 1.423375353515148\n",
      "Epoch 23, Batch 600, Loss: 1.4002267065644265\n",
      "Epoch 23, Batch 800, Loss: 1.4424303671717644\n",
      "Epoch 24, Batch 200, Loss: 1.4684618151187896\n",
      "Epoch 24, Batch 400, Loss: 1.4192971017956735\n",
      "Epoch 24, Batch 600, Loss: 1.4106700724363328\n",
      "Epoch 24, Batch 800, Loss: 1.4292393600940705\n",
      "Epoch 25, Batch 200, Loss: 1.409016061425209\n",
      "Epoch 25, Batch 400, Loss: 1.4571391052007676\n",
      "Epoch 25, Batch 600, Loss: 1.4611484748125076\n",
      "Epoch 25, Batch 800, Loss: 1.3955682826042175\n",
      "Accuracy on test set: 0.7573%\n",
      "Fitting for combination 71\n",
      "784\n",
      "4\n",
      "10\n",
      "[40, 10, 10, 30, 10]\n",
      "True\n",
      "['relu', 'sigmoid', 'relu', 'relu']\n",
      "SGD\n",
      "0.03\n",
      "0.03\n",
      "CrossEntropyLoss\n",
      "100\n",
      "25\n",
      "0\n",
      "Epoch 1, Batch 100, Loss: 2.303076286315918\n",
      "Epoch 1, Batch 200, Loss: 2.2964120602607725\n",
      "Epoch 1, Batch 300, Loss: 2.293074004650116\n",
      "Epoch 1, Batch 400, Loss: 2.2889820313453675\n",
      "Epoch 1, Batch 500, Loss: 2.2802004289627074\n",
      "Epoch 1, Batch 600, Loss: 2.265919840335846\n",
      "Epoch 1, Batch 700, Loss: 2.22826810836792\n",
      "Epoch 1, Batch 800, Loss: 2.136529507637024\n",
      "Epoch 1, Batch 900, Loss: 1.9474292707443237\n",
      "Epoch 2, Batch 100, Loss: 1.7547102344036103\n",
      "Epoch 2, Batch 200, Loss: 1.7153448390960693\n",
      "Epoch 2, Batch 300, Loss: 1.6950033032894134\n",
      "Epoch 2, Batch 400, Loss: 1.6827607679367065\n",
      "Epoch 2, Batch 500, Loss: 1.65858820438385\n",
      "Epoch 2, Batch 600, Loss: 1.6412431442737578\n",
      "Epoch 2, Batch 700, Loss: 1.6274496829509735\n",
      "Epoch 2, Batch 800, Loss: 1.607832019329071\n",
      "Epoch 2, Batch 900, Loss: 1.5966920721530915\n",
      "Epoch 3, Batch 100, Loss: 1.5402260160446166\n",
      "Epoch 3, Batch 200, Loss: 1.4912486398220062\n",
      "Epoch 3, Batch 300, Loss: 1.418992623090744\n",
      "Epoch 3, Batch 400, Loss: 1.3770289862155913\n",
      "Epoch 3, Batch 500, Loss: 1.3444823288917542\n",
      "Epoch 3, Batch 600, Loss: 1.3244190943241119\n",
      "Epoch 3, Batch 700, Loss: 1.2818408489227295\n",
      "Epoch 3, Batch 800, Loss: 1.2665562808513642\n",
      "Epoch 3, Batch 900, Loss: 1.2451681411266327\n",
      "Epoch 4, Batch 100, Loss: 1.232768975496292\n",
      "Epoch 4, Batch 200, Loss: 1.2250304901599884\n",
      "Epoch 4, Batch 300, Loss: 1.2161735427379607\n",
      "Epoch 4, Batch 400, Loss: 1.2067374098300934\n",
      "Epoch 4, Batch 500, Loss: 1.18380717754364\n",
      "Epoch 4, Batch 600, Loss: 1.1736329174041749\n",
      "Epoch 4, Batch 700, Loss: 1.1751576483249664\n",
      "Epoch 4, Batch 800, Loss: 1.173691291809082\n",
      "Epoch 4, Batch 900, Loss: 1.1598532164096833\n",
      "Epoch 5, Batch 100, Loss: 1.1719960510730743\n",
      "Epoch 5, Batch 200, Loss: 1.1697581911087036\n",
      "Epoch 5, Batch 300, Loss: 1.1390454280376434\n",
      "Epoch 5, Batch 400, Loss: 1.1397238498926163\n",
      "Epoch 5, Batch 500, Loss: 1.14262624502182\n",
      "Epoch 5, Batch 600, Loss: 1.1390663653612136\n",
      "Epoch 5, Batch 700, Loss: 1.129786078929901\n",
      "Epoch 5, Batch 800, Loss: 1.1332454466819764\n",
      "Epoch 5, Batch 900, Loss: 1.1302225583791732\n",
      "Epoch 6, Batch 100, Loss: 1.1135877513885497\n",
      "Epoch 6, Batch 200, Loss: 1.1158569794893265\n",
      "Epoch 6, Batch 300, Loss: 1.1243660271167755\n",
      "Epoch 6, Batch 400, Loss: 1.114523577094078\n",
      "Epoch 6, Batch 500, Loss: 1.1211767381429671\n",
      "Epoch 6, Batch 600, Loss: 1.1152833968400955\n",
      "Epoch 6, Batch 700, Loss: 1.1112505060434341\n",
      "Epoch 6, Batch 800, Loss: 1.1199203777313231\n",
      "Epoch 6, Batch 900, Loss: 1.1030881410837174\n",
      "Epoch 7, Batch 100, Loss: 1.0901730662584306\n",
      "Epoch 7, Batch 200, Loss: 1.107288175225258\n",
      "Epoch 7, Batch 300, Loss: 1.0922884082794189\n",
      "Epoch 7, Batch 400, Loss: 1.1068216121196748\n",
      "Epoch 7, Batch 500, Loss: 1.0982832151651383\n",
      "Epoch 7, Batch 600, Loss: 1.0954571026563644\n",
      "Epoch 7, Batch 700, Loss: 1.0953601956367494\n",
      "Epoch 7, Batch 800, Loss: 1.0959648269414901\n",
      "Epoch 7, Batch 900, Loss: 1.0944531679153442\n",
      "Epoch 8, Batch 100, Loss: 1.0788124597072601\n",
      "Epoch 8, Batch 200, Loss: 1.080919098854065\n",
      "Epoch 8, Batch 300, Loss: 1.0955196416378021\n",
      "Epoch 8, Batch 400, Loss: 1.086044060587883\n",
      "Epoch 8, Batch 500, Loss: 1.0864429819583892\n",
      "Epoch 8, Batch 600, Loss: 1.0710926938056946\n",
      "Epoch 8, Batch 700, Loss: 1.0857040750980378\n",
      "Epoch 8, Batch 800, Loss: 1.068367108106613\n",
      "Epoch 8, Batch 900, Loss: 1.0673972123861313\n",
      "Epoch 9, Batch 100, Loss: 1.0689328736066819\n",
      "Epoch 9, Batch 200, Loss: 1.059486848115921\n",
      "Epoch 9, Batch 300, Loss: 1.0786819171905517\n",
      "Epoch 9, Batch 400, Loss: 1.0737480437755584\n",
      "Epoch 9, Batch 500, Loss: 1.061628014445305\n",
      "Epoch 9, Batch 600, Loss: 1.0596378338336945\n",
      "Epoch 9, Batch 700, Loss: 1.069329805970192\n",
      "Epoch 9, Batch 800, Loss: 1.0571750682592391\n",
      "Epoch 9, Batch 900, Loss: 1.0750338292121888\n",
      "Epoch 10, Batch 100, Loss: 1.0498562109470368\n",
      "Epoch 10, Batch 200, Loss: 1.0490772891044617\n",
      "Epoch 10, Batch 300, Loss: 1.0716780990362167\n",
      "Epoch 10, Batch 400, Loss: 1.0575007474422455\n",
      "Epoch 10, Batch 500, Loss: 1.0729835110902786\n",
      "Epoch 10, Batch 600, Loss: 1.0659494119882584\n",
      "Epoch 10, Batch 700, Loss: 1.0590083461999893\n",
      "Epoch 10, Batch 800, Loss: 1.0557248759269715\n",
      "Epoch 10, Batch 900, Loss: 1.064159089922905\n",
      "Epoch 11, Batch 100, Loss: 1.0681371998786926\n",
      "Epoch 11, Batch 200, Loss: 1.0685450118780135\n",
      "Epoch 11, Batch 300, Loss: 1.0557604509592056\n",
      "Epoch 11, Batch 400, Loss: 1.051924329996109\n",
      "Epoch 11, Batch 500, Loss: 1.0585171139240266\n",
      "Epoch 11, Batch 600, Loss: 1.0571261829137801\n",
      "Epoch 11, Batch 700, Loss: 1.0528677576780319\n",
      "Epoch 11, Batch 800, Loss: 1.0452420508861542\n",
      "Epoch 11, Batch 900, Loss: 1.0530449104309083\n",
      "Epoch 12, Batch 100, Loss: 1.0586776793003083\n",
      "Epoch 12, Batch 200, Loss: 1.0559105777740478\n",
      "Epoch 12, Batch 300, Loss: 1.0489437121152878\n",
      "Epoch 12, Batch 400, Loss: 1.0370924973487854\n",
      "Epoch 12, Batch 500, Loss: 1.0524134439229966\n",
      "Epoch 12, Batch 600, Loss: 1.0500701487064361\n",
      "Epoch 12, Batch 700, Loss: 1.042991406917572\n",
      "Epoch 12, Batch 800, Loss: 1.0659517359733581\n",
      "Epoch 12, Batch 900, Loss: 1.0569089317321778\n",
      "Epoch 13, Batch 100, Loss: 1.04685220181942\n",
      "Epoch 13, Batch 200, Loss: 1.038410552740097\n",
      "Epoch 13, Batch 300, Loss: 1.053985562324524\n",
      "Epoch 13, Batch 400, Loss: 1.0485174149274825\n",
      "Epoch 13, Batch 500, Loss: 1.0655102497339248\n",
      "Epoch 13, Batch 600, Loss: 1.0440433758497238\n",
      "Epoch 13, Batch 700, Loss: 1.0400921177864075\n",
      "Epoch 13, Batch 800, Loss: 1.0564402067661285\n",
      "Epoch 13, Batch 900, Loss: 1.0610070323944092\n",
      "Epoch 14, Batch 100, Loss: 1.0443169766664504\n",
      "Epoch 14, Batch 200, Loss: 1.0454878401756287\n",
      "Epoch 14, Batch 300, Loss: 1.0607775604724885\n",
      "Epoch 14, Batch 400, Loss: 1.0424670904874802\n",
      "Epoch 14, Batch 500, Loss: 1.0478909981250764\n",
      "Epoch 14, Batch 600, Loss: 1.0449051374197007\n",
      "Epoch 14, Batch 700, Loss: 1.0468178248405458\n",
      "Epoch 14, Batch 800, Loss: 1.049384311437607\n",
      "Epoch 14, Batch 900, Loss: 1.0507193171977998\n",
      "Epoch 15, Batch 100, Loss: 1.0591751182079314\n",
      "Epoch 15, Batch 200, Loss: 1.0570303779840469\n",
      "Epoch 15, Batch 300, Loss: 1.0485920882225037\n",
      "Epoch 15, Batch 400, Loss: 1.0440610402822494\n",
      "Epoch 15, Batch 500, Loss: 1.0541642665863038\n",
      "Epoch 15, Batch 600, Loss: 1.0386312103271484\n",
      "Epoch 15, Batch 700, Loss: 1.0523938947916032\n",
      "Epoch 15, Batch 800, Loss: 1.0369135767221451\n",
      "Epoch 15, Batch 900, Loss: 1.0423455613851547\n",
      "Epoch 16, Batch 100, Loss: 1.0417053133249283\n",
      "Epoch 16, Batch 200, Loss: 1.0458050054311752\n",
      "Epoch 16, Batch 300, Loss: 1.036204478740692\n",
      "Epoch 16, Batch 400, Loss: 1.0385921210050584\n",
      "Epoch 16, Batch 500, Loss: 1.0371706664562226\n",
      "Epoch 16, Batch 600, Loss: 1.0491850864887238\n",
      "Epoch 16, Batch 700, Loss: 1.0476153630018235\n",
      "Epoch 16, Batch 800, Loss: 1.0545993626117707\n",
      "Epoch 16, Batch 900, Loss: 1.0396465545892715\n",
      "Epoch 17, Batch 100, Loss: 1.0476918870210647\n",
      "Epoch 17, Batch 200, Loss: 1.0574897927045823\n",
      "Epoch 17, Batch 300, Loss: 1.0471486401557923\n",
      "Epoch 17, Batch 400, Loss: 1.0380716544389725\n",
      "Epoch 17, Batch 500, Loss: 1.035954580307007\n",
      "Epoch 17, Batch 600, Loss: 1.046078775525093\n",
      "Epoch 17, Batch 700, Loss: 1.0438376253843307\n",
      "Epoch 17, Batch 800, Loss: 1.0436872494220735\n",
      "Epoch 17, Batch 900, Loss: 1.047520285844803\n",
      "Epoch 18, Batch 100, Loss: 1.0469327640533448\n",
      "Epoch 18, Batch 200, Loss: 1.0511099791526795\n",
      "Epoch 18, Batch 300, Loss: 1.0398139590024948\n",
      "Epoch 18, Batch 400, Loss: 1.027823604941368\n",
      "Epoch 18, Batch 500, Loss: 1.0613587963581086\n",
      "Epoch 18, Batch 600, Loss: 1.0561614137887956\n",
      "Epoch 18, Batch 700, Loss: 1.0451901918649673\n",
      "Epoch 18, Batch 800, Loss: 1.0350989693403243\n",
      "Epoch 18, Batch 900, Loss: 1.0467894715070725\n",
      "Epoch 19, Batch 100, Loss: 1.0380993431806564\n",
      "Epoch 19, Batch 200, Loss: 1.0303974866867065\n",
      "Epoch 19, Batch 300, Loss: 1.0452557176351547\n",
      "Epoch 19, Batch 400, Loss: 1.0382515627145767\n",
      "Epoch 19, Batch 500, Loss: 1.0587840270996094\n",
      "Epoch 19, Batch 600, Loss: 1.0442131674289703\n",
      "Epoch 19, Batch 700, Loss: 1.0419970643520355\n",
      "Epoch 19, Batch 800, Loss: 1.04113234937191\n",
      "Epoch 19, Batch 900, Loss: 1.0550174283981324\n",
      "Epoch 20, Batch 100, Loss: 1.0480916464328767\n",
      "Epoch 20, Batch 200, Loss: 1.0363960963487626\n",
      "Epoch 20, Batch 300, Loss: 1.0459277486801148\n",
      "Epoch 20, Batch 400, Loss: 1.0506335395574569\n",
      "Epoch 20, Batch 500, Loss: 1.0393048930168152\n",
      "Epoch 20, Batch 600, Loss: 1.0354067504405975\n",
      "Epoch 20, Batch 700, Loss: 1.038771902322769\n",
      "Epoch 20, Batch 800, Loss: 1.0591248720884323\n",
      "Epoch 20, Batch 900, Loss: 1.0452947920560838\n",
      "Epoch 21, Batch 100, Loss: 1.0438558715581894\n",
      "Epoch 21, Batch 200, Loss: 1.0450455093383788\n",
      "Epoch 21, Batch 300, Loss: 1.0472532516717912\n",
      "Epoch 21, Batch 400, Loss: 1.0531608259677887\n",
      "Epoch 21, Batch 500, Loss: 1.0406356620788575\n",
      "Epoch 21, Batch 600, Loss: 1.048808951973915\n",
      "Epoch 21, Batch 700, Loss: 1.0492412978410721\n",
      "Epoch 21, Batch 800, Loss: 1.0331134229898453\n",
      "Epoch 21, Batch 900, Loss: 1.0337321329116822\n",
      "Epoch 22, Batch 100, Loss: 1.0437235182523728\n",
      "Epoch 22, Batch 200, Loss: 1.0479014885425568\n",
      "Epoch 22, Batch 300, Loss: 1.0289980810880661\n",
      "Epoch 22, Batch 400, Loss: 1.0447743654251098\n",
      "Epoch 22, Batch 500, Loss: 1.0398639184236527\n",
      "Epoch 22, Batch 600, Loss: 1.035412386059761\n",
      "Epoch 22, Batch 700, Loss: 1.0414852887392043\n",
      "Epoch 22, Batch 800, Loss: 1.0504958176612853\n",
      "Epoch 22, Batch 900, Loss: 1.041684039235115\n",
      "Epoch 23, Batch 100, Loss: 1.044464784860611\n",
      "Epoch 23, Batch 200, Loss: 1.0530201983451843\n",
      "Epoch 23, Batch 300, Loss: 1.0316479885578156\n",
      "Epoch 23, Batch 400, Loss: 1.0372729843854904\n",
      "Epoch 23, Batch 500, Loss: 1.0450390088558197\n",
      "Epoch 23, Batch 600, Loss: 1.0473179906606673\n",
      "Epoch 23, Batch 700, Loss: 1.0541755372285844\n",
      "Epoch 23, Batch 800, Loss: 1.0479461705684663\n",
      "Epoch 23, Batch 900, Loss: 1.0317069154977798\n",
      "Epoch 24, Batch 100, Loss: 1.0427654075622559\n",
      "Epoch 24, Batch 200, Loss: 1.0482733535766602\n",
      "Epoch 24, Batch 300, Loss: 1.0392683923244477\n",
      "Epoch 24, Batch 400, Loss: 1.0390547269582748\n",
      "Epoch 24, Batch 500, Loss: 1.037554202079773\n",
      "Epoch 24, Batch 600, Loss: 1.0260809230804444\n",
      "Epoch 24, Batch 700, Loss: 1.046117212176323\n",
      "Epoch 24, Batch 800, Loss: 1.0439518010616302\n",
      "Epoch 24, Batch 900, Loss: 1.0476153242588042\n",
      "Epoch 25, Batch 100, Loss: 1.0387235909700394\n",
      "Epoch 25, Batch 200, Loss: 1.0365854328870774\n",
      "Epoch 25, Batch 300, Loss: 1.036922225356102\n",
      "Epoch 25, Batch 400, Loss: 1.0334523671865463\n",
      "Epoch 25, Batch 500, Loss: 1.048774672150612\n",
      "Epoch 25, Batch 600, Loss: 1.0295757615566254\n",
      "Epoch 25, Batch 700, Loss: 1.044236565232277\n",
      "Epoch 25, Batch 800, Loss: 1.0376847553253175\n",
      "Epoch 25, Batch 900, Loss: 1.0580808848142624\n",
      "Accuracy on test set: 0.6033%\n",
      "Fitting for combination 72\n",
      "784\n",
      "4\n",
      "10\n",
      "[40, 10, 10, 40, 10]\n",
      "False\n",
      "['relu', 'sigmoid', 'relu', 'sigmoid']\n",
      "Adam\n",
      "0.03\n",
      "0.03\n",
      "CrossEntropyLoss\n",
      "50\n",
      "25\n",
      "0\n",
      "Epoch 1, Batch 50, Loss: 1.182575397491455\n",
      "Epoch 1, Batch 100, Loss: 1.1678323936462403\n",
      "Epoch 1, Batch 150, Loss: 1.1770001101493834\n",
      "Epoch 1, Batch 200, Loss: 1.1779170417785645\n",
      "Epoch 1, Batch 250, Loss: 1.1747840785980224\n",
      "Epoch 1, Batch 300, Loss: 1.1746266055107117\n",
      "Epoch 1, Batch 350, Loss: 1.1724321007728578\n",
      "Epoch 1, Batch 400, Loss: 1.1789071536064148\n",
      "Epoch 1, Batch 450, Loss: 1.1748189520835877\n",
      "Epoch 1, Batch 500, Loss: 1.181588797569275\n",
      "Epoch 1, Batch 550, Loss: 1.1753381538391112\n",
      "Epoch 1, Batch 600, Loss: 1.1749416446685792\n",
      "Epoch 1, Batch 650, Loss: 1.169016420841217\n",
      "Epoch 1, Batch 700, Loss: 1.16894766330719\n",
      "Epoch 1, Batch 750, Loss: 1.1812930846214293\n",
      "Epoch 1, Batch 800, Loss: 1.1774378442764282\n",
      "Epoch 1, Batch 850, Loss: 1.1798279905319213\n",
      "Epoch 1, Batch 900, Loss: 1.1713769292831422\n",
      "Epoch 2, Batch 50, Loss: 1.175039210319519\n",
      "Epoch 2, Batch 100, Loss: 1.173095829486847\n",
      "Epoch 2, Batch 150, Loss: 1.1693291473388672\n",
      "Epoch 2, Batch 200, Loss: 1.1783791065216065\n",
      "Epoch 2, Batch 250, Loss: 1.1716992902755736\n",
      "Epoch 2, Batch 300, Loss: 1.1736931157112123\n",
      "Epoch 2, Batch 350, Loss: 1.1762340354919434\n",
      "Epoch 2, Batch 400, Loss: 1.1726085996627809\n",
      "Epoch 2, Batch 450, Loss: 1.1704017090797425\n",
      "Epoch 2, Batch 500, Loss: 1.1695051169395447\n",
      "Epoch 2, Batch 550, Loss: 1.1689787220954895\n",
      "Epoch 2, Batch 600, Loss: 1.1670140242576599\n",
      "Epoch 2, Batch 650, Loss: 1.1780388402938842\n",
      "Epoch 2, Batch 700, Loss: 1.1786407709121705\n",
      "Epoch 2, Batch 750, Loss: 1.1740104746818543\n",
      "Epoch 2, Batch 800, Loss: 1.1742531323432923\n",
      "Epoch 2, Batch 850, Loss: 1.1806005978584289\n",
      "Epoch 2, Batch 900, Loss: 1.1695543909072876\n",
      "Epoch 3, Batch 50, Loss: 1.1753469181060792\n",
      "Epoch 3, Batch 100, Loss: 1.1719448828697205\n",
      "Epoch 3, Batch 150, Loss: 1.1737486290931702\n",
      "Epoch 3, Batch 200, Loss: 1.169564781188965\n",
      "Epoch 3, Batch 250, Loss: 1.181680028438568\n",
      "Epoch 3, Batch 300, Loss: 1.1766930198669434\n",
      "Epoch 3, Batch 350, Loss: 1.168516492843628\n",
      "Epoch 3, Batch 400, Loss: 1.1772923636436463\n",
      "Epoch 3, Batch 450, Loss: 1.1764996099472045\n",
      "Epoch 3, Batch 500, Loss: 1.17702707529068\n",
      "Epoch 3, Batch 550, Loss: 1.176795117855072\n",
      "Epoch 3, Batch 600, Loss: 1.1657425022125245\n",
      "Epoch 3, Batch 650, Loss: 1.1737722587585449\n",
      "Epoch 3, Batch 700, Loss: 1.1789274883270264\n",
      "Epoch 3, Batch 750, Loss: 1.1645011568069459\n",
      "Epoch 3, Batch 800, Loss: 1.1699700522422791\n",
      "Epoch 3, Batch 850, Loss: 1.1827009844779968\n",
      "Epoch 3, Batch 900, Loss: 1.1735972309112548\n",
      "Epoch 4, Batch 50, Loss: 1.1735553979873656\n",
      "Epoch 4, Batch 100, Loss: 1.170405797958374\n",
      "Epoch 4, Batch 150, Loss: 1.1751918601989746\n",
      "Epoch 4, Batch 200, Loss: 1.1754611325263977\n",
      "Epoch 4, Batch 250, Loss: 1.1738166689872742\n",
      "Epoch 4, Batch 300, Loss: 1.1745553779602051\n",
      "Epoch 4, Batch 350, Loss: 1.1741156935691834\n",
      "Epoch 4, Batch 400, Loss: 1.1761623311042786\n",
      "Epoch 4, Batch 450, Loss: 1.1741282534599304\n",
      "Epoch 4, Batch 500, Loss: 1.1666563367843628\n",
      "Epoch 4, Batch 550, Loss: 1.170721538066864\n",
      "Epoch 4, Batch 600, Loss: 1.173891441822052\n",
      "Epoch 4, Batch 650, Loss: 1.1806200790405272\n",
      "Epoch 4, Batch 700, Loss: 1.184273006916046\n",
      "Epoch 4, Batch 750, Loss: 1.176845712661743\n",
      "Epoch 4, Batch 800, Loss: 1.179788134098053\n",
      "Epoch 4, Batch 850, Loss: 1.1720748090744018\n",
      "Epoch 4, Batch 900, Loss: 1.183507306575775\n",
      "Epoch 5, Batch 50, Loss: 1.176088342666626\n",
      "Epoch 5, Batch 100, Loss: 1.1742944169044494\n",
      "Epoch 5, Batch 150, Loss: 1.190456714630127\n",
      "Epoch 5, Batch 200, Loss: 1.1782586693763732\n",
      "Epoch 5, Batch 250, Loss: 1.176475214958191\n",
      "Epoch 5, Batch 300, Loss: 1.1812356090545655\n",
      "Epoch 5, Batch 350, Loss: 1.1747992753982544\n",
      "Epoch 5, Batch 400, Loss: 1.1733954191207885\n",
      "Epoch 5, Batch 450, Loss: 1.1769763684272767\n",
      "Epoch 5, Batch 500, Loss: 1.1758527374267578\n",
      "Epoch 5, Batch 550, Loss: 1.176593735218048\n",
      "Epoch 5, Batch 600, Loss: 1.1700602746009827\n",
      "Epoch 5, Batch 650, Loss: 1.1709063482284545\n",
      "Epoch 5, Batch 700, Loss: 1.1712786293029784\n",
      "Epoch 5, Batch 750, Loss: 1.1763423418998717\n",
      "Epoch 5, Batch 800, Loss: 1.170975193977356\n",
      "Epoch 5, Batch 850, Loss: 1.168999161720276\n",
      "Epoch 5, Batch 900, Loss: 1.1764160585403443\n",
      "Epoch 6, Batch 50, Loss: 1.1789626836776734\n",
      "Epoch 6, Batch 100, Loss: 1.1672032165527344\n",
      "Epoch 6, Batch 150, Loss: 1.1738265085220336\n",
      "Epoch 6, Batch 200, Loss: 1.1705718159675598\n",
      "Epoch 6, Batch 250, Loss: 1.1716677975654601\n",
      "Epoch 6, Batch 300, Loss: 1.1709834098815919\n",
      "Epoch 6, Batch 350, Loss: 1.1718752241134645\n",
      "Epoch 6, Batch 400, Loss: 1.1698232626914977\n",
      "Epoch 6, Batch 450, Loss: 1.173738923072815\n",
      "Epoch 6, Batch 500, Loss: 1.1711088609695435\n",
      "Epoch 6, Batch 550, Loss: 1.177843723297119\n",
      "Epoch 6, Batch 600, Loss: 1.1722629284858703\n",
      "Epoch 6, Batch 650, Loss: 1.171164674758911\n",
      "Epoch 6, Batch 700, Loss: 1.1729702830314637\n",
      "Epoch 6, Batch 750, Loss: 1.1708643889427186\n",
      "Epoch 6, Batch 800, Loss: 1.1701133489608764\n",
      "Epoch 6, Batch 850, Loss: 1.1764055871963501\n",
      "Epoch 6, Batch 900, Loss: 1.1766750502586365\n",
      "Epoch 7, Batch 50, Loss: 1.1772706842422485\n",
      "Epoch 7, Batch 100, Loss: 1.1822996425628662\n",
      "Epoch 7, Batch 150, Loss: 1.1740734457969666\n",
      "Epoch 7, Batch 200, Loss: 1.1723768973350526\n",
      "Epoch 7, Batch 250, Loss: 1.17212482213974\n",
      "Epoch 7, Batch 300, Loss: 1.1787656497955323\n",
      "Epoch 7, Batch 350, Loss: 1.1696825313568116\n",
      "Epoch 7, Batch 400, Loss: 1.1705397939682007\n",
      "Epoch 7, Batch 450, Loss: 1.1758561444282531\n",
      "Epoch 7, Batch 500, Loss: 1.168700647354126\n",
      "Epoch 7, Batch 550, Loss: 1.1704143285751343\n",
      "Epoch 7, Batch 600, Loss: 1.1698140406608581\n",
      "Epoch 7, Batch 650, Loss: 1.1747293591499328\n",
      "Epoch 7, Batch 700, Loss: 1.175581715106964\n",
      "Epoch 7, Batch 750, Loss: 1.1673527908325196\n",
      "Epoch 7, Batch 800, Loss: 1.1795867896080017\n",
      "Epoch 7, Batch 850, Loss: 1.1800760531425476\n",
      "Epoch 7, Batch 900, Loss: 1.191658022403717\n",
      "Epoch 8, Batch 50, Loss: 1.1708327150344848\n",
      "Epoch 8, Batch 100, Loss: 1.1735527586936951\n",
      "Epoch 8, Batch 150, Loss: 1.176153998374939\n",
      "Epoch 8, Batch 200, Loss: 1.1725699257850648\n",
      "Epoch 8, Batch 250, Loss: 1.1728955960273744\n",
      "Epoch 8, Batch 300, Loss: 1.1770420598983764\n",
      "Epoch 8, Batch 350, Loss: 1.1787975096702576\n",
      "Epoch 8, Batch 400, Loss: 1.1672019648551941\n",
      "Epoch 8, Batch 450, Loss: 1.1810583424568177\n",
      "Epoch 8, Batch 500, Loss: 1.1771495628356934\n",
      "Epoch 8, Batch 550, Loss: 1.1751151943206788\n",
      "Epoch 8, Batch 600, Loss: 1.1759293723106383\n",
      "Epoch 8, Batch 650, Loss: 1.16979998588562\n",
      "Epoch 8, Batch 700, Loss: 1.1737498903274537\n",
      "Epoch 8, Batch 750, Loss: 1.1674104285240174\n",
      "Epoch 8, Batch 800, Loss: 1.1704907894134522\n",
      "Epoch 8, Batch 850, Loss: 1.1736798095703125\n",
      "Epoch 8, Batch 900, Loss: 1.1730083918571472\n",
      "Epoch 9, Batch 50, Loss: 1.177195360660553\n",
      "Epoch 9, Batch 100, Loss: 1.1824542331695556\n",
      "Epoch 9, Batch 150, Loss: 1.1700104928016664\n",
      "Epoch 9, Batch 200, Loss: 1.177618601322174\n",
      "Epoch 9, Batch 250, Loss: 1.1726229643821717\n",
      "Epoch 9, Batch 300, Loss: 1.1716769075393676\n",
      "Epoch 9, Batch 350, Loss: 1.1730570244789122\n",
      "Epoch 9, Batch 400, Loss: 1.1749264264106751\n",
      "Epoch 9, Batch 450, Loss: 1.1727817463874817\n",
      "Epoch 9, Batch 500, Loss: 1.1682273745536804\n",
      "Epoch 9, Batch 550, Loss: 1.1714500093460083\n",
      "Epoch 9, Batch 600, Loss: 1.176575882434845\n",
      "Epoch 9, Batch 650, Loss: 1.174506573677063\n",
      "Epoch 9, Batch 700, Loss: 1.1794245743751526\n",
      "Epoch 9, Batch 750, Loss: 1.171759524345398\n",
      "Epoch 9, Batch 800, Loss: 1.1761576652526855\n",
      "Epoch 9, Batch 850, Loss: 1.1748533654212951\n",
      "Epoch 9, Batch 900, Loss: 1.1680807232856751\n",
      "Epoch 10, Batch 50, Loss: 1.1843679189682006\n",
      "Epoch 10, Batch 100, Loss: 1.1753999590873718\n",
      "Epoch 10, Batch 150, Loss: 1.1752254247665406\n",
      "Epoch 10, Batch 200, Loss: 1.1773293733596801\n",
      "Epoch 10, Batch 250, Loss: 1.1712445211410523\n",
      "Epoch 10, Batch 300, Loss: 1.1744158601760863\n",
      "Epoch 10, Batch 350, Loss: 1.172718777656555\n",
      "Epoch 10, Batch 400, Loss: 1.1675227856636048\n",
      "Epoch 10, Batch 450, Loss: 1.1734030294418334\n",
      "Epoch 10, Batch 500, Loss: 1.1803574824333192\n",
      "Epoch 10, Batch 550, Loss: 1.1842385673522948\n",
      "Epoch 10, Batch 600, Loss: 1.1707749629020692\n",
      "Epoch 10, Batch 650, Loss: 1.1768211030960083\n",
      "Epoch 10, Batch 700, Loss: 1.1822418212890624\n",
      "Epoch 10, Batch 750, Loss: 1.1717898774147033\n",
      "Epoch 10, Batch 800, Loss: 1.1691390752792359\n",
      "Epoch 10, Batch 850, Loss: 1.1734427905082703\n",
      "Epoch 10, Batch 900, Loss: 1.1725782322883607\n",
      "Epoch 11, Batch 50, Loss: 1.1762847900390625\n",
      "Epoch 11, Batch 100, Loss: 1.1726719331741333\n",
      "Epoch 11, Batch 150, Loss: 1.1812370944023132\n",
      "Epoch 11, Batch 200, Loss: 1.1689472603797912\n",
      "Epoch 11, Batch 250, Loss: 1.1759819841384889\n",
      "Epoch 11, Batch 300, Loss: 1.1735783648490905\n",
      "Epoch 11, Batch 350, Loss: 1.1647815275192261\n",
      "Epoch 11, Batch 400, Loss: 1.1730366778373718\n",
      "Epoch 11, Batch 450, Loss: 1.1729156947135926\n",
      "Epoch 11, Batch 500, Loss: 1.1807651710510254\n",
      "Epoch 11, Batch 550, Loss: 1.1734027981758117\n",
      "Epoch 11, Batch 600, Loss: 1.1733231878280639\n",
      "Epoch 11, Batch 650, Loss: 1.1799692964553834\n",
      "Epoch 11, Batch 700, Loss: 1.1619331526756287\n",
      "Epoch 11, Batch 750, Loss: 1.1743230724334717\n",
      "Epoch 11, Batch 800, Loss: 1.168550536632538\n",
      "Epoch 11, Batch 850, Loss: 1.1792864871025086\n",
      "Epoch 11, Batch 900, Loss: 1.1747240042686462\n",
      "Epoch 12, Batch 50, Loss: 1.1739774012565614\n",
      "Epoch 12, Batch 100, Loss: 1.1749398589134217\n",
      "Epoch 12, Batch 150, Loss: 1.1745233607292176\n",
      "Epoch 12, Batch 200, Loss: 1.1729038882255554\n",
      "Epoch 12, Batch 250, Loss: 1.1718905234336854\n",
      "Epoch 12, Batch 300, Loss: 1.175680572986603\n",
      "Epoch 12, Batch 350, Loss: 1.1709162592887878\n",
      "Epoch 12, Batch 400, Loss: 1.172857689857483\n",
      "Epoch 12, Batch 450, Loss: 1.1741624641418458\n",
      "Epoch 12, Batch 500, Loss: 1.178329176902771\n",
      "Epoch 12, Batch 550, Loss: 1.1720701837539673\n",
      "Epoch 12, Batch 600, Loss: 1.170018355846405\n",
      "Epoch 12, Batch 650, Loss: 1.1776763534545898\n",
      "Epoch 12, Batch 700, Loss: 1.1757111239433289\n",
      "Epoch 12, Batch 750, Loss: 1.1696270656585694\n",
      "Epoch 12, Batch 800, Loss: 1.1755193090438842\n",
      "Epoch 12, Batch 850, Loss: 1.1642715907096863\n",
      "Epoch 12, Batch 900, Loss: 1.1773543667793274\n",
      "Epoch 13, Batch 50, Loss: 1.1678916811943054\n",
      "Epoch 13, Batch 100, Loss: 1.171251015663147\n",
      "Epoch 13, Batch 150, Loss: 1.1714398431777955\n",
      "Epoch 13, Batch 200, Loss: 1.1744487476348877\n",
      "Epoch 13, Batch 250, Loss: 1.178060064315796\n",
      "Epoch 13, Batch 300, Loss: 1.1774417114257814\n",
      "Epoch 13, Batch 350, Loss: 1.1689294052124024\n",
      "Epoch 13, Batch 400, Loss: 1.1669945168495177\n",
      "Epoch 13, Batch 450, Loss: 1.1661629533767701\n",
      "Epoch 13, Batch 500, Loss: 1.1739635419845582\n",
      "Epoch 13, Batch 550, Loss: 1.1682367992401124\n",
      "Epoch 13, Batch 600, Loss: 1.180308496952057\n",
      "Epoch 13, Batch 650, Loss: 1.1791595935821533\n",
      "Epoch 13, Batch 700, Loss: 1.1740261149406432\n",
      "Epoch 13, Batch 750, Loss: 1.1713758420944214\n",
      "Epoch 13, Batch 800, Loss: 1.177083022594452\n",
      "Epoch 13, Batch 850, Loss: 1.1720929551124573\n",
      "Epoch 13, Batch 900, Loss: 1.1715399432182312\n",
      "Epoch 14, Batch 50, Loss: 1.1746566152572633\n",
      "Epoch 14, Batch 100, Loss: 1.1830263113975525\n",
      "Epoch 14, Batch 150, Loss: 1.1697145128250122\n",
      "Epoch 14, Batch 200, Loss: 1.1695896458625794\n",
      "Epoch 14, Batch 250, Loss: 1.1778962731361389\n",
      "Epoch 14, Batch 300, Loss: 1.1765539932250977\n",
      "Epoch 14, Batch 350, Loss: 1.1711790323257447\n",
      "Epoch 14, Batch 400, Loss: 1.1715287518501283\n",
      "Epoch 14, Batch 450, Loss: 1.1779375529289247\n",
      "Epoch 14, Batch 500, Loss: 1.1704767107963563\n",
      "Epoch 14, Batch 550, Loss: 1.173361186981201\n",
      "Epoch 14, Batch 600, Loss: 1.1788228344917298\n",
      "Epoch 14, Batch 650, Loss: 1.170762815475464\n",
      "Epoch 14, Batch 700, Loss: 1.1730314660072327\n",
      "Epoch 14, Batch 750, Loss: 1.1706251502037048\n",
      "Epoch 14, Batch 800, Loss: 1.1795973348617554\n",
      "Epoch 14, Batch 850, Loss: 1.1725936841964721\n",
      "Epoch 14, Batch 900, Loss: 1.17187166929245\n",
      "Epoch 15, Batch 50, Loss: 1.1742323327064514\n",
      "Epoch 15, Batch 100, Loss: 1.170762071609497\n",
      "Epoch 15, Batch 150, Loss: 1.1703280854225158\n",
      "Epoch 15, Batch 200, Loss: 1.179973509311676\n",
      "Epoch 15, Batch 250, Loss: 1.178517723083496\n",
      "Epoch 15, Batch 300, Loss: 1.1767314600944518\n",
      "Epoch 15, Batch 350, Loss: 1.1700741839408875\n",
      "Epoch 15, Batch 400, Loss: 1.177602105140686\n",
      "Epoch 15, Batch 450, Loss: 1.1790765571594237\n",
      "Epoch 15, Batch 500, Loss: 1.1744067883491516\n",
      "Epoch 15, Batch 550, Loss: 1.172065920829773\n",
      "Epoch 15, Batch 600, Loss: 1.1757118725776672\n",
      "Epoch 15, Batch 650, Loss: 1.171389367580414\n",
      "Epoch 15, Batch 700, Loss: 1.1771223831176758\n",
      "Epoch 15, Batch 750, Loss: 1.1761909461021423\n",
      "Epoch 15, Batch 800, Loss: 1.1719776463508607\n",
      "Epoch 15, Batch 850, Loss: 1.1739996790885925\n",
      "Epoch 15, Batch 900, Loss: 1.1716281628608705\n",
      "Epoch 16, Batch 50, Loss: 1.172292890548706\n",
      "Epoch 16, Batch 100, Loss: 1.1698598337173463\n",
      "Epoch 16, Batch 150, Loss: 1.1740438961982727\n",
      "Epoch 16, Batch 200, Loss: 1.1692121696472169\n",
      "Epoch 16, Batch 250, Loss: 1.1827419114112854\n",
      "Epoch 16, Batch 300, Loss: 1.1742867469787597\n",
      "Epoch 16, Batch 350, Loss: 1.1740320825576782\n",
      "Epoch 16, Batch 400, Loss: 1.1761035776138307\n",
      "Epoch 16, Batch 450, Loss: 1.177888491153717\n",
      "Epoch 16, Batch 500, Loss: 1.1781015753746034\n",
      "Epoch 16, Batch 550, Loss: 1.1710758900642395\n",
      "Epoch 16, Batch 600, Loss: 1.1862736535072327\n",
      "Epoch 16, Batch 650, Loss: 1.1784584712982178\n",
      "Epoch 16, Batch 700, Loss: 1.1768914699554442\n",
      "Epoch 16, Batch 750, Loss: 1.1687844133377074\n",
      "Epoch 16, Batch 800, Loss: 1.1733231806755067\n",
      "Epoch 16, Batch 850, Loss: 1.1793132638931274\n",
      "Epoch 16, Batch 900, Loss: 1.1817072343826294\n",
      "Epoch 17, Batch 50, Loss: 1.1738864636421205\n",
      "Epoch 17, Batch 100, Loss: 1.174743630886078\n",
      "Epoch 17, Batch 150, Loss: 1.1711830282211304\n",
      "Epoch 17, Batch 200, Loss: 1.1769938707351684\n",
      "Epoch 17, Batch 250, Loss: 1.1716115951538086\n",
      "Epoch 17, Batch 300, Loss: 1.176416072845459\n",
      "Epoch 17, Batch 350, Loss: 1.1730547547340393\n",
      "Epoch 17, Batch 400, Loss: 1.1852168393135072\n",
      "Epoch 17, Batch 450, Loss: 1.1674218773841858\n",
      "Epoch 17, Batch 500, Loss: 1.1739190077781678\n",
      "Epoch 17, Batch 550, Loss: 1.1778187727928162\n",
      "Epoch 17, Batch 600, Loss: 1.1751262474060058\n",
      "Epoch 17, Batch 650, Loss: 1.180081193447113\n",
      "Epoch 17, Batch 700, Loss: 1.1761618399620055\n",
      "Epoch 17, Batch 750, Loss: 1.1687726187705993\n",
      "Epoch 17, Batch 800, Loss: 1.1702729558944702\n",
      "Epoch 17, Batch 850, Loss: 1.1719727230072021\n",
      "Epoch 17, Batch 900, Loss: 1.1728494715690614\n",
      "Epoch 18, Batch 50, Loss: 1.1717615079879762\n",
      "Epoch 18, Batch 100, Loss: 1.1755206966400147\n",
      "Epoch 18, Batch 150, Loss: 1.1838162422180176\n",
      "Epoch 18, Batch 200, Loss: 1.1736627674102784\n",
      "Epoch 18, Batch 250, Loss: 1.1682283997535705\n",
      "Epoch 18, Batch 300, Loss: 1.169501814842224\n",
      "Epoch 18, Batch 350, Loss: 1.1743649673461913\n",
      "Epoch 18, Batch 400, Loss: 1.1769810819625854\n",
      "Epoch 18, Batch 450, Loss: 1.1750260138511657\n",
      "Epoch 18, Batch 500, Loss: 1.1841915607452393\n",
      "Epoch 18, Batch 550, Loss: 1.1750857758522033\n",
      "Epoch 18, Batch 600, Loss: 1.1806231451034546\n",
      "Epoch 18, Batch 650, Loss: 1.171910924911499\n",
      "Epoch 18, Batch 700, Loss: 1.1733292508125306\n",
      "Epoch 18, Batch 750, Loss: 1.178571572303772\n",
      "Epoch 18, Batch 800, Loss: 1.1799171733856202\n",
      "Epoch 18, Batch 850, Loss: 1.1793179631233215\n",
      "Epoch 18, Batch 900, Loss: 1.1683504462242127\n",
      "Epoch 19, Batch 50, Loss: 1.1798717522621154\n",
      "Epoch 19, Batch 100, Loss: 1.1714004683494568\n",
      "Epoch 19, Batch 150, Loss: 1.1775631546974181\n",
      "Epoch 19, Batch 200, Loss: 1.17239084482193\n",
      "Epoch 19, Batch 250, Loss: 1.175563485622406\n",
      "Epoch 19, Batch 300, Loss: 1.1735312843322754\n",
      "Epoch 19, Batch 350, Loss: 1.1749155664443969\n",
      "Epoch 19, Batch 400, Loss: 1.170691568851471\n",
      "Epoch 19, Batch 450, Loss: 1.1784246635437012\n",
      "Epoch 19, Batch 500, Loss: 1.170285234451294\n",
      "Epoch 19, Batch 550, Loss: 1.1724333429336549\n",
      "Epoch 19, Batch 600, Loss: 1.16734322309494\n",
      "Epoch 19, Batch 650, Loss: 1.1715038990974427\n",
      "Epoch 19, Batch 700, Loss: 1.1784200358390808\n",
      "Epoch 19, Batch 750, Loss: 1.1833733773231507\n",
      "Epoch 19, Batch 800, Loss: 1.169082851409912\n",
      "Epoch 19, Batch 850, Loss: 1.1725532150268554\n",
      "Epoch 19, Batch 900, Loss: 1.1694359278678894\n",
      "Epoch 20, Batch 50, Loss: 1.1688025832176208\n",
      "Epoch 20, Batch 100, Loss: 1.1826825857162475\n",
      "Epoch 20, Batch 150, Loss: 1.1927003407478332\n",
      "Epoch 20, Batch 200, Loss: 1.1703033089637755\n",
      "Epoch 20, Batch 250, Loss: 1.1723606562614441\n",
      "Epoch 20, Batch 300, Loss: 1.1686771035194397\n",
      "Epoch 20, Batch 350, Loss: 1.1770855784416199\n",
      "Epoch 20, Batch 400, Loss: 1.1750524258613586\n",
      "Epoch 20, Batch 450, Loss: 1.1687441229820252\n",
      "Epoch 20, Batch 500, Loss: 1.1795483374595641\n",
      "Epoch 20, Batch 550, Loss: 1.1807033705711365\n",
      "Epoch 20, Batch 600, Loss: 1.1735326027870179\n",
      "Epoch 20, Batch 650, Loss: 1.174941258430481\n",
      "Epoch 20, Batch 700, Loss: 1.1710947370529174\n",
      "Epoch 20, Batch 750, Loss: 1.1805058169364928\n",
      "Epoch 20, Batch 800, Loss: 1.1806944966316224\n",
      "Epoch 20, Batch 850, Loss: 1.1705394649505616\n",
      "Epoch 20, Batch 900, Loss: 1.1680650448799133\n",
      "Epoch 21, Batch 50, Loss: 1.1764287567138672\n",
      "Epoch 21, Batch 100, Loss: 1.167806646823883\n",
      "Epoch 21, Batch 150, Loss: 1.1719456458091735\n",
      "Epoch 21, Batch 200, Loss: 1.172049126625061\n",
      "Epoch 21, Batch 250, Loss: 1.171367666721344\n",
      "Epoch 21, Batch 300, Loss: 1.1705312752723693\n",
      "Epoch 21, Batch 350, Loss: 1.1679870200157165\n",
      "Epoch 21, Batch 400, Loss: 1.1656642007827758\n",
      "Epoch 21, Batch 450, Loss: 1.1807728791236878\n",
      "Epoch 21, Batch 500, Loss: 1.1693326830863953\n",
      "Epoch 21, Batch 550, Loss: 1.1744648599624634\n",
      "Epoch 21, Batch 600, Loss: 1.168061110973358\n",
      "Epoch 21, Batch 650, Loss: 1.167962498664856\n",
      "Epoch 21, Batch 700, Loss: 1.1750151777267457\n",
      "Epoch 21, Batch 750, Loss: 1.1748515391349792\n",
      "Epoch 21, Batch 800, Loss: 1.1765143585205078\n",
      "Epoch 21, Batch 850, Loss: 1.1726162981987\n",
      "Epoch 21, Batch 900, Loss: 1.1794067406654358\n",
      "Epoch 22, Batch 50, Loss: 1.1736482930183412\n",
      "Epoch 22, Batch 100, Loss: 1.171909601688385\n",
      "Epoch 22, Batch 150, Loss: 1.175204656124115\n",
      "Epoch 22, Batch 200, Loss: 1.1729275560379029\n",
      "Epoch 22, Batch 250, Loss: 1.1829189324378968\n",
      "Epoch 22, Batch 300, Loss: 1.1758839392662048\n",
      "Epoch 22, Batch 350, Loss: 1.1686451601982117\n",
      "Epoch 22, Batch 400, Loss: 1.1681056070327758\n",
      "Epoch 22, Batch 450, Loss: 1.174653446674347\n",
      "Epoch 22, Batch 500, Loss: 1.18046808719635\n",
      "Epoch 22, Batch 550, Loss: 1.177261848449707\n",
      "Epoch 22, Batch 600, Loss: 1.1769275331497193\n",
      "Epoch 22, Batch 650, Loss: 1.1682091760635376\n",
      "Epoch 22, Batch 700, Loss: 1.1786005067825318\n",
      "Epoch 22, Batch 750, Loss: 1.1823564767837524\n",
      "Epoch 22, Batch 800, Loss: 1.165729374885559\n",
      "Epoch 22, Batch 850, Loss: 1.1756667375564576\n",
      "Epoch 22, Batch 900, Loss: 1.1803550839424133\n",
      "Epoch 23, Batch 50, Loss: 1.1779493546485902\n",
      "Epoch 23, Batch 100, Loss: 1.17218674659729\n",
      "Epoch 23, Batch 150, Loss: 1.1755895829200744\n",
      "Epoch 23, Batch 200, Loss: 1.1723392009735107\n",
      "Epoch 23, Batch 250, Loss: 1.1759294962882996\n",
      "Epoch 23, Batch 300, Loss: 1.1725482296943666\n",
      "Epoch 23, Batch 350, Loss: 1.1781763887405396\n",
      "Epoch 23, Batch 400, Loss: 1.171230046749115\n",
      "Epoch 23, Batch 450, Loss: 1.1737080931663513\n",
      "Epoch 23, Batch 500, Loss: 1.175870168209076\n",
      "Epoch 23, Batch 550, Loss: 1.1752967405319215\n",
      "Epoch 23, Batch 600, Loss: 1.1778026938438415\n",
      "Epoch 23, Batch 650, Loss: 1.1753960347175598\n",
      "Epoch 23, Batch 700, Loss: 1.1794255709648132\n",
      "Epoch 23, Batch 750, Loss: 1.174368965625763\n",
      "Epoch 23, Batch 800, Loss: 1.166921582221985\n",
      "Epoch 23, Batch 850, Loss: 1.1671571326255799\n",
      "Epoch 23, Batch 900, Loss: 1.1724187779426574\n",
      "Epoch 24, Batch 50, Loss: 1.1674146151542664\n",
      "Epoch 24, Batch 100, Loss: 1.173312692642212\n",
      "Epoch 24, Batch 150, Loss: 1.1763418745994567\n",
      "Epoch 24, Batch 200, Loss: 1.1700678658485413\n",
      "Epoch 24, Batch 250, Loss: 1.1710385990142822\n",
      "Epoch 24, Batch 300, Loss: 1.181503129005432\n",
      "Epoch 24, Batch 350, Loss: 1.1823594999313354\n",
      "Epoch 24, Batch 400, Loss: 1.1773291301727296\n",
      "Epoch 24, Batch 450, Loss: 1.176656506061554\n",
      "Epoch 24, Batch 500, Loss: 1.1818507361412047\n",
      "Epoch 24, Batch 550, Loss: 1.1680186343193055\n",
      "Epoch 24, Batch 600, Loss: 1.1780598735809327\n",
      "Epoch 24, Batch 650, Loss: 1.1725672698020935\n",
      "Epoch 24, Batch 700, Loss: 1.1698476052284241\n",
      "Epoch 24, Batch 750, Loss: 1.1726633429527282\n",
      "Epoch 24, Batch 800, Loss: 1.171758198738098\n",
      "Epoch 24, Batch 850, Loss: 1.1728859448432922\n",
      "Epoch 24, Batch 900, Loss: 1.1769444799423219\n",
      "Epoch 25, Batch 50, Loss: 1.172730007171631\n",
      "Epoch 25, Batch 100, Loss: 1.1736659049987792\n",
      "Epoch 25, Batch 150, Loss: 1.1697811317443847\n",
      "Epoch 25, Batch 200, Loss: 1.178935763835907\n",
      "Epoch 25, Batch 250, Loss: 1.1781023788452147\n",
      "Epoch 25, Batch 300, Loss: 1.1703724718093873\n",
      "Epoch 25, Batch 350, Loss: 1.175258650779724\n",
      "Epoch 25, Batch 400, Loss: 1.1733430790901185\n",
      "Epoch 25, Batch 450, Loss: 1.1749454188346862\n",
      "Epoch 25, Batch 500, Loss: 1.1739964413642883\n",
      "Epoch 25, Batch 550, Loss: 1.1676101732254027\n",
      "Epoch 25, Batch 600, Loss: 1.1759547019004821\n",
      "Epoch 25, Batch 650, Loss: 1.172980706691742\n",
      "Epoch 25, Batch 700, Loss: 1.171239457130432\n",
      "Epoch 25, Batch 750, Loss: 1.1714799737930297\n",
      "Epoch 25, Batch 800, Loss: 1.1630409097671508\n",
      "Epoch 25, Batch 850, Loss: 1.1759498023986816\n",
      "Epoch 25, Batch 900, Loss: 1.1764582204818725\n",
      "Accuracy on test set: 0.1%\n",
      "Fitting for combination 73\n",
      "784\n",
      "4\n",
      "10\n",
      "[40, 10, 10, 40, 10]\n",
      "False\n",
      "['relu', 'sigmoid', 'relu', 'tanh']\n",
      "SGD\n",
      "0.1\n",
      "0.1\n",
      "CrossEntropyLoss\n",
      "100\n",
      "25\n",
      "0\n",
      "Epoch 1, Batch 100, Loss: 2.303947968482971\n",
      "Epoch 1, Batch 200, Loss: 2.302849373817444\n",
      "Epoch 1, Batch 300, Loss: 2.302667281627655\n",
      "Epoch 1, Batch 400, Loss: 2.3029824805259707\n",
      "Epoch 1, Batch 500, Loss: 2.3029285717010497\n",
      "Epoch 1, Batch 600, Loss: 2.3028297209739685\n",
      "Epoch 1, Batch 700, Loss: 2.3030141973495484\n",
      "Epoch 1, Batch 800, Loss: 2.30288712978363\n",
      "Epoch 1, Batch 900, Loss: 2.3029453277587892\n",
      "Epoch 2, Batch 100, Loss: 2.3030281829833985\n",
      "Epoch 2, Batch 200, Loss: 2.302969114780426\n",
      "Epoch 2, Batch 300, Loss: 2.302766499519348\n",
      "Epoch 2, Batch 400, Loss: 2.3027909922599794\n",
      "Epoch 2, Batch 500, Loss: 2.302876477241516\n",
      "Epoch 2, Batch 600, Loss: 2.3029920935630797\n",
      "Epoch 2, Batch 700, Loss: 2.302955334186554\n",
      "Epoch 2, Batch 800, Loss: 2.303053810596466\n",
      "Epoch 2, Batch 900, Loss: 2.3027764916419984\n",
      "Epoch 3, Batch 100, Loss: 2.302686026096344\n",
      "Epoch 3, Batch 200, Loss: 2.3028148365020753\n",
      "Epoch 3, Batch 300, Loss: 2.3028449249267577\n",
      "Epoch 3, Batch 400, Loss: 2.302907712459564\n",
      "Epoch 3, Batch 500, Loss: 2.3029478669166563\n",
      "Epoch 3, Batch 600, Loss: 2.302778522968292\n",
      "Epoch 3, Batch 700, Loss: 2.3027934646606445\n",
      "Epoch 3, Batch 800, Loss: 2.303010334968567\n",
      "Epoch 3, Batch 900, Loss: 2.302755746841431\n",
      "Epoch 4, Batch 100, Loss: 2.3027428555488587\n",
      "Epoch 4, Batch 200, Loss: 2.302923939228058\n",
      "Epoch 4, Batch 300, Loss: 2.3029894757270815\n",
      "Epoch 4, Batch 400, Loss: 2.3027340006828307\n",
      "Epoch 4, Batch 500, Loss: 2.302837941646576\n",
      "Epoch 4, Batch 600, Loss: 2.3025053930282593\n",
      "Epoch 4, Batch 700, Loss: 2.30271733045578\n",
      "Epoch 4, Batch 800, Loss: 2.3024547815322878\n",
      "Epoch 4, Batch 900, Loss: 2.3028175330162046\n",
      "Epoch 5, Batch 100, Loss: 2.302871026992798\n",
      "Epoch 5, Batch 200, Loss: 2.302778973579407\n",
      "Epoch 5, Batch 300, Loss: 2.3029246163368224\n",
      "Epoch 5, Batch 400, Loss: 2.3027170157432555\n",
      "Epoch 5, Batch 500, Loss: 2.302476530075073\n",
      "Epoch 5, Batch 600, Loss: 2.3027453637123108\n",
      "Epoch 5, Batch 700, Loss: 2.3028664493560793\n",
      "Epoch 5, Batch 800, Loss: 2.303013150691986\n",
      "Epoch 5, Batch 900, Loss: 2.3028095746040345\n",
      "Epoch 6, Batch 100, Loss: 2.3028942155838013\n",
      "Epoch 6, Batch 200, Loss: 2.3029026079177854\n",
      "Epoch 6, Batch 300, Loss: 2.302879738807678\n",
      "Epoch 6, Batch 400, Loss: 2.302680048942566\n",
      "Epoch 6, Batch 500, Loss: 2.3029807734489443\n",
      "Epoch 6, Batch 600, Loss: 2.302671573162079\n",
      "Epoch 6, Batch 700, Loss: 2.302954466342926\n",
      "Epoch 6, Batch 800, Loss: 2.3028362083435057\n",
      "Epoch 6, Batch 900, Loss: 2.3030888175964357\n",
      "Epoch 7, Batch 100, Loss: 2.3025827908515932\n",
      "Epoch 7, Batch 200, Loss: 2.3029369473457337\n",
      "Epoch 7, Batch 300, Loss: 2.3028214621543883\n",
      "Epoch 7, Batch 400, Loss: 2.302833912372589\n",
      "Epoch 7, Batch 500, Loss: 2.3030594182014466\n",
      "Epoch 7, Batch 600, Loss: 2.3028128433227537\n",
      "Epoch 7, Batch 700, Loss: 2.3028613090515138\n",
      "Epoch 7, Batch 800, Loss: 2.30297988653183\n",
      "Epoch 7, Batch 900, Loss: 2.302731866836548\n",
      "Epoch 8, Batch 100, Loss: 2.302900197505951\n",
      "Epoch 8, Batch 200, Loss: 2.3027774906158447\n",
      "Epoch 8, Batch 300, Loss: 2.30282662153244\n",
      "Epoch 8, Batch 400, Loss: 2.3025562357902527\n",
      "Epoch 8, Batch 500, Loss: 2.302795736789703\n",
      "Epoch 8, Batch 600, Loss: 2.3029228806495667\n",
      "Epoch 8, Batch 700, Loss: 2.302978024482727\n",
      "Epoch 8, Batch 800, Loss: 2.3029963970184326\n",
      "Epoch 8, Batch 900, Loss: 2.302833602428436\n",
      "Epoch 9, Batch 100, Loss: 2.3025768852233885\n",
      "Epoch 9, Batch 200, Loss: 2.3028860449790955\n",
      "Epoch 9, Batch 300, Loss: 2.3028771924972533\n",
      "Epoch 9, Batch 400, Loss: 2.3029316782951357\n",
      "Epoch 9, Batch 500, Loss: 2.3026869678497315\n",
      "Epoch 9, Batch 600, Loss: 2.3030131101608275\n",
      "Epoch 9, Batch 700, Loss: 2.3028677535057067\n",
      "Epoch 9, Batch 800, Loss: 2.3023549699783326\n",
      "Epoch 9, Batch 900, Loss: 2.302562973499298\n",
      "Epoch 10, Batch 100, Loss: 2.3028022384643556\n",
      "Epoch 10, Batch 200, Loss: 2.302946662902832\n",
      "Epoch 10, Batch 300, Loss: 2.3027787446975707\n",
      "Epoch 10, Batch 400, Loss: 2.3029023361206056\n",
      "Epoch 10, Batch 500, Loss: 2.302898716926575\n",
      "Epoch 10, Batch 600, Loss: 2.3031362223625185\n",
      "Epoch 10, Batch 700, Loss: 2.303002986907959\n",
      "Epoch 10, Batch 800, Loss: 2.3027668356895448\n",
      "Epoch 10, Batch 900, Loss: 2.3029592418670655\n",
      "Epoch 11, Batch 100, Loss: 2.302697455883026\n",
      "Epoch 11, Batch 200, Loss: 2.302805118560791\n",
      "Epoch 11, Batch 300, Loss: 2.3024500107765196\n",
      "Epoch 11, Batch 400, Loss: 2.3030702209472658\n",
      "Epoch 11, Batch 500, Loss: 2.3026728940010073\n",
      "Epoch 11, Batch 600, Loss: 2.302939100265503\n",
      "Epoch 11, Batch 700, Loss: 2.302502872943878\n",
      "Epoch 11, Batch 800, Loss: 2.302537245750427\n",
      "Epoch 11, Batch 900, Loss: 2.3029759526252747\n",
      "Epoch 12, Batch 100, Loss: 2.3027719616889955\n",
      "Epoch 12, Batch 200, Loss: 2.3028758192062377\n",
      "Epoch 12, Batch 300, Loss: 2.3029988646507262\n",
      "Epoch 12, Batch 400, Loss: 2.302905509471893\n",
      "Epoch 12, Batch 500, Loss: 2.302932596206665\n",
      "Epoch 12, Batch 600, Loss: 2.3028542518615724\n",
      "Epoch 12, Batch 700, Loss: 2.302973861694336\n",
      "Epoch 12, Batch 800, Loss: 2.303002371788025\n",
      "Epoch 12, Batch 900, Loss: 2.302764205932617\n",
      "Epoch 13, Batch 100, Loss: 2.302922828197479\n",
      "Epoch 13, Batch 200, Loss: 2.30290100812912\n",
      "Epoch 13, Batch 300, Loss: 2.302907497882843\n",
      "Epoch 13, Batch 400, Loss: 2.3028564476966857\n",
      "Epoch 13, Batch 500, Loss: 2.303038775920868\n",
      "Epoch 13, Batch 600, Loss: 2.302852246761322\n",
      "Epoch 13, Batch 700, Loss: 2.3029256534576414\n",
      "Epoch 13, Batch 800, Loss: 2.3028209590911866\n",
      "Epoch 13, Batch 900, Loss: 2.3029836869239806\n",
      "Epoch 14, Batch 100, Loss: 2.3027039194107055\n",
      "Epoch 14, Batch 200, Loss: 2.303008031845093\n",
      "Epoch 14, Batch 300, Loss: 2.3027876257896422\n",
      "Epoch 14, Batch 400, Loss: 2.3026904559135435\n",
      "Epoch 14, Batch 500, Loss: 2.3027822852134703\n",
      "Epoch 14, Batch 600, Loss: 2.302794361114502\n",
      "Epoch 14, Batch 700, Loss: 2.3029916429519655\n",
      "Epoch 14, Batch 800, Loss: 2.3028954577445986\n",
      "Epoch 14, Batch 900, Loss: 2.30266083240509\n",
      "Epoch 15, Batch 100, Loss: 2.3028589200973513\n",
      "Epoch 15, Batch 200, Loss: 2.302666928768158\n",
      "Epoch 15, Batch 300, Loss: 2.3028099441528322\n",
      "Epoch 15, Batch 400, Loss: 2.302575204372406\n",
      "Epoch 15, Batch 500, Loss: 2.30310222864151\n",
      "Epoch 15, Batch 600, Loss: 2.3027159857749937\n",
      "Epoch 15, Batch 700, Loss: 2.3028110480308532\n",
      "Epoch 15, Batch 800, Loss: 2.302905547618866\n",
      "Epoch 15, Batch 900, Loss: 2.302660665512085\n",
      "Epoch 16, Batch 100, Loss: 2.3025832533836366\n",
      "Epoch 16, Batch 200, Loss: 2.3027004957199098\n",
      "Epoch 16, Batch 300, Loss: 2.30275940656662\n",
      "Epoch 16, Batch 400, Loss: 2.302696018218994\n",
      "Epoch 16, Batch 500, Loss: 2.3031857299804686\n",
      "Epoch 16, Batch 600, Loss: 2.3028480339050295\n",
      "Epoch 16, Batch 700, Loss: 2.3029564881324767\n",
      "Epoch 16, Batch 800, Loss: 2.3026418566703795\n",
      "Epoch 16, Batch 900, Loss: 2.3029036784172057\n",
      "Epoch 17, Batch 100, Loss: 2.3027617716789246\n",
      "Epoch 17, Batch 200, Loss: 2.302472448348999\n",
      "Epoch 17, Batch 300, Loss: 2.3027952003479\n",
      "Epoch 17, Batch 400, Loss: 2.3028795957565307\n",
      "Epoch 17, Batch 500, Loss: 2.302822494506836\n",
      "Epoch 17, Batch 600, Loss: 2.3027225708961487\n",
      "Epoch 17, Batch 700, Loss: 2.302824115753174\n",
      "Epoch 17, Batch 800, Loss: 2.3027594542503356\n",
      "Epoch 17, Batch 900, Loss: 2.303004126548767\n",
      "Epoch 18, Batch 100, Loss: 2.302716271877289\n",
      "Epoch 18, Batch 200, Loss: 2.3028834676742553\n",
      "Epoch 18, Batch 300, Loss: 2.302589519023895\n",
      "Epoch 18, Batch 400, Loss: 2.3031210565567015\n",
      "Epoch 18, Batch 500, Loss: 2.3030097842216493\n",
      "Epoch 18, Batch 600, Loss: 2.3027159571647644\n",
      "Epoch 18, Batch 700, Loss: 2.302795374393463\n",
      "Epoch 18, Batch 800, Loss: 2.3027283811569212\n",
      "Epoch 18, Batch 900, Loss: 2.302911036014557\n",
      "Epoch 19, Batch 100, Loss: 2.3027064061164855\n",
      "Epoch 19, Batch 200, Loss: 2.3026678466796877\n",
      "Epoch 19, Batch 300, Loss: 2.3027883338928223\n",
      "Epoch 19, Batch 400, Loss: 2.3026665687561034\n",
      "Epoch 19, Batch 500, Loss: 2.3027707290649415\n",
      "Epoch 19, Batch 600, Loss: 2.3027689337730406\n",
      "Epoch 19, Batch 700, Loss: 2.303060612678528\n",
      "Epoch 19, Batch 800, Loss: 2.3028332209587097\n",
      "Epoch 19, Batch 900, Loss: 2.3026270031929017\n",
      "Epoch 20, Batch 100, Loss: 2.3027287244796755\n",
      "Epoch 20, Batch 200, Loss: 2.3029288458824158\n",
      "Epoch 20, Batch 300, Loss: 2.3025295543670654\n",
      "Epoch 20, Batch 400, Loss: 2.3029897689819334\n",
      "Epoch 20, Batch 500, Loss: 2.302793915271759\n",
      "Epoch 20, Batch 600, Loss: 2.3029496264457703\n",
      "Epoch 20, Batch 700, Loss: 2.302834060192108\n",
      "Epoch 20, Batch 800, Loss: 2.3029210209846496\n",
      "Epoch 20, Batch 900, Loss: 2.3028199315071105\n",
      "Epoch 21, Batch 100, Loss: 2.3029341173171995\n",
      "Epoch 21, Batch 200, Loss: 2.302787263393402\n",
      "Epoch 21, Batch 300, Loss: 2.302841305732727\n",
      "Epoch 21, Batch 400, Loss: 2.302437822818756\n",
      "Epoch 21, Batch 500, Loss: 2.3028856325149536\n",
      "Epoch 21, Batch 600, Loss: 2.302511758804321\n",
      "Epoch 21, Batch 700, Loss: 2.3027848529815675\n",
      "Epoch 21, Batch 800, Loss: 2.302646059989929\n",
      "Epoch 21, Batch 900, Loss: 2.3030942392349245\n",
      "Epoch 22, Batch 100, Loss: 2.3027980518341065\n",
      "Epoch 22, Batch 200, Loss: 2.3027570104599\n",
      "Epoch 22, Batch 300, Loss: 2.302668080329895\n",
      "Epoch 22, Batch 400, Loss: 2.3029828000068666\n",
      "Epoch 22, Batch 500, Loss: 2.3028879404067992\n",
      "Epoch 22, Batch 600, Loss: 2.302834725379944\n",
      "Epoch 22, Batch 700, Loss: 2.3026388096809387\n",
      "Epoch 22, Batch 800, Loss: 2.3027724027633667\n",
      "Epoch 22, Batch 900, Loss: 2.3029381966590883\n",
      "Epoch 23, Batch 100, Loss: 2.303009548187256\n",
      "Epoch 23, Batch 200, Loss: 2.302487108707428\n",
      "Epoch 23, Batch 300, Loss: 2.302579076290131\n",
      "Epoch 23, Batch 400, Loss: 2.302853009700775\n",
      "Epoch 23, Batch 500, Loss: 2.302630076408386\n",
      "Epoch 23, Batch 600, Loss: 2.3030551862716675\n",
      "Epoch 23, Batch 700, Loss: 2.3026946139335633\n",
      "Epoch 23, Batch 800, Loss: 2.302873706817627\n",
      "Epoch 23, Batch 900, Loss: 2.302837162017822\n",
      "Epoch 24, Batch 100, Loss: 2.3028676557540892\n",
      "Epoch 24, Batch 200, Loss: 2.3029393124580384\n",
      "Epoch 24, Batch 300, Loss: 2.3027449297904967\n",
      "Epoch 24, Batch 400, Loss: 2.302816925048828\n",
      "Epoch 24, Batch 500, Loss: 2.303019392490387\n",
      "Epoch 24, Batch 600, Loss: 2.3030664801597593\n",
      "Epoch 24, Batch 700, Loss: 2.3027664828300476\n",
      "Epoch 24, Batch 800, Loss: 2.3028267574310304\n",
      "Epoch 24, Batch 900, Loss: 2.302714149951935\n",
      "Epoch 25, Batch 100, Loss: 2.3028799319267272\n",
      "Epoch 25, Batch 200, Loss: 2.30249495267868\n",
      "Epoch 25, Batch 300, Loss: 2.302852416038513\n",
      "Epoch 25, Batch 400, Loss: 2.3027854442596434\n",
      "Epoch 25, Batch 500, Loss: 2.3029819703102112\n",
      "Epoch 25, Batch 600, Loss: 2.3028490376472472\n",
      "Epoch 25, Batch 700, Loss: 2.3028972363471985\n",
      "Epoch 25, Batch 800, Loss: 2.30260174036026\n",
      "Epoch 25, Batch 900, Loss: 2.3026649236679075\n",
      "Accuracy on test set: 0.1%\n",
      "Fitting for combination 74\n",
      "784\n",
      "4\n",
      "10\n",
      "[40, 10, 10, 40, 10]\n",
      "True\n",
      "['relu', 'sigmoid', 'relu', 'tanh']\n",
      "Adam\n",
      "0.3\n",
      "0.1\n",
      "CrossEntropyLoss\n",
      "200\n",
      "25\n",
      "0\n",
      "Epoch 1, Batch 200, Loss: 4.645445611476898\n",
      "Epoch 1, Batch 400, Loss: 4.6301451635360715\n",
      "Epoch 1, Batch 600, Loss: 4.632559180259705\n",
      "Epoch 1, Batch 800, Loss: 4.675449273586273\n",
      "Epoch 2, Batch 200, Loss: 4.631306357383728\n",
      "Epoch 2, Batch 400, Loss: 4.6242768406867985\n",
      "Epoch 2, Batch 600, Loss: 4.624257214069367\n",
      "Epoch 2, Batch 800, Loss: 4.628863444328308\n",
      "Epoch 3, Batch 200, Loss: 13.350488333702087\n",
      "Epoch 3, Batch 400, Loss: 5.023022975921631\n",
      "Epoch 3, Batch 600, Loss: 4.6197254586219785\n",
      "Epoch 3, Batch 800, Loss: 4.619272015094757\n",
      "Epoch 4, Batch 200, Loss: 4.615845916271209\n",
      "Epoch 4, Batch 400, Loss: 4.619462096691132\n",
      "Epoch 4, Batch 600, Loss: 4.622668130397797\n",
      "Epoch 4, Batch 800, Loss: 4.6210257744789125\n",
      "Epoch 5, Batch 200, Loss: 4.626558396816254\n",
      "Epoch 5, Batch 400, Loss: 4.626585204601287\n",
      "Epoch 5, Batch 600, Loss: 4.628843092918396\n",
      "Epoch 5, Batch 800, Loss: 4.625195274353027\n",
      "Epoch 6, Batch 200, Loss: 4.625877923965454\n",
      "Epoch 6, Batch 400, Loss: 4.629629940986633\n",
      "Epoch 6, Batch 600, Loss: 4.631838974952697\n",
      "Epoch 6, Batch 800, Loss: 4.630626454353332\n",
      "Epoch 7, Batch 200, Loss: 4.628712766170501\n",
      "Epoch 7, Batch 400, Loss: 4.628508627414703\n",
      "Epoch 7, Batch 600, Loss: 4.626488335132599\n",
      "Epoch 7, Batch 800, Loss: 4.626191248893738\n",
      "Epoch 8, Batch 200, Loss: 4.63049012184143\n",
      "Epoch 8, Batch 400, Loss: 4.627210566997528\n",
      "Epoch 8, Batch 600, Loss: 4.622861487865448\n",
      "Epoch 8, Batch 800, Loss: 4.625786700248718\n",
      "Epoch 9, Batch 200, Loss: 4.625588641166687\n",
      "Epoch 9, Batch 400, Loss: 4.630700294971466\n",
      "Epoch 9, Batch 600, Loss: 4.6309547829627995\n",
      "Epoch 9, Batch 800, Loss: 10.99245248556137\n",
      "Epoch 10, Batch 200, Loss: 4.620413625240326\n",
      "Epoch 10, Batch 400, Loss: 4.620595862865448\n",
      "Epoch 10, Batch 600, Loss: 4.618688311576843\n",
      "Epoch 10, Batch 800, Loss: 4.6173400926589965\n",
      "Epoch 11, Batch 200, Loss: 4.619593288898468\n",
      "Epoch 11, Batch 400, Loss: 4.62368269443512\n",
      "Epoch 11, Batch 600, Loss: 4.630199680328369\n",
      "Epoch 11, Batch 800, Loss: 4.623798949718475\n",
      "Epoch 12, Batch 200, Loss: 4.625756223201751\n",
      "Epoch 12, Batch 400, Loss: 4.6267447090148925\n",
      "Epoch 12, Batch 600, Loss: 4.62664144039154\n",
      "Epoch 12, Batch 800, Loss: 4.627803478240967\n",
      "Epoch 13, Batch 200, Loss: 4.6248547649383545\n",
      "Epoch 13, Batch 400, Loss: 4.624644832611084\n",
      "Epoch 13, Batch 600, Loss: 4.627646479606629\n",
      "Epoch 13, Batch 800, Loss: 4.629293954372406\n",
      "Epoch 14, Batch 200, Loss: 4.62624359369278\n",
      "Epoch 14, Batch 400, Loss: 4.625360615253449\n",
      "Epoch 14, Batch 600, Loss: 13.071462323665619\n",
      "Epoch 14, Batch 800, Loss: 4.699318902492523\n",
      "Epoch 15, Batch 200, Loss: 4.624406509399414\n",
      "Epoch 15, Batch 400, Loss: 4.61847336769104\n",
      "Epoch 15, Batch 600, Loss: 4.617092435359955\n",
      "Epoch 15, Batch 800, Loss: 4.624740498065949\n",
      "Epoch 16, Batch 200, Loss: 4.62171594619751\n",
      "Epoch 16, Batch 400, Loss: 4.625832633972168\n",
      "Epoch 16, Batch 600, Loss: 4.623792881965637\n",
      "Epoch 16, Batch 800, Loss: 4.620458946228028\n",
      "Epoch 17, Batch 200, Loss: 4.629846754074097\n",
      "Epoch 17, Batch 400, Loss: 4.626478934288025\n",
      "Epoch 17, Batch 600, Loss: 4.62533296585083\n",
      "Epoch 17, Batch 800, Loss: 4.625314841270447\n",
      "Epoch 18, Batch 200, Loss: 4.6225912737846375\n",
      "Epoch 18, Batch 400, Loss: 4.626829373836517\n",
      "Epoch 18, Batch 600, Loss: 4.626072108745575\n",
      "Epoch 18, Batch 800, Loss: 4.625733621120453\n",
      "Epoch 19, Batch 200, Loss: 4.630222430229187\n",
      "Epoch 19, Batch 400, Loss: 4.628914206027985\n",
      "Epoch 19, Batch 600, Loss: 4.626064491271973\n",
      "Epoch 19, Batch 800, Loss: 4.628974163532257\n",
      "Epoch 20, Batch 200, Loss: 4.633310656547547\n",
      "Epoch 20, Batch 400, Loss: 4.634542157649994\n",
      "Epoch 20, Batch 600, Loss: 4.644398045539856\n",
      "Epoch 20, Batch 800, Loss: 10.000053572654725\n",
      "Epoch 21, Batch 200, Loss: 4.619779603481293\n",
      "Epoch 21, Batch 400, Loss: 4.620640902519226\n",
      "Epoch 21, Batch 600, Loss: 4.622279603481292\n",
      "Epoch 21, Batch 800, Loss: 4.623153305053711\n",
      "Epoch 22, Batch 200, Loss: 4.6242533206939695\n",
      "Epoch 22, Batch 400, Loss: 4.623595416545868\n",
      "Epoch 22, Batch 600, Loss: 4.63117490530014\n",
      "Epoch 22, Batch 800, Loss: 4.627954239845276\n",
      "Epoch 23, Batch 200, Loss: 4.62737679719925\n",
      "Epoch 23, Batch 400, Loss: 4.621539256572723\n",
      "Epoch 23, Batch 600, Loss: 4.622879238128662\n",
      "Epoch 23, Batch 800, Loss: 4.625923037528992\n",
      "Epoch 24, Batch 200, Loss: 4.625736076831817\n",
      "Epoch 24, Batch 400, Loss: 4.624689517021179\n",
      "Epoch 24, Batch 600, Loss: 4.630434653759003\n",
      "Epoch 24, Batch 800, Loss: 4.629326686859131\n",
      "Epoch 25, Batch 200, Loss: 4.627443592548371\n",
      "Epoch 25, Batch 400, Loss: 4.624014332294464\n",
      "Epoch 25, Batch 600, Loss: 4.684505126476288\n",
      "Epoch 25, Batch 800, Loss: 11.918630633354187\n",
      "Accuracy on test set: 0.1%\n",
      "Fitting for combination 75\n",
      "784\n",
      "4\n",
      "10\n",
      "[40, 10, 10, 40, 10]\n",
      "True\n",
      "['relu', 'sigmoid', 'relu', 'sigmoid']\n",
      "SGD\n",
      "0.1\n",
      "0.3\n",
      "CrossEntropyLoss\n",
      "50\n",
      "25\n",
      "0\n",
      "Epoch 1, Batch 50, Loss: 1.1539878487586974\n",
      "Epoch 1, Batch 100, Loss: 1.1530030679702759\n",
      "Epoch 1, Batch 150, Loss: 1.1533685088157655\n",
      "Epoch 1, Batch 200, Loss: 1.1530509567260743\n",
      "Epoch 1, Batch 250, Loss: 1.1527708649635315\n",
      "Epoch 1, Batch 300, Loss: 1.15265478849411\n",
      "Epoch 1, Batch 350, Loss: 1.152999722957611\n",
      "Epoch 1, Batch 400, Loss: 1.1527585554122926\n",
      "Epoch 1, Batch 450, Loss: 1.1525903868675231\n",
      "Epoch 1, Batch 500, Loss: 1.1530603170394897\n",
      "Epoch 1, Batch 550, Loss: 1.1531129813194274\n",
      "Epoch 1, Batch 600, Loss: 1.1533730030059814\n",
      "Epoch 1, Batch 650, Loss: 1.1531336975097657\n",
      "Epoch 1, Batch 700, Loss: 1.1535511708259583\n",
      "Epoch 1, Batch 750, Loss: 1.1531108808517456\n",
      "Epoch 1, Batch 800, Loss: 1.1531952428817749\n",
      "Epoch 1, Batch 850, Loss: 1.1527056074142457\n",
      "Epoch 1, Batch 900, Loss: 1.1534801506996155\n",
      "Epoch 2, Batch 50, Loss: 1.1522345495224\n",
      "Epoch 2, Batch 100, Loss: 1.1530786490440368\n",
      "Epoch 2, Batch 150, Loss: 1.1539363169670105\n",
      "Epoch 2, Batch 200, Loss: 1.1530272150039673\n",
      "Epoch 2, Batch 250, Loss: 1.1531886529922486\n",
      "Epoch 2, Batch 300, Loss: 1.1517705869674684\n",
      "Epoch 2, Batch 350, Loss: 1.1538735342025757\n",
      "Epoch 2, Batch 400, Loss: 1.1529665279388428\n",
      "Epoch 2, Batch 450, Loss: 1.153125491142273\n",
      "Epoch 2, Batch 500, Loss: 1.1529510045051574\n",
      "Epoch 2, Batch 550, Loss: 1.1540274143218994\n",
      "Epoch 2, Batch 600, Loss: 1.1531463384628295\n",
      "Epoch 2, Batch 650, Loss: 1.153702862262726\n",
      "Epoch 2, Batch 700, Loss: 1.1527468299865722\n",
      "Epoch 2, Batch 750, Loss: 1.151996729373932\n",
      "Epoch 2, Batch 800, Loss: 1.1528041553497315\n",
      "Epoch 2, Batch 850, Loss: 1.1527303862571716\n",
      "Epoch 2, Batch 900, Loss: 1.1527895879745484\n",
      "Epoch 3, Batch 50, Loss: 1.1536126160621643\n",
      "Epoch 3, Batch 100, Loss: 1.1529563879966735\n",
      "Epoch 3, Batch 150, Loss: 1.1528616237640381\n",
      "Epoch 3, Batch 200, Loss: 1.1526093888282776\n",
      "Epoch 3, Batch 250, Loss: 1.1526479601860047\n",
      "Epoch 3, Batch 300, Loss: 1.1531421661376953\n",
      "Epoch 3, Batch 350, Loss: 1.1531786823272705\n",
      "Epoch 3, Batch 400, Loss: 1.1525208234786988\n",
      "Epoch 3, Batch 450, Loss: 1.15312091588974\n",
      "Epoch 3, Batch 500, Loss: 1.1517721343040466\n",
      "Epoch 3, Batch 550, Loss: 1.153098292350769\n",
      "Epoch 3, Batch 600, Loss: 1.1531608390808106\n",
      "Epoch 3, Batch 650, Loss: 1.1523695993423462\n",
      "Epoch 3, Batch 700, Loss: 1.1529540181159974\n",
      "Epoch 3, Batch 750, Loss: 1.1520391082763672\n",
      "Epoch 3, Batch 800, Loss: 1.1530941462516784\n",
      "Epoch 3, Batch 850, Loss: 1.1523466324806213\n",
      "Epoch 3, Batch 900, Loss: 1.1532726693153381\n",
      "Epoch 4, Batch 50, Loss: 1.1534749126434327\n",
      "Epoch 4, Batch 100, Loss: 1.1526586627960205\n",
      "Epoch 4, Batch 150, Loss: 1.152527244091034\n",
      "Epoch 4, Batch 200, Loss: 1.1537139749526977\n",
      "Epoch 4, Batch 250, Loss: 1.1528380417823791\n",
      "Epoch 4, Batch 300, Loss: 1.1524110507965089\n",
      "Epoch 4, Batch 350, Loss: 1.1522201871871949\n",
      "Epoch 4, Batch 400, Loss: 1.152440493106842\n",
      "Epoch 4, Batch 450, Loss: 1.1527516222000123\n",
      "Epoch 4, Batch 500, Loss: 1.153460500240326\n",
      "Epoch 4, Batch 550, Loss: 1.152884428501129\n",
      "Epoch 4, Batch 600, Loss: 1.1524551677703858\n",
      "Epoch 4, Batch 650, Loss: 1.1524844837188721\n",
      "Epoch 4, Batch 700, Loss: 1.1534434723854066\n",
      "Epoch 4, Batch 750, Loss: 1.1524038791656495\n",
      "Epoch 4, Batch 800, Loss: 1.1529436326026916\n",
      "Epoch 4, Batch 850, Loss: 1.1534652018547058\n",
      "Epoch 4, Batch 900, Loss: 1.1526530361175538\n",
      "Epoch 5, Batch 50, Loss: 1.1530243015289308\n",
      "Epoch 5, Batch 100, Loss: 1.1534525537490845\n",
      "Epoch 5, Batch 150, Loss: 1.1527533006668091\n",
      "Epoch 5, Batch 200, Loss: 1.1538350415229797\n",
      "Epoch 5, Batch 250, Loss: 1.1527623653411865\n",
      "Epoch 5, Batch 300, Loss: 1.1518353486061097\n",
      "Epoch 5, Batch 350, Loss: 1.1527964115142821\n",
      "Epoch 5, Batch 400, Loss: 1.152276349067688\n",
      "Epoch 5, Batch 450, Loss: 1.1526698136329652\n",
      "Epoch 5, Batch 500, Loss: 1.1533369755744933\n",
      "Epoch 5, Batch 550, Loss: 1.1537382411956787\n",
      "Epoch 5, Batch 600, Loss: 1.1531265020370483\n",
      "Epoch 5, Batch 650, Loss: 1.1526727342605592\n",
      "Epoch 5, Batch 700, Loss: 1.1537020158767701\n",
      "Epoch 5, Batch 750, Loss: 1.153037281036377\n",
      "Epoch 5, Batch 800, Loss: 1.153091206550598\n",
      "Epoch 5, Batch 850, Loss: 1.1528557658195495\n",
      "Epoch 5, Batch 900, Loss: 1.1525902247428894\n",
      "Epoch 6, Batch 50, Loss: 1.152560043334961\n",
      "Epoch 6, Batch 100, Loss: 1.1528347110748292\n",
      "Epoch 6, Batch 150, Loss: 1.1530845308303832\n",
      "Epoch 6, Batch 200, Loss: 1.1523469638824464\n",
      "Epoch 6, Batch 250, Loss: 1.1526696920394897\n",
      "Epoch 6, Batch 300, Loss: 1.1528244400024414\n",
      "Epoch 6, Batch 350, Loss: 1.1531003069877626\n",
      "Epoch 6, Batch 400, Loss: 1.1525188064575196\n",
      "Epoch 6, Batch 450, Loss: 1.1533327412605285\n",
      "Epoch 6, Batch 500, Loss: 1.153162636756897\n",
      "Epoch 6, Batch 550, Loss: 1.1535544204711914\n",
      "Epoch 6, Batch 600, Loss: 1.1534205889701843\n",
      "Epoch 6, Batch 650, Loss: 1.1530007243156433\n",
      "Epoch 6, Batch 700, Loss: 1.1528986120223998\n",
      "Epoch 6, Batch 750, Loss: 1.1521074748039246\n",
      "Epoch 6, Batch 800, Loss: 1.1527150201797485\n",
      "Epoch 6, Batch 850, Loss: 1.1526336145401002\n",
      "Epoch 6, Batch 900, Loss: 1.1529900360107421\n",
      "Epoch 7, Batch 50, Loss: 1.1515371298789978\n",
      "Epoch 7, Batch 100, Loss: 1.153010447025299\n",
      "Epoch 7, Batch 150, Loss: 1.1528614377975464\n",
      "Epoch 7, Batch 200, Loss: 1.1529776549339295\n",
      "Epoch 7, Batch 250, Loss: 1.1534976243972779\n",
      "Epoch 7, Batch 300, Loss: 1.1522669410705566\n",
      "Epoch 7, Batch 350, Loss: 1.1527909016609192\n",
      "Epoch 7, Batch 400, Loss: 1.152366397380829\n",
      "Epoch 7, Batch 450, Loss: 1.153024332523346\n",
      "Epoch 7, Batch 500, Loss: 1.1528998351097106\n",
      "Epoch 7, Batch 550, Loss: 1.1535510993003846\n",
      "Epoch 7, Batch 600, Loss: 1.1534884643554688\n",
      "Epoch 7, Batch 650, Loss: 1.1534094524383545\n",
      "Epoch 7, Batch 700, Loss: 1.152947244644165\n",
      "Epoch 7, Batch 750, Loss: 1.152985007762909\n",
      "Epoch 7, Batch 800, Loss: 1.153675000667572\n",
      "Epoch 7, Batch 850, Loss: 1.1529780864715575\n",
      "Epoch 7, Batch 900, Loss: 1.152691490650177\n",
      "Epoch 8, Batch 50, Loss: 1.1534673762321472\n",
      "Epoch 8, Batch 100, Loss: 1.152351257801056\n",
      "Epoch 8, Batch 150, Loss: 1.1521729564666747\n",
      "Epoch 8, Batch 200, Loss: 1.153132164478302\n",
      "Epoch 8, Batch 250, Loss: 1.1532383847236634\n",
      "Epoch 8, Batch 300, Loss: 1.1524936270713806\n",
      "Epoch 8, Batch 350, Loss: 1.1529872179031373\n",
      "Epoch 8, Batch 400, Loss: 1.1525407576560973\n",
      "Epoch 8, Batch 450, Loss: 1.1527179527282714\n",
      "Epoch 8, Batch 500, Loss: 1.1530469751358032\n",
      "Epoch 8, Batch 550, Loss: 1.1526768159866334\n",
      "Epoch 8, Batch 600, Loss: 1.1530732917785644\n",
      "Epoch 8, Batch 650, Loss: 1.1529424428939818\n",
      "Epoch 8, Batch 700, Loss: 1.1537256836891174\n",
      "Epoch 8, Batch 750, Loss: 1.152373869419098\n",
      "Epoch 8, Batch 800, Loss: 1.1528322196006775\n",
      "Epoch 8, Batch 850, Loss: 1.1533922052383423\n",
      "Epoch 8, Batch 900, Loss: 1.1526129698753358\n",
      "Epoch 9, Batch 50, Loss: 1.1537926745414735\n",
      "Epoch 9, Batch 100, Loss: 1.1535311150550842\n",
      "Epoch 9, Batch 150, Loss: 1.1527513837814332\n",
      "Epoch 9, Batch 200, Loss: 1.1538951373100281\n",
      "Epoch 9, Batch 250, Loss: 1.1533717823028564\n",
      "Epoch 9, Batch 300, Loss: 1.153581566810608\n",
      "Epoch 9, Batch 350, Loss: 1.153507947921753\n",
      "Epoch 9, Batch 400, Loss: 1.1521807599067688\n",
      "Epoch 9, Batch 450, Loss: 1.1529147720336914\n",
      "Epoch 9, Batch 500, Loss: 1.152399046421051\n",
      "Epoch 9, Batch 550, Loss: 1.152491090297699\n",
      "Epoch 9, Batch 600, Loss: 1.1521779561042786\n",
      "Epoch 9, Batch 650, Loss: 1.1529114842414856\n",
      "Epoch 9, Batch 700, Loss: 1.1521597576141358\n",
      "Epoch 9, Batch 750, Loss: 1.1530233001708985\n",
      "Epoch 9, Batch 800, Loss: 1.1536929130554199\n",
      "Epoch 9, Batch 850, Loss: 1.1531795597076415\n",
      "Epoch 9, Batch 900, Loss: 1.1533442521095276\n",
      "Epoch 10, Batch 50, Loss: 1.1530707907676696\n",
      "Epoch 10, Batch 100, Loss: 1.1529560852050782\n",
      "Epoch 10, Batch 150, Loss: 1.1515171575546264\n",
      "Epoch 10, Batch 200, Loss: 1.1529984188079834\n",
      "Epoch 10, Batch 250, Loss: 1.1533265161514281\n",
      "Epoch 10, Batch 300, Loss: 1.1528307867050172\n",
      "Epoch 10, Batch 350, Loss: 1.1531461906433105\n",
      "Epoch 10, Batch 400, Loss: 1.1526521229743958\n",
      "Epoch 10, Batch 450, Loss: 1.1527915287017823\n",
      "Epoch 10, Batch 500, Loss: 1.1528372478485107\n",
      "Epoch 10, Batch 550, Loss: 1.153006157875061\n",
      "Epoch 10, Batch 600, Loss: 1.1526817297935485\n",
      "Epoch 10, Batch 650, Loss: 1.1536178874969483\n",
      "Epoch 10, Batch 700, Loss: 1.1527541780471802\n",
      "Epoch 10, Batch 750, Loss: 1.1527903199195861\n",
      "Epoch 10, Batch 800, Loss: 1.1529313230514526\n",
      "Epoch 10, Batch 850, Loss: 1.153630883693695\n",
      "Epoch 10, Batch 900, Loss: 1.1535199689865112\n",
      "Epoch 11, Batch 50, Loss: 1.1528117704391478\n",
      "Epoch 11, Batch 100, Loss: 1.1532939338684083\n",
      "Epoch 11, Batch 150, Loss: 1.1532067489624023\n",
      "Epoch 11, Batch 200, Loss: 1.1517357802391053\n",
      "Epoch 11, Batch 250, Loss: 1.1532309651374817\n",
      "Epoch 11, Batch 300, Loss: 1.152574255466461\n",
      "Epoch 11, Batch 350, Loss: 1.1534568977355957\n",
      "Epoch 11, Batch 400, Loss: 1.1524740171432495\n",
      "Epoch 11, Batch 450, Loss: 1.1526857566833497\n",
      "Epoch 11, Batch 500, Loss: 1.1537167453765869\n",
      "Epoch 11, Batch 550, Loss: 1.1529554867744445\n",
      "Epoch 11, Batch 600, Loss: 1.1534688115119933\n",
      "Epoch 11, Batch 650, Loss: 1.152606942653656\n",
      "Epoch 11, Batch 700, Loss: 1.1527704310417175\n",
      "Epoch 11, Batch 750, Loss: 1.1522742795944214\n",
      "Epoch 11, Batch 800, Loss: 1.153309826850891\n",
      "Epoch 11, Batch 850, Loss: 1.1529432535171509\n",
      "Epoch 11, Batch 900, Loss: 1.153101496696472\n",
      "Epoch 12, Batch 50, Loss: 1.1533116340637206\n",
      "Epoch 12, Batch 100, Loss: 1.1530933213233947\n",
      "Epoch 12, Batch 150, Loss: 1.152209963798523\n",
      "Epoch 12, Batch 200, Loss: 1.154056408405304\n",
      "Epoch 12, Batch 250, Loss: 1.153074016571045\n",
      "Epoch 12, Batch 300, Loss: 1.1520570588111878\n",
      "Epoch 12, Batch 350, Loss: 1.1527860236167908\n",
      "Epoch 12, Batch 400, Loss: 1.1523534393310546\n",
      "Epoch 12, Batch 450, Loss: 1.1534144473075867\n",
      "Epoch 12, Batch 500, Loss: 1.15318523645401\n",
      "Epoch 12, Batch 550, Loss: 1.1534246873855591\n",
      "Epoch 12, Batch 600, Loss: 1.153215320110321\n",
      "Epoch 12, Batch 650, Loss: 1.1531714153289796\n",
      "Epoch 12, Batch 700, Loss: 1.1526758480072021\n",
      "Epoch 12, Batch 750, Loss: 1.1535475039482117\n",
      "Epoch 12, Batch 800, Loss: 1.1533487915992737\n",
      "Epoch 12, Batch 850, Loss: 1.1533489322662354\n",
      "Epoch 12, Batch 900, Loss: 1.1529296088218688\n",
      "Epoch 13, Batch 50, Loss: 1.1528931117057801\n",
      "Epoch 13, Batch 100, Loss: 1.1537286448478699\n",
      "Epoch 13, Batch 150, Loss: 1.152662489414215\n",
      "Epoch 13, Batch 200, Loss: 1.153081283569336\n",
      "Epoch 13, Batch 250, Loss: 1.1528691148757935\n",
      "Epoch 13, Batch 300, Loss: 1.1536000061035157\n",
      "Epoch 13, Batch 350, Loss: 1.1529495787620545\n",
      "Epoch 13, Batch 400, Loss: 1.153004093170166\n",
      "Epoch 13, Batch 450, Loss: 1.1531541681289672\n",
      "Epoch 13, Batch 500, Loss: 1.1529975891113282\n",
      "Epoch 13, Batch 550, Loss: 1.152780122756958\n",
      "Epoch 13, Batch 600, Loss: 1.153411090373993\n",
      "Epoch 13, Batch 650, Loss: 1.1529560351371766\n",
      "Epoch 13, Batch 700, Loss: 1.1526599311828614\n",
      "Epoch 13, Batch 750, Loss: 1.1531951308250428\n",
      "Epoch 13, Batch 800, Loss: 1.1530639672279357\n",
      "Epoch 13, Batch 850, Loss: 1.1530085062980653\n",
      "Epoch 13, Batch 900, Loss: 1.1531886625289918\n",
      "Epoch 14, Batch 50, Loss: 1.15255446434021\n",
      "Epoch 14, Batch 100, Loss: 1.1524404001235962\n",
      "Epoch 14, Batch 150, Loss: 1.1530448818206787\n",
      "Epoch 14, Batch 200, Loss: 1.152419445514679\n",
      "Epoch 14, Batch 250, Loss: 1.1532208943367004\n",
      "Epoch 14, Batch 300, Loss: 1.1538968420028686\n",
      "Epoch 14, Batch 350, Loss: 1.1529157185554504\n",
      "Epoch 14, Batch 400, Loss: 1.1526260137557984\n",
      "Epoch 14, Batch 450, Loss: 1.1530137276649475\n",
      "Epoch 14, Batch 500, Loss: 1.1525771594047547\n",
      "Epoch 14, Batch 550, Loss: 1.1523614740371704\n",
      "Epoch 14, Batch 600, Loss: 1.1525983500480652\n",
      "Epoch 14, Batch 650, Loss: 1.15271963596344\n",
      "Epoch 14, Batch 700, Loss: 1.1531235408782958\n",
      "Epoch 14, Batch 750, Loss: 1.152819242477417\n",
      "Epoch 14, Batch 800, Loss: 1.1522221779823303\n",
      "Epoch 14, Batch 850, Loss: 1.1528764843940735\n",
      "Epoch 14, Batch 900, Loss: 1.1533863162994384\n",
      "Epoch 15, Batch 50, Loss: 1.1534225296974183\n",
      "Epoch 15, Batch 100, Loss: 1.153028597831726\n",
      "Epoch 15, Batch 150, Loss: 1.152924816608429\n",
      "Epoch 15, Batch 200, Loss: 1.1536287331581117\n",
      "Epoch 15, Batch 250, Loss: 1.1526651573181153\n",
      "Epoch 15, Batch 300, Loss: 1.152935140132904\n",
      "Epoch 15, Batch 350, Loss: 1.1534666347503661\n",
      "Epoch 15, Batch 400, Loss: 1.153094003200531\n",
      "Epoch 15, Batch 450, Loss: 1.1526287841796874\n",
      "Epoch 15, Batch 500, Loss: 1.1525507116317748\n",
      "Epoch 15, Batch 550, Loss: 1.1526811337471008\n",
      "Epoch 15, Batch 600, Loss: 1.1524294853210448\n",
      "Epoch 15, Batch 650, Loss: 1.1537485671043397\n",
      "Epoch 15, Batch 700, Loss: 1.1529578518867494\n",
      "Epoch 15, Batch 750, Loss: 1.1524923133850098\n",
      "Epoch 15, Batch 800, Loss: 1.1532093477249146\n",
      "Epoch 15, Batch 850, Loss: 1.1529540133476257\n",
      "Epoch 15, Batch 900, Loss: 1.1531512451171875\n",
      "Epoch 16, Batch 50, Loss: 1.1524329733848573\n",
      "Epoch 16, Batch 100, Loss: 1.1527118873596192\n",
      "Epoch 16, Batch 150, Loss: 1.1526079869270325\n",
      "Epoch 16, Batch 200, Loss: 1.15379741191864\n",
      "Epoch 16, Batch 250, Loss: 1.1531207799911498\n",
      "Epoch 16, Batch 300, Loss: 1.1529317951202394\n",
      "Epoch 16, Batch 350, Loss: 1.152879774570465\n",
      "Epoch 16, Batch 400, Loss: 1.152133162021637\n",
      "Epoch 16, Batch 450, Loss: 1.1535881686210632\n",
      "Epoch 16, Batch 500, Loss: 1.152019829750061\n",
      "Epoch 16, Batch 550, Loss: 1.1532113313674928\n",
      "Epoch 16, Batch 600, Loss: 1.1528058314323426\n",
      "Epoch 16, Batch 650, Loss: 1.1535275435447694\n",
      "Epoch 16, Batch 700, Loss: 1.1528422927856445\n",
      "Epoch 16, Batch 750, Loss: 1.1528014183044433\n",
      "Epoch 16, Batch 800, Loss: 1.1523787474632263\n",
      "Epoch 16, Batch 850, Loss: 1.1524672770500184\n",
      "Epoch 16, Batch 900, Loss: 1.1532813692092896\n",
      "Epoch 17, Batch 50, Loss: 1.1527855539321898\n",
      "Epoch 17, Batch 100, Loss: 1.1527225518226623\n",
      "Epoch 17, Batch 150, Loss: 1.1533868265151979\n",
      "Epoch 17, Batch 200, Loss: 1.153205895423889\n",
      "Epoch 17, Batch 250, Loss: 1.1530508041381835\n",
      "Epoch 17, Batch 300, Loss: 1.1528236293792724\n",
      "Epoch 17, Batch 350, Loss: 1.153095898628235\n",
      "Epoch 17, Batch 400, Loss: 1.1528710865974425\n",
      "Epoch 17, Batch 450, Loss: 1.1538240170478822\n",
      "Epoch 17, Batch 500, Loss: 1.152527370452881\n",
      "Epoch 17, Batch 550, Loss: 1.1527289867401123\n",
      "Epoch 17, Batch 600, Loss: 1.1533479762077332\n",
      "Epoch 17, Batch 650, Loss: 1.1530865120887757\n",
      "Epoch 17, Batch 700, Loss: 1.1521532034873962\n",
      "Epoch 17, Batch 750, Loss: 1.153355152606964\n",
      "Epoch 17, Batch 800, Loss: 1.153111436367035\n",
      "Epoch 17, Batch 850, Loss: 1.1531025958061218\n",
      "Epoch 17, Batch 900, Loss: 1.1526406145095824\n",
      "Epoch 18, Batch 50, Loss: 1.152965931892395\n",
      "Epoch 18, Batch 100, Loss: 1.1523833537101746\n",
      "Epoch 18, Batch 150, Loss: 1.1525959038734437\n",
      "Epoch 18, Batch 200, Loss: 1.1532186770439148\n",
      "Epoch 18, Batch 250, Loss: 1.1530059552192689\n",
      "Epoch 18, Batch 300, Loss: 1.1533955931663513\n",
      "Epoch 18, Batch 350, Loss: 1.15337131023407\n",
      "Epoch 18, Batch 400, Loss: 1.1528841757774353\n",
      "Epoch 18, Batch 450, Loss: 1.1529223823547363\n",
      "Epoch 18, Batch 500, Loss: 1.1531360054016113\n",
      "Epoch 18, Batch 550, Loss: 1.153408372402191\n",
      "Epoch 18, Batch 600, Loss: 1.1533707809448241\n",
      "Epoch 18, Batch 650, Loss: 1.1530017495155334\n",
      "Epoch 18, Batch 700, Loss: 1.1527028226852416\n",
      "Epoch 18, Batch 750, Loss: 1.1530408239364625\n",
      "Epoch 18, Batch 800, Loss: 1.1536073279380799\n",
      "Epoch 18, Batch 850, Loss: 1.1534604716300965\n",
      "Epoch 18, Batch 900, Loss: 1.1536823987960816\n",
      "Epoch 19, Batch 50, Loss: 1.1535251426696778\n",
      "Epoch 19, Batch 100, Loss: 1.1532221412658692\n",
      "Epoch 19, Batch 150, Loss: 1.153159511089325\n",
      "Epoch 19, Batch 200, Loss: 1.1518914103507996\n",
      "Epoch 19, Batch 250, Loss: 1.1530945253372193\n",
      "Epoch 19, Batch 300, Loss: 1.1529309034347535\n",
      "Epoch 19, Batch 350, Loss: 1.152895565032959\n",
      "Epoch 19, Batch 400, Loss: 1.1532289433479308\n",
      "Epoch 19, Batch 450, Loss: 1.152004098892212\n",
      "Epoch 19, Batch 500, Loss: 1.1528827095031737\n",
      "Epoch 19, Batch 550, Loss: 1.153409206867218\n",
      "Epoch 19, Batch 600, Loss: 1.1523913884162902\n",
      "Epoch 19, Batch 650, Loss: 1.1533480191230774\n",
      "Epoch 19, Batch 700, Loss: 1.152438497543335\n",
      "Epoch 19, Batch 750, Loss: 1.1536783075332642\n",
      "Epoch 19, Batch 800, Loss: 1.152447452545166\n",
      "Epoch 19, Batch 850, Loss: 1.1528448247909546\n",
      "Epoch 19, Batch 900, Loss: 1.1529644155502319\n",
      "Epoch 20, Batch 50, Loss: 1.1524799656867981\n",
      "Epoch 20, Batch 100, Loss: 1.152796401977539\n",
      "Epoch 20, Batch 150, Loss: 1.1528271651268005\n",
      "Epoch 20, Batch 200, Loss: 1.1529462337493896\n",
      "Epoch 20, Batch 250, Loss: 1.1522520112991332\n",
      "Epoch 20, Batch 300, Loss: 1.1529595804214479\n",
      "Epoch 20, Batch 350, Loss: 1.1531361246109009\n",
      "Epoch 20, Batch 400, Loss: 1.1530814051628113\n",
      "Epoch 20, Batch 450, Loss: 1.1525151968002318\n",
      "Epoch 20, Batch 500, Loss: 1.1526930570602416\n",
      "Epoch 20, Batch 550, Loss: 1.1538245725631713\n",
      "Epoch 20, Batch 600, Loss: 1.1535012578964234\n",
      "Epoch 20, Batch 650, Loss: 1.1525111150741578\n",
      "Epoch 20, Batch 700, Loss: 1.1540095901489258\n",
      "Epoch 20, Batch 750, Loss: 1.1527432870864869\n",
      "Epoch 20, Batch 800, Loss: 1.1533756709098817\n",
      "Epoch 20, Batch 850, Loss: 1.1531434440612793\n",
      "Epoch 20, Batch 900, Loss: 1.1533075857162476\n",
      "Epoch 21, Batch 50, Loss: 1.1530143404006958\n",
      "Epoch 21, Batch 100, Loss: 1.1535922718048095\n",
      "Epoch 21, Batch 150, Loss: 1.153320529460907\n",
      "Epoch 21, Batch 200, Loss: 1.1527004265785217\n",
      "Epoch 21, Batch 250, Loss: 1.1530229902267457\n",
      "Epoch 21, Batch 300, Loss: 1.153533697128296\n",
      "Epoch 21, Batch 350, Loss: 1.1534366965293885\n",
      "Epoch 21, Batch 400, Loss: 1.1520830035209655\n",
      "Epoch 21, Batch 450, Loss: 1.1525561332702636\n",
      "Epoch 21, Batch 500, Loss: 1.1533100557327272\n",
      "Epoch 21, Batch 550, Loss: 1.1529948234558105\n",
      "Epoch 21, Batch 600, Loss: 1.152778663635254\n",
      "Epoch 21, Batch 650, Loss: 1.1518659019470214\n",
      "Epoch 21, Batch 700, Loss: 1.1525646328926087\n",
      "Epoch 21, Batch 750, Loss: 1.1532255911827087\n",
      "Epoch 21, Batch 800, Loss: 1.152337429523468\n",
      "Epoch 21, Batch 850, Loss: 1.1538019251823426\n",
      "Epoch 21, Batch 900, Loss: 1.1533747696876526\n",
      "Epoch 22, Batch 50, Loss: 1.1532232284545898\n",
      "Epoch 22, Batch 100, Loss: 1.1533647871017456\n",
      "Epoch 22, Batch 150, Loss: 1.1532038044929505\n",
      "Epoch 22, Batch 200, Loss: 1.1531491947174073\n",
      "Epoch 22, Batch 250, Loss: 1.1538603281974793\n",
      "Epoch 22, Batch 300, Loss: 1.1528143286705017\n",
      "Epoch 22, Batch 350, Loss: 1.1534673762321472\n",
      "Epoch 22, Batch 400, Loss: 1.1535376930236816\n",
      "Epoch 22, Batch 450, Loss: 1.1537060308456422\n",
      "Epoch 22, Batch 500, Loss: 1.1538109469413758\n",
      "Epoch 22, Batch 550, Loss: 1.1520783734321594\n",
      "Epoch 22, Batch 600, Loss: 1.1532937645912171\n",
      "Epoch 22, Batch 650, Loss: 1.1529317283630371\n",
      "Epoch 22, Batch 700, Loss: 1.153136065006256\n",
      "Epoch 22, Batch 750, Loss: 1.1535125470161438\n",
      "Epoch 22, Batch 800, Loss: 1.153142066001892\n",
      "Epoch 22, Batch 850, Loss: 1.1528879952430726\n",
      "Epoch 22, Batch 900, Loss: 1.1533530497550963\n",
      "Epoch 23, Batch 50, Loss: 1.152367238998413\n",
      "Epoch 23, Batch 100, Loss: 1.1536902642250062\n",
      "Epoch 23, Batch 150, Loss: 1.15314679145813\n",
      "Epoch 23, Batch 200, Loss: 1.1524515056610107\n",
      "Epoch 23, Batch 250, Loss: 1.1522104454040527\n",
      "Epoch 23, Batch 300, Loss: 1.1533625769615172\n",
      "Epoch 23, Batch 350, Loss: 1.1533444499969483\n",
      "Epoch 23, Batch 400, Loss: 1.1527889609336852\n",
      "Epoch 23, Batch 450, Loss: 1.1525391674041747\n",
      "Epoch 23, Batch 500, Loss: 1.1533603167533875\n",
      "Epoch 23, Batch 550, Loss: 1.1530981636047364\n",
      "Epoch 23, Batch 600, Loss: 1.1531001567840575\n",
      "Epoch 23, Batch 650, Loss: 1.1528124594688416\n",
      "Epoch 23, Batch 700, Loss: 1.1524355959892274\n",
      "Epoch 23, Batch 750, Loss: 1.1525969362258912\n",
      "Epoch 23, Batch 800, Loss: 1.152845380306244\n",
      "Epoch 23, Batch 850, Loss: 1.1519859862327575\n",
      "Epoch 23, Batch 900, Loss: 1.1531751990318297\n",
      "Epoch 24, Batch 50, Loss: 1.1530076718330384\n",
      "Epoch 24, Batch 100, Loss: 1.1525784063339233\n",
      "Epoch 24, Batch 150, Loss: 1.1534142208099365\n",
      "Epoch 24, Batch 200, Loss: 1.1525494837760926\n",
      "Epoch 24, Batch 250, Loss: 1.152692666053772\n",
      "Epoch 24, Batch 300, Loss: 1.1528541707992555\n",
      "Epoch 24, Batch 350, Loss: 1.1522903394699098\n",
      "Epoch 24, Batch 400, Loss: 1.1526930379867553\n",
      "Epoch 24, Batch 450, Loss: 1.1529578399658202\n",
      "Epoch 24, Batch 500, Loss: 1.1529327750205993\n",
      "Epoch 24, Batch 550, Loss: 1.152972629070282\n",
      "Epoch 24, Batch 600, Loss: 1.1532076072692872\n",
      "Epoch 24, Batch 650, Loss: 1.1525588011741639\n",
      "Epoch 24, Batch 700, Loss: 1.1527390146255494\n",
      "Epoch 24, Batch 750, Loss: 1.1530251216888427\n",
      "Epoch 24, Batch 800, Loss: 1.1534061813354493\n",
      "Epoch 24, Batch 850, Loss: 1.1522552633285523\n",
      "Epoch 24, Batch 900, Loss: 1.1532707762718202\n",
      "Epoch 25, Batch 50, Loss: 1.152113196849823\n",
      "Epoch 25, Batch 100, Loss: 1.1530302500724792\n",
      "Epoch 25, Batch 150, Loss: 1.1533026313781738\n",
      "Epoch 25, Batch 200, Loss: 1.152480411529541\n",
      "Epoch 25, Batch 250, Loss: 1.1529591798782348\n",
      "Epoch 25, Batch 300, Loss: 1.1533587074279785\n",
      "Epoch 25, Batch 350, Loss: 1.1533636474609374\n",
      "Epoch 25, Batch 400, Loss: 1.152541959285736\n",
      "Epoch 25, Batch 450, Loss: 1.1543321084976197\n",
      "Epoch 25, Batch 500, Loss: 1.1529155683517456\n",
      "Epoch 25, Batch 550, Loss: 1.1525714302062988\n",
      "Epoch 25, Batch 600, Loss: 1.1523332571983338\n",
      "Epoch 25, Batch 650, Loss: 1.1534450626373292\n",
      "Epoch 25, Batch 700, Loss: 1.1529148411750794\n",
      "Epoch 25, Batch 750, Loss: 1.152765462398529\n",
      "Epoch 25, Batch 800, Loss: 1.1525107669830321\n",
      "Epoch 25, Batch 850, Loss: 1.1528877758979796\n",
      "Epoch 25, Batch 900, Loss: 1.1533513069152832\n",
      "Accuracy on test set: 0.1%\n",
      "Fitting for combination 76\n",
      "784\n",
      "4\n",
      "10\n",
      "[40, 10, 10, 50, 10]\n",
      "False\n",
      "['relu', 'sigmoid', 'relu', 'sigmoid']\n",
      "Adam\n",
      "0.01\n",
      "0.03\n",
      "CrossEntropyLoss\n",
      "50\n",
      "25\n",
      "0\n",
      "Epoch 1, Batch 50, Loss: 1.1634638452529906\n",
      "Epoch 1, Batch 100, Loss: 1.159707772731781\n",
      "Epoch 1, Batch 150, Loss: 1.1606030106544494\n",
      "Epoch 1, Batch 200, Loss: 1.1659507584571838\n",
      "Epoch 1, Batch 250, Loss: 1.1612549352645873\n",
      "Epoch 1, Batch 300, Loss: 1.161465892791748\n",
      "Epoch 1, Batch 350, Loss: 1.1628224849700928\n",
      "Epoch 1, Batch 400, Loss: 1.1645343494415283\n",
      "Epoch 1, Batch 450, Loss: 1.1599853372573852\n",
      "Epoch 1, Batch 500, Loss: 1.1649810028076173\n",
      "Epoch 1, Batch 550, Loss: 1.1601793122291566\n",
      "Epoch 1, Batch 600, Loss: 1.1576630210876464\n",
      "Epoch 1, Batch 650, Loss: 1.1642289519309998\n",
      "Epoch 1, Batch 700, Loss: 1.1637752366065979\n",
      "Epoch 1, Batch 750, Loss: 1.1649366760253905\n",
      "Epoch 1, Batch 800, Loss: 1.161593770980835\n",
      "Epoch 1, Batch 850, Loss: 1.1613951873779298\n",
      "Epoch 1, Batch 900, Loss: 1.1609367203712464\n",
      "Epoch 2, Batch 50, Loss: 1.1623901700973511\n",
      "Epoch 2, Batch 100, Loss: 1.163512659072876\n",
      "Epoch 2, Batch 150, Loss: 1.1597194838523865\n",
      "Epoch 2, Batch 200, Loss: 1.1641481757164\n",
      "Epoch 2, Batch 250, Loss: 1.1616172981262207\n",
      "Epoch 2, Batch 300, Loss: 1.1655172491073609\n",
      "Epoch 2, Batch 350, Loss: 1.1596220088005067\n",
      "Epoch 2, Batch 400, Loss: 1.1632977843284606\n",
      "Epoch 2, Batch 450, Loss: 1.1583548974990845\n",
      "Epoch 2, Batch 500, Loss: 1.161391396522522\n",
      "Epoch 2, Batch 550, Loss: 1.1641593265533448\n",
      "Epoch 2, Batch 600, Loss: 1.161971800327301\n",
      "Epoch 2, Batch 650, Loss: 1.1607307553291322\n",
      "Epoch 2, Batch 700, Loss: 1.1650247383117676\n",
      "Epoch 2, Batch 750, Loss: 1.1623880076408386\n",
      "Epoch 2, Batch 800, Loss: 1.1597406315803527\n",
      "Epoch 2, Batch 850, Loss: 1.1607105016708374\n",
      "Epoch 2, Batch 900, Loss: 1.1595686841011048\n",
      "Epoch 3, Batch 50, Loss: 1.160034351348877\n",
      "Epoch 3, Batch 100, Loss: 1.1605873227119445\n",
      "Epoch 3, Batch 150, Loss: 1.1635584092140199\n",
      "Epoch 3, Batch 200, Loss: 1.1586190176010132\n",
      "Epoch 3, Batch 250, Loss: 1.1599802422523497\n",
      "Epoch 3, Batch 300, Loss: 1.1636546540260315\n",
      "Epoch 3, Batch 350, Loss: 1.1647038340568543\n",
      "Epoch 3, Batch 400, Loss: 1.1639651608467103\n",
      "Epoch 3, Batch 450, Loss: 1.1601714658737183\n",
      "Epoch 3, Batch 500, Loss: 1.1614291977882385\n",
      "Epoch 3, Batch 550, Loss: 1.1658938002586365\n",
      "Epoch 3, Batch 600, Loss: 1.160308940410614\n",
      "Epoch 3, Batch 650, Loss: 1.1606616878509521\n",
      "Epoch 3, Batch 700, Loss: 1.165045371055603\n",
      "Epoch 3, Batch 750, Loss: 1.1645582795143128\n",
      "Epoch 3, Batch 800, Loss: 1.1608893966674805\n",
      "Epoch 3, Batch 850, Loss: 1.1597150135040284\n",
      "Epoch 3, Batch 900, Loss: 1.1602206110954285\n",
      "Epoch 4, Batch 50, Loss: 1.1594338464736937\n",
      "Epoch 4, Batch 100, Loss: 1.1622501492500306\n",
      "Epoch 4, Batch 150, Loss: 1.1625473880767823\n",
      "Epoch 4, Batch 200, Loss: 1.162772500514984\n",
      "Epoch 4, Batch 250, Loss: 1.167919843196869\n",
      "Epoch 4, Batch 300, Loss: 1.1620980501174927\n",
      "Epoch 4, Batch 350, Loss: 1.1616892957687377\n",
      "Epoch 4, Batch 400, Loss: 1.1599043607711792\n",
      "Epoch 4, Batch 450, Loss: 1.1586002039909362\n",
      "Epoch 4, Batch 500, Loss: 1.1595308256149293\n",
      "Epoch 4, Batch 550, Loss: 1.1597755002975463\n",
      "Epoch 4, Batch 600, Loss: 1.162556493282318\n",
      "Epoch 4, Batch 650, Loss: 1.1567737126350404\n",
      "Epoch 4, Batch 700, Loss: 1.1603371739387511\n",
      "Epoch 4, Batch 750, Loss: 1.1628714680671692\n",
      "Epoch 4, Batch 800, Loss: 1.1586139059066773\n",
      "Epoch 4, Batch 850, Loss: 1.1608400535583496\n",
      "Epoch 4, Batch 900, Loss: 1.1653564429283143\n",
      "Epoch 5, Batch 50, Loss: 1.1683328795433043\n",
      "Epoch 5, Batch 100, Loss: 1.1621668887138368\n",
      "Epoch 5, Batch 150, Loss: 1.1620907664299012\n",
      "Epoch 5, Batch 200, Loss: 1.1618248677253724\n",
      "Epoch 5, Batch 250, Loss: 1.1591317796707152\n",
      "Epoch 5, Batch 300, Loss: 1.1627310132980346\n",
      "Epoch 5, Batch 350, Loss: 1.1603305196762086\n",
      "Epoch 5, Batch 400, Loss: 1.1618259501457215\n",
      "Epoch 5, Batch 450, Loss: 1.1623582339286804\n",
      "Epoch 5, Batch 500, Loss: 1.1655382013320923\n",
      "Epoch 5, Batch 550, Loss: 1.1602976822853088\n",
      "Epoch 5, Batch 600, Loss: 1.16475989818573\n",
      "Epoch 5, Batch 650, Loss: 1.1587476110458375\n",
      "Epoch 5, Batch 700, Loss: 1.1589173340797425\n",
      "Epoch 5, Batch 750, Loss: 1.163842420578003\n",
      "Epoch 5, Batch 800, Loss: 1.1618038749694823\n",
      "Epoch 5, Batch 850, Loss: 1.1611968159675599\n",
      "Epoch 5, Batch 900, Loss: 1.1649327540397645\n",
      "Epoch 6, Batch 50, Loss: 1.1604241585731507\n",
      "Epoch 6, Batch 100, Loss: 1.1606455397605897\n",
      "Epoch 6, Batch 150, Loss: 1.1582650566101074\n",
      "Epoch 6, Batch 200, Loss: 1.1580070066452026\n",
      "Epoch 6, Batch 250, Loss: 1.1624165725708009\n",
      "Epoch 6, Batch 300, Loss: 1.1612337613105774\n",
      "Epoch 6, Batch 350, Loss: 1.1620421123504638\n",
      "Epoch 6, Batch 400, Loss: 1.1653259634971618\n",
      "Epoch 6, Batch 450, Loss: 1.160406265258789\n",
      "Epoch 6, Batch 500, Loss: 1.1644241404533386\n",
      "Epoch 6, Batch 550, Loss: 1.1616838598251342\n",
      "Epoch 6, Batch 600, Loss: 1.1643438529968262\n",
      "Epoch 6, Batch 650, Loss: 1.1603943347930907\n",
      "Epoch 6, Batch 700, Loss: 1.1633673071861268\n",
      "Epoch 6, Batch 750, Loss: 1.164077513217926\n",
      "Epoch 6, Batch 800, Loss: 1.1601003623008728\n",
      "Epoch 6, Batch 850, Loss: 1.1613242197036744\n",
      "Epoch 6, Batch 900, Loss: 1.1615686917304993\n",
      "Epoch 7, Batch 50, Loss: 1.1674138593673706\n",
      "Epoch 7, Batch 100, Loss: 1.1632942008972167\n",
      "Epoch 7, Batch 150, Loss: 1.1584375\n",
      "Epoch 7, Batch 200, Loss: 1.1602451181411744\n",
      "Epoch 7, Batch 250, Loss: 1.1621418833732604\n",
      "Epoch 7, Batch 300, Loss: 1.1651427054405212\n",
      "Epoch 7, Batch 350, Loss: 1.1592990946769715\n",
      "Epoch 7, Batch 400, Loss: 1.1580198121070862\n",
      "Epoch 7, Batch 450, Loss: 1.1638250946998596\n",
      "Epoch 7, Batch 500, Loss: 1.1637049579620362\n",
      "Epoch 7, Batch 550, Loss: 1.1573544931411743\n",
      "Epoch 7, Batch 600, Loss: 1.1617399382591247\n",
      "Epoch 7, Batch 650, Loss: 1.1606776380538941\n",
      "Epoch 7, Batch 700, Loss: 1.1657610630989075\n",
      "Epoch 7, Batch 750, Loss: 1.1646846199035645\n",
      "Epoch 7, Batch 800, Loss: 1.160846040248871\n",
      "Epoch 7, Batch 850, Loss: 1.1588320851325988\n",
      "Epoch 7, Batch 900, Loss: 1.1612721276283264\n",
      "Epoch 8, Batch 50, Loss: 1.159445698261261\n",
      "Epoch 8, Batch 100, Loss: 1.163878996372223\n",
      "Epoch 8, Batch 150, Loss: 1.160264902114868\n",
      "Epoch 8, Batch 200, Loss: 1.161572597026825\n",
      "Epoch 8, Batch 250, Loss: 1.1625583338737489\n",
      "Epoch 8, Batch 300, Loss: 1.1616641068458557\n",
      "Epoch 8, Batch 350, Loss: 1.1615252852439881\n",
      "Epoch 8, Batch 400, Loss: 1.1614739418029785\n",
      "Epoch 8, Batch 450, Loss: 1.1592920851707458\n",
      "Epoch 8, Batch 500, Loss: 1.1605398845672608\n",
      "Epoch 8, Batch 550, Loss: 1.1584715485572814\n",
      "Epoch 8, Batch 600, Loss: 1.1619895434379577\n",
      "Epoch 8, Batch 650, Loss: 1.160701606273651\n",
      "Epoch 8, Batch 700, Loss: 1.1606336164474487\n",
      "Epoch 8, Batch 750, Loss: 1.1587271356582642\n",
      "Epoch 8, Batch 800, Loss: 1.1636016321182252\n",
      "Epoch 8, Batch 850, Loss: 1.1622422528266907\n",
      "Epoch 8, Batch 900, Loss: 1.1631644296646118\n",
      "Epoch 9, Batch 50, Loss: 1.1605724096298218\n",
      "Epoch 9, Batch 100, Loss: 1.161256549358368\n",
      "Epoch 9, Batch 150, Loss: 1.1604794621467591\n",
      "Epoch 9, Batch 200, Loss: 1.16266939163208\n",
      "Epoch 9, Batch 250, Loss: 1.162064895629883\n",
      "Epoch 9, Batch 300, Loss: 1.1585767793655395\n",
      "Epoch 9, Batch 350, Loss: 1.1606927371025086\n",
      "Epoch 9, Batch 400, Loss: 1.159246039390564\n",
      "Epoch 9, Batch 450, Loss: 1.1620428204536437\n",
      "Epoch 9, Batch 500, Loss: 1.1606554007530212\n",
      "Epoch 9, Batch 550, Loss: 1.1646366238594055\n",
      "Epoch 9, Batch 600, Loss: 1.163907105922699\n",
      "Epoch 9, Batch 650, Loss: 1.1590029907226562\n",
      "Epoch 9, Batch 700, Loss: 1.1634790325164794\n",
      "Epoch 9, Batch 750, Loss: 1.1631308841705321\n",
      "Epoch 9, Batch 800, Loss: 1.162041893005371\n",
      "Epoch 9, Batch 850, Loss: 1.1597979259490967\n",
      "Epoch 9, Batch 900, Loss: 1.1590004611015319\n",
      "Epoch 10, Batch 50, Loss: 1.1648907041549683\n",
      "Epoch 10, Batch 100, Loss: 1.162143120765686\n",
      "Epoch 10, Batch 150, Loss: 1.1620399403572081\n",
      "Epoch 10, Batch 200, Loss: 1.1618271684646606\n",
      "Epoch 10, Batch 250, Loss: 1.1608261585235595\n",
      "Epoch 10, Batch 300, Loss: 1.1588077354431152\n",
      "Epoch 10, Batch 350, Loss: 1.1620704913139344\n",
      "Epoch 10, Batch 400, Loss: 1.1594740891456603\n",
      "Epoch 10, Batch 450, Loss: 1.1628617286682128\n",
      "Epoch 10, Batch 500, Loss: 1.1606753277778625\n",
      "Epoch 10, Batch 550, Loss: 1.1600713324546814\n",
      "Epoch 10, Batch 600, Loss: 1.1600558805465697\n",
      "Epoch 10, Batch 650, Loss: 1.162183084487915\n",
      "Epoch 10, Batch 700, Loss: 1.1633427476882934\n",
      "Epoch 10, Batch 750, Loss: 1.1590467309951782\n",
      "Epoch 10, Batch 800, Loss: 1.161172754764557\n",
      "Epoch 10, Batch 850, Loss: 1.1662512445449829\n",
      "Epoch 10, Batch 900, Loss: 1.1633270573616028\n",
      "Epoch 11, Batch 50, Loss: 1.166109573841095\n",
      "Epoch 11, Batch 100, Loss: 1.1658251571655274\n",
      "Epoch 11, Batch 150, Loss: 1.1612569332122802\n",
      "Epoch 11, Batch 200, Loss: 1.1588174748420714\n",
      "Epoch 11, Batch 250, Loss: 1.1637089323997498\n",
      "Epoch 11, Batch 300, Loss: 1.1648464441299438\n",
      "Epoch 11, Batch 350, Loss: 1.1617364072799683\n",
      "Epoch 11, Batch 400, Loss: 1.163034737110138\n",
      "Epoch 11, Batch 450, Loss: 1.1628225231170655\n",
      "Epoch 11, Batch 500, Loss: 1.159162495136261\n",
      "Epoch 11, Batch 550, Loss: 1.1604184889793396\n",
      "Epoch 11, Batch 600, Loss: 1.1639060187339783\n",
      "Epoch 11, Batch 650, Loss: 1.162922682762146\n",
      "Epoch 11, Batch 700, Loss: 1.1613374710083009\n",
      "Epoch 11, Batch 750, Loss: 1.15845600605011\n",
      "Epoch 11, Batch 800, Loss: 1.1584333658218384\n",
      "Epoch 11, Batch 850, Loss: 1.1641994190216065\n",
      "Epoch 11, Batch 900, Loss: 1.1627202939987182\n",
      "Epoch 12, Batch 50, Loss: 1.1648248171806335\n",
      "Epoch 12, Batch 100, Loss: 1.1650363612174988\n",
      "Epoch 12, Batch 150, Loss: 1.1590922141075135\n",
      "Epoch 12, Batch 200, Loss: 1.1602507948875427\n",
      "Epoch 12, Batch 250, Loss: 1.1629646468162536\n",
      "Epoch 12, Batch 300, Loss: 1.161980803012848\n",
      "Epoch 12, Batch 350, Loss: 1.1655743885040284\n",
      "Epoch 12, Batch 400, Loss: 1.1634008145332337\n",
      "Epoch 12, Batch 450, Loss: 1.1623231101036071\n",
      "Epoch 12, Batch 500, Loss: 1.159480197429657\n",
      "Epoch 12, Batch 550, Loss: 1.1643427467346192\n",
      "Epoch 12, Batch 600, Loss: 1.1623805618286134\n",
      "Epoch 12, Batch 650, Loss: 1.1601590061187743\n",
      "Epoch 12, Batch 700, Loss: 1.1610696125030517\n",
      "Epoch 12, Batch 750, Loss: 1.1603993105888366\n",
      "Epoch 12, Batch 800, Loss: 1.1612529182434081\n",
      "Epoch 12, Batch 850, Loss: 1.1616984510421753\n",
      "Epoch 12, Batch 900, Loss: 1.1622501969337464\n",
      "Epoch 13, Batch 50, Loss: 1.162804126739502\n",
      "Epoch 13, Batch 100, Loss: 1.1621814489364624\n",
      "Epoch 13, Batch 150, Loss: 1.1597492241859435\n",
      "Epoch 13, Batch 200, Loss: 1.1636097502708436\n",
      "Epoch 13, Batch 250, Loss: 1.164139404296875\n",
      "Epoch 13, Batch 300, Loss: 1.1615263962745666\n",
      "Epoch 13, Batch 350, Loss: 1.1624708104133605\n",
      "Epoch 13, Batch 400, Loss: 1.1667613530158996\n",
      "Epoch 13, Batch 450, Loss: 1.1637845969200133\n",
      "Epoch 13, Batch 500, Loss: 1.1613032579421998\n",
      "Epoch 13, Batch 550, Loss: 1.1656589770317078\n",
      "Epoch 13, Batch 600, Loss: 1.168533387184143\n",
      "Epoch 13, Batch 650, Loss: 1.1618111538887024\n",
      "Epoch 13, Batch 700, Loss: 1.1618346691131591\n",
      "Epoch 13, Batch 750, Loss: 1.1590733885765077\n",
      "Epoch 13, Batch 800, Loss: 1.1589671897888183\n",
      "Epoch 13, Batch 850, Loss: 1.162927258014679\n",
      "Epoch 13, Batch 900, Loss: 1.1620933985710145\n",
      "Epoch 14, Batch 50, Loss: 1.1580035138130187\n",
      "Epoch 14, Batch 100, Loss: 1.1638043379783631\n",
      "Epoch 14, Batch 150, Loss: 1.161037459373474\n",
      "Epoch 14, Batch 200, Loss: 1.1582851815223694\n",
      "Epoch 14, Batch 250, Loss: 1.1626915216445923\n",
      "Epoch 14, Batch 300, Loss: 1.1623506879806518\n",
      "Epoch 14, Batch 350, Loss: 1.1626111221313478\n",
      "Epoch 14, Batch 400, Loss: 1.1620703005790711\n",
      "Epoch 14, Batch 450, Loss: 1.1603871607780456\n",
      "Epoch 14, Batch 500, Loss: 1.16113436460495\n",
      "Epoch 14, Batch 550, Loss: 1.1614481949806212\n",
      "Epoch 14, Batch 600, Loss: 1.1592457699775696\n",
      "Epoch 14, Batch 650, Loss: 1.1591440558433532\n",
      "Epoch 14, Batch 700, Loss: 1.1567468166351318\n",
      "Epoch 14, Batch 750, Loss: 1.1605927395820617\n",
      "Epoch 14, Batch 800, Loss: 1.162199945449829\n",
      "Epoch 14, Batch 850, Loss: 1.1623104190826417\n",
      "Epoch 14, Batch 900, Loss: 1.162365233898163\n",
      "Epoch 15, Batch 50, Loss: 1.1600170850753784\n",
      "Epoch 15, Batch 100, Loss: 1.1676482486724853\n",
      "Epoch 15, Batch 150, Loss: 1.1630538892745972\n",
      "Epoch 15, Batch 200, Loss: 1.16030277967453\n",
      "Epoch 15, Batch 250, Loss: 1.166364197731018\n",
      "Epoch 15, Batch 300, Loss: 1.1624852776527406\n",
      "Epoch 15, Batch 350, Loss: 1.1663450598716736\n",
      "Epoch 15, Batch 400, Loss: 1.167679467201233\n",
      "Epoch 15, Batch 450, Loss: 1.1603790569305419\n",
      "Epoch 15, Batch 500, Loss: 1.1608580327033997\n",
      "Epoch 15, Batch 550, Loss: 1.1657587432861327\n",
      "Epoch 15, Batch 600, Loss: 1.1611436676979066\n",
      "Epoch 15, Batch 650, Loss: 1.1584461760520934\n",
      "Epoch 15, Batch 700, Loss: 1.1606051516532898\n",
      "Epoch 15, Batch 750, Loss: 1.1592004203796387\n",
      "Epoch 15, Batch 800, Loss: 1.1592891192436219\n",
      "Epoch 15, Batch 850, Loss: 1.167229244709015\n",
      "Epoch 15, Batch 900, Loss: 1.1650688934326172\n",
      "Epoch 16, Batch 50, Loss: 1.160515511035919\n",
      "Epoch 16, Batch 100, Loss: 1.164368736743927\n",
      "Epoch 16, Batch 150, Loss: 1.1586402320861817\n",
      "Epoch 16, Batch 200, Loss: 1.1617236375808715\n",
      "Epoch 16, Batch 250, Loss: 1.160423903465271\n",
      "Epoch 16, Batch 300, Loss: 1.162743947505951\n",
      "Epoch 16, Batch 350, Loss: 1.1600315618515014\n",
      "Epoch 16, Batch 400, Loss: 1.1628972172737122\n",
      "Epoch 16, Batch 450, Loss: 1.1610100293159484\n",
      "Epoch 16, Batch 500, Loss: 1.1565930509567262\n",
      "Epoch 16, Batch 550, Loss: 1.159606521129608\n",
      "Epoch 16, Batch 600, Loss: 1.1608384490013122\n",
      "Epoch 16, Batch 650, Loss: 1.160828354358673\n",
      "Epoch 16, Batch 700, Loss: 1.1634119582176208\n",
      "Epoch 16, Batch 750, Loss: 1.1611729264259338\n",
      "Epoch 16, Batch 800, Loss: 1.160491168498993\n",
      "Epoch 16, Batch 850, Loss: 1.1632073783874513\n",
      "Epoch 16, Batch 900, Loss: 1.1618507599830628\n",
      "Epoch 17, Batch 50, Loss: 1.1608988285064696\n",
      "Epoch 17, Batch 100, Loss: 1.1630492949485778\n",
      "Epoch 17, Batch 150, Loss: 1.1607449913024903\n",
      "Epoch 17, Batch 200, Loss: 1.1703397822380066\n",
      "Epoch 17, Batch 250, Loss: 1.1614865922927857\n",
      "Epoch 17, Batch 300, Loss: 1.1640934443473816\n",
      "Epoch 17, Batch 350, Loss: 1.159778254032135\n",
      "Epoch 17, Batch 400, Loss: 1.1641820549964905\n",
      "Epoch 17, Batch 450, Loss: 1.162819917201996\n",
      "Epoch 17, Batch 500, Loss: 1.1618373107910156\n",
      "Epoch 17, Batch 550, Loss: 1.1595311617851258\n",
      "Epoch 17, Batch 600, Loss: 1.1596538233757019\n",
      "Epoch 17, Batch 650, Loss: 1.1617519664764404\n",
      "Epoch 17, Batch 700, Loss: 1.15882807970047\n",
      "Epoch 17, Batch 750, Loss: 1.1623833274841309\n",
      "Epoch 17, Batch 800, Loss: 1.162065646648407\n",
      "Epoch 17, Batch 850, Loss: 1.1648949193954468\n",
      "Epoch 17, Batch 900, Loss: 1.1679111170768737\n",
      "Epoch 18, Batch 50, Loss: 1.1628617787361144\n",
      "Epoch 18, Batch 100, Loss: 1.1606109952926635\n",
      "Epoch 18, Batch 150, Loss: 1.1617923045158387\n",
      "Epoch 18, Batch 200, Loss: 1.158883044719696\n",
      "Epoch 18, Batch 250, Loss: 1.1593556380271912\n",
      "Epoch 18, Batch 300, Loss: 1.1611047577857971\n",
      "Epoch 18, Batch 350, Loss: 1.1606769418716432\n",
      "Epoch 18, Batch 400, Loss: 1.1612204122543335\n",
      "Epoch 18, Batch 450, Loss: 1.1603440952301025\n",
      "Epoch 18, Batch 500, Loss: 1.1616991639137269\n",
      "Epoch 18, Batch 550, Loss: 1.1646619176864623\n",
      "Epoch 18, Batch 600, Loss: 1.1638211607933044\n",
      "Epoch 18, Batch 650, Loss: 1.1668652772903443\n",
      "Epoch 18, Batch 700, Loss: 1.1614436316490173\n",
      "Epoch 18, Batch 750, Loss: 1.1611906433105468\n",
      "Epoch 18, Batch 800, Loss: 1.1640442562103273\n",
      "Epoch 18, Batch 850, Loss: 1.1598782658576965\n",
      "Epoch 18, Batch 900, Loss: 1.1614346289634705\n",
      "Epoch 19, Batch 50, Loss: 1.1622512936592102\n",
      "Epoch 19, Batch 100, Loss: 1.163033516407013\n",
      "Epoch 19, Batch 150, Loss: 1.1612489581108094\n",
      "Epoch 19, Batch 200, Loss: 1.162594518661499\n",
      "Epoch 19, Batch 250, Loss: 1.1623283648490905\n",
      "Epoch 19, Batch 300, Loss: 1.1598700857162476\n",
      "Epoch 19, Batch 350, Loss: 1.1645153450965882\n",
      "Epoch 19, Batch 400, Loss: 1.159633708000183\n",
      "Epoch 19, Batch 450, Loss: 1.1602785992622375\n",
      "Epoch 19, Batch 500, Loss: 1.1601073789596557\n",
      "Epoch 19, Batch 550, Loss: 1.1607735061645508\n",
      "Epoch 19, Batch 600, Loss: 1.1619059658050537\n",
      "Epoch 19, Batch 650, Loss: 1.1611084055900573\n",
      "Epoch 19, Batch 700, Loss: 1.1624948167800904\n",
      "Epoch 19, Batch 750, Loss: 1.1629166030883789\n",
      "Epoch 19, Batch 800, Loss: 1.1588068342208862\n",
      "Epoch 19, Batch 850, Loss: 1.1596326160430908\n",
      "Epoch 19, Batch 900, Loss: 1.1606586980819702\n",
      "Epoch 20, Batch 50, Loss: 1.1622870182991027\n",
      "Epoch 20, Batch 100, Loss: 1.160971872806549\n",
      "Epoch 20, Batch 150, Loss: 1.1635938143730165\n",
      "Epoch 20, Batch 200, Loss: 1.1593957257270813\n",
      "Epoch 20, Batch 250, Loss: 1.1594817733764649\n",
      "Epoch 20, Batch 300, Loss: 1.1611273193359375\n",
      "Epoch 20, Batch 350, Loss: 1.1631467580795287\n",
      "Epoch 20, Batch 400, Loss: 1.166405930519104\n",
      "Epoch 20, Batch 450, Loss: 1.1618851828575134\n",
      "Epoch 20, Batch 500, Loss: 1.1609451460838318\n",
      "Epoch 20, Batch 550, Loss: 1.1635035252571106\n",
      "Epoch 20, Batch 600, Loss: 1.1643649053573608\n",
      "Epoch 20, Batch 650, Loss: 1.1622992110252381\n",
      "Epoch 20, Batch 700, Loss: 1.1607724475860595\n",
      "Epoch 20, Batch 750, Loss: 1.1574680757522584\n",
      "Epoch 20, Batch 800, Loss: 1.1635336351394654\n",
      "Epoch 20, Batch 850, Loss: 1.161465048789978\n",
      "Epoch 20, Batch 900, Loss: 1.1625256586074828\n",
      "Epoch 21, Batch 50, Loss: 1.1645187902450562\n",
      "Epoch 21, Batch 100, Loss: 1.1644136428833007\n",
      "Epoch 21, Batch 150, Loss: 1.1614001703262329\n",
      "Epoch 21, Batch 200, Loss: 1.1632196879386902\n",
      "Epoch 21, Batch 250, Loss: 1.1605957055091858\n",
      "Epoch 21, Batch 300, Loss: 1.1599115180969237\n",
      "Epoch 21, Batch 350, Loss: 1.1607767486572265\n",
      "Epoch 21, Batch 400, Loss: 1.1634689140319825\n",
      "Epoch 21, Batch 450, Loss: 1.161149661540985\n",
      "Epoch 21, Batch 500, Loss: 1.1637936520576477\n",
      "Epoch 21, Batch 550, Loss: 1.1597357702255249\n",
      "Epoch 21, Batch 600, Loss: 1.1601735281944274\n",
      "Epoch 21, Batch 650, Loss: 1.1611825847625732\n",
      "Epoch 21, Batch 700, Loss: 1.166996886730194\n",
      "Epoch 21, Batch 750, Loss: 1.1661885595321655\n",
      "Epoch 21, Batch 800, Loss: 1.165606942176819\n",
      "Epoch 21, Batch 850, Loss: 1.159263892173767\n",
      "Epoch 21, Batch 900, Loss: 1.1615577030181885\n",
      "Epoch 22, Batch 50, Loss: 1.161742765903473\n",
      "Epoch 22, Batch 100, Loss: 1.1621233534812927\n",
      "Epoch 22, Batch 150, Loss: 1.1613602757453918\n",
      "Epoch 22, Batch 200, Loss: 1.1591596388816834\n",
      "Epoch 22, Batch 250, Loss: 1.163585422039032\n",
      "Epoch 22, Batch 300, Loss: 1.164672236442566\n",
      "Epoch 22, Batch 350, Loss: 1.158232581615448\n",
      "Epoch 22, Batch 400, Loss: 1.1621406698226928\n",
      "Epoch 22, Batch 450, Loss: 1.1620398139953614\n",
      "Epoch 22, Batch 500, Loss: 1.1626697611808776\n",
      "Epoch 22, Batch 550, Loss: 1.161336934566498\n",
      "Epoch 22, Batch 600, Loss: 1.1586865258216859\n",
      "Epoch 22, Batch 650, Loss: 1.159865779876709\n",
      "Epoch 22, Batch 700, Loss: 1.1623361229896545\n",
      "Epoch 22, Batch 750, Loss: 1.1619536137580873\n",
      "Epoch 22, Batch 800, Loss: 1.158985824584961\n",
      "Epoch 22, Batch 850, Loss: 1.1611588978767395\n",
      "Epoch 22, Batch 900, Loss: 1.1601497197151185\n",
      "Epoch 23, Batch 50, Loss: 1.1592336201667786\n",
      "Epoch 23, Batch 100, Loss: 1.1607596039772035\n",
      "Epoch 23, Batch 150, Loss: 1.1625575065612792\n",
      "Epoch 23, Batch 200, Loss: 1.1611160278320312\n",
      "Epoch 23, Batch 250, Loss: 1.1664411926269531\n",
      "Epoch 23, Batch 300, Loss: 1.1626328873634337\n",
      "Epoch 23, Batch 350, Loss: 1.1661932182312011\n",
      "Epoch 23, Batch 400, Loss: 1.163916380405426\n",
      "Epoch 23, Batch 450, Loss: 1.1614407444000243\n",
      "Epoch 23, Batch 500, Loss: 1.1631433463096619\n",
      "Epoch 23, Batch 550, Loss: 1.1677618098258973\n",
      "Epoch 23, Batch 600, Loss: 1.1622136974334716\n",
      "Epoch 23, Batch 650, Loss: 1.1592615222930909\n",
      "Epoch 23, Batch 700, Loss: 1.1634251284599304\n",
      "Epoch 23, Batch 750, Loss: 1.1625207138061524\n",
      "Epoch 23, Batch 800, Loss: 1.1621742939949036\n",
      "Epoch 23, Batch 850, Loss: 1.1628829145431518\n",
      "Epoch 23, Batch 900, Loss: 1.1636287760734558\n",
      "Epoch 24, Batch 50, Loss: 1.1612998819351197\n",
      "Epoch 24, Batch 100, Loss: 1.1591820859909057\n",
      "Epoch 24, Batch 150, Loss: 1.1614114332199097\n",
      "Epoch 24, Batch 200, Loss: 1.1609073448181153\n",
      "Epoch 24, Batch 250, Loss: 1.1579012417793273\n",
      "Epoch 24, Batch 300, Loss: 1.1595327615737916\n",
      "Epoch 24, Batch 350, Loss: 1.161502285003662\n",
      "Epoch 24, Batch 400, Loss: 1.1614307951927185\n",
      "Epoch 24, Batch 450, Loss: 1.1615474796295167\n",
      "Epoch 24, Batch 500, Loss: 1.1641587591171265\n",
      "Epoch 24, Batch 550, Loss: 1.16760840177536\n",
      "Epoch 24, Batch 600, Loss: 1.1604541587829589\n",
      "Epoch 24, Batch 650, Loss: 1.1606988573074342\n",
      "Epoch 24, Batch 700, Loss: 1.1611878228187562\n",
      "Epoch 24, Batch 750, Loss: 1.1601901030540467\n",
      "Epoch 24, Batch 800, Loss: 1.1622174191474914\n",
      "Epoch 24, Batch 850, Loss: 1.1592454886436463\n",
      "Epoch 24, Batch 900, Loss: 1.1613319277763368\n",
      "Epoch 25, Batch 50, Loss: 1.1634124946594238\n",
      "Epoch 25, Batch 100, Loss: 1.1592839241027832\n",
      "Epoch 25, Batch 150, Loss: 1.1595609593391418\n",
      "Epoch 25, Batch 200, Loss: 1.1587008428573609\n",
      "Epoch 25, Batch 250, Loss: 1.1610776090621948\n",
      "Epoch 25, Batch 300, Loss: 1.159659013748169\n",
      "Epoch 25, Batch 350, Loss: 1.1633251452445983\n",
      "Epoch 25, Batch 400, Loss: 1.1621661114692687\n",
      "Epoch 25, Batch 450, Loss: 1.1621159172058106\n",
      "Epoch 25, Batch 500, Loss: 1.1635114645957947\n",
      "Epoch 25, Batch 550, Loss: 1.1609176039695739\n",
      "Epoch 25, Batch 600, Loss: 1.1630168914794923\n",
      "Epoch 25, Batch 650, Loss: 1.1615351343154907\n",
      "Epoch 25, Batch 700, Loss: 1.161029143333435\n",
      "Epoch 25, Batch 750, Loss: 1.1641874170303346\n",
      "Epoch 25, Batch 800, Loss: 1.1650947475433349\n",
      "Epoch 25, Batch 850, Loss: 1.160943214893341\n",
      "Epoch 25, Batch 900, Loss: 1.1617516684532165\n",
      "Accuracy on test set: 0.1%\n",
      "Fitting for combination 77\n",
      "784\n",
      "4\n",
      "10\n",
      "[40, 10, 10, 50, 10]\n",
      "True\n",
      "['relu', 'sigmoid', 'relu', 'tanh']\n",
      "SGD\n",
      "0.03\n",
      "0.3\n",
      "CrossEntropyLoss\n",
      "200\n",
      "25\n",
      "0\n",
      "Epoch 1, Batch 200, Loss: 4.609342014789581\n",
      "Epoch 1, Batch 400, Loss: 4.605483043193817\n",
      "Epoch 1, Batch 600, Loss: 4.605143222808838\n",
      "Epoch 1, Batch 800, Loss: 4.605117905139923\n",
      "Epoch 2, Batch 200, Loss: 4.605338051319122\n",
      "Epoch 2, Batch 400, Loss: 4.605223009586334\n",
      "Epoch 2, Batch 600, Loss: 4.605193357467652\n",
      "Epoch 2, Batch 800, Loss: 4.605293228626251\n",
      "Epoch 3, Batch 200, Loss: 4.605274922847748\n",
      "Epoch 3, Batch 400, Loss: 4.60533816576004\n",
      "Epoch 3, Batch 600, Loss: 4.605377473831177\n",
      "Epoch 3, Batch 800, Loss: 4.605146670341492\n",
      "Epoch 4, Batch 200, Loss: 4.605390057563782\n",
      "Epoch 4, Batch 400, Loss: 4.605324947834015\n",
      "Epoch 4, Batch 600, Loss: 4.605220456123352\n",
      "Epoch 4, Batch 800, Loss: 4.605229508876801\n",
      "Epoch 5, Batch 200, Loss: 4.60511402130127\n",
      "Epoch 5, Batch 400, Loss: 4.6052421188354495\n",
      "Epoch 5, Batch 600, Loss: 4.605420067310333\n",
      "Epoch 5, Batch 800, Loss: 4.6051377892494205\n",
      "Epoch 6, Batch 200, Loss: 4.605306947231293\n",
      "Epoch 6, Batch 400, Loss: 4.605296947956085\n",
      "Epoch 6, Batch 600, Loss: 4.605227077007294\n",
      "Epoch 6, Batch 800, Loss: 4.6052661752700805\n",
      "Epoch 7, Batch 200, Loss: 4.605132954120636\n",
      "Epoch 7, Batch 400, Loss: 4.60540011882782\n",
      "Epoch 7, Batch 600, Loss: 4.605384676456452\n",
      "Epoch 7, Batch 800, Loss: 4.605132677555084\n",
      "Epoch 8, Batch 200, Loss: 4.6053081178665165\n",
      "Epoch 8, Batch 400, Loss: 4.605324497222901\n",
      "Epoch 8, Batch 600, Loss: 4.605419487953186\n",
      "Epoch 8, Batch 800, Loss: 4.605099196434021\n",
      "Epoch 9, Batch 200, Loss: 4.605308182239533\n",
      "Epoch 9, Batch 400, Loss: 4.605301821231842\n",
      "Epoch 9, Batch 600, Loss: 4.605272443294525\n",
      "Epoch 9, Batch 800, Loss: 4.605337934494019\n",
      "Epoch 10, Batch 200, Loss: 4.605334417819977\n",
      "Epoch 10, Batch 400, Loss: 4.605278255939484\n",
      "Epoch 10, Batch 600, Loss: 4.605270042419433\n",
      "Epoch 10, Batch 800, Loss: 4.605396752357483\n",
      "Epoch 11, Batch 200, Loss: 4.605338745117187\n",
      "Epoch 11, Batch 400, Loss: 4.605209119319916\n",
      "Epoch 11, Batch 600, Loss: 4.60539705991745\n",
      "Epoch 11, Batch 800, Loss: 4.60528014421463\n",
      "Epoch 12, Batch 200, Loss: 4.605178689956665\n",
      "Epoch 12, Batch 400, Loss: 4.6052546882629395\n",
      "Epoch 12, Batch 600, Loss: 4.605294394493103\n",
      "Epoch 12, Batch 800, Loss: 4.605340478420257\n",
      "Epoch 13, Batch 200, Loss: 4.605391108989716\n",
      "Epoch 13, Batch 400, Loss: 4.6051942825317385\n",
      "Epoch 13, Batch 600, Loss: 4.605311660766602\n",
      "Epoch 13, Batch 800, Loss: 4.605480513572693\n",
      "Epoch 14, Batch 200, Loss: 4.605266833305359\n",
      "Epoch 14, Batch 400, Loss: 4.605316247940063\n",
      "Epoch 14, Batch 600, Loss: 4.605290937423706\n",
      "Epoch 14, Batch 800, Loss: 4.605383307933807\n",
      "Epoch 15, Batch 200, Loss: 4.605344653129578\n",
      "Epoch 15, Batch 400, Loss: 4.6053828573226925\n",
      "Epoch 15, Batch 600, Loss: 4.605283343791962\n",
      "Epoch 15, Batch 800, Loss: 4.605398628711701\n",
      "Epoch 16, Batch 200, Loss: 4.605219151973724\n",
      "Epoch 16, Batch 400, Loss: 4.60526752948761\n",
      "Epoch 16, Batch 600, Loss: 4.605358283519745\n",
      "Epoch 16, Batch 800, Loss: 4.605325169563294\n",
      "Epoch 17, Batch 200, Loss: 4.6052289700508116\n",
      "Epoch 17, Batch 400, Loss: 4.605361790657043\n",
      "Epoch 17, Batch 600, Loss: 4.605326571464539\n",
      "Epoch 17, Batch 800, Loss: 4.605238540172577\n",
      "Epoch 18, Batch 200, Loss: 4.605315809249878\n",
      "Epoch 18, Batch 400, Loss: 4.6053813409805295\n",
      "Epoch 18, Batch 600, Loss: 4.605372190475464\n",
      "Epoch 18, Batch 800, Loss: 4.605290515422821\n",
      "Epoch 19, Batch 200, Loss: 4.605334136486054\n",
      "Epoch 19, Batch 400, Loss: 4.6053054547309875\n",
      "Epoch 19, Batch 600, Loss: 4.605328724384308\n",
      "Epoch 19, Batch 800, Loss: 4.605263454914093\n",
      "Epoch 20, Batch 200, Loss: 4.605222625732422\n",
      "Epoch 20, Batch 400, Loss: 4.605374999046326\n",
      "Epoch 20, Batch 600, Loss: 4.605365808010101\n",
      "Epoch 20, Batch 800, Loss: 4.605340099334716\n",
      "Epoch 21, Batch 200, Loss: 4.605329382419586\n",
      "Epoch 21, Batch 400, Loss: 4.6053001403808596\n",
      "Epoch 21, Batch 600, Loss: 4.605372385978699\n",
      "Epoch 21, Batch 800, Loss: 4.605288286209106\n",
      "Epoch 22, Batch 200, Loss: 4.605224606990814\n",
      "Epoch 22, Batch 400, Loss: 4.605207667350769\n",
      "Epoch 22, Batch 600, Loss: 4.605312397480011\n",
      "Epoch 22, Batch 800, Loss: 4.605165338516235\n",
      "Epoch 23, Batch 200, Loss: 4.605373070240021\n",
      "Epoch 23, Batch 400, Loss: 4.605306105613709\n",
      "Epoch 23, Batch 600, Loss: 4.605307874679565\n",
      "Epoch 23, Batch 800, Loss: 4.605227530002594\n",
      "Epoch 24, Batch 200, Loss: 4.605398972034454\n",
      "Epoch 24, Batch 400, Loss: 4.605391945838928\n",
      "Epoch 24, Batch 600, Loss: 4.6052166366577145\n",
      "Epoch 24, Batch 800, Loss: 4.605313820838928\n",
      "Epoch 25, Batch 200, Loss: 4.60536108493805\n",
      "Epoch 25, Batch 400, Loss: 4.605275678634643\n",
      "Epoch 25, Batch 600, Loss: 4.605315442085266\n",
      "Epoch 25, Batch 800, Loss: 4.605316359996795\n",
      "Accuracy on test set: 0.1%\n",
      "Fitting for combination 78\n",
      "784\n",
      "4\n",
      "10\n",
      "[40, 10, 10, 50, 10]\n",
      "False\n",
      "['relu', 'sigmoid', 'relu', 'tanh']\n",
      "Adam\n",
      "0.1\n",
      "0.1\n",
      "CrossEntropyLoss\n",
      "100\n",
      "25\n",
      "0\n",
      "Epoch 1, Batch 100, Loss: 2.3063982224464414\n",
      "Epoch 1, Batch 200, Loss: 2.30635858297348\n",
      "Epoch 1, Batch 300, Loss: 2.306111533641815\n",
      "Epoch 1, Batch 400, Loss: 2.306807973384857\n",
      "Epoch 1, Batch 500, Loss: 2.305356140136719\n",
      "Epoch 1, Batch 600, Loss: 2.304995045661926\n",
      "Epoch 1, Batch 700, Loss: 2.307238960266113\n",
      "Epoch 1, Batch 800, Loss: 2.306649241447449\n",
      "Epoch 1, Batch 900, Loss: 2.3069414710998535\n",
      "Epoch 2, Batch 100, Loss: 2.307180860042572\n",
      "Epoch 2, Batch 200, Loss: 2.304822483062744\n",
      "Epoch 2, Batch 300, Loss: 2.308228402137756\n",
      "Epoch 2, Batch 400, Loss: 2.3063796162605286\n",
      "Epoch 2, Batch 500, Loss: 2.3075513434410095\n",
      "Epoch 2, Batch 600, Loss: 2.303783509731293\n",
      "Epoch 2, Batch 700, Loss: 2.307501871585846\n",
      "Epoch 2, Batch 800, Loss: 2.3064718079566955\n",
      "Epoch 2, Batch 900, Loss: 2.3055021595954894\n",
      "Epoch 3, Batch 100, Loss: 2.3056863713264466\n",
      "Epoch 3, Batch 200, Loss: 2.3066929960250855\n",
      "Epoch 3, Batch 300, Loss: 2.3069437956809997\n",
      "Epoch 3, Batch 400, Loss: 2.3071014714241027\n",
      "Epoch 3, Batch 500, Loss: 2.3071691393852234\n",
      "Epoch 3, Batch 600, Loss: 2.309042854309082\n",
      "Epoch 3, Batch 700, Loss: 2.30514625787735\n",
      "Epoch 3, Batch 800, Loss: 2.3060544443130495\n",
      "Epoch 3, Batch 900, Loss: 2.3093384766578673\n",
      "Epoch 4, Batch 100, Loss: 2.3077742648124695\n",
      "Epoch 4, Batch 200, Loss: 2.306481101512909\n",
      "Epoch 4, Batch 300, Loss: 2.3075983548164367\n",
      "Epoch 4, Batch 400, Loss: 2.3073336029052736\n",
      "Epoch 4, Batch 500, Loss: 2.304631819725037\n",
      "Epoch 4, Batch 600, Loss: 2.307126433849335\n",
      "Epoch 4, Batch 700, Loss: 2.306375093460083\n",
      "Epoch 4, Batch 800, Loss: 2.3058530068397523\n",
      "Epoch 4, Batch 900, Loss: 2.3060636472702027\n",
      "Epoch 5, Batch 100, Loss: 2.3078728461265565\n",
      "Epoch 5, Batch 200, Loss: 2.307420153617859\n",
      "Epoch 5, Batch 300, Loss: 2.308640582561493\n",
      "Epoch 5, Batch 400, Loss: 2.309245147705078\n",
      "Epoch 5, Batch 500, Loss: 2.304538702964783\n",
      "Epoch 5, Batch 600, Loss: 2.3074830889701845\n",
      "Epoch 5, Batch 700, Loss: 2.3070654344558714\n",
      "Epoch 5, Batch 800, Loss: 2.309282147884369\n",
      "Epoch 5, Batch 900, Loss: 2.3067987513542176\n",
      "Epoch 6, Batch 100, Loss: 2.305619978904724\n",
      "Epoch 6, Batch 200, Loss: 2.304889998435974\n",
      "Epoch 6, Batch 300, Loss: 2.3071730756759643\n",
      "Epoch 6, Batch 400, Loss: 2.306753361225128\n",
      "Epoch 6, Batch 500, Loss: 2.3064168763160704\n",
      "Epoch 6, Batch 600, Loss: 2.3088821744918824\n",
      "Epoch 6, Batch 700, Loss: 2.3069313764572144\n",
      "Epoch 6, Batch 800, Loss: 2.3052601647377013\n",
      "Epoch 6, Batch 900, Loss: 2.3070699048042296\n",
      "Epoch 7, Batch 100, Loss: 2.3078594756126405\n",
      "Epoch 7, Batch 200, Loss: 2.307192726135254\n",
      "Epoch 7, Batch 300, Loss: 2.3069721364974978\n",
      "Epoch 7, Batch 400, Loss: 2.307655656337738\n",
      "Epoch 7, Batch 500, Loss: 2.3068814063072205\n",
      "Epoch 7, Batch 600, Loss: 2.3074882340431215\n",
      "Epoch 7, Batch 700, Loss: 2.3073512053489686\n",
      "Epoch 7, Batch 800, Loss: 2.3067703294754027\n",
      "Epoch 7, Batch 900, Loss: 2.308821053504944\n",
      "Epoch 8, Batch 100, Loss: 2.3068823122978213\n",
      "Epoch 8, Batch 200, Loss: 2.3081158804893493\n",
      "Epoch 8, Batch 300, Loss: 2.3068827033042907\n",
      "Epoch 8, Batch 400, Loss: 2.30544780254364\n",
      "Epoch 8, Batch 500, Loss: 2.3064529490470886\n",
      "Epoch 8, Batch 600, Loss: 2.3087287044525144\n",
      "Epoch 8, Batch 700, Loss: 2.3079089903831482\n",
      "Epoch 8, Batch 800, Loss: 2.3057771825790407\n",
      "Epoch 8, Batch 900, Loss: 2.3051926445961\n",
      "Epoch 9, Batch 100, Loss: 2.3060946655273438\n",
      "Epoch 9, Batch 200, Loss: 2.306739642620087\n",
      "Epoch 9, Batch 300, Loss: 2.3063443684577942\n",
      "Epoch 9, Batch 400, Loss: 2.308550922870636\n",
      "Epoch 9, Batch 500, Loss: 2.3074068999290467\n",
      "Epoch 9, Batch 600, Loss: 2.307736780643463\n",
      "Epoch 9, Batch 700, Loss: 2.3058827900886536\n",
      "Epoch 9, Batch 800, Loss: 2.307165307998657\n",
      "Epoch 9, Batch 900, Loss: 2.3088553404808043\n",
      "Epoch 10, Batch 100, Loss: 2.3098706912994387\n",
      "Epoch 10, Batch 200, Loss: 2.306644072532654\n",
      "Epoch 10, Batch 300, Loss: 2.306256034374237\n",
      "Epoch 10, Batch 400, Loss: 2.3071232652664184\n",
      "Epoch 10, Batch 500, Loss: 2.3063440537452697\n",
      "Epoch 10, Batch 600, Loss: 2.3063006138801576\n",
      "Epoch 10, Batch 700, Loss: 2.306399166584015\n",
      "Epoch 10, Batch 800, Loss: 2.3063605499267577\n",
      "Epoch 10, Batch 900, Loss: 2.3062700486183165\n",
      "Epoch 11, Batch 100, Loss: 2.30737019777298\n",
      "Epoch 11, Batch 200, Loss: 2.3067833518981935\n",
      "Epoch 11, Batch 300, Loss: 2.3079127264022827\n",
      "Epoch 11, Batch 400, Loss: 2.307745859622955\n",
      "Epoch 11, Batch 500, Loss: 2.3079778742790222\n",
      "Epoch 11, Batch 600, Loss: 2.307758319377899\n",
      "Epoch 11, Batch 700, Loss: 2.308056139945984\n",
      "Epoch 11, Batch 800, Loss: 2.3060753917694092\n",
      "Epoch 11, Batch 900, Loss: 2.3083429527282715\n",
      "Epoch 12, Batch 100, Loss: 2.309038541316986\n",
      "Epoch 12, Batch 200, Loss: 2.3068794298171995\n",
      "Epoch 12, Batch 300, Loss: 2.3070672249794004\n",
      "Epoch 12, Batch 400, Loss: 2.307993335723877\n",
      "Epoch 12, Batch 500, Loss: 2.3066284608840943\n",
      "Epoch 12, Batch 600, Loss: 2.305078959465027\n",
      "Epoch 12, Batch 700, Loss: 2.3075403070449827\n",
      "Epoch 12, Batch 800, Loss: 2.307180120944977\n",
      "Epoch 12, Batch 900, Loss: 2.307018322944641\n",
      "Epoch 13, Batch 100, Loss: 2.3093238854408265\n",
      "Epoch 13, Batch 200, Loss: 2.305421154499054\n",
      "Epoch 13, Batch 300, Loss: 2.3070670533180238\n",
      "Epoch 13, Batch 400, Loss: 2.3062420868873597\n",
      "Epoch 13, Batch 500, Loss: 2.3065088772773743\n",
      "Epoch 13, Batch 600, Loss: 2.307682044506073\n",
      "Epoch 13, Batch 700, Loss: 2.3061600017547605\n",
      "Epoch 13, Batch 800, Loss: 2.3057875180244447\n",
      "Epoch 13, Batch 900, Loss: 2.3081636905670164\n",
      "Epoch 14, Batch 100, Loss: 2.310783507823944\n",
      "Epoch 14, Batch 200, Loss: 2.306483066082001\n",
      "Epoch 14, Batch 300, Loss: 2.306267144680023\n",
      "Epoch 14, Batch 400, Loss: 2.305611779689789\n",
      "Epoch 14, Batch 500, Loss: 2.307263548374176\n",
      "Epoch 14, Batch 600, Loss: 2.3058547735214234\n",
      "Epoch 14, Batch 700, Loss: 2.307119998931885\n",
      "Epoch 14, Batch 800, Loss: 2.307752265930176\n",
      "Epoch 14, Batch 900, Loss: 2.3066318583488465\n",
      "Epoch 15, Batch 100, Loss: 2.305206458568573\n",
      "Epoch 15, Batch 200, Loss: 2.3054784107208253\n",
      "Epoch 15, Batch 300, Loss: 2.307402787208557\n",
      "Epoch 15, Batch 400, Loss: 2.3051979660987856\n",
      "Epoch 15, Batch 500, Loss: 2.3051857709884644\n",
      "Epoch 15, Batch 600, Loss: 2.308130695819855\n",
      "Epoch 15, Batch 700, Loss: 2.3047883796691893\n",
      "Epoch 15, Batch 800, Loss: 2.3070386052131653\n",
      "Epoch 15, Batch 900, Loss: 2.305009915828705\n",
      "Epoch 16, Batch 100, Loss: 2.3086425113677977\n",
      "Epoch 16, Batch 200, Loss: 2.306858229637146\n",
      "Epoch 16, Batch 300, Loss: 2.3070611548423767\n",
      "Epoch 16, Batch 400, Loss: 2.3080914187431336\n",
      "Epoch 16, Batch 500, Loss: 2.3072343850135804\n",
      "Epoch 16, Batch 600, Loss: 2.306612644195557\n",
      "Epoch 16, Batch 700, Loss: 2.3065844917297365\n",
      "Epoch 16, Batch 800, Loss: 2.307435443401337\n",
      "Epoch 16, Batch 900, Loss: 2.306456642150879\n",
      "Epoch 17, Batch 100, Loss: 2.3069937324523924\n",
      "Epoch 17, Batch 200, Loss: 2.306455705165863\n",
      "Epoch 17, Batch 300, Loss: 2.30576416015625\n",
      "Epoch 17, Batch 400, Loss: 2.306432192325592\n",
      "Epoch 17, Batch 500, Loss: 2.307438843250275\n",
      "Epoch 17, Batch 600, Loss: 2.307531802654266\n",
      "Epoch 17, Batch 700, Loss: 2.307654523849487\n",
      "Epoch 17, Batch 800, Loss: 2.309221930503845\n",
      "Epoch 17, Batch 900, Loss: 2.31021210193634\n",
      "Epoch 18, Batch 100, Loss: 2.3060035419464113\n",
      "Epoch 18, Batch 200, Loss: 2.306687841415405\n",
      "Epoch 18, Batch 300, Loss: 2.308764998912811\n",
      "Epoch 18, Batch 400, Loss: 2.307863619327545\n",
      "Epoch 18, Batch 500, Loss: 2.306979956626892\n",
      "Epoch 18, Batch 600, Loss: 2.308767728805542\n",
      "Epoch 18, Batch 700, Loss: 2.306101689338684\n",
      "Epoch 18, Batch 800, Loss: 2.307356429100037\n",
      "Epoch 18, Batch 900, Loss: 2.306650321483612\n",
      "Epoch 19, Batch 100, Loss: 2.3054837155342103\n",
      "Epoch 19, Batch 200, Loss: 2.308164324760437\n",
      "Epoch 19, Batch 300, Loss: 2.3059481239318846\n",
      "Epoch 19, Batch 400, Loss: 2.308202631473541\n",
      "Epoch 19, Batch 500, Loss: 2.3079878950119017\n",
      "Epoch 19, Batch 600, Loss: 2.3052296471595763\n",
      "Epoch 19, Batch 700, Loss: 2.3071551537513733\n",
      "Epoch 19, Batch 800, Loss: 2.306686804294586\n",
      "Epoch 19, Batch 900, Loss: 2.3074193239212035\n",
      "Epoch 20, Batch 100, Loss: 2.3057338571548462\n",
      "Epoch 20, Batch 200, Loss: 2.3069785022735596\n",
      "Epoch 20, Batch 300, Loss: 2.3061825442314148\n",
      "Epoch 20, Batch 400, Loss: 2.307667429447174\n",
      "Epoch 20, Batch 500, Loss: 2.30717084646225\n",
      "Epoch 20, Batch 600, Loss: 2.3053139519691466\n",
      "Epoch 20, Batch 700, Loss: 2.306438307762146\n",
      "Epoch 20, Batch 800, Loss: 2.3068008637428283\n",
      "Epoch 20, Batch 900, Loss: 2.307934446334839\n",
      "Epoch 21, Batch 100, Loss: 2.306230764389038\n",
      "Epoch 21, Batch 200, Loss: 2.3067436480522154\n",
      "Epoch 21, Batch 300, Loss: 2.306391830444336\n",
      "Epoch 21, Batch 400, Loss: 2.3071889686584472\n",
      "Epoch 21, Batch 500, Loss: 2.307071225643158\n",
      "Epoch 21, Batch 600, Loss: 2.307344524860382\n",
      "Epoch 21, Batch 700, Loss: 2.308270959854126\n",
      "Epoch 21, Batch 800, Loss: 2.307955687046051\n",
      "Epoch 21, Batch 900, Loss: 2.306332042217255\n",
      "Epoch 22, Batch 100, Loss: 2.3075567436218263\n",
      "Epoch 22, Batch 200, Loss: 2.3059949398040773\n",
      "Epoch 22, Batch 300, Loss: 2.3069485092163085\n",
      "Epoch 22, Batch 400, Loss: 2.3066428780555723\n",
      "Epoch 22, Batch 500, Loss: 2.3094501280784607\n",
      "Epoch 22, Batch 600, Loss: 2.306645226478577\n",
      "Epoch 22, Batch 700, Loss: 2.3071610641479494\n",
      "Epoch 22, Batch 800, Loss: 2.307004199028015\n",
      "Epoch 22, Batch 900, Loss: 2.3065414237976074\n",
      "Epoch 23, Batch 100, Loss: 2.306149733066559\n",
      "Epoch 23, Batch 200, Loss: 2.306986827850342\n",
      "Epoch 23, Batch 300, Loss: 2.305713901519775\n",
      "Epoch 23, Batch 400, Loss: 2.307865059375763\n",
      "Epoch 23, Batch 500, Loss: 2.3079099225997926\n",
      "Epoch 23, Batch 600, Loss: 2.3073359727859497\n",
      "Epoch 23, Batch 700, Loss: 2.3066258668899535\n",
      "Epoch 23, Batch 800, Loss: 2.307515821456909\n",
      "Epoch 23, Batch 900, Loss: 2.3066156339645385\n",
      "Epoch 24, Batch 100, Loss: 2.3082671856880186\n",
      "Epoch 24, Batch 200, Loss: 2.30755108833313\n",
      "Epoch 24, Batch 300, Loss: 2.306461853981018\n",
      "Epoch 24, Batch 400, Loss: 2.3075797390937804\n",
      "Epoch 24, Batch 500, Loss: 2.3069501376152037\n",
      "Epoch 24, Batch 600, Loss: 2.306919000148773\n",
      "Epoch 24, Batch 700, Loss: 2.307886345386505\n",
      "Epoch 24, Batch 800, Loss: 2.3056825733184816\n",
      "Epoch 24, Batch 900, Loss: 2.3068515467643738\n",
      "Epoch 25, Batch 100, Loss: 2.306916341781616\n",
      "Epoch 25, Batch 200, Loss: 2.305170166492462\n",
      "Epoch 25, Batch 300, Loss: 2.306027157306671\n",
      "Epoch 25, Batch 400, Loss: 2.306129274368286\n",
      "Epoch 25, Batch 500, Loss: 2.306198103427887\n",
      "Epoch 25, Batch 600, Loss: 2.305928044319153\n",
      "Epoch 25, Batch 700, Loss: 2.307931668758392\n",
      "Epoch 25, Batch 800, Loss: 2.30695360660553\n",
      "Epoch 25, Batch 900, Loss: 2.3080590319633485\n",
      "Accuracy on test set: 0.1%\n",
      "Fitting for combination 79\n",
      "784\n",
      "4\n",
      "10\n",
      "[40, 10, 10, 50, 10]\n",
      "False\n",
      "['relu', 'sigmoid', 'relu', 'tanh']\n",
      "SGD\n",
      "0.3\n",
      "0.1\n",
      "CrossEntropyLoss\n",
      "200\n",
      "25\n",
      "0\n",
      "Epoch 1, Batch 200, Loss: 4.607263045310974\n",
      "Epoch 1, Batch 400, Loss: 4.606109509468078\n",
      "Epoch 1, Batch 600, Loss: 4.606454267501831\n",
      "Epoch 1, Batch 800, Loss: 4.60714852809906\n",
      "Epoch 2, Batch 200, Loss: 4.6059320306777956\n",
      "Epoch 2, Batch 400, Loss: 4.606202640533447\n",
      "Epoch 2, Batch 600, Loss: 4.606537358760834\n",
      "Epoch 2, Batch 800, Loss: 4.6060406112670895\n",
      "Epoch 3, Batch 200, Loss: 4.606005547046661\n",
      "Epoch 3, Batch 400, Loss: 4.606224131584168\n",
      "Epoch 3, Batch 600, Loss: 4.606225147247314\n",
      "Epoch 3, Batch 800, Loss: 4.606528263092041\n",
      "Epoch 4, Batch 200, Loss: 4.606404132843018\n",
      "Epoch 4, Batch 400, Loss: 4.606247956752777\n",
      "Epoch 4, Batch 600, Loss: 4.60670827627182\n",
      "Epoch 4, Batch 800, Loss: 4.605357162952423\n",
      "Epoch 5, Batch 200, Loss: 4.606000659465789\n",
      "Epoch 5, Batch 400, Loss: 4.6069200897216795\n",
      "Epoch 5, Batch 600, Loss: 4.606677184104919\n",
      "Epoch 5, Batch 800, Loss: 4.607004547119141\n",
      "Epoch 6, Batch 200, Loss: 4.606695091724395\n",
      "Epoch 6, Batch 400, Loss: 4.6069981741905215\n",
      "Epoch 6, Batch 600, Loss: 4.606497805118561\n",
      "Epoch 6, Batch 800, Loss: 4.605959393978119\n",
      "Epoch 7, Batch 200, Loss: 4.6065436196327205\n",
      "Epoch 7, Batch 400, Loss: 4.606789433956147\n",
      "Epoch 7, Batch 600, Loss: 4.6064484548568725\n",
      "Epoch 7, Batch 800, Loss: 4.606303238868714\n",
      "Epoch 8, Batch 200, Loss: 4.606143872737885\n",
      "Epoch 8, Batch 400, Loss: 4.606792407035828\n",
      "Epoch 8, Batch 600, Loss: 4.606678910255432\n",
      "Epoch 8, Batch 800, Loss: 4.6067344450950625\n",
      "Epoch 9, Batch 200, Loss: 4.6058709955215456\n",
      "Epoch 9, Batch 400, Loss: 4.606932141780853\n",
      "Epoch 9, Batch 600, Loss: 4.6066535115242\n",
      "Epoch 9, Batch 800, Loss: 4.606775929927826\n",
      "Epoch 10, Batch 200, Loss: 4.6063270139694215\n",
      "Epoch 10, Batch 400, Loss: 4.605061130523682\n",
      "Epoch 10, Batch 600, Loss: 4.60549729347229\n",
      "Epoch 10, Batch 800, Loss: 4.606445505619049\n",
      "Epoch 11, Batch 200, Loss: 4.606357958316803\n",
      "Epoch 11, Batch 400, Loss: 4.606096246242523\n",
      "Epoch 11, Batch 600, Loss: 4.60624989271164\n",
      "Epoch 11, Batch 800, Loss: 4.60627393245697\n",
      "Epoch 12, Batch 200, Loss: 4.6063641571998595\n",
      "Epoch 12, Batch 400, Loss: 4.606555387973786\n",
      "Epoch 12, Batch 600, Loss: 4.606403753757477\n",
      "Epoch 12, Batch 800, Loss: 4.606729238033295\n",
      "Epoch 13, Batch 200, Loss: 4.606679923534394\n",
      "Epoch 13, Batch 400, Loss: 4.605943522453308\n",
      "Epoch 13, Batch 600, Loss: 4.606640043258667\n",
      "Epoch 13, Batch 800, Loss: 4.605842795372009\n",
      "Epoch 14, Batch 200, Loss: 4.606491520404815\n",
      "Epoch 14, Batch 400, Loss: 4.606727030277252\n",
      "Epoch 14, Batch 600, Loss: 4.606590716838837\n",
      "Epoch 14, Batch 800, Loss: 4.6066504573822025\n",
      "Epoch 15, Batch 200, Loss: 4.6071219992637635\n",
      "Epoch 15, Batch 400, Loss: 4.605970430374145\n",
      "Epoch 15, Batch 600, Loss: 4.606418316364288\n",
      "Epoch 15, Batch 800, Loss: 4.606216013431549\n",
      "Epoch 16, Batch 200, Loss: 4.605993378162384\n",
      "Epoch 16, Batch 400, Loss: 4.606214501857758\n",
      "Epoch 16, Batch 600, Loss: 4.60639678478241\n",
      "Epoch 16, Batch 800, Loss: 4.60589781999588\n",
      "Epoch 17, Batch 200, Loss: 4.606754257678985\n",
      "Epoch 17, Batch 400, Loss: 4.606130831241607\n",
      "Epoch 17, Batch 600, Loss: 4.606384470462799\n",
      "Epoch 17, Batch 800, Loss: 4.606274104118347\n",
      "Epoch 18, Batch 200, Loss: 4.606765608787537\n",
      "Epoch 18, Batch 400, Loss: 4.6066498184204105\n",
      "Epoch 18, Batch 600, Loss: 4.606664254665374\n",
      "Epoch 18, Batch 800, Loss: 4.606002907752991\n",
      "Epoch 19, Batch 200, Loss: 4.606366515159607\n",
      "Epoch 19, Batch 400, Loss: 4.605620064735413\n",
      "Epoch 19, Batch 600, Loss: 4.606487898826599\n",
      "Epoch 19, Batch 800, Loss: 4.606226072311402\n",
      "Epoch 20, Batch 200, Loss: 4.605866942405701\n",
      "Epoch 20, Batch 400, Loss: 4.606393623352051\n",
      "Epoch 20, Batch 600, Loss: 4.605506062507629\n",
      "Epoch 20, Batch 800, Loss: 4.605613193511963\n",
      "Epoch 21, Batch 200, Loss: 4.606830008029938\n",
      "Epoch 21, Batch 400, Loss: 4.6060847306251524\n",
      "Epoch 21, Batch 600, Loss: 4.606535432338714\n",
      "Epoch 21, Batch 800, Loss: 4.60654777765274\n",
      "Epoch 22, Batch 200, Loss: 4.606249351501464\n",
      "Epoch 22, Batch 400, Loss: 4.606455590724945\n",
      "Epoch 22, Batch 600, Loss: 4.606261470317841\n",
      "Epoch 22, Batch 800, Loss: 4.605725655555725\n",
      "Epoch 23, Batch 200, Loss: 4.606510896682739\n",
      "Epoch 23, Batch 400, Loss: 4.606600496768952\n",
      "Epoch 23, Batch 600, Loss: 4.6064864540100094\n",
      "Epoch 23, Batch 800, Loss: 4.605304636955261\n",
      "Epoch 24, Batch 200, Loss: 4.60648553609848\n",
      "Epoch 24, Batch 400, Loss: 4.606884400844574\n",
      "Epoch 24, Batch 600, Loss: 4.60642736196518\n",
      "Epoch 24, Batch 800, Loss: 4.606584153175354\n",
      "Epoch 25, Batch 200, Loss: 4.606364235877991\n",
      "Epoch 25, Batch 400, Loss: 4.606780266761779\n",
      "Epoch 25, Batch 600, Loss: 4.606506991386413\n",
      "Epoch 25, Batch 800, Loss: 4.606790478229523\n",
      "Accuracy on test set: 0.1%\n",
      "Best parameters: {'inputs': 784, 'number_of_layers': 1, 'outputs': 10, 'neurons_per_layer': [40, 10, 10, 10], 'dropout_layers': True, 'activation_functions': ['relu', 'sigmoid', 'relu'], 'optimizers': 'SGD', 'learning_rates': 0.03, 'weight_decays': 0, 'loss_functions': 'CrossEntropyLoss', 'batches': 100, 'epochs': 25, 'score': 0.8762}\n"
     ]
    }
   ],
   "source": [
    "# Much more reasonable Randomized Greedy grid search that only brute forces the number of layers, neurons per layer, and optimization functions,\n",
    "# which has been modified from the above to do ~80 combinations, as each number of neurons per layer is tried twice. The weight decay and learning\n",
    "# rate are also more restrictive\n",
    "for number_of_layers in parameters['number_of_layers']:\n",
    "    for neurons_per_layer in parameters['neurons_per_layer']:\n",
    "        for optimizer in parameters['optimizers']:\n",
    "\n",
    "            #{'neurons': [30], 'activation': ['relu']}\n",
    "            #number_of_layers = 3\n",
    "\n",
    "            dropout_layer = random.choice(parameters['dropout_layers'])\n",
    "            activation_function = random.choice(parameters['activation_functions'])\n",
    "            learning_rate = random.choice(parameters['learning_rates'])\n",
    "            weight_decay = random.choice(parameters['weight_decays'])\n",
    "            loss_function = random.choice(parameters['loss_functions']) # I'm aware that there is only a single loss function, I'm just treating it the same as everything else\n",
    "            batch = random.choice(parameters['batches'])\n",
    "            epochs = random.choice(parameters['epochs'])\n",
    "            \n",
    "                                     \n",
    "            # Create the network architecture                       \n",
    "            neurons = [] # Neurons array to store the neurons_per_layer\n",
    "            activations = [] # Activations array to store the activations per layer\n",
    "            if len(local_layers['neurons']) < number_of_layers: # If there aren't yet enough \"best performing\" neurons for the layer it's on, toss something in it\n",
    "                local_layers['neurons'].append(neurons_per_layer)\n",
    "                local_layers['activations'].append(activation_function)\n",
    "            for layer in range(0,number_of_layers-1): # Add in the \"best\" results from the previous layers, if any\n",
    "                neurons.append(local_layers['neurons'][layer])\n",
    "                activations.append(local_layers['activations'][layer])\n",
    "            neurons.append(neurons_per_layer) # Add the current number of neurons being tested for this layer\n",
    "            activations.append(activation_function) # Add the current (random) activation function being tested for this layer\n",
    "            neurons.append(parameters['outputs']) # For the final output layer, meaning we always have number_of_layers + 1 total layers\n",
    "            # PLEASE NOTE: the optimizer handles the softmax function on the output, so no softmax activation is added, meaning\n",
    "            # the activations array will always be one shorter than the neurons array\n",
    "    \n",
    "            print(f\"Fitting for combination {combination}\")\n",
    "\n",
    "            # Save the current hyperparameters into results\n",
    "            results['number_of_layers'].append(number_of_layers)\n",
    "            results['neurons_per_layer'].append(neurons)\n",
    "            results['dropout_layers'].append(dropout_layer)\n",
    "            results['activation_functions'].append(activations)\n",
    "            results['optimizers'].append(optimizer)\n",
    "            results['learning_rates'].append(learning_rate)\n",
    "            results['weight_decays'].append(weight_decay)\n",
    "            results['loss_functions'].append(loss_function)\n",
    "            results['batches'].append(batch)\n",
    "            results['epochs'].append(epochs)\n",
    "            results['inputs'].append(parameters['inputs'])\n",
    "            results['outputs'].append(parameters['outputs'])\n",
    "            results['score'].append(0)\n",
    "\n",
    "            for parameter in results:\n",
    "                print(results[parameter][combination])\n",
    "            \n",
    "            model = MLP((parameters['inputs']), neurons, dropout_layer, activations)\n",
    "            model.to(device)\n",
    "            model.fit(train_loader, optimizer, loss_function, learning_rate, weight_decay, batch, epochs)\n",
    "            \n",
    "            # Evaluate the model on the validation data\n",
    "            score = model.score(test_loader)\n",
    "            results['score'][combination] = score\n",
    "            \n",
    "            # If the current score is better than the best score, update the best score, best parameters, and save the architecture into local_layers\n",
    "            # This is what makes the algorithm \"greedy\" instead of brute force\n",
    "            if score > best_parameters['score']:\n",
    "                for parameter in results:\n",
    "                    best_parameters[parameter] = results[parameter][combination]\n",
    "                local_layers['neurons'] = neurons\n",
    "                local_layers['activations'] = activations\n",
    "            elif score < worst_parameters['score']:\n",
    "                for parameter in results:\n",
    "                    worst_parameters[parameter] = results[parameter][combination]\n",
    "            \n",
    "            combination += 1\n",
    "\n",
    "print('Best parameters:', best_parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'inputs': 784, 'number_of_layers': 1, 'outputs': 10, 'neurons_per_layer': [40, 10, 10, 10], 'dropout_layers': True, 'activation_functions': ['relu', 'sigmoid', 'relu'], 'optimizers': 'SGD', 'learning_rates': 0.03, 'weight_decays': 0, 'loss_functions': 'CrossEntropyLoss', 'batches': 100, 'epochs': 25, 'score': 0.8762}\n",
      "{'inputs': 784, 'number_of_layers': 2, 'outputs': 10, 'neurons_per_layer': [40, 30, 10], 'dropout_layers': True, 'activation_functions': ['relu', 'tanh'], 'optimizers': 'Adam', 'learning_rates': 0.3, 'weight_decays': 0.01, 'loss_functions': 'CrossEntropyLoss', 'batches': 200, 'epochs': 25, 'score': 0.0902}\n"
     ]
    }
   ],
   "source": [
    "print(best_parameters)\n",
    "print(worst_parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like with the above, the best_parameters dict hates me.  These are the true best parameters:\n",
    "\n",
    "784\n",
    "1\n",
    "10\n",
    "[40, 10]\n",
    "True+\n",
    "['relu']\n",
    "SGD\n",
    "0.03\n",
    "0\n",
    "CrossEntropyLoss\n",
    "100\n",
    "25\n",
    "0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"fmnist_without_augments.csv\", \"w\", newline=\"\") as file:\n",
    "    writer = csv.DictWriter(file, results.keys())\n",
    "    writer.writeheader()\n",
    "    writer.writerow(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Batch 100, Loss: 1.22136863052845\n",
      "Epoch 1, Batch 200, Loss: 0.7058444607257843\n",
      "Epoch 1, Batch 300, Loss: 0.6346086165308953\n",
      "Epoch 1, Batch 400, Loss: 0.5791755375266076\n",
      "Epoch 1, Batch 500, Loss: 0.5363850125670433\n",
      "Epoch 1, Batch 600, Loss: 0.5299435216188431\n",
      "Epoch 1, Batch 700, Loss: 0.5316382467746734\n",
      "Epoch 1, Batch 800, Loss: 0.493985775411129\n",
      "Epoch 1, Batch 900, Loss: 0.4894328972697258\n",
      "Epoch 2, Batch 100, Loss: 0.4958282372355461\n",
      "Epoch 2, Batch 200, Loss: 0.4603382143378258\n",
      "Epoch 2, Batch 300, Loss: 0.4666664496064186\n",
      "Epoch 2, Batch 400, Loss: 0.47181703269481656\n",
      "Epoch 2, Batch 500, Loss: 0.4661540172994137\n",
      "Epoch 2, Batch 600, Loss: 0.4397779616713524\n",
      "Epoch 2, Batch 700, Loss: 0.43731684803962706\n",
      "Epoch 2, Batch 800, Loss: 0.43729307502508163\n",
      "Epoch 2, Batch 900, Loss: 0.4297910232841968\n",
      "Epoch 3, Batch 100, Loss: 0.4190805853903294\n",
      "Epoch 3, Batch 200, Loss: 0.42640316888689994\n",
      "Epoch 3, Batch 300, Loss: 0.4432858529686928\n",
      "Epoch 3, Batch 400, Loss: 0.4356191121041775\n",
      "Epoch 3, Batch 500, Loss: 0.38977635964751245\n",
      "Epoch 3, Batch 600, Loss: 0.4055425216257572\n",
      "Epoch 3, Batch 700, Loss: 0.4221260905265808\n",
      "Epoch 3, Batch 800, Loss: 0.4150451962649822\n",
      "Epoch 3, Batch 900, Loss: 0.41227173179388044\n",
      "Epoch 4, Batch 100, Loss: 0.39795007959008216\n",
      "Epoch 4, Batch 200, Loss: 0.4008985735476017\n",
      "Epoch 4, Batch 300, Loss: 0.39021644681692125\n",
      "Epoch 4, Batch 400, Loss: 0.40866640612483024\n",
      "Epoch 4, Batch 500, Loss: 0.37973521515727043\n",
      "Epoch 4, Batch 600, Loss: 0.400834391862154\n",
      "Epoch 4, Batch 700, Loss: 0.40577874943614006\n",
      "Epoch 4, Batch 800, Loss: 0.38452407777309416\n",
      "Epoch 4, Batch 900, Loss: 0.38906442761421206\n",
      "Epoch 5, Batch 100, Loss: 0.40539040088653566\n",
      "Epoch 5, Batch 200, Loss: 0.3948916579782963\n",
      "Epoch 5, Batch 300, Loss: 0.36387795507907866\n",
      "Epoch 5, Batch 400, Loss: 0.387945389598608\n",
      "Epoch 5, Batch 500, Loss: 0.3619949623942375\n",
      "Epoch 5, Batch 600, Loss: 0.37147834166884425\n",
      "Epoch 5, Batch 700, Loss: 0.36159000888466836\n",
      "Epoch 5, Batch 800, Loss: 0.3831546960771084\n",
      "Epoch 5, Batch 900, Loss: 0.3817143850028515\n",
      "Epoch 6, Batch 100, Loss: 0.3699092674255371\n",
      "Epoch 6, Batch 200, Loss: 0.3759064611792564\n",
      "Epoch 6, Batch 300, Loss: 0.3514454954862595\n",
      "Epoch 6, Batch 400, Loss: 0.37002513140439985\n",
      "Epoch 6, Batch 500, Loss: 0.3634304401278496\n",
      "Epoch 6, Batch 600, Loss: 0.3736779333651066\n",
      "Epoch 6, Batch 700, Loss: 0.3731761136651039\n",
      "Epoch 6, Batch 800, Loss: 0.35633892863988875\n",
      "Epoch 6, Batch 900, Loss: 0.3565290203690529\n",
      "Epoch 7, Batch 100, Loss: 0.3407192289829254\n",
      "Epoch 7, Batch 200, Loss: 0.3601548865437508\n",
      "Epoch 7, Batch 300, Loss: 0.3633587361127138\n",
      "Epoch 7, Batch 400, Loss: 0.34344500973820685\n",
      "Epoch 7, Batch 500, Loss: 0.3593967053294182\n",
      "Epoch 7, Batch 600, Loss: 0.3500229661166668\n",
      "Epoch 7, Batch 700, Loss: 0.35085648313164713\n",
      "Epoch 7, Batch 800, Loss: 0.33970568314194677\n",
      "Epoch 7, Batch 900, Loss: 0.3811287638545036\n",
      "Epoch 8, Batch 100, Loss: 0.345614188760519\n",
      "Epoch 8, Batch 200, Loss: 0.3405947421491146\n",
      "Epoch 8, Batch 300, Loss: 0.3497236420214176\n",
      "Epoch 8, Batch 400, Loss: 0.3519419318437576\n",
      "Epoch 8, Batch 500, Loss: 0.3328362061083317\n",
      "Epoch 8, Batch 600, Loss: 0.35785450354218484\n",
      "Epoch 8, Batch 700, Loss: 0.33079817876219747\n",
      "Epoch 8, Batch 800, Loss: 0.3512903906404972\n",
      "Epoch 8, Batch 900, Loss: 0.3479118782281876\n",
      "Epoch 9, Batch 100, Loss: 0.3396465693414211\n",
      "Epoch 9, Batch 200, Loss: 0.3478253327310085\n",
      "Epoch 9, Batch 300, Loss: 0.320052110850811\n",
      "Epoch 9, Batch 400, Loss: 0.34642790257930756\n",
      "Epoch 9, Batch 500, Loss: 0.3329999642074108\n",
      "Epoch 9, Batch 600, Loss: 0.3291048853099346\n",
      "Epoch 9, Batch 700, Loss: 0.35404079392552373\n",
      "Epoch 9, Batch 800, Loss: 0.313193474560976\n",
      "Epoch 9, Batch 900, Loss: 0.34960982248187067\n",
      "Epoch 10, Batch 100, Loss: 0.3355459372699261\n",
      "Epoch 10, Batch 200, Loss: 0.33230118349194526\n",
      "Epoch 10, Batch 300, Loss: 0.32151888132095335\n",
      "Epoch 10, Batch 400, Loss: 0.3356407758593559\n",
      "Epoch 10, Batch 500, Loss: 0.32853307858109476\n",
      "Epoch 10, Batch 600, Loss: 0.32610402166843416\n",
      "Epoch 10, Batch 700, Loss: 0.3359039725363255\n",
      "Epoch 10, Batch 800, Loss: 0.3344037464261055\n",
      "Epoch 10, Batch 900, Loss: 0.32489968597888946\n",
      "Epoch 11, Batch 100, Loss: 0.31817104123532775\n",
      "Epoch 11, Batch 200, Loss: 0.330637286528945\n",
      "Epoch 11, Batch 300, Loss: 0.3230319032073021\n",
      "Epoch 11, Batch 400, Loss: 0.3221012945473194\n",
      "Epoch 11, Batch 500, Loss: 0.3207511545717716\n",
      "Epoch 11, Batch 600, Loss: 0.3391753913462162\n",
      "Epoch 11, Batch 700, Loss: 0.3112992757558823\n",
      "Epoch 11, Batch 800, Loss: 0.3237091763317585\n",
      "Epoch 11, Batch 900, Loss: 0.3261781723797321\n",
      "Epoch 12, Batch 100, Loss: 0.30086587443947793\n",
      "Epoch 12, Batch 200, Loss: 0.30990201339125634\n",
      "Epoch 12, Batch 300, Loss: 0.32040133982896807\n",
      "Epoch 12, Batch 400, Loss: 0.3161318428069353\n",
      "Epoch 12, Batch 500, Loss: 0.3115195381641388\n",
      "Epoch 12, Batch 600, Loss: 0.32414450250566007\n",
      "Epoch 12, Batch 700, Loss: 0.3223504315316677\n",
      "Epoch 12, Batch 800, Loss: 0.3205554139614105\n",
      "Epoch 12, Batch 900, Loss: 0.3141440116614103\n",
      "Epoch 13, Batch 100, Loss: 0.3144995728135109\n",
      "Epoch 13, Batch 200, Loss: 0.3121124041825533\n",
      "Epoch 13, Batch 300, Loss: 0.3023348764330149\n",
      "Epoch 13, Batch 400, Loss: 0.3152035304903984\n",
      "Epoch 13, Batch 500, Loss: 0.28890289932489394\n",
      "Epoch 13, Batch 600, Loss: 0.3173932059109211\n",
      "Epoch 13, Batch 700, Loss: 0.30894740350544453\n",
      "Epoch 13, Batch 800, Loss: 0.31021568104624747\n",
      "Epoch 13, Batch 900, Loss: 0.31804993212223054\n",
      "Epoch 14, Batch 100, Loss: 0.30816732689738274\n",
      "Epoch 14, Batch 200, Loss: 0.3051159558445215\n",
      "Epoch 14, Batch 300, Loss: 0.3082551177591085\n",
      "Epoch 14, Batch 400, Loss: 0.30508582934737205\n",
      "Epoch 14, Batch 500, Loss: 0.30900172919034957\n",
      "Epoch 14, Batch 600, Loss: 0.3027182574570179\n",
      "Epoch 14, Batch 700, Loss: 0.29161546409130096\n",
      "Epoch 14, Batch 800, Loss: 0.2944881671667099\n",
      "Epoch 14, Batch 900, Loss: 0.3103704898059368\n",
      "Epoch 15, Batch 100, Loss: 0.3037580500543118\n",
      "Epoch 15, Batch 200, Loss: 0.30033122554421426\n",
      "Epoch 15, Batch 300, Loss: 0.28667448438704013\n",
      "Epoch 15, Batch 400, Loss: 0.3014208718389273\n",
      "Epoch 15, Batch 500, Loss: 0.29593087516725064\n",
      "Epoch 15, Batch 600, Loss: 0.3058064141124487\n",
      "Epoch 15, Batch 700, Loss: 0.3063188105821609\n",
      "Epoch 15, Batch 800, Loss: 0.2825087935477495\n",
      "Epoch 15, Batch 900, Loss: 0.30331242986023427\n",
      "Epoch 16, Batch 100, Loss: 0.29252865076065065\n",
      "Epoch 16, Batch 200, Loss: 0.28523498490452764\n",
      "Epoch 16, Batch 300, Loss: 0.30935279540717603\n",
      "Epoch 16, Batch 400, Loss: 0.289780106395483\n",
      "Epoch 16, Batch 500, Loss: 0.28834465324878694\n",
      "Epoch 16, Batch 600, Loss: 0.3074408109486103\n",
      "Epoch 16, Batch 700, Loss: 0.29296242982149123\n",
      "Epoch 16, Batch 800, Loss: 0.2902256505191326\n",
      "Epoch 16, Batch 900, Loss: 0.3043452731519938\n",
      "Epoch 17, Batch 100, Loss: 0.29393733605742456\n",
      "Epoch 17, Batch 200, Loss: 0.28971418879926203\n",
      "Epoch 17, Batch 300, Loss: 0.2825740586221218\n",
      "Epoch 17, Batch 400, Loss: 0.29852474309504035\n",
      "Epoch 17, Batch 500, Loss: 0.2841346229612827\n",
      "Epoch 17, Batch 600, Loss: 0.2847579004615545\n",
      "Epoch 17, Batch 700, Loss: 0.2998139773309231\n",
      "Epoch 17, Batch 800, Loss: 0.28572916001081466\n",
      "Epoch 17, Batch 900, Loss: 0.28920741498470304\n",
      "Epoch 18, Batch 100, Loss: 0.28282668806612493\n",
      "Epoch 18, Batch 200, Loss: 0.28656481131911277\n",
      "Epoch 18, Batch 300, Loss: 0.2917082186043263\n",
      "Epoch 18, Batch 400, Loss: 0.293060065433383\n",
      "Epoch 18, Batch 500, Loss: 0.2865359827876091\n",
      "Epoch 18, Batch 600, Loss: 0.2796345794945955\n",
      "Epoch 18, Batch 700, Loss: 0.28253850869834424\n",
      "Epoch 18, Batch 800, Loss: 0.2894904454797506\n",
      "Epoch 18, Batch 900, Loss: 0.2865986979007721\n",
      "Epoch 19, Batch 100, Loss: 0.27392776265740393\n",
      "Epoch 19, Batch 200, Loss: 0.27832054138183593\n",
      "Epoch 19, Batch 300, Loss: 0.2902035976946354\n",
      "Epoch 19, Batch 400, Loss: 0.29017941281199455\n",
      "Epoch 19, Batch 500, Loss: 0.285627861097455\n",
      "Epoch 19, Batch 600, Loss: 0.2867532229423523\n",
      "Epoch 19, Batch 700, Loss: 0.27228772863745687\n",
      "Epoch 19, Batch 800, Loss: 0.2804186637699604\n",
      "Epoch 19, Batch 900, Loss: 0.28068675339221955\n",
      "Epoch 20, Batch 100, Loss: 0.2688696525245905\n",
      "Epoch 20, Batch 200, Loss: 0.26082777671515944\n",
      "Epoch 20, Batch 300, Loss: 0.27407891817390917\n",
      "Epoch 20, Batch 400, Loss: 0.2731650472432375\n",
      "Epoch 20, Batch 500, Loss: 0.2795847123861313\n",
      "Epoch 20, Batch 600, Loss: 0.2891280838102102\n",
      "Epoch 20, Batch 700, Loss: 0.29157730005681515\n",
      "Epoch 20, Batch 800, Loss: 0.27353872001171115\n",
      "Epoch 20, Batch 900, Loss: 0.27921538524329664\n",
      "Epoch 21, Batch 100, Loss: 0.2885979891568422\n",
      "Epoch 21, Batch 200, Loss: 0.25743223950266836\n",
      "Epoch 21, Batch 300, Loss: 0.27249057464301585\n",
      "Epoch 21, Batch 400, Loss: 0.2758249106258154\n",
      "Epoch 21, Batch 500, Loss: 0.27042402811348437\n",
      "Epoch 21, Batch 600, Loss: 0.2656115671992302\n",
      "Epoch 21, Batch 700, Loss: 0.2823072776198387\n",
      "Epoch 21, Batch 800, Loss: 0.28435002617537974\n",
      "Epoch 21, Batch 900, Loss: 0.2698432894051075\n",
      "Epoch 22, Batch 100, Loss: 0.2664395386725664\n",
      "Epoch 22, Batch 200, Loss: 0.27164774693548677\n",
      "Epoch 22, Batch 300, Loss: 0.2669376407563686\n",
      "Epoch 22, Batch 400, Loss: 0.26390508890151976\n",
      "Epoch 22, Batch 500, Loss: 0.27387365385890006\n",
      "Epoch 22, Batch 600, Loss: 0.2836201611161232\n",
      "Epoch 22, Batch 700, Loss: 0.2864335834980011\n",
      "Epoch 22, Batch 800, Loss: 0.2738523031026125\n",
      "Epoch 22, Batch 900, Loss: 0.2663945583999157\n",
      "Epoch 23, Batch 100, Loss: 0.2656734676659107\n",
      "Epoch 23, Batch 200, Loss: 0.2738089432567358\n",
      "Epoch 23, Batch 300, Loss: 0.27352433919906616\n",
      "Epoch 23, Batch 400, Loss: 0.25681914642453196\n",
      "Epoch 23, Batch 500, Loss: 0.27190241910517216\n",
      "Epoch 23, Batch 600, Loss: 0.27008541248738765\n",
      "Epoch 23, Batch 700, Loss: 0.2632078210264444\n",
      "Epoch 23, Batch 800, Loss: 0.27049244321882726\n",
      "Epoch 23, Batch 900, Loss: 0.27533200033009053\n",
      "Epoch 24, Batch 100, Loss: 0.24983497381210326\n",
      "Epoch 24, Batch 200, Loss: 0.2677171026915312\n",
      "Epoch 24, Batch 300, Loss: 0.2658855776488781\n",
      "Epoch 24, Batch 400, Loss: 0.26915671601891517\n",
      "Epoch 24, Batch 500, Loss: 0.2690164863318205\n",
      "Epoch 24, Batch 600, Loss: 0.266380695477128\n",
      "Epoch 24, Batch 700, Loss: 0.268324162364006\n",
      "Epoch 24, Batch 800, Loss: 0.27541360013186933\n",
      "Epoch 24, Batch 900, Loss: 0.26426566049456596\n",
      "Epoch 25, Batch 100, Loss: 0.26933044858276844\n",
      "Epoch 25, Batch 200, Loss: 0.26659809656441213\n",
      "Epoch 25, Batch 300, Loss: 0.250621537566185\n",
      "Epoch 25, Batch 400, Loss: 0.2673356073349714\n",
      "Epoch 25, Batch 500, Loss: 0.27067117869853974\n",
      "Epoch 25, Batch 600, Loss: 0.24549731552600862\n",
      "Epoch 25, Batch 700, Loss: 0.2591132351756096\n",
      "Epoch 25, Batch 800, Loss: 0.2764264985173941\n",
      "Epoch 25, Batch 900, Loss: 0.264769679158926\n",
      "Accuracy on test set: 0.8774%\n"
     ]
    }
   ],
   "source": [
    "model = MLP((28*28), [40, 10], False, ['relu'])\n",
    "model.to(device)\n",
    "model.fit(train_loader, 'SGD', 'CrossEntropyLoss', 0.03, 0, 100, 25)\n",
    "score = model.score(test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 428
    },
    "id": "J2GkmLeQEeZV",
    "outputId": "cb5bae66-d3d2-4163-deef-5828aad6b068"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "image_index = 27\n",
    "test_image, test_label = test_dataset[image_index]\n",
    "\n",
    "cpu_image = test_image\n",
    "test_image = torch.Tensor(test_image).to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    output = model(test_image.unsqueeze(0))\n",
    "    _, predicted_label = torch.max(output, 1)\n",
    "\n",
    "test_image_numpy = cpu_image.squeeze().numpy()\n",
    "\n",
    "plt.imshow(test_image_numpy, cmap='gray')\n",
    "plt.title(f'Predicted Label: {predicted_label.item()}, Actual Label: {test_label}')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K5nxrEoAHAUX"
   },
   "source": [
    "## Part 3\n",
    "\n",
    "### FMNIST CNN Implimentation with Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "id": "AEoqWEFz5Ms-"
   },
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()\n",
    "\n",
    "# Preprocess the data\n",
    "x_train = x_train.reshape(x_train.shape[0], 28, 28, 1).astype('float32') / 255\n",
    "x_test = x_test.reshape(x_test.shape[0], 28, 28, 1).astype('float32') / 255\n",
    "\n",
    "# Convert labels to one-hot encoding\n",
    "y_train = to_categorical(y_train, 10)\n",
    "y_test = to_categorical(y_test, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "id": "k41uN-aAIH6Y"
   },
   "outputs": [],
   "source": [
    "# Mapping the labels for the MNIST dataset\n",
    "labels_map = {\n",
    "    0: \"0\", 1: \"1\", 2: \"2\", 3: \"3\", 4: \"4\",\n",
    "    5: \"5\", 6: \"6\", 7: \"7\", 8: \"8\", 9: \"9\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to create the CNN model. I wanted to do a class like the above two, but I'm lazy\n",
    "def cnn_model(inputs, conv2d_outputs, kernels, dense_outputs, conv2d_activations, dense_activations, dropout):\n",
    "    model = Sequential()\n",
    "    model.add(Input(inputs))\n",
    "    for layer in range(0,len(conv2d_outputs)):\n",
    "        model.add(Conv2D(conv2d_outputs[layer], kernels[layer], activation=conv2d_activations[layer]))\n",
    "        if (layer % 2 == 1):\n",
    "            model.add(MaxPooling2D(kernels[layer]))\n",
    "        elif (layer != 0) and (layer % 2 == 0) and (dropout == True):\n",
    "            model.add(Dropout(0.1))\n",
    "    model.add(Flatten())\n",
    "    for layer in range(0,(len(dense_outputs)-1)):\n",
    "        model.add(Dense(dense_outputs[layer], activation=dense_activations[layer]))\n",
    "        if (dropout == True):\n",
    "            model.add(Dropout(0.5))\n",
    "    model.add(Dense(dense_outputs[(len(dense_outputs)-1)], activation=dense_activations[(len(dense_activations)-1)]))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "nPfHtKytJd9Q"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-13 19:03:20.200078: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:982] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-10-13 19:03:20.200125: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:982] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-10-13 19:03:20.200135: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:982] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-10-13 19:03:20.276033: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:982] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-10-13 19:03:20.276066: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:982] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-10-13 19:03:20.276070: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1722] Could not identify NUMA node of platform GPU id 0, defaulting to 0.  Your kernel may not have been built with NUMA support.\n",
      "2024-10-13 19:03:20.276082: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:982] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-10-13 19:03:20.276306: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1635] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 9511 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 4080 Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-13 19:03:21.598587: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:424] Loaded cuDNN version 8902\n",
      "2024-10-13 19:03:21.723721: W tensorflow/compiler/xla/stream_executor/gpu/asm_compiler.cc:231] Falling back to the CUDA driver for PTX compilation; ptxas does not support CC 8.9\n",
      "2024-10-13 19:03:21.723744: W tensorflow/compiler/xla/stream_executor/gpu/asm_compiler.cc:234] Used ptxas at ptxas\n",
      "2024-10-13 19:03:21.723790: W tensorflow/compiler/xla/stream_executor/gpu/redzone_allocator.cc:317] UNIMPLEMENTED: ptxas ptxas too old. Falling back to the driver to compile.\n",
      "Relying on driver to perform ptx compilation. \n",
      "Modify $PATH to customize ptxas location.\n",
      "This message will be only logged once.\n",
      "2024-10-13 19:03:22.936573: I tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:637] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1250/1250 [==============================] - 5s 3ms/step - loss: 2.0546 - accuracy: 0.3072\n",
      "Epoch 2/5\n",
      "1250/1250 [==============================] - 3s 3ms/step - loss: 0.9293 - accuracy: 0.6917\n",
      "Epoch 3/5\n",
      "1250/1250 [==============================] - 3s 2ms/step - loss: 0.7309 - accuracy: 0.7372\n",
      "Epoch 4/5\n",
      "1250/1250 [==============================] - 3s 2ms/step - loss: 0.6604 - accuracy: 0.7563\n",
      "Epoch 5/5\n",
      "1250/1250 [==============================] - 3s 2ms/step - loss: 0.6188 - accuracy: 0.7695\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 0.6227 - accuracy: 0.7677\n",
      "Test accuracy: 0.7677000164985657\n"
     ]
    }
   ],
   "source": [
    "# Test the above with an example model\n",
    "model = cnn_model((28, 28, 1), [32, 64], [(5,5), (3,3)], [128, 10], ['sigmoid', 'tanh'], ['sigmoid'], False)\n",
    "\n",
    "model.compile(optimizer=SGD(learning_rate=0.01),\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(x_train, y_train, epochs=5, batch_size=48, verbose=1)\n",
    "\n",
    "loss, accuracy = model.evaluate(x_test, y_test)\n",
    "print('Test accuracy:', accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<keras.layers.convolutional.conv2d.Conv2D at 0x7f2ba1b97cd0>,\n",
       " <keras.layers.convolutional.conv2d.Conv2D at 0x7f2ba20d37d0>,\n",
       " <keras.layers.pooling.max_pooling2d.MaxPooling2D at 0x7f2ba222c310>,\n",
       " <keras.layers.reshaping.flatten.Flatten at 0x7f2ba1c1ba90>,\n",
       " <keras.layers.core.dense.Dense at 0x7f2ba1bfb750>,\n",
       " <keras.layers.core.dense.Dense at 0x7f2ba1d8b190>]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the range of parameters for the grid search\n",
    "parameters = {\n",
    "    'network_inputs': (28, 28, 1),\n",
    "    'conv2d_layers': [1, 2, 3, 4],\n",
    "    'conv2d_outputs': [32, 64, 128],\n",
    "    'kernels': [(2, 2), (3, 3), (4, 4), (5, 5)],\n",
    "    'dense_layers': [1, 2, 3],\n",
    "    'dense_outputs': [16, 32, 64, 128],\n",
    "    'network_output': 10,\n",
    "    'dropout_layers': [False, True],\n",
    "    'conv2d_activations': ['relu', 'tanh', 'sigmoid'],\n",
    "    'dense_activations': ['relu', 'tanh', 'sigmoid'],\n",
    "    'optimizers': ['Adam', 'SGD'],\n",
    "    'learning_rates': [0.01, 0.03, 0.1, 0.3],\n",
    "    'weight_decays': [0, 0.01, 0.03],\n",
    "    'loss_functions': ['categorical_crossentropy'],\n",
    "    'batches': [50, 100, 150, 200, 300],\n",
    "    'epochs': [25] # This is going to take so long that I decided to keep it short\n",
    "}\n",
    "\n",
    "results = {}\n",
    "best_parameters = {}\n",
    "worst_parameters = {}\n",
    "for parameter in parameters:\n",
    "    results[parameter] = []\n",
    "    best_parameters[parameter] = None\n",
    "    worst_parameters[parameter] = None\n",
    "\n",
    "results['accuracy'] = []\n",
    "best_parameters['accuracy'] = -1\n",
    "worst_parameters['accuracy'] = 1\n",
    "combination = 0\n",
    "local_layers = {'conv2d_outputs':[], 'kernels':[], 'conv2d_activations':[]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting for combination 0\n",
      "(28, 28, 1)\n",
      "1\n",
      "[32]\n",
      "[(2, 2)]\n",
      "1\n",
      "[128, 10]\n",
      "10\n",
      "True\n",
      "['relu']\n",
      "['relu', 'softmax']\n",
      "Adam\n",
      "0.03\n",
      "0.03\n",
      "categorical_crossentropy\n",
      "300\n",
      "25\n",
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-14 08:53:03.201593: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:982] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-10-14 08:53:03.201635: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:982] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-10-14 08:53:03.201643: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:982] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-10-14 08:53:03.274130: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:982] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-10-14 08:53:03.274163: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:982] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-10-14 08:53:03.274167: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1722] Could not identify NUMA node of platform GPU id 0, defaulting to 0.  Your kernel may not have been built with NUMA support.\n",
      "2024-10-14 08:53:03.274178: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:982] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-10-14 08:53:03.274197: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1635] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 9511 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 4080 Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-14 08:53:04.623163: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:424] Loaded cuDNN version 8902\n",
      "2024-10-14 08:53:04.739081: W tensorflow/compiler/xla/stream_executor/gpu/asm_compiler.cc:231] Falling back to the CUDA driver for PTX compilation; ptxas does not support CC 8.9\n",
      "2024-10-14 08:53:04.739106: W tensorflow/compiler/xla/stream_executor/gpu/asm_compiler.cc:234] Used ptxas at ptxas\n",
      "2024-10-14 08:53:04.739138: W tensorflow/compiler/xla/stream_executor/gpu/redzone_allocator.cc:317] UNIMPLEMENTED: ptxas ptxas too old. Falling back to the driver to compile.\n",
      "Relying on driver to perform ptx compilation. \n",
      "Modify $PATH to customize ptxas location.\n",
      "This message will be only logged once.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  1/200 [..............................] - ETA: 6:09 - loss: 2.3013 - accuracy: 0.0967"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-14 08:53:06.110493: I tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:637] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200/200 [==============================] - 3s 3ms/step - loss: 0.8887 - accuracy: 0.7944\n",
      "Epoch 2/25\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.3631 - accuracy: 0.8707\n",
      "Epoch 3/25\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.3235 - accuracy: 0.8852\n",
      "Epoch 4/25\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.3032 - accuracy: 0.8913\n",
      "Epoch 5/25\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.2905 - accuracy: 0.8967\n",
      "Epoch 6/25\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.2806 - accuracy: 0.8996\n",
      "Epoch 7/25\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.2699 - accuracy: 0.9044\n",
      "Epoch 8/25\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.2642 - accuracy: 0.9056\n",
      "Epoch 9/25\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.2568 - accuracy: 0.9077\n",
      "Epoch 10/25\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.2532 - accuracy: 0.9091\n",
      "Epoch 11/25\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.2477 - accuracy: 0.9117\n",
      "Epoch 12/25\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.2456 - accuracy: 0.9120\n",
      "Epoch 13/25\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.2386 - accuracy: 0.9148\n",
      "Epoch 14/25\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.2375 - accuracy: 0.9158\n",
      "Epoch 15/25\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.2345 - accuracy: 0.9158\n",
      "Epoch 16/25\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.2280 - accuracy: 0.9180\n",
      "Epoch 17/25\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.2290 - accuracy: 0.9185\n",
      "Epoch 18/25\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.2277 - accuracy: 0.9169\n",
      "Epoch 19/25\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.2244 - accuracy: 0.9184\n",
      "Epoch 20/25\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.2222 - accuracy: 0.9212\n",
      "Epoch 21/25\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.2210 - accuracy: 0.9199\n",
      "Epoch 22/25\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.2201 - accuracy: 0.9212\n",
      "Epoch 23/25\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.2183 - accuracy: 0.9217\n",
      "Epoch 24/25\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.2154 - accuracy: 0.9223\n",
      "Epoch 25/25\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.2125 - accuracy: 0.9245\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 0.2864 - accuracy: 0.8992\n",
      "Fitting for combination 1\n",
      "(28, 28, 1)\n",
      "1\n",
      "[32]\n",
      "[(2, 2)]\n",
      "1\n",
      "[32, 10]\n",
      "10\n",
      "False\n",
      "['relu']\n",
      "['tanh', 'softmax']\n",
      "Adam\n",
      "0.01\n",
      "0.03\n",
      "categorical_crossentropy\n",
      "300\n",
      "25\n",
      "0\n",
      "Epoch 1/25\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.4765 - accuracy: 0.8389\n",
      "Epoch 2/25\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.2937 - accuracy: 0.8999\n",
      "Epoch 3/25\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.2599 - accuracy: 0.9108\n",
      "Epoch 4/25\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.2419 - accuracy: 0.9171\n",
      "Epoch 5/25\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.2290 - accuracy: 0.9216\n",
      "Epoch 6/25\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.2197 - accuracy: 0.9247\n",
      "Epoch 7/25\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.2124 - accuracy: 0.9273\n",
      "Epoch 8/25\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.2062 - accuracy: 0.9296\n",
      "Epoch 9/25\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.2008 - accuracy: 0.9319\n",
      "Epoch 10/25\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.1962 - accuracy: 0.9335\n",
      "Epoch 11/25\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.1923 - accuracy: 0.9347\n",
      "Epoch 12/25\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.1887 - accuracy: 0.9359\n",
      "Epoch 13/25\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.1857 - accuracy: 0.9373\n",
      "Epoch 14/25\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.1828 - accuracy: 0.9383\n",
      "Epoch 15/25\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.1804 - accuracy: 0.9391\n",
      "Epoch 16/25\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.1778 - accuracy: 0.9401\n",
      "Epoch 17/25\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.1756 - accuracy: 0.9411\n",
      "Epoch 18/25\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.1735 - accuracy: 0.9420\n",
      "Epoch 19/25\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.1716 - accuracy: 0.9420\n",
      "Epoch 20/25\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.1697 - accuracy: 0.9434\n",
      "Epoch 21/25\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.1680 - accuracy: 0.9435\n",
      "Epoch 22/25\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.1665 - accuracy: 0.9441\n",
      "Epoch 23/25\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.1648 - accuracy: 0.9446\n",
      "Epoch 24/25\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.1634 - accuracy: 0.9452\n",
      "Epoch 25/25\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.1621 - accuracy: 0.9459\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 0.2722 - accuracy: 0.9029\n",
      "Fitting for combination 2\n",
      "(28, 28, 1)\n",
      "1\n",
      "[32]\n",
      "[(4, 4)]\n",
      "1\n",
      "[16, 10]\n",
      "10\n",
      "False\n",
      "['sigmoid']\n",
      "['relu', 'softmax']\n",
      "Adam\n",
      "0.1\n",
      "0.01\n",
      "categorical_crossentropy\n",
      "100\n",
      "25\n",
      "0\n",
      "Epoch 1/25\n",
      "600/600 [==============================] - 2s 2ms/step - loss: 3.7864 - accuracy: 0.1003\n",
      "Epoch 2/25\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 2.3033 - accuracy: 0.0997\n",
      "Epoch 3/25\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 2.3030 - accuracy: 0.0992\n",
      "Epoch 4/25\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 2.3030 - accuracy: 0.0986\n",
      "Epoch 5/25\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 2.3029 - accuracy: 0.0983\n",
      "Epoch 6/25\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 2.3029 - accuracy: 0.0990\n",
      "Epoch 7/25\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 2.3028 - accuracy: 0.0992\n",
      "Epoch 8/25\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 2.3028 - accuracy: 0.0975\n",
      "Epoch 9/25\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 2.3028 - accuracy: 0.0986\n",
      "Epoch 10/25\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 2.3028 - accuracy: 0.0985\n",
      "Epoch 11/25\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 2.3027 - accuracy: 0.1000\n",
      "Epoch 12/25\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 2.3027 - accuracy: 0.0994\n",
      "Epoch 13/25\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 2.3027 - accuracy: 0.0971\n",
      "Epoch 14/25\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 2.3027 - accuracy: 0.0986\n",
      "Epoch 15/25\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 2.3027 - accuracy: 0.0976\n",
      "Epoch 16/25\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 2.3027 - accuracy: 0.0990\n",
      "Epoch 17/25\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 2.3027 - accuracy: 0.0979\n",
      "Epoch 18/25\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 2.3027 - accuracy: 0.0987\n",
      "Epoch 19/25\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 2.3027 - accuracy: 0.0960\n",
      "Epoch 20/25\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 2.3027 - accuracy: 0.0981\n",
      "Epoch 21/25\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 2.3027 - accuracy: 0.0987\n",
      "Epoch 22/25\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 2.3027 - accuracy: 0.0970\n",
      "Epoch 23/25\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 2.3027 - accuracy: 0.0977\n",
      "Epoch 24/25\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 2.3027 - accuracy: 0.0962\n",
      "Epoch 25/25\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 2.3027 - accuracy: 0.0997\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 2.3026 - accuracy: 0.1000\n",
      "Fitting for combination 3\n",
      "(28, 28, 1)\n",
      "1\n",
      "[32]\n",
      "[(5, 5)]\n",
      "1\n",
      "[128, 10]\n",
      "10\n",
      "True\n",
      "['tanh']\n",
      "['relu', 'softmax']\n",
      "Adam\n",
      "0.1\n",
      "0\n",
      "categorical_crossentropy\n",
      "300\n",
      "25\n",
      "0\n",
      "Epoch 1/25\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 48.3378 - accuracy: 0.1022\n",
      "Epoch 2/25\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.3053 - accuracy: 0.1005\n",
      "Epoch 3/25\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.3053 - accuracy: 0.0989\n",
      "Epoch 4/25\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.3059 - accuracy: 0.0989\n",
      "Epoch 5/25\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.3066 - accuracy: 0.1005\n",
      "Epoch 6/25\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.3065 - accuracy: 0.1001\n",
      "Epoch 7/25\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.3063 - accuracy: 0.0998\n",
      "Epoch 8/25\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.3072 - accuracy: 0.0986\n",
      "Epoch 9/25\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.3064 - accuracy: 0.0992\n",
      "Epoch 10/25\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.3068 - accuracy: 0.1015\n",
      "Epoch 11/25\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.3067 - accuracy: 0.1001\n",
      "Epoch 12/25\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.3066 - accuracy: 0.1006\n",
      "Epoch 13/25\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.3066 - accuracy: 0.0994\n",
      "Epoch 14/25\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.3065 - accuracy: 0.0999\n",
      "Epoch 15/25\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.3065 - accuracy: 0.0987\n",
      "Epoch 16/25\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.3065 - accuracy: 0.1011\n",
      "Epoch 17/25\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.3064 - accuracy: 0.0997\n",
      "Epoch 18/25\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.3073 - accuracy: 0.0992\n",
      "Epoch 19/25\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.3068 - accuracy: 0.0989\n",
      "Epoch 20/25\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.3064 - accuracy: 0.0994\n",
      "Epoch 21/25\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.3066 - accuracy: 0.1009\n",
      "Epoch 22/25\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.3072 - accuracy: 0.0980\n",
      "Epoch 23/25\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.3062 - accuracy: 0.0968\n",
      "Epoch 24/25\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.3067 - accuracy: 0.0984\n",
      "Epoch 25/25\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.3069 - accuracy: 0.1006\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 2.3072 - accuracy: 0.1000\n",
      "Fitting for combination 4\n",
      "(28, 28, 1)\n",
      "1\n",
      "[32]\n",
      "[(4, 4)]\n",
      "3\n",
      "[32, 128, 128, 10]\n",
      "10\n",
      "True\n",
      "['sigmoid']\n",
      "['relu', 'tanh', 'sigmoid', 'softmax']\n",
      "Adam\n",
      "0.01\n",
      "0\n",
      "categorical_crossentropy\n",
      "150\n",
      "25\n",
      "0\n",
      "Epoch 1/25\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 2.3186 - accuracy: 0.1007\n",
      "Epoch 2/25\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 2.3040 - accuracy: 0.1008\n",
      "Epoch 3/25\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 2.3036 - accuracy: 0.0987\n",
      "Epoch 4/25\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 2.3038 - accuracy: 0.0996\n",
      "Epoch 5/25\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 2.3038 - accuracy: 0.1000\n",
      "Epoch 6/25\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 2.3038 - accuracy: 0.0977\n",
      "Epoch 7/25\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 2.3037 - accuracy: 0.0998\n",
      "Epoch 8/25\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 2.3043 - accuracy: 0.0984\n",
      "Epoch 9/25\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 2.3042 - accuracy: 0.1006\n",
      "Epoch 10/25\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 2.3040 - accuracy: 0.0994\n",
      "Epoch 11/25\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 2.3043 - accuracy: 0.0981\n",
      "Epoch 12/25\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 2.3041 - accuracy: 0.1012\n",
      "Epoch 13/25\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 2.3040 - accuracy: 0.0983\n",
      "Epoch 14/25\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 2.3049 - accuracy: 0.1004\n",
      "Epoch 15/25\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 2.3039 - accuracy: 0.1001\n",
      "Epoch 16/25\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 2.3044 - accuracy: 0.1011\n",
      "Epoch 17/25\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 2.3043 - accuracy: 0.0997\n",
      "Epoch 18/25\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 2.3043 - accuracy: 0.0993\n",
      "Epoch 19/25\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 2.3044 - accuracy: 0.0989\n",
      "Epoch 20/25\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 2.3037 - accuracy: 0.0979\n",
      "Epoch 21/25\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 2.3040 - accuracy: 0.0982\n",
      "Epoch 22/25\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 2.3036 - accuracy: 0.0978\n",
      "Epoch 23/25\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 2.3035 - accuracy: 0.0986\n",
      "Epoch 24/25\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 2.3039 - accuracy: 0.0984\n",
      "Epoch 25/25\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 2.3130 - accuracy: 0.0999\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 2.3026 - accuracy: 0.1000\n",
      "Fitting for combination 5\n",
      "(28, 28, 1)\n",
      "1\n",
      "[32]\n",
      "[(4, 4)]\n",
      "1\n",
      "[32, 10]\n",
      "10\n",
      "True\n",
      "['tanh']\n",
      "['relu', 'softmax']\n",
      "SGD\n",
      "0.1\n",
      "0.03\n",
      "categorical_crossentropy\n",
      "300\n",
      "25\n",
      "0\n",
      "Epoch 1/25\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 1.1311 - accuracy: 0.6016\n",
      "Epoch 2/25\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.8310 - accuracy: 0.7058\n",
      "Epoch 3/25\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.7906 - accuracy: 0.7205\n",
      "Epoch 4/25\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.7699 - accuracy: 0.7286\n",
      "Epoch 5/25\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.7506 - accuracy: 0.7366\n",
      "Epoch 6/25\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.7452 - accuracy: 0.7364\n",
      "Epoch 7/25\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.7389 - accuracy: 0.7395\n",
      "Epoch 8/25\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.7323 - accuracy: 0.7422\n",
      "Epoch 9/25\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.7291 - accuracy: 0.7426\n",
      "Epoch 10/25\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.7243 - accuracy: 0.7458\n",
      "Epoch 11/25\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.7183 - accuracy: 0.7472\n",
      "Epoch 12/25\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.7160 - accuracy: 0.7487\n",
      "Epoch 13/25\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.7132 - accuracy: 0.7516\n",
      "Epoch 14/25\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.7142 - accuracy: 0.7494\n",
      "Epoch 15/25\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.7097 - accuracy: 0.7522\n",
      "Epoch 16/25\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.7097 - accuracy: 0.7498\n",
      "Epoch 17/25\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.7070 - accuracy: 0.7518\n",
      "Epoch 18/25\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.7057 - accuracy: 0.7515\n",
      "Epoch 19/25\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.7021 - accuracy: 0.7542\n",
      "Epoch 20/25\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.6997 - accuracy: 0.7524\n",
      "Epoch 21/25\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.6946 - accuracy: 0.7541\n",
      "Epoch 22/25\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.6984 - accuracy: 0.7564\n",
      "Epoch 23/25\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.6974 - accuracy: 0.7567\n",
      "Epoch 24/25\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.6942 - accuracy: 0.7576\n",
      "Epoch 25/25\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.6906 - accuracy: 0.7589\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 0.5179 - accuracy: 0.8145\n",
      "Fitting for combination 6\n",
      "(28, 28, 1)\n",
      "1\n",
      "[32]\n",
      "[(5, 5)]\n",
      "2\n",
      "[16, 128, 10]\n",
      "10\n",
      "True\n",
      "['sigmoid']\n",
      "['tanh', 'tanh', 'softmax']\n",
      "SGD\n",
      "0.03\n",
      "0.03\n",
      "categorical_crossentropy\n",
      "50\n",
      "25\n",
      "0\n",
      "Epoch 1/25\n",
      "1200/1200 [==============================] - 3s 2ms/step - loss: 2.4522 - accuracy: 0.1059\n",
      "Epoch 2/25\n",
      "1200/1200 [==============================] - 3s 2ms/step - loss: 2.3571 - accuracy: 0.1237\n",
      "Epoch 3/25\n",
      "1200/1200 [==============================] - 3s 2ms/step - loss: 2.2971 - accuracy: 0.1463\n",
      "Epoch 4/25\n",
      "1200/1200 [==============================] - 3s 2ms/step - loss: 2.2462 - accuracy: 0.1626\n",
      "Epoch 5/25\n",
      "1200/1200 [==============================] - 3s 2ms/step - loss: 2.2114 - accuracy: 0.1699\n",
      "Epoch 6/25\n",
      "1200/1200 [==============================] - 3s 2ms/step - loss: 2.1818 - accuracy: 0.1792\n",
      "Epoch 7/25\n",
      "1200/1200 [==============================] - 3s 2ms/step - loss: 2.1560 - accuracy: 0.1874\n",
      "Epoch 8/25\n",
      "1200/1200 [==============================] - 3s 2ms/step - loss: 2.1397 - accuracy: 0.1881\n",
      "Epoch 9/25\n",
      "1200/1200 [==============================] - 3s 2ms/step - loss: 2.1218 - accuracy: 0.1943\n",
      "Epoch 10/25\n",
      "1200/1200 [==============================] - 3s 2ms/step - loss: 2.1035 - accuracy: 0.1976\n",
      "Epoch 11/25\n",
      "1200/1200 [==============================] - 3s 2ms/step - loss: 2.0939 - accuracy: 0.2003\n",
      "Epoch 12/25\n",
      "1200/1200 [==============================] - 3s 2ms/step - loss: 2.0826 - accuracy: 0.2008\n",
      "Epoch 13/25\n",
      "1200/1200 [==============================] - 3s 2ms/step - loss: 2.0726 - accuracy: 0.2050\n",
      "Epoch 14/25\n",
      "1200/1200 [==============================] - 3s 2ms/step - loss: 2.0627 - accuracy: 0.2077\n",
      "Epoch 15/25\n",
      "1200/1200 [==============================] - 3s 2ms/step - loss: 2.0580 - accuracy: 0.2066\n",
      "Epoch 16/25\n",
      "1200/1200 [==============================] - 3s 2ms/step - loss: 2.0469 - accuracy: 0.2110\n",
      "Epoch 17/25\n",
      "1200/1200 [==============================] - 3s 2ms/step - loss: 2.0418 - accuracy: 0.2113\n",
      "Epoch 18/25\n",
      "1200/1200 [==============================] - 3s 2ms/step - loss: 2.0314 - accuracy: 0.2139\n",
      "Epoch 19/25\n",
      "1200/1200 [==============================] - 3s 2ms/step - loss: 2.0236 - accuracy: 0.2165\n",
      "Epoch 20/25\n",
      "1200/1200 [==============================] - 3s 2ms/step - loss: 2.0230 - accuracy: 0.2146\n",
      "Epoch 21/25\n",
      "1200/1200 [==============================] - 3s 2ms/step - loss: 2.0140 - accuracy: 0.2183\n",
      "Epoch 22/25\n",
      "1200/1200 [==============================] - 3s 2ms/step - loss: 2.0094 - accuracy: 0.2202\n",
      "Epoch 23/25\n",
      "1200/1200 [==============================] - 3s 2ms/step - loss: 2.0024 - accuracy: 0.2224\n",
      "Epoch 24/25\n",
      "1200/1200 [==============================] - 3s 2ms/step - loss: 1.9959 - accuracy: 0.2225\n",
      "Epoch 25/25\n",
      "1200/1200 [==============================] - 3s 2ms/step - loss: 1.9930 - accuracy: 0.2240\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 1.7861 - accuracy: 0.4630\n",
      "Fitting for combination 7\n",
      "(28, 28, 1)\n",
      "1\n",
      "[32]\n",
      "[(2, 2)]\n",
      "1\n",
      "[32, 10]\n",
      "10\n",
      "True\n",
      "['relu']\n",
      "['relu', 'softmax']\n",
      "SGD\n",
      "0.3\n",
      "0.01\n",
      "categorical_crossentropy\n",
      "100\n",
      "25\n",
      "0\n",
      "Epoch 1/25\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 1.2657 - accuracy: 0.4848\n",
      "Epoch 2/25\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 1.1095 - accuracy: 0.5509\n",
      "Epoch 3/25\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 1.0703 - accuracy: 0.5648\n",
      "Epoch 4/25\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 1.0316 - accuracy: 0.5836\n",
      "Epoch 5/25\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 1.0108 - accuracy: 0.5936\n",
      "Epoch 6/25\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 1.0093 - accuracy: 0.5958\n",
      "Epoch 7/25\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 1.0015 - accuracy: 0.5996\n",
      "Epoch 8/25\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.9991 - accuracy: 0.6005\n",
      "Epoch 9/25\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.9855 - accuracy: 0.6046\n",
      "Epoch 10/25\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.9911 - accuracy: 0.6063\n",
      "Epoch 11/25\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.9823 - accuracy: 0.6077\n",
      "Epoch 12/25\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.9847 - accuracy: 0.6041\n",
      "Epoch 13/25\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.9783 - accuracy: 0.6081\n",
      "Epoch 14/25\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.9785 - accuracy: 0.6060\n",
      "Epoch 15/25\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.9768 - accuracy: 0.6083\n",
      "Epoch 16/25\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.9709 - accuracy: 0.6096\n",
      "Epoch 17/25\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.9747 - accuracy: 0.6084\n",
      "Epoch 18/25\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.9727 - accuracy: 0.6091\n",
      "Epoch 19/25\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.9664 - accuracy: 0.6103\n",
      "Epoch 20/25\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.9613 - accuracy: 0.6154\n",
      "Epoch 21/25\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.9638 - accuracy: 0.6123\n",
      "Epoch 22/25\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.9570 - accuracy: 0.6148\n",
      "Epoch 23/25\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.9663 - accuracy: 0.6083\n",
      "Epoch 24/25\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.9628 - accuracy: 0.6103\n",
      "Epoch 25/25\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.9655 - accuracy: 0.6124\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 0.6077 - accuracy: 0.7614\n",
      "Fitting for combination 8\n",
      "(28, 28, 1)\n",
      "1\n",
      "[32]\n",
      "[(4, 4)]\n",
      "3\n",
      "[16, 16, 32, 10]\n",
      "10\n",
      "True\n",
      "['sigmoid']\n",
      "['relu', 'relu', 'sigmoid', 'softmax']\n",
      "SGD\n",
      "0.1\n",
      "0\n",
      "categorical_crossentropy\n",
      "300\n",
      "25\n",
      "0\n",
      "Epoch 1/25\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.4205 - accuracy: 0.1012\n",
      "Epoch 2/25\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.3491 - accuracy: 0.1002\n",
      "Epoch 3/25\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.3215 - accuracy: 0.0990\n",
      "Epoch 4/25\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.3109 - accuracy: 0.0991\n",
      "Epoch 5/25\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.3069 - accuracy: 0.1000\n",
      "Epoch 6/25\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.3054 - accuracy: 0.0992\n",
      "Epoch 7/25\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.3039 - accuracy: 0.0997\n",
      "Epoch 8/25\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.3041 - accuracy: 0.0990\n",
      "Epoch 9/25\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.3037 - accuracy: 0.1015\n",
      "Epoch 10/25\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.3039 - accuracy: 0.0988\n",
      "Epoch 11/25\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.3037 - accuracy: 0.0998\n",
      "Epoch 12/25\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.3039 - accuracy: 0.1009\n",
      "Epoch 13/25\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.3042 - accuracy: 0.0978\n",
      "Epoch 14/25\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.3038 - accuracy: 0.1000\n",
      "Epoch 15/25\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.3038 - accuracy: 0.0964\n",
      "Epoch 16/25\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.3036 - accuracy: 0.1001\n",
      "Epoch 17/25\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.3038 - accuracy: 0.0981\n",
      "Epoch 18/25\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.3039 - accuracy: 0.0989\n",
      "Epoch 19/25\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.3039 - accuracy: 0.0981\n",
      "Epoch 20/25\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.3040 - accuracy: 0.0999\n",
      "Epoch 21/25\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.3039 - accuracy: 0.1001\n",
      "Epoch 22/25\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.3038 - accuracy: 0.1000\n",
      "Epoch 23/25\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.3039 - accuracy: 0.0981\n",
      "Epoch 24/25\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.3040 - accuracy: 0.0990\n",
      "Epoch 25/25\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.3037 - accuracy: 0.0987\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 2.3027 - accuracy: 0.1000\n",
      "Fitting for combination 9\n",
      "(28, 28, 1)\n",
      "1\n",
      "[32]\n",
      "[(5, 5)]\n",
      "3\n",
      "[16, 64, 64, 10]\n",
      "10\n",
      "False\n",
      "['tanh']\n",
      "['sigmoid', 'sigmoid', 'relu', 'softmax']\n",
      "SGD\n",
      "0.3\n",
      "0.03\n",
      "categorical_crossentropy\n",
      "300\n",
      "25\n",
      "0\n",
      "Epoch 1/25\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 1.8515 - accuracy: 0.3653\n",
      "Epoch 2/25\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 1.3197 - accuracy: 0.5237\n",
      "Epoch 3/25\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 1.1644 - accuracy: 0.5576\n",
      "Epoch 4/25\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 1.0920 - accuracy: 0.5701\n",
      "Epoch 5/25\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 1.0498 - accuracy: 0.5816\n",
      "Epoch 6/25\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 1.0214 - accuracy: 0.5913\n",
      "Epoch 7/25\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 1.0002 - accuracy: 0.6005\n",
      "Epoch 8/25\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.9826 - accuracy: 0.6087\n",
      "Epoch 9/25\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.9670 - accuracy: 0.6164\n",
      "Epoch 10/25\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.9531 - accuracy: 0.6246\n",
      "Epoch 11/25\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.9402 - accuracy: 0.6331\n",
      "Epoch 12/25\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.9283 - accuracy: 0.6405\n",
      "Epoch 13/25\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.9174 - accuracy: 0.6478\n",
      "Epoch 14/25\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.9072 - accuracy: 0.6535\n",
      "Epoch 15/25\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.8978 - accuracy: 0.6586\n",
      "Epoch 16/25\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.8889 - accuracy: 0.6617\n",
      "Epoch 17/25\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.8806 - accuracy: 0.6666\n",
      "Epoch 18/25\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.8726 - accuracy: 0.6696\n",
      "Epoch 19/25\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.8651 - accuracy: 0.6732\n",
      "Epoch 20/25\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.8579 - accuracy: 0.6751\n",
      "Epoch 21/25\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.8513 - accuracy: 0.6788\n",
      "Epoch 22/25\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.8448 - accuracy: 0.6804\n",
      "Epoch 23/25\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.8390 - accuracy: 0.6834\n",
      "Epoch 24/25\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.8334 - accuracy: 0.6853\n",
      "Epoch 25/25\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.8281 - accuracy: 0.6884\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 0.8399 - accuracy: 0.6872\n",
      "Fitting for combination 10\n",
      "(28, 28, 1)\n",
      "1\n",
      "[64]\n",
      "[(4, 4)]\n",
      "2\n",
      "[64, 32, 10]\n",
      "10\n",
      "False\n",
      "['tanh']\n",
      "['sigmoid', 'relu', 'softmax']\n",
      "Adam\n",
      "0.1\n",
      "0.03\n",
      "categorical_crossentropy\n",
      "300\n",
      "25\n",
      "0\n",
      "Epoch 1/25\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 1.5988 - accuracy: 0.3299\n",
      "Epoch 2/25\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 1.1966 - accuracy: 0.5117\n",
      "Epoch 3/25\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 1.0320 - accuracy: 0.5821\n",
      "Epoch 4/25\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.9632 - accuracy: 0.6046\n",
      "Epoch 5/25\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.9501 - accuracy: 0.6078\n",
      "Epoch 6/25\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.9278 - accuracy: 0.6177\n",
      "Epoch 7/25\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.8957 - accuracy: 0.6297\n",
      "Epoch 8/25\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.8720 - accuracy: 0.6367\n",
      "Epoch 9/25\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.8643 - accuracy: 0.6420\n",
      "Epoch 10/25\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.8510 - accuracy: 0.6519\n",
      "Epoch 11/25\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.8357 - accuracy: 0.6513\n",
      "Epoch 12/25\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.8285 - accuracy: 0.6588\n",
      "Epoch 13/25\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.8166 - accuracy: 0.6781\n",
      "Epoch 14/25\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.8181 - accuracy: 0.6659\n",
      "Epoch 15/25\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.7967 - accuracy: 0.6681\n",
      "Epoch 16/25\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.7878 - accuracy: 0.6835\n",
      "Epoch 17/25\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.7784 - accuracy: 0.6861\n",
      "Epoch 18/25\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.7687 - accuracy: 0.7034\n",
      "Epoch 19/25\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.7593 - accuracy: 0.7034\n",
      "Epoch 20/25\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.7544 - accuracy: 0.7034\n",
      "Epoch 21/25\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.7496 - accuracy: 0.7048\n",
      "Epoch 22/25\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.7441 - accuracy: 0.7080\n",
      "Epoch 23/25\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.7370 - accuracy: 0.7088\n",
      "Epoch 24/25\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.7334 - accuracy: 0.7083\n",
      "Epoch 25/25\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.7213 - accuracy: 0.7163\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 0.7431 - accuracy: 0.7092\n",
      "Fitting for combination 11\n",
      "(28, 28, 1)\n",
      "1\n",
      "[64]\n",
      "[(4, 4)]\n",
      "1\n",
      "[32, 10]\n",
      "10\n",
      "False\n",
      "['sigmoid']\n",
      "['relu', 'softmax']\n",
      "Adam\n",
      "0.3\n",
      "0\n",
      "categorical_crossentropy\n",
      "150\n",
      "25\n",
      "0\n",
      "Epoch 1/25\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 62.2048 - accuracy: 0.1012\n",
      "Epoch 2/25\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 2.3140 - accuracy: 0.0996\n",
      "Epoch 3/25\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 2.3159 - accuracy: 0.0987\n",
      "Epoch 4/25\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 2.3178 - accuracy: 0.0995\n",
      "Epoch 5/25\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 2.3166 - accuracy: 0.1005\n",
      "Epoch 6/25\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 2.3161 - accuracy: 0.1010\n",
      "Epoch 7/25\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 2.3168 - accuracy: 0.0983\n",
      "Epoch 8/25\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 2.3202 - accuracy: 0.1001\n",
      "Epoch 9/25\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 2.3186 - accuracy: 0.0989\n",
      "Epoch 10/25\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 2.3164 - accuracy: 0.0985\n",
      "Epoch 11/25\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 2.3195 - accuracy: 0.0978\n",
      "Epoch 12/25\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 2.3176 - accuracy: 0.0994\n",
      "Epoch 13/25\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 2.3189 - accuracy: 0.1001\n",
      "Epoch 14/25\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 2.3190 - accuracy: 0.0983\n",
      "Epoch 15/25\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 2.3179 - accuracy: 0.0992\n",
      "Epoch 16/25\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 2.3181 - accuracy: 0.1001\n",
      "Epoch 17/25\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 2.3186 - accuracy: 0.0992\n",
      "Epoch 18/25\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 2.3165 - accuracy: 0.1009\n",
      "Epoch 19/25\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 2.3187 - accuracy: 0.0989\n",
      "Epoch 20/25\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 2.3182 - accuracy: 0.0998\n",
      "Epoch 21/25\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 2.3178 - accuracy: 0.1006\n",
      "Epoch 22/25\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 2.3190 - accuracy: 0.1006\n",
      "Epoch 23/25\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 2.3183 - accuracy: 0.0990\n",
      "Epoch 24/25\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 2.3199 - accuracy: 0.0986\n",
      "Epoch 25/25\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 2.3176 - accuracy: 0.0993\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 2.3187 - accuracy: 0.1000\n",
      "Fitting for combination 12\n",
      "(28, 28, 1)\n",
      "1\n",
      "[64]\n",
      "[(3, 3)]\n",
      "3\n",
      "[32, 128, 32, 10]\n",
      "10\n",
      "False\n",
      "['relu']\n",
      "['tanh', 'relu', 'sigmoid', 'softmax']\n",
      "Adam\n",
      "0.3\n",
      "0\n",
      "categorical_crossentropy\n",
      "100\n",
      "25\n",
      "0\n",
      "Epoch 1/25\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 2.4101 - accuracy: 0.1005\n",
      "Epoch 2/25\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 2.4033 - accuracy: 0.1009\n",
      "Epoch 3/25\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 2.4035 - accuracy: 0.1008\n",
      "Epoch 4/25\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 2.4121 - accuracy: 0.1003\n",
      "Epoch 5/25\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 2.4104 - accuracy: 0.0996\n",
      "Epoch 6/25\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 2.3937 - accuracy: 0.0998\n",
      "Epoch 7/25\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 2.4127 - accuracy: 0.1010\n",
      "Epoch 8/25\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 2.4135 - accuracy: 0.1001\n",
      "Epoch 9/25\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 2.3994 - accuracy: 0.1011\n",
      "Epoch 10/25\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 2.4030 - accuracy: 0.0998\n",
      "Epoch 11/25\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 2.4098 - accuracy: 0.1000\n",
      "Epoch 12/25\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 2.4065 - accuracy: 0.1008\n",
      "Epoch 13/25\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 2.4077 - accuracy: 0.1011\n",
      "Epoch 14/25\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 2.4016 - accuracy: 0.0987\n",
      "Epoch 15/25\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 2.4077 - accuracy: 0.0956\n",
      "Epoch 16/25\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 2.4122 - accuracy: 0.0995\n",
      "Epoch 17/25\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 2.3975 - accuracy: 0.1009\n",
      "Epoch 18/25\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 2.4057 - accuracy: 0.1005\n",
      "Epoch 19/25\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 2.4031 - accuracy: 0.0992\n",
      "Epoch 20/25\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 2.4016 - accuracy: 0.0992\n",
      "Epoch 21/25\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 2.4129 - accuracy: 0.0976\n",
      "Epoch 22/25\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 2.4019 - accuracy: 0.0992\n",
      "Epoch 23/25\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 2.4014 - accuracy: 0.0994\n",
      "Epoch 24/25\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 2.4195 - accuracy: 0.1013\n",
      "Epoch 25/25\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 2.3983 - accuracy: 0.0998\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 2.4276 - accuracy: 0.1000\n",
      "Fitting for combination 13\n",
      "(28, 28, 1)\n",
      "1\n",
      "[64]\n",
      "[(4, 4)]\n",
      "2\n",
      "[64, 32, 10]\n",
      "10\n",
      "True\n",
      "['tanh']\n",
      "['relu', 'sigmoid', 'softmax']\n",
      "Adam\n",
      "0.3\n",
      "0\n",
      "categorical_crossentropy\n",
      "100\n",
      "25\n",
      "0\n",
      "Epoch 1/25\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 2.7092 - accuracy: 0.1061\n",
      "Epoch 2/25\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 2.7057 - accuracy: 0.0987\n",
      "Epoch 3/25\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 2.6806 - accuracy: 0.1016\n",
      "Epoch 4/25\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 2.6484 - accuracy: 0.0999\n",
      "Epoch 5/25\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 2.6263 - accuracy: 0.1033\n",
      "Epoch 6/25\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 2.6382 - accuracy: 0.0996\n",
      "Epoch 7/25\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 2.6375 - accuracy: 0.1014\n",
      "Epoch 8/25\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 2.6307 - accuracy: 0.1005\n",
      "Epoch 9/25\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 2.6257 - accuracy: 0.1002\n",
      "Epoch 10/25\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 2.6710 - accuracy: 0.1016\n",
      "Epoch 11/25\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 2.6365 - accuracy: 0.1005\n",
      "Epoch 12/25\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 2.6332 - accuracy: 0.1008\n",
      "Epoch 13/25\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 2.6195 - accuracy: 0.1017\n",
      "Epoch 14/25\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 2.6385 - accuracy: 0.1010\n",
      "Epoch 15/25\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 2.6280 - accuracy: 0.0975\n",
      "Epoch 16/25\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 2.6417 - accuracy: 0.1000\n",
      "Epoch 17/25\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 2.6080 - accuracy: 0.0992\n",
      "Epoch 18/25\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 2.6281 - accuracy: 0.0991\n",
      "Epoch 19/25\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 2.6009 - accuracy: 0.1004\n",
      "Epoch 20/25\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 2.6106 - accuracy: 0.0981\n",
      "Epoch 21/25\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 2.6154 - accuracy: 0.0978\n",
      "Epoch 22/25\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 2.6045 - accuracy: 0.0992\n",
      "Epoch 23/25\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 2.5925 - accuracy: 0.0989\n",
      "Epoch 24/25\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 2.6315 - accuracy: 0.1006\n",
      "Epoch 25/25\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 2.6080 - accuracy: 0.0992\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 2.4278 - accuracy: 0.1000\n",
      "Fitting for combination 14\n",
      "(28, 28, 1)\n",
      "1\n",
      "[64]\n",
      "[(3, 3)]\n",
      "1\n",
      "[16, 10]\n",
      "10\n",
      "False\n",
      "['sigmoid']\n",
      "['sigmoid', 'softmax']\n",
      "Adam\n",
      "0.01\n",
      "0\n",
      "categorical_crossentropy\n",
      "50\n",
      "25\n",
      "0\n",
      "Epoch 1/25\n",
      "1200/1200 [==============================] - 3s 2ms/step - loss: 2.3088 - accuracy: 0.1007\n",
      "Epoch 2/25\n",
      "1200/1200 [==============================] - 3s 2ms/step - loss: 2.3081 - accuracy: 0.0990\n",
      "Epoch 3/25\n",
      "1200/1200 [==============================] - 3s 2ms/step - loss: 2.3081 - accuracy: 0.0987\n",
      "Epoch 4/25\n",
      "1200/1200 [==============================] - 3s 2ms/step - loss: 2.3079 - accuracy: 0.1012\n",
      "Epoch 5/25\n",
      "1200/1200 [==============================] - 3s 2ms/step - loss: 2.3078 - accuracy: 0.0981\n",
      "Epoch 6/25\n",
      "1200/1200 [==============================] - 3s 2ms/step - loss: 2.3073 - accuracy: 0.1005\n",
      "Epoch 7/25\n",
      "1200/1200 [==============================] - 3s 2ms/step - loss: 2.3078 - accuracy: 0.0993\n",
      "Epoch 8/25\n",
      "1200/1200 [==============================] - 3s 2ms/step - loss: 2.3084 - accuracy: 0.0991\n",
      "Epoch 9/25\n",
      "1200/1200 [==============================] - 3s 2ms/step - loss: 2.3082 - accuracy: 0.0999\n",
      "Epoch 10/25\n",
      "1200/1200 [==============================] - 3s 2ms/step - loss: 2.3076 - accuracy: 0.0995\n",
      "Epoch 11/25\n",
      "1200/1200 [==============================] - 3s 2ms/step - loss: 2.3081 - accuracy: 0.0982\n",
      "Epoch 12/25\n",
      "1200/1200 [==============================] - 3s 2ms/step - loss: 2.3080 - accuracy: 0.0984\n",
      "Epoch 13/25\n",
      "1200/1200 [==============================] - 3s 2ms/step - loss: 2.3081 - accuracy: 0.0990\n",
      "Epoch 14/25\n",
      "1200/1200 [==============================] - 3s 2ms/step - loss: 2.3079 - accuracy: 0.0988\n",
      "Epoch 15/25\n",
      "1200/1200 [==============================] - 3s 2ms/step - loss: 2.3075 - accuracy: 0.1000\n",
      "Epoch 16/25\n",
      "1200/1200 [==============================] - 3s 2ms/step - loss: 2.3077 - accuracy: 0.1015\n",
      "Epoch 17/25\n",
      "1200/1200 [==============================] - 3s 2ms/step - loss: 2.3077 - accuracy: 0.1003\n",
      "Epoch 18/25\n",
      "1200/1200 [==============================] - 3s 2ms/step - loss: 2.3076 - accuracy: 0.1005\n",
      "Epoch 19/25\n",
      "1200/1200 [==============================] - 3s 2ms/step - loss: 2.3081 - accuracy: 0.0998\n",
      "Epoch 20/25\n",
      "1200/1200 [==============================] - 3s 2ms/step - loss: 2.3079 - accuracy: 0.0994\n",
      "Epoch 21/25\n",
      "1200/1200 [==============================] - 3s 2ms/step - loss: 2.3079 - accuracy: 0.0999\n",
      "Epoch 22/25\n",
      "1200/1200 [==============================] - 3s 2ms/step - loss: 2.3077 - accuracy: 0.1016\n",
      "Epoch 23/25\n",
      "1200/1200 [==============================] - 3s 2ms/step - loss: 2.3081 - accuracy: 0.0977\n",
      "Epoch 24/25\n",
      "1200/1200 [==============================] - 3s 2ms/step - loss: 2.3084 - accuracy: 0.0998\n",
      "Epoch 25/25\n",
      "1200/1200 [==============================] - 3s 2ms/step - loss: 2.3077 - accuracy: 0.1001\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 2.3064 - accuracy: 0.1000\n",
      "Fitting for combination 15\n",
      "(28, 28, 1)\n",
      "1\n",
      "[64]\n",
      "[(3, 3)]\n",
      "2\n",
      "[128, 16, 10]\n",
      "10\n",
      "True\n",
      "['relu']\n",
      "['relu', 'relu', 'softmax']\n",
      "SGD\n",
      "0.03\n",
      "0.03\n",
      "categorical_crossentropy\n",
      "100\n",
      "25\n",
      "0\n",
      "Epoch 1/25\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 1.6678 - accuracy: 0.4037\n",
      "Epoch 2/25\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 1.4611 - accuracy: 0.4689\n",
      "Epoch 3/25\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 1.4171 - accuracy: 0.4769\n",
      "Epoch 4/25\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 1.3827 - accuracy: 0.4897\n",
      "Epoch 5/25\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 1.3632 - accuracy: 0.4904\n",
      "Epoch 6/25\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 1.3447 - accuracy: 0.5000\n",
      "Epoch 7/25\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 1.3345 - accuracy: 0.5058\n",
      "Epoch 8/25\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 1.3262 - accuracy: 0.5073\n",
      "Epoch 9/25\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 1.3224 - accuracy: 0.5059\n",
      "Epoch 10/25\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 1.3129 - accuracy: 0.5112\n",
      "Epoch 11/25\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 1.3077 - accuracy: 0.5091\n",
      "Epoch 12/25\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 1.2942 - accuracy: 0.5139\n",
      "Epoch 13/25\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 1.2912 - accuracy: 0.5149\n",
      "Epoch 14/25\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 1.2938 - accuracy: 0.5148\n",
      "Epoch 15/25\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 1.2834 - accuracy: 0.5213\n",
      "Epoch 16/25\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 1.2799 - accuracy: 0.5205\n",
      "Epoch 17/25\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 1.2732 - accuracy: 0.5242\n",
      "Epoch 18/25\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 1.2704 - accuracy: 0.5246\n",
      "Epoch 19/25\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 1.2740 - accuracy: 0.5236\n",
      "Epoch 20/25\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 1.2688 - accuracy: 0.5240\n",
      "Epoch 21/25\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 1.2646 - accuracy: 0.5264\n",
      "Epoch 22/25\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 1.2625 - accuracy: 0.5260\n",
      "Epoch 23/25\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 1.2605 - accuracy: 0.5284\n",
      "Epoch 24/25\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 1.2568 - accuracy: 0.5305\n",
      "Epoch 25/25\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 1.2507 - accuracy: 0.5348\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 0.8468 - accuracy: 0.7327\n",
      "Fitting for combination 16\n",
      "(28, 28, 1)\n",
      "1\n",
      "[64]\n",
      "[(5, 5)]\n",
      "2\n",
      "[64, 32, 10]\n",
      "10\n",
      "True\n",
      "['tanh']\n",
      "['relu', 'relu', 'softmax']\n",
      "SGD\n",
      "0.01\n",
      "0.03\n",
      "categorical_crossentropy\n",
      "50\n",
      "25\n",
      "0\n",
      "Epoch 1/25\n",
      "1200/1200 [==============================] - 3s 2ms/step - loss: 1.8143 - accuracy: 0.3455\n",
      "Epoch 2/25\n",
      "1200/1200 [==============================] - 3s 2ms/step - loss: 1.6885 - accuracy: 0.3927\n",
      "Epoch 3/25\n",
      "1200/1200 [==============================] - 3s 2ms/step - loss: 1.6510 - accuracy: 0.4044\n",
      "Epoch 4/25\n",
      "1200/1200 [==============================] - 3s 2ms/step - loss: 1.6254 - accuracy: 0.4130\n",
      "Epoch 5/25\n",
      "1200/1200 [==============================] - 3s 2ms/step - loss: 1.6077 - accuracy: 0.4195\n",
      "Epoch 6/25\n",
      "1200/1200 [==============================] - 3s 2ms/step - loss: 1.5986 - accuracy: 0.4223\n",
      "Epoch 7/25\n",
      "1200/1200 [==============================] - 3s 2ms/step - loss: 1.5871 - accuracy: 0.4273\n",
      "Epoch 8/25\n",
      "1200/1200 [==============================] - 3s 2ms/step - loss: 1.5778 - accuracy: 0.4313\n",
      "Epoch 9/25\n",
      "1200/1200 [==============================] - 3s 2ms/step - loss: 1.5692 - accuracy: 0.4354\n",
      "Epoch 10/25\n",
      "1200/1200 [==============================] - 3s 2ms/step - loss: 1.5625 - accuracy: 0.4377\n",
      "Epoch 11/25\n",
      "1200/1200 [==============================] - 3s 2ms/step - loss: 1.5534 - accuracy: 0.4407\n",
      "Epoch 12/25\n",
      "1200/1200 [==============================] - 3s 2ms/step - loss: 1.5467 - accuracy: 0.4439\n",
      "Epoch 13/25\n",
      "1200/1200 [==============================] - 3s 2ms/step - loss: 1.5463 - accuracy: 0.4423\n",
      "Epoch 14/25\n",
      "1200/1200 [==============================] - 3s 2ms/step - loss: 1.5448 - accuracy: 0.4417\n",
      "Epoch 15/25\n",
      "1200/1200 [==============================] - 3s 2ms/step - loss: 1.5380 - accuracy: 0.4469\n",
      "Epoch 16/25\n",
      "1200/1200 [==============================] - 3s 2ms/step - loss: 1.5358 - accuracy: 0.4460\n",
      "Epoch 17/25\n",
      "1200/1200 [==============================] - 3s 2ms/step - loss: 1.5261 - accuracy: 0.4528\n",
      "Epoch 18/25\n",
      "1200/1200 [==============================] - 3s 2ms/step - loss: 1.5286 - accuracy: 0.4484\n",
      "Epoch 19/25\n",
      "1200/1200 [==============================] - 3s 2ms/step - loss: 1.5283 - accuracy: 0.4486\n",
      "Epoch 20/25\n",
      "1200/1200 [==============================] - 3s 2ms/step - loss: 1.5186 - accuracy: 0.4526\n",
      "Epoch 21/25\n",
      "1200/1200 [==============================] - 3s 2ms/step - loss: 1.5154 - accuracy: 0.4541\n",
      "Epoch 22/25\n",
      "1200/1200 [==============================] - 3s 2ms/step - loss: 1.5128 - accuracy: 0.4536\n",
      "Epoch 23/25\n",
      "1200/1200 [==============================] - 3s 2ms/step - loss: 1.5176 - accuracy: 0.4535\n",
      "Epoch 24/25\n",
      "1200/1200 [==============================] - 3s 2ms/step - loss: 1.5098 - accuracy: 0.4540\n",
      "Epoch 25/25\n",
      "1200/1200 [==============================] - 3s 2ms/step - loss: 1.5097 - accuracy: 0.4557\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 1.1526 - accuracy: 0.6644\n",
      "Fitting for combination 17\n",
      "(28, 28, 1)\n",
      "1\n",
      "[64]\n",
      "[(3, 3)]\n",
      "3\n",
      "[128, 16, 16, 10]\n",
      "10\n",
      "True\n",
      "['relu']\n",
      "['sigmoid', 'relu', 'relu', 'softmax']\n",
      "SGD\n",
      "0.03\n",
      "0.01\n",
      "categorical_crossentropy\n",
      "100\n",
      "25\n",
      "0\n",
      "Epoch 1/25\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 2.3129 - accuracy: 0.1050\n",
      "Epoch 2/25\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 2.3035 - accuracy: 0.1089\n",
      "Epoch 3/25\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 2.3022 - accuracy: 0.1117\n",
      "Epoch 4/25\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 2.3021 - accuracy: 0.1127\n",
      "Epoch 5/25\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 2.3014 - accuracy: 0.1117\n",
      "Epoch 6/25\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 2.3013 - accuracy: 0.1104\n",
      "Epoch 7/25\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 2.3010 - accuracy: 0.1099\n",
      "Epoch 8/25\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 2.3007 - accuracy: 0.1136\n",
      "Epoch 9/25\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 2.3007 - accuracy: 0.1146\n",
      "Epoch 10/25\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 2.2990 - accuracy: 0.1138\n",
      "Epoch 11/25\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 2.2996 - accuracy: 0.1138\n",
      "Epoch 12/25\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 2.2995 - accuracy: 0.1154\n",
      "Epoch 13/25\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 2.2990 - accuracy: 0.1169\n",
      "Epoch 14/25\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 2.2983 - accuracy: 0.1166\n",
      "Epoch 15/25\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 2.2987 - accuracy: 0.1165\n",
      "Epoch 16/25\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 2.2979 - accuracy: 0.1173\n",
      "Epoch 17/25\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 2.2976 - accuracy: 0.1180\n",
      "Epoch 18/25\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 2.2974 - accuracy: 0.1187\n",
      "Epoch 19/25\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 2.2974 - accuracy: 0.1183\n",
      "Epoch 20/25\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 2.2963 - accuracy: 0.1200\n",
      "Epoch 21/25\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 2.2969 - accuracy: 0.1179\n",
      "Epoch 22/25\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 2.2970 - accuracy: 0.1196\n",
      "Epoch 23/25\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 2.2959 - accuracy: 0.1210\n",
      "Epoch 24/25\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 2.2965 - accuracy: 0.1199\n",
      "Epoch 25/25\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 2.2967 - accuracy: 0.1200\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 2.2996 - accuracy: 0.1030\n",
      "Fitting for combination 18\n",
      "(28, 28, 1)\n",
      "1\n",
      "[64]\n",
      "[(3, 3)]\n",
      "1\n",
      "[16, 10]\n",
      "10\n",
      "True\n",
      "['tanh']\n",
      "['tanh', 'softmax']\n",
      "SGD\n",
      "0.03\n",
      "0.01\n",
      "categorical_crossentropy\n",
      "100\n",
      "25\n",
      "0\n",
      "Epoch 1/25\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 1.1059 - accuracy: 0.6639\n",
      "Epoch 2/25\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 0.9023 - accuracy: 0.7235\n",
      "Epoch 3/25\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 0.8625 - accuracy: 0.7330\n",
      "Epoch 4/25\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 0.8418 - accuracy: 0.7389\n",
      "Epoch 5/25\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 0.8311 - accuracy: 0.7402\n",
      "Epoch 6/25\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 0.8166 - accuracy: 0.7457\n",
      "Epoch 7/25\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 0.8078 - accuracy: 0.7478\n",
      "Epoch 8/25\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.7979 - accuracy: 0.7509\n",
      "Epoch 9/25\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.7974 - accuracy: 0.7516\n",
      "Epoch 10/25\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 0.7903 - accuracy: 0.7546\n",
      "Epoch 11/25\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 0.7865 - accuracy: 0.7541\n",
      "Epoch 12/25\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 0.7825 - accuracy: 0.7544\n",
      "Epoch 13/25\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 0.7813 - accuracy: 0.7541\n",
      "Epoch 14/25\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 0.7763 - accuracy: 0.7580\n",
      "Epoch 15/25\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 0.7725 - accuracy: 0.7592\n",
      "Epoch 16/25\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 0.7714 - accuracy: 0.7562\n",
      "Epoch 17/25\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 0.7694 - accuracy: 0.7576\n",
      "Epoch 18/25\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 0.7705 - accuracy: 0.7576\n",
      "Epoch 19/25\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 0.7625 - accuracy: 0.7616\n",
      "Epoch 20/25\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 0.7617 - accuracy: 0.7607\n",
      "Epoch 21/25\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 0.7649 - accuracy: 0.7602\n",
      "Epoch 22/25\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 0.7588 - accuracy: 0.7645\n",
      "Epoch 23/25\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 0.7588 - accuracy: 0.7621\n",
      "Epoch 24/25\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 0.7571 - accuracy: 0.7620\n",
      "Epoch 25/25\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 0.7576 - accuracy: 0.7625\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 0.5787 - accuracy: 0.8088\n",
      "Fitting for combination 19\n",
      "(28, 28, 1)\n",
      "1\n",
      "[64]\n",
      "[(4, 4)]\n",
      "2\n",
      "[16, 16, 10]\n",
      "10\n",
      "True\n",
      "['tanh']\n",
      "['sigmoid', 'sigmoid', 'softmax']\n",
      "SGD\n",
      "0.3\n",
      "0\n",
      "categorical_crossentropy\n",
      "150\n",
      "25\n",
      "0\n",
      "Epoch 1/25\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 1.8004 - accuracy: 0.2657\n",
      "Epoch 2/25\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 1.5201 - accuracy: 0.3316\n",
      "Epoch 3/25\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 1.4619 - accuracy: 0.3488\n",
      "Epoch 4/25\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 1.4191 - accuracy: 0.3703\n",
      "Epoch 5/25\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 1.3852 - accuracy: 0.3864\n",
      "Epoch 6/25\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 1.3610 - accuracy: 0.4004\n",
      "Epoch 7/25\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 1.3385 - accuracy: 0.4093\n",
      "Epoch 8/25\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 1.3207 - accuracy: 0.4233\n",
      "Epoch 9/25\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 1.3005 - accuracy: 0.4302\n",
      "Epoch 10/25\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 1.2933 - accuracy: 0.4321\n",
      "Epoch 11/25\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 1.2857 - accuracy: 0.4342\n",
      "Epoch 12/25\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 1.2767 - accuracy: 0.4368\n",
      "Epoch 13/25\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 1.2718 - accuracy: 0.4369\n",
      "Epoch 14/25\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 1.2719 - accuracy: 0.4352\n",
      "Epoch 15/25\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 1.2669 - accuracy: 0.4415\n",
      "Epoch 16/25\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 1.2666 - accuracy: 0.4419\n",
      "Epoch 17/25\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 1.2613 - accuracy: 0.4435\n",
      "Epoch 18/25\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 1.2538 - accuracy: 0.4433\n",
      "Epoch 19/25\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 1.2568 - accuracy: 0.4415\n",
      "Epoch 20/25\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 1.2539 - accuracy: 0.4417\n",
      "Epoch 21/25\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 1.2534 - accuracy: 0.4442\n",
      "Epoch 22/25\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 1.2494 - accuracy: 0.4450\n",
      "Epoch 23/25\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 1.2432 - accuracy: 0.4487\n",
      "Epoch 24/25\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 1.2444 - accuracy: 0.4471\n",
      "Epoch 25/25\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 1.2454 - accuracy: 0.4495\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 0.9520 - accuracy: 0.6586\n",
      "Fitting for combination 20\n",
      "(28, 28, 1)\n",
      "1\n",
      "[128]\n",
      "[(4, 4)]\n",
      "1\n",
      "[16, 10]\n",
      "10\n",
      "False\n",
      "['sigmoid']\n",
      "['sigmoid', 'softmax']\n",
      "Adam\n",
      "0.03\n",
      "0\n",
      "categorical_crossentropy\n",
      "100\n",
      "25\n",
      "0\n",
      "Epoch 1/25\n",
      "600/600 [==============================] - 2s 4ms/step - loss: 2.3101 - accuracy: 0.1006\n",
      "Epoch 2/25\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 2.3089 - accuracy: 0.1005\n",
      "Epoch 3/25\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 2.3090 - accuracy: 0.0986\n",
      "Epoch 4/25\n",
      "600/600 [==============================] - 2s 4ms/step - loss: 2.3095 - accuracy: 0.0994\n",
      "Epoch 5/25\n",
      "600/600 [==============================] - 2s 4ms/step - loss: 2.3084 - accuracy: 0.0989\n",
      "Epoch 6/25\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 2.3086 - accuracy: 0.1014\n",
      "Epoch 7/25\n",
      "600/600 [==============================] - 2s 4ms/step - loss: 2.3087 - accuracy: 0.1009\n",
      "Epoch 8/25\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 2.3101 - accuracy: 0.0980\n",
      "Epoch 9/25\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 2.3090 - accuracy: 0.0992\n",
      "Epoch 10/25\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 2.3086 - accuracy: 0.1015\n",
      "Epoch 11/25\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 2.3094 - accuracy: 0.0982\n",
      "Epoch 12/25\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 2.3091 - accuracy: 0.0975\n",
      "Epoch 13/25\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 2.3091 - accuracy: 0.0986\n",
      "Epoch 14/25\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 2.3096 - accuracy: 0.0988\n",
      "Epoch 15/25\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 2.3087 - accuracy: 0.0986\n",
      "Epoch 16/25\n",
      "600/600 [==============================] - 2s 4ms/step - loss: 2.3086 - accuracy: 0.1015\n",
      "Epoch 17/25\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 2.3090 - accuracy: 0.1004\n",
      "Epoch 18/25\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 2.3088 - accuracy: 0.1006\n",
      "Epoch 19/25\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 2.3094 - accuracy: 0.0993\n",
      "Epoch 20/25\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 2.3091 - accuracy: 0.0983\n",
      "Epoch 21/25\n",
      "600/600 [==============================] - 2s 4ms/step - loss: 2.3091 - accuracy: 0.1000\n",
      "Epoch 22/25\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 2.3092 - accuracy: 0.1009\n",
      "Epoch 23/25\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 2.3095 - accuracy: 0.0985\n",
      "Epoch 24/25\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 2.3091 - accuracy: 0.1006\n",
      "Epoch 25/25\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 2.3086 - accuracy: 0.1006\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 2.3068 - accuracy: 0.1000\n",
      "Fitting for combination 21\n",
      "(28, 28, 1)\n",
      "1\n",
      "[128]\n",
      "[(3, 3)]\n",
      "2\n",
      "[64, 64, 10]\n",
      "10\n",
      "False\n",
      "['sigmoid']\n",
      "['relu', 'relu', 'softmax']\n",
      "Adam\n",
      "0.01\n",
      "0.03\n",
      "categorical_crossentropy\n",
      "200\n",
      "25\n",
      "0\n",
      "Epoch 1/25\n",
      "300/300 [==============================] - 2s 7ms/step - loss: 10.1260 - accuracy: 0.1003\n",
      "Epoch 2/25\n",
      "300/300 [==============================] - 2s 7ms/step - loss: 2.3027 - accuracy: 0.1000\n",
      "Epoch 3/25\n",
      "300/300 [==============================] - 2s 7ms/step - loss: 2.3027 - accuracy: 0.1000\n",
      "Epoch 4/25\n",
      "300/300 [==============================] - 2s 7ms/step - loss: 2.3026 - accuracy: 0.1000\n",
      "Epoch 5/25\n",
      "300/300 [==============================] - 2s 7ms/step - loss: 2.3026 - accuracy: 0.0994\n",
      "Epoch 6/25\n",
      "300/300 [==============================] - 2s 7ms/step - loss: 2.3026 - accuracy: 0.1000\n",
      "Epoch 7/25\n",
      "300/300 [==============================] - 2s 7ms/step - loss: 2.3026 - accuracy: 0.1000\n",
      "Epoch 8/25\n",
      "300/300 [==============================] - 2s 7ms/step - loss: 2.3026 - accuracy: 0.1000\n",
      "Epoch 9/25\n",
      "300/300 [==============================] - 2s 7ms/step - loss: 2.3026 - accuracy: 0.1000\n",
      "Epoch 10/25\n",
      "300/300 [==============================] - 2s 7ms/step - loss: 2.3026 - accuracy: 0.1000\n",
      "Epoch 11/25\n",
      "300/300 [==============================] - 2s 7ms/step - loss: 2.3026 - accuracy: 0.1000\n",
      "Epoch 12/25\n",
      "300/300 [==============================] - 2s 7ms/step - loss: 2.3026 - accuracy: 0.1000\n",
      "Epoch 13/25\n",
      "300/300 [==============================] - 2s 7ms/step - loss: 2.3026 - accuracy: 0.1000\n",
      "Epoch 14/25\n",
      "300/300 [==============================] - 2s 7ms/step - loss: 2.3026 - accuracy: 0.0996\n",
      "Epoch 15/25\n",
      "300/300 [==============================] - 2s 7ms/step - loss: 2.3026 - accuracy: 0.1000\n",
      "Epoch 16/25\n",
      "300/300 [==============================] - 2s 7ms/step - loss: 2.3026 - accuracy: 0.1000\n",
      "Epoch 17/25\n",
      "300/300 [==============================] - 2s 7ms/step - loss: 2.3026 - accuracy: 0.1000\n",
      "Epoch 18/25\n",
      "300/300 [==============================] - 2s 7ms/step - loss: 2.3026 - accuracy: 0.0992\n",
      "Epoch 19/25\n",
      "300/300 [==============================] - 2s 7ms/step - loss: 2.3026 - accuracy: 0.1000\n",
      "Epoch 20/25\n",
      "300/300 [==============================] - 2s 7ms/step - loss: 2.3026 - accuracy: 0.0992\n",
      "Epoch 21/25\n",
      "300/300 [==============================] - 2s 7ms/step - loss: 2.3026 - accuracy: 0.1000\n",
      "Epoch 22/25\n",
      "300/300 [==============================] - 2s 7ms/step - loss: 2.3026 - accuracy: 0.0990\n",
      "Epoch 23/25\n",
      "300/300 [==============================] - 2s 7ms/step - loss: 2.3026 - accuracy: 0.1000\n",
      "Epoch 24/25\n",
      "300/300 [==============================] - 2s 7ms/step - loss: 2.3026 - accuracy: 0.1000\n",
      "Epoch 25/25\n",
      "300/300 [==============================] - 2s 7ms/step - loss: 2.3026 - accuracy: 0.1000\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 2.3026 - accuracy: 0.1000\n",
      "Fitting for combination 22\n",
      "(28, 28, 1)\n",
      "1\n",
      "[128]\n",
      "[(4, 4)]\n",
      "2\n",
      "[128, 16, 10]\n",
      "10\n",
      "False\n",
      "['sigmoid']\n",
      "['sigmoid', 'relu', 'softmax']\n",
      "Adam\n",
      "0.1\n",
      "0.01\n",
      "categorical_crossentropy\n",
      "50\n",
      "25\n",
      "0\n",
      "Epoch 1/25\n",
      "1200/1200 [==============================] - 5s 4ms/step - loss: 2.3082 - accuracy: 0.0988\n",
      "Epoch 2/25\n",
      "1200/1200 [==============================] - 5s 4ms/step - loss: 2.3032 - accuracy: 0.0988\n",
      "Epoch 3/25\n",
      "1200/1200 [==============================] - 5s 4ms/step - loss: 2.3029 - accuracy: 0.0998\n",
      "Epoch 4/25\n",
      "1200/1200 [==============================] - 5s 4ms/step - loss: 2.3029 - accuracy: 0.0979\n",
      "Epoch 5/25\n",
      "1200/1200 [==============================] - 5s 4ms/step - loss: 2.3028 - accuracy: 0.0983\n",
      "Epoch 6/25\n",
      "1200/1200 [==============================] - 5s 4ms/step - loss: 2.3028 - accuracy: 0.0988\n",
      "Epoch 7/25\n",
      "1200/1200 [==============================] - 5s 4ms/step - loss: 2.3028 - accuracy: 0.0987\n",
      "Epoch 8/25\n",
      "1200/1200 [==============================] - 5s 4ms/step - loss: 2.3028 - accuracy: 0.0966\n",
      "Epoch 9/25\n",
      "1200/1200 [==============================] - 5s 4ms/step - loss: 2.3027 - accuracy: 0.0986\n",
      "Epoch 10/25\n",
      "1200/1200 [==============================] - 5s 4ms/step - loss: 2.3027 - accuracy: 0.0984\n",
      "Epoch 11/25\n",
      "1200/1200 [==============================] - 5s 4ms/step - loss: 2.3027 - accuracy: 0.0995\n",
      "Epoch 12/25\n",
      "1200/1200 [==============================] - 5s 4ms/step - loss: 2.3027 - accuracy: 0.0989\n",
      "Epoch 13/25\n",
      "1200/1200 [==============================] - 5s 4ms/step - loss: 2.3027 - accuracy: 0.0962\n",
      "Epoch 14/25\n",
      "1200/1200 [==============================] - 5s 4ms/step - loss: 2.3027 - accuracy: 0.0980\n",
      "Epoch 15/25\n",
      "1200/1200 [==============================] - 5s 4ms/step - loss: 2.3027 - accuracy: 0.0979\n",
      "Epoch 16/25\n",
      "1200/1200 [==============================] - 5s 4ms/step - loss: 2.3027 - accuracy: 0.0982\n",
      "Epoch 17/25\n",
      "1200/1200 [==============================] - 5s 4ms/step - loss: 2.3027 - accuracy: 0.0980\n",
      "Epoch 18/25\n",
      "1200/1200 [==============================] - 5s 4ms/step - loss: 2.3027 - accuracy: 0.0975\n",
      "Epoch 19/25\n",
      "1200/1200 [==============================] - 5s 4ms/step - loss: 2.3027 - accuracy: 0.0968\n",
      "Epoch 20/25\n",
      "1200/1200 [==============================] - 5s 4ms/step - loss: 2.3027 - accuracy: 0.0977\n",
      "Epoch 21/25\n",
      "1200/1200 [==============================] - 5s 4ms/step - loss: 2.3027 - accuracy: 0.0978\n",
      "Epoch 22/25\n",
      "1200/1200 [==============================] - 5s 4ms/step - loss: 2.3027 - accuracy: 0.0969\n",
      "Epoch 23/25\n",
      "1200/1200 [==============================] - 5s 4ms/step - loss: 2.3027 - accuracy: 0.0978\n",
      "Epoch 24/25\n",
      "1200/1200 [==============================] - 5s 4ms/step - loss: 2.3027 - accuracy: 0.0971\n",
      "Epoch 25/25\n",
      "1200/1200 [==============================] - 5s 4ms/step - loss: 2.3026 - accuracy: 0.0998\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 2.3026 - accuracy: 0.1000\n",
      "Fitting for combination 23\n",
      "(28, 28, 1)\n",
      "1\n",
      "[128]\n",
      "[(3, 3)]\n",
      "3\n",
      "[128, 64, 16, 10]\n",
      "10\n",
      "True\n",
      "['relu']\n",
      "['sigmoid', 'sigmoid', 'sigmoid', 'softmax']\n",
      "Adam\n",
      "0.01\n",
      "0\n",
      "categorical_crossentropy\n",
      "150\n",
      "25\n",
      "0\n",
      "Epoch 1/25\n",
      "400/400 [==============================] - 3s 6ms/step - loss: 1.5651 - accuracy: 0.3242\n",
      "Epoch 2/25\n",
      "400/400 [==============================] - 2s 6ms/step - loss: 1.3771 - accuracy: 0.3876\n",
      "Epoch 3/25\n",
      "400/400 [==============================] - 2s 6ms/step - loss: 1.3595 - accuracy: 0.3969\n",
      "Epoch 4/25\n",
      "400/400 [==============================] - 2s 6ms/step - loss: 1.2980 - accuracy: 0.4200\n",
      "Epoch 5/25\n",
      "400/400 [==============================] - 2s 6ms/step - loss: 1.2531 - accuracy: 0.4438\n",
      "Epoch 6/25\n",
      "400/400 [==============================] - 2s 6ms/step - loss: 1.2245 - accuracy: 0.4581\n",
      "Epoch 7/25\n",
      "400/400 [==============================] - 2s 6ms/step - loss: 1.2164 - accuracy: 0.4654\n",
      "Epoch 8/25\n",
      "400/400 [==============================] - 2s 6ms/step - loss: 1.1866 - accuracy: 0.4727\n",
      "Epoch 9/25\n",
      "400/400 [==============================] - 2s 6ms/step - loss: 1.1888 - accuracy: 0.4723\n",
      "Epoch 10/25\n",
      "400/400 [==============================] - 2s 6ms/step - loss: 1.1543 - accuracy: 0.4791\n",
      "Epoch 11/25\n",
      "400/400 [==============================] - 2s 6ms/step - loss: 1.1409 - accuracy: 0.4862\n",
      "Epoch 12/25\n",
      "400/400 [==============================] - 2s 6ms/step - loss: 1.1519 - accuracy: 0.4791\n",
      "Epoch 13/25\n",
      "400/400 [==============================] - 2s 6ms/step - loss: 1.1556 - accuracy: 0.4817\n",
      "Epoch 14/25\n",
      "400/400 [==============================] - 2s 6ms/step - loss: 1.1189 - accuracy: 0.4916\n",
      "Epoch 15/25\n",
      "400/400 [==============================] - 2s 6ms/step - loss: 1.1243 - accuracy: 0.4978\n",
      "Epoch 16/25\n",
      "400/400 [==============================] - 2s 6ms/step - loss: 1.1356 - accuracy: 0.4966\n",
      "Epoch 17/25\n",
      "400/400 [==============================] - 2s 6ms/step - loss: 1.1184 - accuracy: 0.4920\n",
      "Epoch 18/25\n",
      "400/400 [==============================] - 2s 6ms/step - loss: 1.0998 - accuracy: 0.4963\n",
      "Epoch 19/25\n",
      "400/400 [==============================] - 2s 6ms/step - loss: 1.1061 - accuracy: 0.4959\n",
      "Epoch 20/25\n",
      "400/400 [==============================] - 2s 6ms/step - loss: 1.1073 - accuracy: 0.5035\n",
      "Epoch 21/25\n",
      "400/400 [==============================] - 2s 6ms/step - loss: 1.0958 - accuracy: 0.5068\n",
      "Epoch 22/25\n",
      "400/400 [==============================] - 2s 6ms/step - loss: 1.1123 - accuracy: 0.5007\n",
      "Epoch 23/25\n",
      "400/400 [==============================] - 2s 6ms/step - loss: 1.0939 - accuracy: 0.5062\n",
      "Epoch 24/25\n",
      "400/400 [==============================] - 2s 6ms/step - loss: 1.0941 - accuracy: 0.5083\n",
      "Epoch 25/25\n",
      "400/400 [==============================] - 2s 6ms/step - loss: 1.0883 - accuracy: 0.5063\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.8811 - accuracy: 0.5714\n",
      "Fitting for combination 24\n",
      "(28, 28, 1)\n",
      "1\n",
      "[128]\n",
      "[(4, 4)]\n",
      "3\n",
      "[32, 64, 64, 10]\n",
      "10\n",
      "False\n",
      "['relu']\n",
      "['sigmoid', 'relu', 'sigmoid', 'softmax']\n",
      "Adam\n",
      "0.01\n",
      "0\n",
      "categorical_crossentropy\n",
      "150\n",
      "25\n",
      "0\n",
      "Epoch 1/25\n",
      "400/400 [==============================] - 2s 4ms/step - loss: 0.6049 - accuracy: 0.7866\n",
      "Epoch 2/25\n",
      "400/400 [==============================] - 2s 4ms/step - loss: 0.4477 - accuracy: 0.8400\n",
      "Epoch 3/25\n",
      "400/400 [==============================] - 2s 4ms/step - loss: 0.3959 - accuracy: 0.8565\n",
      "Epoch 4/25\n",
      "400/400 [==============================] - 2s 4ms/step - loss: 0.3862 - accuracy: 0.8606\n",
      "Epoch 5/25\n",
      "400/400 [==============================] - 2s 4ms/step - loss: 0.3655 - accuracy: 0.8666\n",
      "Epoch 6/25\n",
      "400/400 [==============================] - 2s 4ms/step - loss: 0.3549 - accuracy: 0.8701\n",
      "Epoch 7/25\n",
      "400/400 [==============================] - 2s 4ms/step - loss: 0.3520 - accuracy: 0.8715\n",
      "Epoch 8/25\n",
      "400/400 [==============================] - 2s 4ms/step - loss: 0.3373 - accuracy: 0.8796\n",
      "Epoch 9/25\n",
      "400/400 [==============================] - 2s 4ms/step - loss: 0.3394 - accuracy: 0.8760\n",
      "Epoch 10/25\n",
      "400/400 [==============================] - 2s 4ms/step - loss: 0.3291 - accuracy: 0.8809\n",
      "Epoch 11/25\n",
      "400/400 [==============================] - 2s 4ms/step - loss: 0.3228 - accuracy: 0.8824\n",
      "Epoch 12/25\n",
      "400/400 [==============================] - 2s 4ms/step - loss: 0.3273 - accuracy: 0.8809\n",
      "Epoch 13/25\n",
      "400/400 [==============================] - 2s 4ms/step - loss: 0.3285 - accuracy: 0.8806\n",
      "Epoch 14/25\n",
      "400/400 [==============================] - 2s 4ms/step - loss: 0.3170 - accuracy: 0.8850\n",
      "Epoch 15/25\n",
      "400/400 [==============================] - 2s 4ms/step - loss: 0.3094 - accuracy: 0.8858\n",
      "Epoch 16/25\n",
      "400/400 [==============================] - 2s 4ms/step - loss: 0.3132 - accuracy: 0.8851\n",
      "Epoch 17/25\n",
      "400/400 [==============================] - 2s 4ms/step - loss: 0.3281 - accuracy: 0.8797\n",
      "Epoch 18/25\n",
      "400/400 [==============================] - 2s 4ms/step - loss: 0.3136 - accuracy: 0.8839\n",
      "Epoch 19/25\n",
      "400/400 [==============================] - 2s 4ms/step - loss: 0.3088 - accuracy: 0.8869\n",
      "Epoch 20/25\n",
      "400/400 [==============================] - 2s 4ms/step - loss: 0.3033 - accuracy: 0.8889\n",
      "Epoch 21/25\n",
      "400/400 [==============================] - 2s 4ms/step - loss: 0.2951 - accuracy: 0.8905\n",
      "Epoch 22/25\n",
      "400/400 [==============================] - 2s 4ms/step - loss: 0.3048 - accuracy: 0.8875\n",
      "Epoch 23/25\n",
      "400/400 [==============================] - 2s 4ms/step - loss: 0.3034 - accuracy: 0.8885\n",
      "Epoch 24/25\n",
      "400/400 [==============================] - 2s 4ms/step - loss: 0.2960 - accuracy: 0.8924\n",
      "Epoch 25/25\n",
      "400/400 [==============================] - 2s 4ms/step - loss: 0.2982 - accuracy: 0.8909\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 0.3725 - accuracy: 0.8704\n",
      "Fitting for combination 25\n",
      "(28, 28, 1)\n",
      "1\n",
      "[128]\n",
      "[(5, 5)]\n",
      "2\n",
      "[32, 64, 10]\n",
      "10\n",
      "False\n",
      "['relu']\n",
      "['sigmoid', 'relu', 'softmax']\n",
      "SGD\n",
      "0.3\n",
      "0.01\n",
      "categorical_crossentropy\n",
      "50\n",
      "25\n",
      "0\n",
      "Epoch 1/25\n",
      "1200/1200 [==============================] - 3s 2ms/step - loss: 0.6112 - accuracy: 0.7717\n",
      "Epoch 2/25\n",
      "1200/1200 [==============================] - 3s 2ms/step - loss: 0.4494 - accuracy: 0.8411\n",
      "Epoch 3/25\n",
      "1200/1200 [==============================] - 3s 2ms/step - loss: 0.4188 - accuracy: 0.8528\n",
      "Epoch 4/25\n",
      "1200/1200 [==============================] - 3s 2ms/step - loss: 0.4022 - accuracy: 0.8572\n",
      "Epoch 5/25\n",
      "1200/1200 [==============================] - 3s 2ms/step - loss: 0.3921 - accuracy: 0.8619\n",
      "Epoch 6/25\n",
      "1200/1200 [==============================] - 3s 2ms/step - loss: 0.3844 - accuracy: 0.8633\n",
      "Epoch 7/25\n",
      "1200/1200 [==============================] - 3s 2ms/step - loss: 0.3782 - accuracy: 0.8657\n",
      "Epoch 8/25\n",
      "1200/1200 [==============================] - 3s 2ms/step - loss: 0.3735 - accuracy: 0.8676\n",
      "Epoch 9/25\n",
      "1200/1200 [==============================] - 3s 2ms/step - loss: 0.3695 - accuracy: 0.8689\n",
      "Epoch 10/25\n",
      "1200/1200 [==============================] - 3s 2ms/step - loss: 0.3663 - accuracy: 0.8706\n",
      "Epoch 11/25\n",
      "1200/1200 [==============================] - 3s 2ms/step - loss: 0.3632 - accuracy: 0.8712\n",
      "Epoch 12/25\n",
      "1200/1200 [==============================] - 3s 2ms/step - loss: 0.3605 - accuracy: 0.8724\n",
      "Epoch 13/25\n",
      "1200/1200 [==============================] - 3s 2ms/step - loss: 0.3583 - accuracy: 0.8726\n",
      "Epoch 14/25\n",
      "1200/1200 [==============================] - 3s 2ms/step - loss: 0.3560 - accuracy: 0.8740\n",
      "Epoch 15/25\n",
      "1200/1200 [==============================] - 3s 2ms/step - loss: 0.3543 - accuracy: 0.8744\n",
      "Epoch 16/25\n",
      "1200/1200 [==============================] - 3s 2ms/step - loss: 0.3526 - accuracy: 0.8746\n",
      "Epoch 17/25\n",
      "1200/1200 [==============================] - 3s 2ms/step - loss: 0.3508 - accuracy: 0.8752\n",
      "Epoch 18/25\n",
      "1200/1200 [==============================] - 3s 2ms/step - loss: 0.3493 - accuracy: 0.8756\n",
      "Epoch 19/25\n",
      "1200/1200 [==============================] - 3s 2ms/step - loss: 0.3479 - accuracy: 0.8763\n",
      "Epoch 20/25\n",
      "1200/1200 [==============================] - 3s 2ms/step - loss: 0.3466 - accuracy: 0.8776\n",
      "Epoch 21/25\n",
      "1200/1200 [==============================] - 3s 2ms/step - loss: 0.3452 - accuracy: 0.8774\n",
      "Epoch 22/25\n",
      "1200/1200 [==============================] - 3s 2ms/step - loss: 0.3438 - accuracy: 0.8778\n",
      "Epoch 23/25\n",
      "1200/1200 [==============================] - 3s 3ms/step - loss: 0.3429 - accuracy: 0.8779\n",
      "Epoch 24/25\n",
      "1200/1200 [==============================] - 3s 3ms/step - loss: 0.3419 - accuracy: 0.8785\n",
      "Epoch 25/25\n",
      "1200/1200 [==============================] - 3s 3ms/step - loss: 0.3410 - accuracy: 0.8794\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 0.3808 - accuracy: 0.8657\n",
      "Fitting for combination 26\n",
      "(28, 28, 1)\n",
      "1\n",
      "[128]\n",
      "[(5, 5)]\n",
      "3\n",
      "[64, 32, 32, 10]\n",
      "10\n",
      "True\n",
      "['sigmoid']\n",
      "['sigmoid', 'tanh', 'relu', 'softmax']\n",
      "SGD\n",
      "0.01\n",
      "0\n",
      "categorical_crossentropy\n",
      "300\n",
      "25\n",
      "0\n",
      "Epoch 1/25\n",
      "200/200 [==============================] - 2s 7ms/step - loss: 2.3137 - accuracy: 0.1005\n",
      "Epoch 2/25\n",
      "200/200 [==============================] - 1s 7ms/step - loss: 2.3028 - accuracy: 0.0988\n",
      "Epoch 3/25\n",
      "200/200 [==============================] - 1s 7ms/step - loss: 2.3027 - accuracy: 0.0995\n",
      "Epoch 4/25\n",
      "200/200 [==============================] - 1s 7ms/step - loss: 2.3026 - accuracy: 0.1022\n",
      "Epoch 5/25\n",
      "200/200 [==============================] - 1s 7ms/step - loss: 2.3026 - accuracy: 0.1015\n",
      "Epoch 6/25\n",
      "200/200 [==============================] - 1s 7ms/step - loss: 2.3026 - accuracy: 0.1014\n",
      "Epoch 7/25\n",
      "200/200 [==============================] - 1s 7ms/step - loss: 2.3025 - accuracy: 0.1003\n",
      "Epoch 8/25\n",
      "200/200 [==============================] - 1s 7ms/step - loss: 2.3026 - accuracy: 0.0999\n",
      "Epoch 9/25\n",
      "200/200 [==============================] - 1s 7ms/step - loss: 2.3026 - accuracy: 0.1021\n",
      "Epoch 10/25\n",
      "200/200 [==============================] - 1s 7ms/step - loss: 2.3025 - accuracy: 0.1033\n",
      "Epoch 11/25\n",
      "200/200 [==============================] - 1s 7ms/step - loss: 2.3024 - accuracy: 0.1005\n",
      "Epoch 12/25\n",
      "200/200 [==============================] - 1s 7ms/step - loss: 2.3023 - accuracy: 0.1055\n",
      "Epoch 13/25\n",
      "200/200 [==============================] - 1s 7ms/step - loss: 2.3022 - accuracy: 0.1034\n",
      "Epoch 14/25\n",
      "200/200 [==============================] - 1s 7ms/step - loss: 2.3018 - accuracy: 0.1054\n",
      "Epoch 15/25\n",
      "200/200 [==============================] - 1s 7ms/step - loss: 2.3016 - accuracy: 0.1081\n",
      "Epoch 16/25\n",
      "200/200 [==============================] - 1s 7ms/step - loss: 2.3008 - accuracy: 0.1085\n",
      "Epoch 17/25\n",
      "200/200 [==============================] - 1s 7ms/step - loss: 2.3007 - accuracy: 0.1113\n",
      "Epoch 18/25\n",
      "200/200 [==============================] - 1s 7ms/step - loss: 2.2989 - accuracy: 0.1129\n",
      "Epoch 19/25\n",
      "200/200 [==============================] - 1s 7ms/step - loss: 2.2968 - accuracy: 0.1172\n",
      "Epoch 20/25\n",
      "200/200 [==============================] - 1s 7ms/step - loss: 2.2935 - accuracy: 0.1235\n",
      "Epoch 21/25\n",
      "200/200 [==============================] - 1s 7ms/step - loss: 2.2896 - accuracy: 0.1291\n",
      "Epoch 22/25\n",
      "200/200 [==============================] - 1s 7ms/step - loss: 2.2789 - accuracy: 0.1401\n",
      "Epoch 23/25\n",
      "200/200 [==============================] - 1s 7ms/step - loss: 2.2688 - accuracy: 0.1502\n",
      "Epoch 24/25\n",
      "200/200 [==============================] - 1s 7ms/step - loss: 2.2561 - accuracy: 0.1598\n",
      "Epoch 25/25\n",
      "200/200 [==============================] - 1s 7ms/step - loss: 2.2338 - accuracy: 0.1773\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 2.1516 - accuracy: 0.1612\n",
      "Fitting for combination 27\n",
      "(28, 28, 1)\n",
      "1\n",
      "[128]\n",
      "[(4, 4)]\n",
      "2\n",
      "[32, 32, 10]\n",
      "10\n",
      "False\n",
      "['tanh']\n",
      "['relu', 'relu', 'softmax']\n",
      "SGD\n",
      "0.03\n",
      "0.03\n",
      "categorical_crossentropy\n",
      "150\n",
      "25\n",
      "0\n",
      "Epoch 1/25\n",
      "400/400 [==============================] - 2s 4ms/step - loss: 0.8797 - accuracy: 0.7042\n",
      "Epoch 2/25\n",
      "400/400 [==============================] - 2s 4ms/step - loss: 0.6434 - accuracy: 0.7808\n",
      "Epoch 3/25\n",
      "400/400 [==============================] - 2s 4ms/step - loss: 0.6086 - accuracy: 0.7929\n",
      "Epoch 4/25\n",
      "400/400 [==============================] - 2s 4ms/step - loss: 0.5901 - accuracy: 0.7997\n",
      "Epoch 5/25\n",
      "400/400 [==============================] - 2s 4ms/step - loss: 0.5785 - accuracy: 0.8039\n",
      "Epoch 6/25\n",
      "400/400 [==============================] - 2s 4ms/step - loss: 0.5697 - accuracy: 0.8055\n",
      "Epoch 7/25\n",
      "400/400 [==============================] - 2s 4ms/step - loss: 0.5633 - accuracy: 0.8081\n",
      "Epoch 8/25\n",
      "400/400 [==============================] - 2s 4ms/step - loss: 0.5580 - accuracy: 0.8093\n",
      "Epoch 9/25\n",
      "400/400 [==============================] - 2s 4ms/step - loss: 0.5534 - accuracy: 0.8104\n",
      "Epoch 10/25\n",
      "400/400 [==============================] - 2s 4ms/step - loss: 0.5498 - accuracy: 0.8118\n",
      "Epoch 11/25\n",
      "400/400 [==============================] - 2s 4ms/step - loss: 0.5466 - accuracy: 0.8130\n",
      "Epoch 12/25\n",
      "400/400 [==============================] - 2s 4ms/step - loss: 0.5437 - accuracy: 0.8131\n",
      "Epoch 13/25\n",
      "400/400 [==============================] - 2s 4ms/step - loss: 0.5413 - accuracy: 0.8138\n",
      "Epoch 14/25\n",
      "400/400 [==============================] - 2s 4ms/step - loss: 0.5389 - accuracy: 0.8148\n",
      "Epoch 15/25\n",
      "400/400 [==============================] - 2s 4ms/step - loss: 0.5369 - accuracy: 0.8155\n",
      "Epoch 16/25\n",
      "400/400 [==============================] - 2s 4ms/step - loss: 0.5351 - accuracy: 0.8163\n",
      "Epoch 17/25\n",
      "400/400 [==============================] - 2s 4ms/step - loss: 0.5334 - accuracy: 0.8163\n",
      "Epoch 18/25\n",
      "400/400 [==============================] - 2s 4ms/step - loss: 0.5318 - accuracy: 0.8173\n",
      "Epoch 19/25\n",
      "400/400 [==============================] - 2s 4ms/step - loss: 0.5303 - accuracy: 0.8173\n",
      "Epoch 20/25\n",
      "400/400 [==============================] - 2s 4ms/step - loss: 0.5289 - accuracy: 0.8182\n",
      "Epoch 21/25\n",
      "400/400 [==============================] - 2s 4ms/step - loss: 0.5276 - accuracy: 0.8178\n",
      "Epoch 22/25\n",
      "400/400 [==============================] - 2s 4ms/step - loss: 0.5264 - accuracy: 0.8186\n",
      "Epoch 23/25\n",
      "400/400 [==============================] - 2s 4ms/step - loss: 0.5253 - accuracy: 0.8192\n",
      "Epoch 24/25\n",
      "400/400 [==============================] - 2s 4ms/step - loss: 0.5243 - accuracy: 0.8194\n",
      "Epoch 25/25\n",
      "400/400 [==============================] - 2s 4ms/step - loss: 0.5232 - accuracy: 0.8195\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 0.5474 - accuracy: 0.8087\n",
      "Fitting for combination 28\n",
      "(28, 28, 1)\n",
      "1\n",
      "[128]\n",
      "[(5, 5)]\n",
      "1\n",
      "[16, 10]\n",
      "10\n",
      "False\n",
      "['sigmoid']\n",
      "['tanh', 'softmax']\n",
      "SGD\n",
      "0.3\n",
      "0.03\n",
      "categorical_crossentropy\n",
      "150\n",
      "25\n",
      "0\n",
      "Epoch 1/25\n",
      "400/400 [==============================] - 2s 4ms/step - loss: 2.3092 - accuracy: 0.0989\n",
      "Epoch 2/25\n",
      "400/400 [==============================] - 2s 4ms/step - loss: 2.3031 - accuracy: 0.0986\n",
      "Epoch 3/25\n",
      "400/400 [==============================] - 2s 4ms/step - loss: 2.3029 - accuracy: 0.0991\n",
      "Epoch 4/25\n",
      "400/400 [==============================] - 2s 4ms/step - loss: 2.3028 - accuracy: 0.0982\n",
      "Epoch 5/25\n",
      "400/400 [==============================] - 2s 4ms/step - loss: 2.3028 - accuracy: 0.0991\n",
      "Epoch 6/25\n",
      "400/400 [==============================] - 2s 4ms/step - loss: 2.3028 - accuracy: 0.0991\n",
      "Epoch 7/25\n",
      "400/400 [==============================] - 2s 4ms/step - loss: 2.3027 - accuracy: 0.0980\n",
      "Epoch 8/25\n",
      "400/400 [==============================] - 2s 4ms/step - loss: 2.3027 - accuracy: 0.0980\n",
      "Epoch 9/25\n",
      "400/400 [==============================] - 2s 4ms/step - loss: 2.3027 - accuracy: 0.0997\n",
      "Epoch 10/25\n",
      "400/400 [==============================] - 2s 4ms/step - loss: 2.3027 - accuracy: 0.0986\n",
      "Epoch 11/25\n",
      "400/400 [==============================] - 2s 4ms/step - loss: 2.3027 - accuracy: 0.0987\n",
      "Epoch 12/25\n",
      "400/400 [==============================] - 2s 4ms/step - loss: 2.3027 - accuracy: 0.0992\n",
      "Epoch 13/25\n",
      "400/400 [==============================] - 2s 4ms/step - loss: 2.3027 - accuracy: 0.0967\n",
      "Epoch 14/25\n",
      "400/400 [==============================] - 2s 4ms/step - loss: 2.3027 - accuracy: 0.0979\n",
      "Epoch 15/25\n",
      "400/400 [==============================] - 2s 4ms/step - loss: 2.3027 - accuracy: 0.0980\n",
      "Epoch 16/25\n",
      "400/400 [==============================] - 2s 4ms/step - loss: 2.3027 - accuracy: 0.0983\n",
      "Epoch 17/25\n",
      "400/400 [==============================] - 2s 4ms/step - loss: 2.3027 - accuracy: 0.0987\n",
      "Epoch 18/25\n",
      "400/400 [==============================] - 2s 4ms/step - loss: 2.3027 - accuracy: 0.0970\n",
      "Epoch 19/25\n",
      "400/400 [==============================] - 2s 4ms/step - loss: 2.3027 - accuracy: 0.0969\n",
      "Epoch 20/25\n",
      "400/400 [==============================] - 2s 4ms/step - loss: 2.3026 - accuracy: 0.0979\n",
      "Epoch 21/25\n",
      "400/400 [==============================] - 2s 4ms/step - loss: 2.3026 - accuracy: 0.0981\n",
      "Epoch 22/25\n",
      "400/400 [==============================] - 2s 4ms/step - loss: 2.3026 - accuracy: 0.0965\n",
      "Epoch 23/25\n",
      "400/400 [==============================] - 2s 4ms/step - loss: 2.3026 - accuracy: 0.0968\n",
      "Epoch 24/25\n",
      "400/400 [==============================] - 2s 4ms/step - loss: 2.3026 - accuracy: 0.0972\n",
      "Epoch 25/25\n",
      "400/400 [==============================] - 2s 4ms/step - loss: 2.3026 - accuracy: 0.0989\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 2.3026 - accuracy: 0.1000\n",
      "Fitting for combination 29\n",
      "(28, 28, 1)\n",
      "1\n",
      "[128]\n",
      "[(2, 2)]\n",
      "2\n",
      "[32, 64, 10]\n",
      "10\n",
      "False\n",
      "['relu']\n",
      "['tanh', 'tanh', 'softmax']\n",
      "SGD\n",
      "0.1\n",
      "0.01\n",
      "categorical_crossentropy\n",
      "100\n",
      "25\n",
      "0\n",
      "Epoch 1/25\n",
      "600/600 [==============================] - 2s 4ms/step - loss: 0.6000 - accuracy: 0.7914\n",
      "Epoch 2/25\n",
      "600/600 [==============================] - 2s 4ms/step - loss: 0.4544 - accuracy: 0.8401\n",
      "Epoch 3/25\n",
      "600/600 [==============================] - 2s 4ms/step - loss: 0.4286 - accuracy: 0.8498\n",
      "Epoch 4/25\n",
      "600/600 [==============================] - 2s 4ms/step - loss: 0.4146 - accuracy: 0.8556\n",
      "Epoch 5/25\n",
      "600/600 [==============================] - 2s 4ms/step - loss: 0.4056 - accuracy: 0.8587\n",
      "Epoch 6/25\n",
      "600/600 [==============================] - 2s 4ms/step - loss: 0.3986 - accuracy: 0.8605\n",
      "Epoch 7/25\n",
      "600/600 [==============================] - 2s 4ms/step - loss: 0.3934 - accuracy: 0.8625\n",
      "Epoch 8/25\n",
      "600/600 [==============================] - 2s 4ms/step - loss: 0.3889 - accuracy: 0.8641\n",
      "Epoch 9/25\n",
      "600/600 [==============================] - 2s 4ms/step - loss: 0.3852 - accuracy: 0.8649\n",
      "Epoch 10/25\n",
      "600/600 [==============================] - 2s 4ms/step - loss: 0.3823 - accuracy: 0.8662\n",
      "Epoch 11/25\n",
      "600/600 [==============================] - 2s 4ms/step - loss: 0.3797 - accuracy: 0.8671\n",
      "Epoch 12/25\n",
      "600/600 [==============================] - 2s 4ms/step - loss: 0.3772 - accuracy: 0.8678\n",
      "Epoch 13/25\n",
      "600/600 [==============================] - 2s 4ms/step - loss: 0.3753 - accuracy: 0.8686\n",
      "Epoch 14/25\n",
      "600/600 [==============================] - 2s 4ms/step - loss: 0.3732 - accuracy: 0.8695\n",
      "Epoch 15/25\n",
      "600/600 [==============================] - 2s 4ms/step - loss: 0.3715 - accuracy: 0.8699\n",
      "Epoch 16/25\n",
      "600/600 [==============================] - 2s 4ms/step - loss: 0.3700 - accuracy: 0.8701\n",
      "Epoch 17/25\n",
      "600/600 [==============================] - 2s 4ms/step - loss: 0.3685 - accuracy: 0.8707\n",
      "Epoch 18/25\n",
      "600/600 [==============================] - 2s 4ms/step - loss: 0.3671 - accuracy: 0.8711\n",
      "Epoch 19/25\n",
      "600/600 [==============================] - 2s 4ms/step - loss: 0.3659 - accuracy: 0.8713\n",
      "Epoch 20/25\n",
      "600/600 [==============================] - 2s 4ms/step - loss: 0.3647 - accuracy: 0.8719\n",
      "Epoch 21/25\n",
      "600/600 [==============================] - 2s 4ms/step - loss: 0.3636 - accuracy: 0.8724\n",
      "Epoch 22/25\n",
      "600/600 [==============================] - 2s 4ms/step - loss: 0.3623 - accuracy: 0.8730\n",
      "Epoch 23/25\n",
      "600/600 [==============================] - 2s 4ms/step - loss: 0.3616 - accuracy: 0.8728\n",
      "Epoch 24/25\n",
      "600/600 [==============================] - 2s 4ms/step - loss: 0.3607 - accuracy: 0.8738\n",
      "Epoch 25/25\n",
      "600/600 [==============================] - 2s 4ms/step - loss: 0.3597 - accuracy: 0.8737\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 0.4011 - accuracy: 0.8632\n",
      "Fitting for combination 30\n",
      "(28, 28, 1)\n",
      "2\n",
      "[32, 32]\n",
      "[(2, 2), (2, 2)]\n",
      "2\n",
      "[64, 64, 10]\n",
      "10\n",
      "True\n",
      "['relu', 'sigmoid']\n",
      "['relu', 'relu', 'softmax']\n",
      "Adam\n",
      "0.3\n",
      "0.03\n",
      "categorical_crossentropy\n",
      "50\n",
      "25\n",
      "0\n",
      "Epoch 1/25\n",
      "1200/1200 [==============================] - 4s 3ms/step - loss: 8.5278 - accuracy: 0.0983\n",
      "Epoch 2/25\n",
      "1200/1200 [==============================] - 3s 3ms/step - loss: 2.3033 - accuracy: 0.0982\n",
      "Epoch 3/25\n",
      "1200/1200 [==============================] - 3s 3ms/step - loss: 2.3030 - accuracy: 0.1001\n",
      "Epoch 4/25\n",
      "1200/1200 [==============================] - 3s 3ms/step - loss: 2.3029 - accuracy: 0.0977\n",
      "Epoch 5/25\n",
      "1200/1200 [==============================] - 3s 3ms/step - loss: 2.3028 - accuracy: 0.0983\n",
      "Epoch 6/25\n",
      "1200/1200 [==============================] - 3s 3ms/step - loss: 2.3028 - accuracy: 0.0989\n",
      "Epoch 7/25\n",
      "1200/1200 [==============================] - 3s 3ms/step - loss: 2.3028 - accuracy: 0.0987\n",
      "Epoch 8/25\n",
      "1200/1200 [==============================] - 3s 3ms/step - loss: 2.3028 - accuracy: 0.0966\n",
      "Epoch 9/25\n",
      "1200/1200 [==============================] - 3s 3ms/step - loss: 2.3027 - accuracy: 0.0987\n",
      "Epoch 10/25\n",
      "1200/1200 [==============================] - 3s 3ms/step - loss: 2.3027 - accuracy: 0.0984\n",
      "Epoch 11/25\n",
      "1200/1200 [==============================] - 3s 3ms/step - loss: 2.3027 - accuracy: 0.0995\n",
      "Epoch 12/25\n",
      "1200/1200 [==============================] - 3s 3ms/step - loss: 2.3027 - accuracy: 0.0989\n",
      "Epoch 13/25\n",
      "1200/1200 [==============================] - 3s 3ms/step - loss: 2.3027 - accuracy: 0.0964\n",
      "Epoch 14/25\n",
      "1200/1200 [==============================] - 3s 3ms/step - loss: 2.3027 - accuracy: 0.0979\n",
      "Epoch 15/25\n",
      "1200/1200 [==============================] - 3s 3ms/step - loss: 2.3027 - accuracy: 0.0981\n",
      "Epoch 16/25\n",
      "1200/1200 [==============================] - 3s 3ms/step - loss: 2.3027 - accuracy: 0.0982\n",
      "Epoch 17/25\n",
      "1200/1200 [==============================] - 3s 3ms/step - loss: 2.3027 - accuracy: 0.0980\n",
      "Epoch 18/25\n",
      "1200/1200 [==============================] - 3s 3ms/step - loss: 2.3027 - accuracy: 0.0977\n",
      "Epoch 19/25\n",
      "1200/1200 [==============================] - 3s 3ms/step - loss: 2.3027 - accuracy: 0.0968\n",
      "Epoch 20/25\n",
      "1200/1200 [==============================] - 3s 3ms/step - loss: 2.3027 - accuracy: 0.0977\n",
      "Epoch 21/25\n",
      "1200/1200 [==============================] - 3s 3ms/step - loss: 2.3027 - accuracy: 0.0977\n",
      "Epoch 22/25\n",
      "1200/1200 [==============================] - 3s 3ms/step - loss: 2.3027 - accuracy: 0.0969\n",
      "Epoch 23/25\n",
      "1200/1200 [==============================] - 3s 3ms/step - loss: 2.3027 - accuracy: 0.0979\n",
      "Epoch 24/25\n",
      "1200/1200 [==============================] - 3s 3ms/step - loss: 2.3027 - accuracy: 0.0971\n",
      "Epoch 25/25\n",
      "1200/1200 [==============================] - 3s 3ms/step - loss: 2.3026 - accuracy: 0.0998\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 2.3026 - accuracy: 0.1000\n",
      "Fitting for combination 31\n",
      "(28, 28, 1)\n",
      "2\n",
      "[32, 32]\n",
      "[(2, 2), (2, 2)]\n",
      "3\n",
      "[64, 128, 32, 10]\n",
      "10\n",
      "True\n",
      "['relu', 'tanh']\n",
      "['relu', 'relu', 'tanh', 'softmax']\n",
      "Adam\n",
      "0.1\n",
      "0.01\n",
      "categorical_crossentropy\n",
      "200\n",
      "25\n",
      "0\n",
      "Epoch 1/25\n",
      "300/300 [==============================] - 1s 3ms/step - loss: 2.4204 - accuracy: 0.0993\n",
      "Epoch 2/25\n",
      "300/300 [==============================] - 1s 3ms/step - loss: 2.3390 - accuracy: 0.0996\n",
      "Epoch 3/25\n",
      "300/300 [==============================] - 1s 4ms/step - loss: 2.3267 - accuracy: 0.1003\n",
      "Epoch 4/25\n",
      "300/300 [==============================] - 1s 4ms/step - loss: 2.3211 - accuracy: 0.1006\n",
      "Epoch 5/25\n",
      "300/300 [==============================] - 1s 4ms/step - loss: 2.3177 - accuracy: 0.0985\n",
      "Epoch 6/25\n",
      "300/300 [==============================] - 1s 4ms/step - loss: 2.3158 - accuracy: 0.0997\n",
      "Epoch 7/25\n",
      "300/300 [==============================] - 1s 4ms/step - loss: 2.3140 - accuracy: 0.1020\n",
      "Epoch 8/25\n",
      "300/300 [==============================] - 1s 4ms/step - loss: 2.3130 - accuracy: 0.0993\n",
      "Epoch 9/25\n",
      "300/300 [==============================] - 1s 4ms/step - loss: 2.3106 - accuracy: 0.0997\n",
      "Epoch 10/25\n",
      "300/300 [==============================] - 1s 4ms/step - loss: 2.3110 - accuracy: 0.1003\n",
      "Epoch 11/25\n",
      "300/300 [==============================] - 1s 4ms/step - loss: 2.3101 - accuracy: 0.0986\n",
      "Epoch 12/25\n",
      "300/300 [==============================] - 1s 3ms/step - loss: 2.3095 - accuracy: 0.0997\n",
      "Epoch 13/25\n",
      "300/300 [==============================] - 1s 3ms/step - loss: 2.3090 - accuracy: 0.1000\n",
      "Epoch 14/25\n",
      "300/300 [==============================] - 1s 4ms/step - loss: 2.3084 - accuracy: 0.0987\n",
      "Epoch 15/25\n",
      "300/300 [==============================] - 1s 4ms/step - loss: 2.3079 - accuracy: 0.0991\n",
      "Epoch 16/25\n",
      "300/300 [==============================] - 1s 4ms/step - loss: 2.3077 - accuracy: 0.1002\n",
      "Epoch 17/25\n",
      "300/300 [==============================] - 1s 4ms/step - loss: 2.3074 - accuracy: 0.0994\n",
      "Epoch 18/25\n",
      "300/300 [==============================] - 1s 3ms/step - loss: 2.3073 - accuracy: 0.1010\n",
      "Epoch 19/25\n",
      "300/300 [==============================] - 1s 3ms/step - loss: 2.3069 - accuracy: 0.1005\n",
      "Epoch 20/25\n",
      "300/300 [==============================] - 1s 4ms/step - loss: 2.3067 - accuracy: 0.1009\n",
      "Epoch 21/25\n",
      "300/300 [==============================] - 1s 4ms/step - loss: 2.3068 - accuracy: 0.1005\n",
      "Epoch 22/25\n",
      "300/300 [==============================] - 1s 3ms/step - loss: 2.3065 - accuracy: 0.0999\n",
      "Epoch 23/25\n",
      "300/300 [==============================] - 1s 3ms/step - loss: 2.3061 - accuracy: 0.1002\n",
      "Epoch 24/25\n",
      "300/300 [==============================] - 1s 4ms/step - loss: 2.3065 - accuracy: 0.0992\n",
      "Epoch 25/25\n",
      "300/300 [==============================] - 1s 4ms/step - loss: 2.3061 - accuracy: 0.1008\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 2.3034 - accuracy: 0.1000\n",
      "Fitting for combination 32\n",
      "(28, 28, 1)\n",
      "2\n",
      "[32, 32]\n",
      "[(2, 2), (5, 5)]\n",
      "1\n",
      "[128, 10]\n",
      "10\n",
      "True\n",
      "['relu', 'sigmoid']\n",
      "['tanh', 'softmax']\n",
      "Adam\n",
      "0.01\n",
      "0.03\n",
      "categorical_crossentropy\n",
      "300\n",
      "25\n",
      "0\n",
      "Epoch 1/25\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 1.7260 - accuracy: 0.3952\n",
      "Epoch 2/25\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.9642 - accuracy: 0.6545\n",
      "Epoch 3/25\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.8262 - accuracy: 0.7097\n",
      "Epoch 4/25\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.7629 - accuracy: 0.7333\n",
      "Epoch 5/25\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.7270 - accuracy: 0.7443\n",
      "Epoch 6/25\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.6997 - accuracy: 0.7548\n",
      "Epoch 7/25\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.6812 - accuracy: 0.7597\n",
      "Epoch 8/25\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.6693 - accuracy: 0.7646\n",
      "Epoch 9/25\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.6560 - accuracy: 0.7681\n",
      "Epoch 10/25\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.6441 - accuracy: 0.7725\n",
      "Epoch 11/25\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.6346 - accuracy: 0.7740\n",
      "Epoch 12/25\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.6284 - accuracy: 0.7758\n",
      "Epoch 13/25\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.6236 - accuracy: 0.7799\n",
      "Epoch 14/25\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.6128 - accuracy: 0.7836\n",
      "Epoch 15/25\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.6112 - accuracy: 0.7835\n",
      "Epoch 16/25\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.6073 - accuracy: 0.7826\n",
      "Epoch 17/25\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.6017 - accuracy: 0.7862\n",
      "Epoch 18/25\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.5993 - accuracy: 0.7866\n",
      "Epoch 19/25\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.5976 - accuracy: 0.7870\n",
      "Epoch 20/25\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.5943 - accuracy: 0.7889\n",
      "Epoch 21/25\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.5902 - accuracy: 0.7897\n",
      "Epoch 22/25\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.5866 - accuracy: 0.7905\n",
      "Epoch 23/25\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.5852 - accuracy: 0.7923\n",
      "Epoch 24/25\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.5804 - accuracy: 0.7921\n",
      "Epoch 25/25\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.5830 - accuracy: 0.7904\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 0.4429 - accuracy: 0.8449\n",
      "Fitting for combination 33\n",
      "(28, 28, 1)\n",
      "2\n",
      "[32, 32]\n",
      "[(2, 2), (5, 5)]\n",
      "3\n",
      "[128, 16, 64, 10]\n",
      "10\n",
      "True\n",
      "['relu', 'sigmoid']\n",
      "['sigmoid', 'tanh', 'relu', 'softmax']\n",
      "Adam\n",
      "0.03\n",
      "0.01\n",
      "categorical_crossentropy\n",
      "100\n",
      "25\n",
      "0\n",
      "Epoch 1/25\n",
      "600/600 [==============================] - 2s 4ms/step - loss: 2.3074 - accuracy: 0.0998\n",
      "Epoch 2/25\n",
      "600/600 [==============================] - 2s 4ms/step - loss: 2.3029 - accuracy: 0.0987\n",
      "Epoch 3/25\n",
      "600/600 [==============================] - 2s 4ms/step - loss: 2.3028 - accuracy: 0.0999\n",
      "Epoch 4/25\n",
      "600/600 [==============================] - 2s 4ms/step - loss: 2.3028 - accuracy: 0.0983\n",
      "Epoch 5/25\n",
      "600/600 [==============================] - 2s 4ms/step - loss: 2.3028 - accuracy: 0.0985\n",
      "Epoch 6/25\n",
      "600/600 [==============================] - 2s 4ms/step - loss: 2.3027 - accuracy: 0.0970\n",
      "Epoch 7/25\n",
      "600/600 [==============================] - 2s 4ms/step - loss: 2.3027 - accuracy: 0.0978\n",
      "Epoch 8/25\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 2.3027 - accuracy: 0.0985\n",
      "Epoch 9/25\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 2.3027 - accuracy: 0.0983\n",
      "Epoch 10/25\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 2.3027 - accuracy: 0.0983\n",
      "Epoch 11/25\n",
      "600/600 [==============================] - 2s 4ms/step - loss: 2.3026 - accuracy: 0.0983\n",
      "Epoch 12/25\n",
      "600/600 [==============================] - 2s 4ms/step - loss: 2.3026 - accuracy: 0.0996\n",
      "Epoch 13/25\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 2.3027 - accuracy: 0.0969\n",
      "Epoch 14/25\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 2.3026 - accuracy: 0.0981\n",
      "Epoch 15/25\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 2.3026 - accuracy: 0.0986\n",
      "Epoch 16/25\n",
      "600/600 [==============================] - 2s 4ms/step - loss: 2.3027 - accuracy: 0.0978\n",
      "Epoch 17/25\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 2.3027 - accuracy: 0.0980\n",
      "Epoch 18/25\n",
      "600/600 [==============================] - 2s 4ms/step - loss: 2.3026 - accuracy: 0.0977\n",
      "Epoch 19/25\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 2.3026 - accuracy: 0.0978\n",
      "Epoch 20/25\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 2.3026 - accuracy: 0.0970\n",
      "Epoch 21/25\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 2.3026 - accuracy: 0.0979\n",
      "Epoch 22/25\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 2.3026 - accuracy: 0.0973\n",
      "Epoch 23/25\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 2.3026 - accuracy: 0.0975\n",
      "Epoch 24/25\n",
      "600/600 [==============================] - 2s 4ms/step - loss: 2.3026 - accuracy: 0.0971\n",
      "Epoch 25/25\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 2.3026 - accuracy: 0.0986\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 2.3026 - accuracy: 0.1000\n",
      "Fitting for combination 34\n",
      "(28, 28, 1)\n",
      "2\n",
      "[32, 32]\n",
      "[(2, 2), (5, 5)]\n",
      "3\n",
      "[32, 16, 64, 10]\n",
      "10\n",
      "True\n",
      "['relu', 'tanh']\n",
      "['sigmoid', 'tanh', 'relu', 'softmax']\n",
      "Adam\n",
      "0.3\n",
      "0.03\n",
      "categorical_crossentropy\n",
      "300\n",
      "25\n",
      "0\n",
      "Epoch 1/25\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 2.3703 - accuracy: 0.1006\n",
      "Epoch 2/25\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 2.3043 - accuracy: 0.0996\n",
      "Epoch 3/25\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 2.3035 - accuracy: 0.0997\n",
      "Epoch 4/25\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 2.3047 - accuracy: 0.0992\n",
      "Epoch 5/25\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 2.3032 - accuracy: 0.0997\n",
      "Epoch 6/25\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 2.3041 - accuracy: 0.0975\n",
      "Epoch 7/25\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 2.3030 - accuracy: 0.0989\n",
      "Epoch 8/25\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 2.3030 - accuracy: 0.0996\n",
      "Epoch 9/25\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 2.3034 - accuracy: 0.0986\n",
      "Epoch 10/25\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 2.3037 - accuracy: 0.0974\n",
      "Epoch 11/25\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 2.3031 - accuracy: 0.0997\n",
      "Epoch 12/25\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 2.3028 - accuracy: 0.0994\n",
      "Epoch 13/25\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 2.3029 - accuracy: 0.0964\n",
      "Epoch 14/25\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 2.3028 - accuracy: 0.0988\n",
      "Epoch 15/25\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 2.3028 - accuracy: 0.0981\n",
      "Epoch 16/25\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 2.3028 - accuracy: 0.0993\n",
      "Epoch 17/25\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 2.3028 - accuracy: 0.0988\n",
      "Epoch 18/25\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 2.3032 - accuracy: 0.0967\n",
      "Epoch 19/25\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 2.3027 - accuracy: 0.0977\n",
      "Epoch 20/25\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 2.3027 - accuracy: 0.0979\n",
      "Epoch 21/25\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 2.3027 - accuracy: 0.1001\n",
      "Epoch 22/25\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 2.3029 - accuracy: 0.0974\n",
      "Epoch 23/25\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 2.3030 - accuracy: 0.0988\n",
      "Epoch 24/25\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 2.3031 - accuracy: 0.0958\n",
      "Epoch 25/25\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 2.3027 - accuracy: 0.0994\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 2.3026 - accuracy: 0.1000\n",
      "Fitting for combination 35\n",
      "(28, 28, 1)\n",
      "2\n",
      "[32, 32]\n",
      "[(2, 2), (3, 3)]\n",
      "3\n",
      "[16, 128, 32, 10]\n",
      "10\n",
      "True\n",
      "['relu', 'relu']\n",
      "['tanh', 'sigmoid', 'sigmoid', 'softmax']\n",
      "SGD\n",
      "0.03\n",
      "0\n",
      "categorical_crossentropy\n",
      "100\n",
      "25\n",
      "0\n",
      "Epoch 1/25\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 2.3719 - accuracy: 0.1014\n",
      "Epoch 2/25\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 2.3074 - accuracy: 0.1108\n",
      "Epoch 3/25\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 2.2215 - accuracy: 0.1602\n",
      "Epoch 4/25\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 1.9220 - accuracy: 0.2224\n",
      "Epoch 5/25\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 1.7187 - accuracy: 0.2624\n",
      "Epoch 6/25\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 1.5942 - accuracy: 0.3039\n",
      "Epoch 7/25\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 1.4976 - accuracy: 0.3461\n",
      "Epoch 8/25\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 1.4360 - accuracy: 0.3673\n",
      "Epoch 9/25\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 1.3899 - accuracy: 0.3845\n",
      "Epoch 10/25\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 1.3574 - accuracy: 0.3929\n",
      "Epoch 11/25\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 1.3396 - accuracy: 0.3995\n",
      "Epoch 12/25\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 1.3165 - accuracy: 0.4070\n",
      "Epoch 13/25\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 1.2933 - accuracy: 0.4172\n",
      "Epoch 14/25\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 1.2722 - accuracy: 0.4266\n",
      "Epoch 15/25\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 1.2544 - accuracy: 0.4360\n",
      "Epoch 16/25\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 1.2367 - accuracy: 0.4458\n",
      "Epoch 17/25\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 1.2172 - accuracy: 0.4576\n",
      "Epoch 18/25\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 1.2007 - accuracy: 0.4656\n",
      "Epoch 19/25\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 1.1843 - accuracy: 0.4733\n",
      "Epoch 20/25\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 1.1735 - accuracy: 0.4828\n",
      "Epoch 21/25\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 1.1589 - accuracy: 0.4918\n",
      "Epoch 22/25\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 1.1397 - accuracy: 0.4971\n",
      "Epoch 23/25\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 1.1291 - accuracy: 0.5031\n",
      "Epoch 24/25\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 1.1166 - accuracy: 0.5089\n",
      "Epoch 25/25\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 1.1070 - accuracy: 0.5118\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 0.8622 - accuracy: 0.6858\n",
      "Fitting for combination 36\n",
      "(28, 28, 1)\n",
      "2\n",
      "[32, 32]\n",
      "[(2, 2), (5, 5)]\n",
      "2\n",
      "[128, 32, 10]\n",
      "10\n",
      "True\n",
      "['relu', 'relu']\n",
      "['relu', 'relu', 'softmax']\n",
      "SGD\n",
      "0.01\n",
      "0\n",
      "categorical_crossentropy\n",
      "100\n",
      "25\n",
      "0\n",
      "Epoch 1/25\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 2.1105 - accuracy: 0.2305\n",
      "Epoch 2/25\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 1.4330 - accuracy: 0.4735\n",
      "Epoch 3/25\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 1.0863 - accuracy: 0.5952\n",
      "Epoch 4/25\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 0.9433 - accuracy: 0.6492\n",
      "Epoch 5/25\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 0.8752 - accuracy: 0.6765\n",
      "Epoch 6/25\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 0.8195 - accuracy: 0.6997\n",
      "Epoch 7/25\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 0.7840 - accuracy: 0.7123\n",
      "Epoch 8/25\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 0.7483 - accuracy: 0.7272\n",
      "Epoch 9/25\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 0.7254 - accuracy: 0.7370\n",
      "Epoch 10/25\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 0.7004 - accuracy: 0.7462\n",
      "Epoch 11/25\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 0.6811 - accuracy: 0.7527\n",
      "Epoch 12/25\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 0.6612 - accuracy: 0.7625\n",
      "Epoch 13/25\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 0.6504 - accuracy: 0.7660\n",
      "Epoch 14/25\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 0.6365 - accuracy: 0.7734\n",
      "Epoch 15/25\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 0.6268 - accuracy: 0.7770\n",
      "Epoch 16/25\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 0.6169 - accuracy: 0.7802\n",
      "Epoch 17/25\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 0.6029 - accuracy: 0.7857\n",
      "Epoch 18/25\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 0.5939 - accuracy: 0.7882\n",
      "Epoch 19/25\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 0.5850 - accuracy: 0.7934\n",
      "Epoch 20/25\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 0.5783 - accuracy: 0.7976\n",
      "Epoch 21/25\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 0.5716 - accuracy: 0.7991\n",
      "Epoch 22/25\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 0.5631 - accuracy: 0.8030\n",
      "Epoch 23/25\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 0.5520 - accuracy: 0.8052\n",
      "Epoch 24/25\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 0.5473 - accuracy: 0.8075\n",
      "Epoch 25/25\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 0.5407 - accuracy: 0.8104\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 0.4150 - accuracy: 0.8483\n",
      "Fitting for combination 37\n",
      "(28, 28, 1)\n",
      "2\n",
      "[32, 32]\n",
      "[(2, 2), (5, 5)]\n",
      "1\n",
      "[32, 10]\n",
      "10\n",
      "True\n",
      "['relu', 'sigmoid']\n",
      "['sigmoid', 'softmax']\n",
      "SGD\n",
      "0.1\n",
      "0.01\n",
      "categorical_crossentropy\n",
      "300\n",
      "25\n",
      "0\n",
      "Epoch 1/25\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 2.3239 - accuracy: 0.1028\n",
      "Epoch 2/25\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 2.3018 - accuracy: 0.1084\n",
      "Epoch 3/25\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 2.2986 - accuracy: 0.1156\n",
      "Epoch 4/25\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 2.2948 - accuracy: 0.1214\n",
      "Epoch 5/25\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 2.2922 - accuracy: 0.1219\n",
      "Epoch 6/25\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 2.2890 - accuracy: 0.1282\n",
      "Epoch 7/25\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 2.2861 - accuracy: 0.1290\n",
      "Epoch 8/25\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 2.2815 - accuracy: 0.1366\n",
      "Epoch 9/25\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 2.2758 - accuracy: 0.1415\n",
      "Epoch 10/25\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 2.2709 - accuracy: 0.1459\n",
      "Epoch 11/25\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 2.2675 - accuracy: 0.1455\n",
      "Epoch 12/25\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 2.2612 - accuracy: 0.1507\n",
      "Epoch 13/25\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 2.2528 - accuracy: 0.1584\n",
      "Epoch 14/25\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 2.2451 - accuracy: 0.1593\n",
      "Epoch 15/25\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 2.2388 - accuracy: 0.1659\n",
      "Epoch 16/25\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 2.2315 - accuracy: 0.1697\n",
      "Epoch 17/25\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 2.2239 - accuracy: 0.1724\n",
      "Epoch 18/25\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 2.2136 - accuracy: 0.1819\n",
      "Epoch 19/25\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 2.2042 - accuracy: 0.1841\n",
      "Epoch 20/25\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 2.1955 - accuracy: 0.1877\n",
      "Epoch 21/25\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 2.1865 - accuracy: 0.1936\n",
      "Epoch 22/25\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 2.1759 - accuracy: 0.1964\n",
      "Epoch 23/25\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 2.1646 - accuracy: 0.2007\n",
      "Epoch 24/25\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 2.1563 - accuracy: 0.2036\n",
      "Epoch 25/25\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 2.1457 - accuracy: 0.2061\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 2.1046 - accuracy: 0.3578\n",
      "Fitting for combination 38\n",
      "(28, 28, 1)\n",
      "2\n",
      "[32, 32]\n",
      "[(2, 2), (5, 5)]\n",
      "2\n",
      "[128, 64, 10]\n",
      "10\n",
      "True\n",
      "['relu', 'relu']\n",
      "['relu', 'relu', 'softmax']\n",
      "SGD\n",
      "0.1\n",
      "0.03\n",
      "categorical_crossentropy\n",
      "100\n",
      "25\n",
      "0\n",
      "Epoch 1/25\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 1.4023 - accuracy: 0.4811\n",
      "Epoch 2/25\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 1.0768 - accuracy: 0.6021\n",
      "Epoch 3/25\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 1.0040 - accuracy: 0.6333\n",
      "Epoch 4/25\n",
      "600/600 [==============================] - 2s 4ms/step - loss: 0.9685 - accuracy: 0.6467\n",
      "Epoch 5/25\n",
      "600/600 [==============================] - 2s 4ms/step - loss: 0.9421 - accuracy: 0.6564\n",
      "Epoch 6/25\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 0.9167 - accuracy: 0.6646\n",
      "Epoch 7/25\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 0.9112 - accuracy: 0.6665\n",
      "Epoch 8/25\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 0.8965 - accuracy: 0.6722\n",
      "Epoch 9/25\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 0.8909 - accuracy: 0.6739\n",
      "Epoch 10/25\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 0.8827 - accuracy: 0.6747\n",
      "Epoch 11/25\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 0.8793 - accuracy: 0.6789\n",
      "Epoch 12/25\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 0.8705 - accuracy: 0.6841\n",
      "Epoch 13/25\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 0.8621 - accuracy: 0.6825\n",
      "Epoch 14/25\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 0.8607 - accuracy: 0.6842\n",
      "Epoch 15/25\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 0.8595 - accuracy: 0.6868\n",
      "Epoch 16/25\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 0.8547 - accuracy: 0.6884\n",
      "Epoch 17/25\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 0.8504 - accuracy: 0.6894\n",
      "Epoch 18/25\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 0.8485 - accuracy: 0.6932\n",
      "Epoch 19/25\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 0.8406 - accuracy: 0.6945\n",
      "Epoch 20/25\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 0.8440 - accuracy: 0.6910\n",
      "Epoch 21/25\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 0.8380 - accuracy: 0.6942\n",
      "Epoch 22/25\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 0.8380 - accuracy: 0.6944\n",
      "Epoch 23/25\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 0.8340 - accuracy: 0.6970\n",
      "Epoch 24/25\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 0.8298 - accuracy: 0.6951\n",
      "Epoch 25/25\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 0.8273 - accuracy: 0.6969\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 0.6426 - accuracy: 0.7604\n",
      "Fitting for combination 39\n",
      "(28, 28, 1)\n",
      "2\n",
      "[32, 32]\n",
      "[(2, 2), (4, 4)]\n",
      "2\n",
      "[32, 128, 10]\n",
      "10\n",
      "True\n",
      "['relu', 'sigmoid']\n",
      "['relu', 'tanh', 'softmax']\n",
      "SGD\n",
      "0.03\n",
      "0\n",
      "categorical_crossentropy\n",
      "200\n",
      "25\n",
      "0\n",
      "Epoch 1/25\n",
      "300/300 [==============================] - 2s 4ms/step - loss: 2.3043 - accuracy: 0.0973\n",
      "Epoch 2/25\n",
      "300/300 [==============================] - 1s 4ms/step - loss: 2.3028 - accuracy: 0.0977\n",
      "Epoch 3/25\n",
      "300/300 [==============================] - 1s 4ms/step - loss: 2.3027 - accuracy: 0.0999\n",
      "Epoch 4/25\n",
      "300/300 [==============================] - 1s 4ms/step - loss: 2.3028 - accuracy: 0.1014\n",
      "Epoch 5/25\n",
      "300/300 [==============================] - 1s 4ms/step - loss: 2.3028 - accuracy: 0.1009\n",
      "Epoch 6/25\n",
      "300/300 [==============================] - 1s 4ms/step - loss: 2.3027 - accuracy: 0.0988\n",
      "Epoch 7/25\n",
      "300/300 [==============================] - 1s 4ms/step - loss: 2.3028 - accuracy: 0.0983\n",
      "Epoch 8/25\n",
      "300/300 [==============================] - 1s 4ms/step - loss: 2.3028 - accuracy: 0.0988\n",
      "Epoch 9/25\n",
      "300/300 [==============================] - 1s 4ms/step - loss: 2.3028 - accuracy: 0.0981\n",
      "Epoch 10/25\n",
      "300/300 [==============================] - 1s 4ms/step - loss: 2.3029 - accuracy: 0.0994\n",
      "Epoch 11/25\n",
      "300/300 [==============================] - 1s 4ms/step - loss: 2.3028 - accuracy: 0.0985\n",
      "Epoch 12/25\n",
      "300/300 [==============================] - 1s 4ms/step - loss: 2.3028 - accuracy: 0.0975\n",
      "Epoch 13/25\n",
      "300/300 [==============================] - 1s 4ms/step - loss: 2.3028 - accuracy: 0.0977\n",
      "Epoch 14/25\n",
      "300/300 [==============================] - 1s 4ms/step - loss: 2.3027 - accuracy: 0.1002\n",
      "Epoch 15/25\n",
      "300/300 [==============================] - 1s 4ms/step - loss: 2.3028 - accuracy: 0.0985\n",
      "Epoch 16/25\n",
      "300/300 [==============================] - 1s 4ms/step - loss: 2.3028 - accuracy: 0.0989\n",
      "Epoch 17/25\n",
      "300/300 [==============================] - 1s 4ms/step - loss: 2.3028 - accuracy: 0.0970\n",
      "Epoch 18/25\n",
      "300/300 [==============================] - 1s 4ms/step - loss: 2.3027 - accuracy: 0.1014\n",
      "Epoch 19/25\n",
      "300/300 [==============================] - 1s 4ms/step - loss: 2.3028 - accuracy: 0.0981\n",
      "Epoch 20/25\n",
      "300/300 [==============================] - 1s 4ms/step - loss: 2.3027 - accuracy: 0.1021\n",
      "Epoch 21/25\n",
      "300/300 [==============================] - 1s 4ms/step - loss: 2.3028 - accuracy: 0.0985\n",
      "Epoch 22/25\n",
      "300/300 [==============================] - 1s 4ms/step - loss: 2.3027 - accuracy: 0.1013\n",
      "Epoch 23/25\n",
      "300/300 [==============================] - 1s 4ms/step - loss: 2.3027 - accuracy: 0.0992\n",
      "Epoch 24/25\n",
      "300/300 [==============================] - 1s 4ms/step - loss: 2.3028 - accuracy: 0.0981\n",
      "Epoch 25/25\n",
      "300/300 [==============================] - 1s 4ms/step - loss: 2.3029 - accuracy: 0.0974\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 2.3026 - accuracy: 0.1000\n",
      "Fitting for combination 40\n",
      "(28, 28, 1)\n",
      "2\n",
      "[32, 64]\n",
      "[(2, 2), (4, 4)]\n",
      "3\n",
      "[128, 16, 64, 10]\n",
      "10\n",
      "True\n",
      "['relu', 'tanh']\n",
      "['tanh', 'tanh', 'tanh', 'softmax']\n",
      "Adam\n",
      "0.03\n",
      "0.01\n",
      "categorical_crossentropy\n",
      "300\n",
      "25\n",
      "0\n",
      "Epoch 1/25\n",
      "200/200 [==============================] - 2s 6ms/step - loss: 1.5118 - accuracy: 0.3779\n",
      "Epoch 2/25\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 1.2232 - accuracy: 0.4810\n",
      "Epoch 3/25\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 1.1268 - accuracy: 0.5281\n",
      "Epoch 4/25\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 1.0843 - accuracy: 0.5518\n",
      "Epoch 5/25\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 1.0498 - accuracy: 0.5732\n",
      "Epoch 6/25\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 1.0151 - accuracy: 0.5974\n",
      "Epoch 7/25\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 0.9776 - accuracy: 0.6143\n",
      "Epoch 8/25\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 0.9500 - accuracy: 0.6271\n",
      "Epoch 9/25\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 0.9331 - accuracy: 0.6380\n",
      "Epoch 10/25\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 0.9257 - accuracy: 0.6432\n",
      "Epoch 11/25\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 0.9192 - accuracy: 0.6437\n",
      "Epoch 12/25\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 0.9095 - accuracy: 0.6474\n",
      "Epoch 13/25\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 0.9012 - accuracy: 0.6564\n",
      "Epoch 14/25\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 0.8886 - accuracy: 0.6611\n",
      "Epoch 15/25\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 0.8784 - accuracy: 0.6652\n",
      "Epoch 16/25\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 0.8798 - accuracy: 0.6648\n",
      "Epoch 17/25\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 0.8806 - accuracy: 0.6645\n",
      "Epoch 18/25\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 0.8737 - accuracy: 0.6658\n",
      "Epoch 19/25\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 0.8615 - accuracy: 0.6713\n",
      "Epoch 20/25\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 0.8571 - accuracy: 0.6707\n",
      "Epoch 21/25\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 0.8563 - accuracy: 0.6721\n",
      "Epoch 22/25\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 0.8577 - accuracy: 0.6749\n",
      "Epoch 23/25\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 0.8482 - accuracy: 0.6757\n",
      "Epoch 24/25\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 0.8465 - accuracy: 0.6805\n",
      "Epoch 25/25\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 0.8486 - accuracy: 0.6764\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 0.6890 - accuracy: 0.7406\n",
      "Fitting for combination 41\n",
      "(28, 28, 1)\n",
      "2\n",
      "[32, 64]\n",
      "[(2, 2), (5, 5)]\n",
      "3\n",
      "[64, 128, 32, 10]\n",
      "10\n",
      "False\n",
      "['relu', 'tanh']\n",
      "['sigmoid', 'sigmoid', 'tanh', 'softmax']\n",
      "Adam\n",
      "0.1\n",
      "0.01\n",
      "categorical_crossentropy\n",
      "50\n",
      "25\n",
      "0\n",
      "Epoch 1/25\n",
      "1200/1200 [==============================] - 4s 3ms/step - loss: 2.3610 - accuracy: 0.0988\n",
      "Epoch 2/25\n",
      "1200/1200 [==============================] - 4s 3ms/step - loss: 2.3210 - accuracy: 0.0988\n",
      "Epoch 3/25\n",
      "1200/1200 [==============================] - 4s 3ms/step - loss: 2.3141 - accuracy: 0.0990\n",
      "Epoch 4/25\n",
      "1200/1200 [==============================] - 4s 3ms/step - loss: 2.3104 - accuracy: 0.0990\n",
      "Epoch 5/25\n",
      "1200/1200 [==============================] - 4s 3ms/step - loss: 2.3089 - accuracy: 0.0986\n",
      "Epoch 6/25\n",
      "1200/1200 [==============================] - 4s 3ms/step - loss: 2.3072 - accuracy: 0.1013\n",
      "Epoch 7/25\n",
      "1200/1200 [==============================] - 4s 3ms/step - loss: 2.3069 - accuracy: 0.1008\n",
      "Epoch 8/25\n",
      "1200/1200 [==============================] - 4s 3ms/step - loss: 2.3070 - accuracy: 0.1001\n",
      "Epoch 9/25\n",
      "1200/1200 [==============================] - 4s 3ms/step - loss: 2.3061 - accuracy: 0.0997\n",
      "Epoch 10/25\n",
      "1200/1200 [==============================] - 4s 3ms/step - loss: 2.3055 - accuracy: 0.1015\n",
      "Epoch 11/25\n",
      "1200/1200 [==============================] - 4s 3ms/step - loss: 2.3055 - accuracy: 0.0975\n",
      "Epoch 12/25\n",
      "1200/1200 [==============================] - 4s 3ms/step - loss: 2.3052 - accuracy: 0.0986\n",
      "Epoch 13/25\n",
      "1200/1200 [==============================] - 4s 3ms/step - loss: 2.3051 - accuracy: 0.0984\n",
      "Epoch 14/25\n",
      "1200/1200 [==============================] - 4s 3ms/step - loss: 2.3050 - accuracy: 0.1001\n",
      "Epoch 15/25\n",
      "1200/1200 [==============================] - 4s 3ms/step - loss: 2.3045 - accuracy: 0.1000\n",
      "Epoch 16/25\n",
      "1200/1200 [==============================] - 4s 3ms/step - loss: 2.3044 - accuracy: 0.1019\n",
      "Epoch 17/25\n",
      "1200/1200 [==============================] - 4s 3ms/step - loss: 2.3043 - accuracy: 0.1001\n",
      "Epoch 18/25\n",
      "1200/1200 [==============================] - 4s 3ms/step - loss: 2.3044 - accuracy: 0.0997\n",
      "Epoch 19/25\n",
      "1200/1200 [==============================] - 4s 3ms/step - loss: 2.3044 - accuracy: 0.0989\n",
      "Epoch 20/25\n",
      "1200/1200 [==============================] - 4s 3ms/step - loss: 2.3042 - accuracy: 0.0997\n",
      "Epoch 21/25\n",
      "1200/1200 [==============================] - 4s 3ms/step - loss: 2.3041 - accuracy: 0.1010\n",
      "Epoch 22/25\n",
      "1200/1200 [==============================] - 4s 3ms/step - loss: 2.3041 - accuracy: 0.0982\n",
      "Epoch 23/25\n",
      "1200/1200 [==============================] - 4s 3ms/step - loss: 2.3040 - accuracy: 0.0976\n",
      "Epoch 24/25\n",
      "1200/1200 [==============================] - 4s 3ms/step - loss: 2.3040 - accuracy: 0.0993\n",
      "Epoch 25/25\n",
      "1200/1200 [==============================] - 4s 3ms/step - loss: 2.3037 - accuracy: 0.1009\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 2.3030 - accuracy: 0.1000\n",
      "Fitting for combination 42\n",
      "(28, 28, 1)\n",
      "2\n",
      "[32, 64]\n",
      "[(2, 2), (3, 3)]\n",
      "3\n",
      "[32, 64, 64, 10]\n",
      "10\n",
      "False\n",
      "['relu', 'tanh']\n",
      "['relu', 'tanh', 'relu', 'softmax']\n",
      "Adam\n",
      "0.01\n",
      "0\n",
      "categorical_crossentropy\n",
      "100\n",
      "25\n",
      "0\n",
      "Epoch 1/25\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 0.5602 - accuracy: 0.7972\n",
      "Epoch 2/25\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 0.5506 - accuracy: 0.8044\n",
      "Epoch 3/25\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 0.6519 - accuracy: 0.7621\n",
      "Epoch 4/25\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 0.6620 - accuracy: 0.7573\n",
      "Epoch 5/25\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 0.5903 - accuracy: 0.7785\n",
      "Epoch 6/25\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 0.6565 - accuracy: 0.7508\n",
      "Epoch 7/25\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 0.6467 - accuracy: 0.7514\n",
      "Epoch 8/25\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 0.8901 - accuracy: 0.6639\n",
      "Epoch 9/25\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 0.6211 - accuracy: 0.7697\n",
      "Epoch 10/25\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 0.6612 - accuracy: 0.7516\n",
      "Epoch 11/25\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 0.7158 - accuracy: 0.7209\n",
      "Epoch 12/25\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 0.7723 - accuracy: 0.7027\n",
      "Epoch 13/25\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 0.7478 - accuracy: 0.7139\n",
      "Epoch 14/25\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 0.7478 - accuracy: 0.7027\n",
      "Epoch 15/25\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 0.7586 - accuracy: 0.7113\n",
      "Epoch 16/25\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 0.8425 - accuracy: 0.6780\n",
      "Epoch 17/25\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 1.3677 - accuracy: 0.4412\n",
      "Epoch 18/25\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 1.1810 - accuracy: 0.5386\n",
      "Epoch 19/25\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 1.2626 - accuracy: 0.4721\n",
      "Epoch 20/25\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 1.2878 - accuracy: 0.4825\n",
      "Epoch 21/25\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 1.1557 - accuracy: 0.5423\n",
      "Epoch 22/25\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 0.9685 - accuracy: 0.6191\n",
      "Epoch 23/25\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 0.8700 - accuracy: 0.6674\n",
      "Epoch 24/25\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 1.5153 - accuracy: 0.4148\n",
      "Epoch 25/25\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 1.3490 - accuracy: 0.4126\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 1.4597 - accuracy: 0.3341\n",
      "Fitting for combination 43\n",
      "(28, 28, 1)\n",
      "2\n",
      "[32, 64]\n",
      "[(2, 2), (5, 5)]\n",
      "2\n",
      "[16, 32, 10]\n",
      "10\n",
      "True\n",
      "['relu', 'tanh']\n",
      "['tanh', 'sigmoid', 'softmax']\n",
      "Adam\n",
      "0.3\n",
      "0.01\n",
      "categorical_crossentropy\n",
      "200\n",
      "25\n",
      "0\n",
      "Epoch 1/25\n",
      "300/300 [==============================] - 2s 5ms/step - loss: 2.3269 - accuracy: 0.0997\n",
      "Epoch 2/25\n",
      "300/300 [==============================] - 1s 5ms/step - loss: 2.3054 - accuracy: 0.1006\n",
      "Epoch 3/25\n",
      "300/300 [==============================] - 1s 5ms/step - loss: 2.3042 - accuracy: 0.1002\n",
      "Epoch 4/25\n",
      "300/300 [==============================] - 1s 5ms/step - loss: 2.3041 - accuracy: 0.1010\n",
      "Epoch 5/25\n",
      "300/300 [==============================] - 1s 5ms/step - loss: 2.3038 - accuracy: 0.0990\n",
      "Epoch 6/25\n",
      "300/300 [==============================] - 1s 5ms/step - loss: 2.3037 - accuracy: 0.0992\n",
      "Epoch 7/25\n",
      "300/300 [==============================] - 1s 5ms/step - loss: 2.3034 - accuracy: 0.0990\n",
      "Epoch 8/25\n",
      "300/300 [==============================] - 1s 5ms/step - loss: 2.3034 - accuracy: 0.0980\n",
      "Epoch 9/25\n",
      "300/300 [==============================] - 1s 5ms/step - loss: 2.3033 - accuracy: 0.1006\n",
      "Epoch 10/25\n",
      "300/300 [==============================] - 1s 5ms/step - loss: 2.3033 - accuracy: 0.0988\n",
      "Epoch 11/25\n",
      "300/300 [==============================] - 1s 5ms/step - loss: 2.3031 - accuracy: 0.0984\n",
      "Epoch 12/25\n",
      "300/300 [==============================] - 1s 5ms/step - loss: 2.3031 - accuracy: 0.1010\n",
      "Epoch 13/25\n",
      "300/300 [==============================] - 1s 5ms/step - loss: 2.3031 - accuracy: 0.0983\n",
      "Epoch 14/25\n",
      "300/300 [==============================] - 1s 5ms/step - loss: 2.3030 - accuracy: 0.0994\n",
      "Epoch 15/25\n",
      "300/300 [==============================] - 1s 5ms/step - loss: 2.3030 - accuracy: 0.1007\n",
      "Epoch 16/25\n",
      "300/300 [==============================] - 1s 5ms/step - loss: 2.3030 - accuracy: 0.1006\n",
      "Epoch 17/25\n",
      "300/300 [==============================] - 1s 5ms/step - loss: 2.3031 - accuracy: 0.0978\n",
      "Epoch 18/25\n",
      "300/300 [==============================] - 1s 5ms/step - loss: 2.3031 - accuracy: 0.0964\n",
      "Epoch 19/25\n",
      "300/300 [==============================] - 1s 5ms/step - loss: 2.3029 - accuracy: 0.0976\n",
      "Epoch 20/25\n",
      "300/300 [==============================] - 1s 5ms/step - loss: 2.3030 - accuracy: 0.0969\n",
      "Epoch 21/25\n",
      "300/300 [==============================] - 1s 5ms/step - loss: 2.3029 - accuracy: 0.0985\n",
      "Epoch 22/25\n",
      "300/300 [==============================] - 1s 5ms/step - loss: 2.3029 - accuracy: 0.0980\n",
      "Epoch 23/25\n",
      "300/300 [==============================] - 1s 5ms/step - loss: 2.3029 - accuracy: 0.0980\n",
      "Epoch 24/25\n",
      "300/300 [==============================] - 1s 5ms/step - loss: 2.3029 - accuracy: 0.0974\n",
      "Epoch 25/25\n",
      "300/300 [==============================] - 1s 5ms/step - loss: 2.3028 - accuracy: 0.1000\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 2.3027 - accuracy: 0.1000\n",
      "Fitting for combination 44\n",
      "(28, 28, 1)\n",
      "2\n",
      "[32, 64]\n",
      "[(2, 2), (2, 2)]\n",
      "3\n",
      "[128, 16, 32, 10]\n",
      "10\n",
      "False\n",
      "['relu', 'relu']\n",
      "['sigmoid', 'sigmoid', 'tanh', 'softmax']\n",
      "Adam\n",
      "0.03\n",
      "0\n",
      "categorical_crossentropy\n",
      "100\n",
      "25\n",
      "0\n",
      "Epoch 1/25\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 1.6295 - accuracy: 0.3378\n",
      "Epoch 2/25\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 1.5078 - accuracy: 0.3776\n",
      "Epoch 3/25\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 1.4735 - accuracy: 0.3666\n",
      "Epoch 4/25\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 1.4260 - accuracy: 0.3785\n",
      "Epoch 5/25\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 1.3273 - accuracy: 0.4495\n",
      "Epoch 6/25\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 1.2461 - accuracy: 0.4854\n",
      "Epoch 7/25\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 2.0244 - accuracy: 0.2110\n",
      "Epoch 8/25\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 2.3097 - accuracy: 0.0992\n",
      "Epoch 9/25\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 2.3079 - accuracy: 0.0991\n",
      "Epoch 10/25\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 2.3072 - accuracy: 0.1001\n",
      "Epoch 11/25\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 2.3072 - accuracy: 0.0977\n",
      "Epoch 12/25\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 2.3068 - accuracy: 0.0992\n",
      "Epoch 13/25\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 2.3070 - accuracy: 0.0989\n",
      "Epoch 14/25\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 2.3071 - accuracy: 0.0997\n",
      "Epoch 15/25\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 2.3061 - accuracy: 0.1014\n",
      "Epoch 16/25\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 2.3066 - accuracy: 0.1005\n",
      "Epoch 17/25\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 2.3073 - accuracy: 0.1009\n",
      "Epoch 18/25\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 2.3072 - accuracy: 0.0991\n",
      "Epoch 19/25\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 2.3078 - accuracy: 0.1000\n",
      "Epoch 20/25\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 2.3069 - accuracy: 0.1001\n",
      "Epoch 21/25\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 2.3067 - accuracy: 0.0995\n",
      "Epoch 22/25\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 2.3074 - accuracy: 0.1001\n",
      "Epoch 23/25\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 2.3076 - accuracy: 0.0985\n",
      "Epoch 24/25\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 2.3083 - accuracy: 0.0981\n",
      "Epoch 25/25\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 2.3070 - accuracy: 0.1005\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 2.3080 - accuracy: 0.1000\n",
      "Fitting for combination 45\n",
      "(28, 28, 1)\n",
      "2\n",
      "[32, 64]\n",
      "[(2, 2), (4, 4)]\n",
      "2\n",
      "[64, 128, 10]\n",
      "10\n",
      "False\n",
      "['relu', 'sigmoid']\n",
      "['sigmoid', 'tanh', 'softmax']\n",
      "SGD\n",
      "0.01\n",
      "0\n",
      "categorical_crossentropy\n",
      "50\n",
      "25\n",
      "0\n",
      "Epoch 1/25\n",
      "1200/1200 [==============================] - 4s 3ms/step - loss: 2.2981 - accuracy: 0.1179\n",
      "Epoch 2/25\n",
      "1200/1200 [==============================] - 3s 3ms/step - loss: 1.8568 - accuracy: 0.4140\n",
      "Epoch 3/25\n",
      "1200/1200 [==============================] - 3s 3ms/step - loss: 0.9711 - accuracy: 0.6624\n",
      "Epoch 4/25\n",
      "1200/1200 [==============================] - 3s 3ms/step - loss: 0.7559 - accuracy: 0.7281\n",
      "Epoch 5/25\n",
      "1200/1200 [==============================] - 3s 3ms/step - loss: 0.6549 - accuracy: 0.7517\n",
      "Epoch 6/25\n",
      "1200/1200 [==============================] - 3s 3ms/step - loss: 0.5991 - accuracy: 0.7720\n",
      "Epoch 7/25\n",
      "1200/1200 [==============================] - 4s 3ms/step - loss: 0.5590 - accuracy: 0.7883\n",
      "Epoch 8/25\n",
      "1200/1200 [==============================] - 3s 3ms/step - loss: 0.5273 - accuracy: 0.8026\n",
      "Epoch 9/25\n",
      "1200/1200 [==============================] - 3s 3ms/step - loss: 0.5008 - accuracy: 0.8140\n",
      "Epoch 10/25\n",
      "1200/1200 [==============================] - 3s 3ms/step - loss: 0.4787 - accuracy: 0.8233\n",
      "Epoch 11/25\n",
      "1200/1200 [==============================] - 3s 3ms/step - loss: 0.4590 - accuracy: 0.8301\n",
      "Epoch 12/25\n",
      "1200/1200 [==============================] - 3s 3ms/step - loss: 0.4422 - accuracy: 0.8377\n",
      "Epoch 13/25\n",
      "1200/1200 [==============================] - 4s 3ms/step - loss: 0.4263 - accuracy: 0.8450\n",
      "Epoch 14/25\n",
      "1200/1200 [==============================] - 3s 3ms/step - loss: 0.4125 - accuracy: 0.8506\n",
      "Epoch 15/25\n",
      "1200/1200 [==============================] - 3s 3ms/step - loss: 0.3998 - accuracy: 0.8554\n",
      "Epoch 16/25\n",
      "1200/1200 [==============================] - 4s 3ms/step - loss: 0.3893 - accuracy: 0.8573\n",
      "Epoch 17/25\n",
      "1200/1200 [==============================] - 4s 3ms/step - loss: 0.3795 - accuracy: 0.8629\n",
      "Epoch 18/25\n",
      "1200/1200 [==============================] - 3s 3ms/step - loss: 0.3715 - accuracy: 0.8658\n",
      "Epoch 19/25\n",
      "1200/1200 [==============================] - 3s 3ms/step - loss: 0.3629 - accuracy: 0.8679\n",
      "Epoch 20/25\n",
      "1200/1200 [==============================] - 4s 3ms/step - loss: 0.3559 - accuracy: 0.8713\n",
      "Epoch 21/25\n",
      "1200/1200 [==============================] - 4s 3ms/step - loss: 0.3493 - accuracy: 0.8731\n",
      "Epoch 22/25\n",
      "1200/1200 [==============================] - 3s 3ms/step - loss: 0.3423 - accuracy: 0.8766\n",
      "Epoch 23/25\n",
      "1200/1200 [==============================] - 3s 3ms/step - loss: 0.3371 - accuracy: 0.8779\n",
      "Epoch 24/25\n",
      "1200/1200 [==============================] - 3s 3ms/step - loss: 0.3321 - accuracy: 0.8800\n",
      "Epoch 25/25\n",
      "1200/1200 [==============================] - 3s 3ms/step - loss: 0.3267 - accuracy: 0.8810\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 0.3522 - accuracy: 0.8732\n",
      "Fitting for combination 46\n",
      "(28, 28, 1)\n",
      "2\n",
      "[32, 64]\n",
      "[(2, 2), (3, 3)]\n",
      "1\n",
      "[64, 10]\n",
      "10\n",
      "False\n",
      "['relu', 'relu']\n",
      "['sigmoid', 'softmax']\n",
      "SGD\n",
      "0.1\n",
      "0.03\n",
      "categorical_crossentropy\n",
      "300\n",
      "25\n",
      "0\n",
      "Epoch 1/25\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 1.3963 - accuracy: 0.5626\n",
      "Epoch 2/25\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.9104 - accuracy: 0.7019\n",
      "Epoch 3/25\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.8424 - accuracy: 0.7237\n",
      "Epoch 4/25\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 0.8078 - accuracy: 0.7333\n",
      "Epoch 5/25\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.7852 - accuracy: 0.7386\n",
      "Epoch 6/25\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.7685 - accuracy: 0.7424\n",
      "Epoch 7/25\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.7559 - accuracy: 0.7455\n",
      "Epoch 8/25\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.7459 - accuracy: 0.7488\n",
      "Epoch 9/25\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.7375 - accuracy: 0.7505\n",
      "Epoch 10/25\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.7304 - accuracy: 0.7524\n",
      "Epoch 11/25\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.7243 - accuracy: 0.7538\n",
      "Epoch 12/25\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.7190 - accuracy: 0.7552\n",
      "Epoch 13/25\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.7142 - accuracy: 0.7564\n",
      "Epoch 14/25\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 0.7099 - accuracy: 0.7577\n",
      "Epoch 15/25\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 0.7061 - accuracy: 0.7583\n",
      "Epoch 16/25\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 0.7026 - accuracy: 0.7593\n",
      "Epoch 17/25\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.6994 - accuracy: 0.7603\n",
      "Epoch 18/25\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.6964 - accuracy: 0.7612\n",
      "Epoch 19/25\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.6937 - accuracy: 0.7620\n",
      "Epoch 20/25\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.6912 - accuracy: 0.7629\n",
      "Epoch 21/25\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.6888 - accuracy: 0.7634\n",
      "Epoch 22/25\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 0.6865 - accuracy: 0.7644\n",
      "Epoch 23/25\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.6844 - accuracy: 0.7639\n",
      "Epoch 24/25\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.6825 - accuracy: 0.7647\n",
      "Epoch 25/25\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 0.6806 - accuracy: 0.7653\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 0.7017 - accuracy: 0.7544\n",
      "Fitting for combination 47\n",
      "(28, 28, 1)\n",
      "2\n",
      "[32, 64]\n",
      "[(2, 2), (5, 5)]\n",
      "3\n",
      "[128, 128, 32, 10]\n",
      "10\n",
      "True\n",
      "['relu', 'relu']\n",
      "['tanh', 'sigmoid', 'tanh', 'softmax']\n",
      "SGD\n",
      "0.1\n",
      "0\n",
      "categorical_crossentropy\n",
      "200\n",
      "25\n",
      "0\n",
      "Epoch 1/25\n",
      "300/300 [==============================] - 2s 5ms/step - loss: 1.6281 - accuracy: 0.3632\n",
      "Epoch 2/25\n",
      "300/300 [==============================] - 1s 5ms/step - loss: 1.0407 - accuracy: 0.5919\n",
      "Epoch 3/25\n",
      "300/300 [==============================] - 1s 5ms/step - loss: 0.8307 - accuracy: 0.6839\n",
      "Epoch 4/25\n",
      "300/300 [==============================] - 1s 5ms/step - loss: 0.7433 - accuracy: 0.7177\n",
      "Epoch 5/25\n",
      "300/300 [==============================] - 1s 5ms/step - loss: 0.6925 - accuracy: 0.7384\n",
      "Epoch 6/25\n",
      "300/300 [==============================] - 1s 5ms/step - loss: 0.6555 - accuracy: 0.7543\n",
      "Epoch 7/25\n",
      "300/300 [==============================] - 1s 5ms/step - loss: 0.6274 - accuracy: 0.7674\n",
      "Epoch 8/25\n",
      "300/300 [==============================] - 1s 5ms/step - loss: 0.6051 - accuracy: 0.7772\n",
      "Epoch 9/25\n",
      "300/300 [==============================] - 1s 5ms/step - loss: 0.5858 - accuracy: 0.7878\n",
      "Epoch 10/25\n",
      "300/300 [==============================] - 1s 5ms/step - loss: 0.5654 - accuracy: 0.7960\n",
      "Epoch 11/25\n",
      "300/300 [==============================] - 1s 5ms/step - loss: 0.5542 - accuracy: 0.8014\n",
      "Epoch 12/25\n",
      "300/300 [==============================] - 1s 5ms/step - loss: 0.5360 - accuracy: 0.8118\n",
      "Epoch 13/25\n",
      "300/300 [==============================] - 1s 5ms/step - loss: 0.5217 - accuracy: 0.8173\n",
      "Epoch 14/25\n",
      "300/300 [==============================] - 1s 5ms/step - loss: 0.5078 - accuracy: 0.8247\n",
      "Epoch 15/25\n",
      "300/300 [==============================] - 1s 5ms/step - loss: 0.4955 - accuracy: 0.8307\n",
      "Epoch 16/25\n",
      "300/300 [==============================] - 1s 5ms/step - loss: 0.4802 - accuracy: 0.8362\n",
      "Epoch 17/25\n",
      "300/300 [==============================] - 1s 5ms/step - loss: 0.4764 - accuracy: 0.8374\n",
      "Epoch 18/25\n",
      "300/300 [==============================] - 1s 5ms/step - loss: 0.4641 - accuracy: 0.8434\n",
      "Epoch 19/25\n",
      "300/300 [==============================] - 1s 5ms/step - loss: 0.4561 - accuracy: 0.8475\n",
      "Epoch 20/25\n",
      "300/300 [==============================] - 1s 5ms/step - loss: 0.4525 - accuracy: 0.8484\n",
      "Epoch 21/25\n",
      "300/300 [==============================] - 1s 5ms/step - loss: 0.4442 - accuracy: 0.8526\n",
      "Epoch 22/25\n",
      "300/300 [==============================] - 1s 5ms/step - loss: 0.4344 - accuracy: 0.8532\n",
      "Epoch 23/25\n",
      "300/300 [==============================] - 1s 5ms/step - loss: 0.4300 - accuracy: 0.8553\n",
      "Epoch 24/25\n",
      "300/300 [==============================] - 1s 5ms/step - loss: 0.4280 - accuracy: 0.8575\n",
      "Epoch 25/25\n",
      "300/300 [==============================] - 1s 5ms/step - loss: 0.4190 - accuracy: 0.8597\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 0.3685 - accuracy: 0.8715\n",
      "Fitting for combination 48\n",
      "(28, 28, 1)\n",
      "2\n",
      "[32, 64]\n",
      "[(2, 2), (3, 3)]\n",
      "3\n",
      "[64, 16, 128, 10]\n",
      "10\n",
      "True\n",
      "['relu', 'sigmoid']\n",
      "['relu', 'tanh', 'relu', 'softmax']\n",
      "SGD\n",
      "0.3\n",
      "0.01\n",
      "categorical_crossentropy\n",
      "300\n",
      "25\n",
      "0\n",
      "Epoch 1/25\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 2.3068 - accuracy: 0.1003\n",
      "Epoch 2/25\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 2.3028 - accuracy: 0.0985\n",
      "Epoch 3/25\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 2.3027 - accuracy: 0.0996\n",
      "Epoch 4/25\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 2.3027 - accuracy: 0.1002\n",
      "Epoch 5/25\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 2.3027 - accuracy: 0.0980\n",
      "Epoch 6/25\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 2.3027 - accuracy: 0.0974\n",
      "Epoch 7/25\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 2.3026 - accuracy: 0.1016\n",
      "Epoch 8/25\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 2.3026 - accuracy: 0.0985\n",
      "Epoch 9/25\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 2.3027 - accuracy: 0.0988\n",
      "Epoch 10/25\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 2.3026 - accuracy: 0.0993\n",
      "Epoch 11/25\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 2.3026 - accuracy: 0.1000\n",
      "Epoch 12/25\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 2.3026 - accuracy: 0.1005\n",
      "Epoch 13/25\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 2.3026 - accuracy: 0.1007\n",
      "Epoch 14/25\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 2.3026 - accuracy: 0.0984\n",
      "Epoch 15/25\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 2.3026 - accuracy: 0.1001\n",
      "Epoch 16/25\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 2.3026 - accuracy: 0.0991\n",
      "Epoch 17/25\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 2.3026 - accuracy: 0.1016\n",
      "Epoch 18/25\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 2.3026 - accuracy: 0.1003\n",
      "Epoch 19/25\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 2.3026 - accuracy: 0.0992\n",
      "Epoch 20/25\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 2.3026 - accuracy: 0.1005\n",
      "Epoch 21/25\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 2.3026 - accuracy: 0.0998\n",
      "Epoch 22/25\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 2.3026 - accuracy: 0.0972\n",
      "Epoch 23/25\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 2.3026 - accuracy: 0.0989\n",
      "Epoch 24/25\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 2.3026 - accuracy: 0.1007\n",
      "Epoch 25/25\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 2.3026 - accuracy: 0.0998\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 2.3026 - accuracy: 0.1000\n",
      "Fitting for combination 49\n",
      "(28, 28, 1)\n",
      "2\n",
      "[32, 64]\n",
      "[(2, 2), (5, 5)]\n",
      "3\n",
      "[64, 64, 64, 10]\n",
      "10\n",
      "True\n",
      "['relu', 'tanh']\n",
      "['tanh', 'tanh', 'sigmoid', 'softmax']\n",
      "SGD\n",
      "0.3\n",
      "0.01\n",
      "categorical_crossentropy\n",
      "200\n",
      "25\n",
      "0\n",
      "Epoch 1/25\n",
      "300/300 [==============================] - 2s 5ms/step - loss: 1.3993 - accuracy: 0.4600\n",
      "Epoch 2/25\n",
      "300/300 [==============================] - 1s 5ms/step - loss: 0.9701 - accuracy: 0.6423\n",
      "Epoch 3/25\n",
      "300/300 [==============================] - 1s 5ms/step - loss: 0.8604 - accuracy: 0.6880\n",
      "Epoch 4/25\n",
      "300/300 [==============================] - 2s 5ms/step - loss: 0.8097 - accuracy: 0.7062\n",
      "Epoch 5/25\n",
      "300/300 [==============================] - 1s 5ms/step - loss: 0.7744 - accuracy: 0.7176\n",
      "Epoch 6/25\n",
      "300/300 [==============================] - 1s 5ms/step - loss: 0.7514 - accuracy: 0.7275\n",
      "Epoch 7/25\n",
      "300/300 [==============================] - 1s 5ms/step - loss: 0.7355 - accuracy: 0.7327\n",
      "Epoch 8/25\n",
      "300/300 [==============================] - 1s 5ms/step - loss: 0.7249 - accuracy: 0.7380\n",
      "Epoch 9/25\n",
      "300/300 [==============================] - 1s 5ms/step - loss: 0.7121 - accuracy: 0.7419\n",
      "Epoch 10/25\n",
      "300/300 [==============================] - 1s 5ms/step - loss: 0.7048 - accuracy: 0.7458\n",
      "Epoch 11/25\n",
      "300/300 [==============================] - 1s 5ms/step - loss: 0.6940 - accuracy: 0.7483\n",
      "Epoch 12/25\n",
      "300/300 [==============================] - 1s 5ms/step - loss: 0.6869 - accuracy: 0.7511\n",
      "Epoch 13/25\n",
      "300/300 [==============================] - 1s 5ms/step - loss: 0.6827 - accuracy: 0.7538\n",
      "Epoch 14/25\n",
      "300/300 [==============================] - 1s 5ms/step - loss: 0.6763 - accuracy: 0.7566\n",
      "Epoch 15/25\n",
      "300/300 [==============================] - 1s 5ms/step - loss: 0.6710 - accuracy: 0.7594\n",
      "Epoch 16/25\n",
      "300/300 [==============================] - 2s 5ms/step - loss: 0.6673 - accuracy: 0.7592\n",
      "Epoch 17/25\n",
      "300/300 [==============================] - 1s 5ms/step - loss: 0.6622 - accuracy: 0.7612\n",
      "Epoch 18/25\n",
      "300/300 [==============================] - 1s 5ms/step - loss: 0.6608 - accuracy: 0.7609\n",
      "Epoch 19/25\n",
      "300/300 [==============================] - 1s 5ms/step - loss: 0.6589 - accuracy: 0.7606\n",
      "Epoch 20/25\n",
      "300/300 [==============================] - 1s 5ms/step - loss: 0.6523 - accuracy: 0.7647\n",
      "Epoch 21/25\n",
      "300/300 [==============================] - 1s 5ms/step - loss: 0.6505 - accuracy: 0.7653\n",
      "Epoch 22/25\n",
      "300/300 [==============================] - 1s 5ms/step - loss: 0.6475 - accuracy: 0.7663\n",
      "Epoch 23/25\n",
      "300/300 [==============================] - 1s 5ms/step - loss: 0.6454 - accuracy: 0.7687\n",
      "Epoch 24/25\n",
      "300/300 [==============================] - 1s 5ms/step - loss: 0.6424 - accuracy: 0.7674\n",
      "Epoch 25/25\n",
      "300/300 [==============================] - 1s 5ms/step - loss: 0.6441 - accuracy: 0.7687\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 0.5255 - accuracy: 0.7997\n",
      "Fitting for combination 50\n",
      "(28, 28, 1)\n",
      "2\n",
      "[32, 128]\n",
      "[(2, 2), (2, 2)]\n",
      "3\n",
      "[128, 16, 32, 10]\n",
      "10\n",
      "False\n",
      "['relu', 'relu']\n",
      "['relu', 'tanh', 'tanh', 'softmax']\n",
      "Adam\n",
      "0.3\n",
      "0.01\n",
      "categorical_crossentropy\n",
      "200\n",
      "25\n",
      "0\n",
      "Epoch 1/25\n",
      "300/300 [==============================] - 2s 6ms/step - loss: 2.7918 - accuracy: 0.1002\n",
      "Epoch 2/25\n",
      "300/300 [==============================] - 2s 6ms/step - loss: 2.3428 - accuracy: 0.1001\n",
      "Epoch 3/25\n",
      "300/300 [==============================] - 2s 6ms/step - loss: 2.3326 - accuracy: 0.0996\n",
      "Epoch 4/25\n",
      "300/300 [==============================] - 2s 6ms/step - loss: 2.3280 - accuracy: 0.1005\n",
      "Epoch 5/25\n",
      "300/300 [==============================] - 2s 6ms/step - loss: 2.3227 - accuracy: 0.1005\n",
      "Epoch 6/25\n",
      "300/300 [==============================] - 2s 6ms/step - loss: 2.3194 - accuracy: 0.1024\n",
      "Epoch 7/25\n",
      "300/300 [==============================] - 2s 6ms/step - loss: 2.3183 - accuracy: 0.0991\n",
      "Epoch 8/25\n",
      "300/300 [==============================] - 2s 6ms/step - loss: 2.3212 - accuracy: 0.0988\n",
      "Epoch 9/25\n",
      "300/300 [==============================] - 2s 6ms/step - loss: 2.3161 - accuracy: 0.0993\n",
      "Epoch 10/25\n",
      "300/300 [==============================] - 2s 6ms/step - loss: 2.3150 - accuracy: 0.1002\n",
      "Epoch 11/25\n",
      "300/300 [==============================] - 2s 6ms/step - loss: 2.3157 - accuracy: 0.0975\n",
      "Epoch 12/25\n",
      "300/300 [==============================] - 2s 6ms/step - loss: 2.3144 - accuracy: 0.0982\n",
      "Epoch 13/25\n",
      "300/300 [==============================] - 2s 6ms/step - loss: 2.3130 - accuracy: 0.0993\n",
      "Epoch 14/25\n",
      "300/300 [==============================] - 2s 6ms/step - loss: 2.3139 - accuracy: 0.1001\n",
      "Epoch 15/25\n",
      "300/300 [==============================] - 2s 6ms/step - loss: 2.3117 - accuracy: 0.0997\n",
      "Epoch 16/25\n",
      "300/300 [==============================] - 2s 6ms/step - loss: 2.3111 - accuracy: 0.0996\n",
      "Epoch 17/25\n",
      "300/300 [==============================] - 2s 6ms/step - loss: 2.3107 - accuracy: 0.1010\n",
      "Epoch 18/25\n",
      "300/300 [==============================] - 2s 6ms/step - loss: 2.3114 - accuracy: 0.0990\n",
      "Epoch 19/25\n",
      "300/300 [==============================] - 2s 6ms/step - loss: 2.3113 - accuracy: 0.0989\n",
      "Epoch 20/25\n",
      "300/300 [==============================] - 2s 6ms/step - loss: 2.3099 - accuracy: 0.1002\n",
      "Epoch 21/25\n",
      "300/300 [==============================] - 2s 6ms/step - loss: 2.3100 - accuracy: 0.1007\n",
      "Epoch 22/25\n",
      "300/300 [==============================] - 2s 6ms/step - loss: 2.3107 - accuracy: 0.1017\n",
      "Epoch 23/25\n",
      "300/300 [==============================] - 2s 6ms/step - loss: 2.3096 - accuracy: 0.0986\n",
      "Epoch 24/25\n",
      "300/300 [==============================] - 2s 6ms/step - loss: 2.3094 - accuracy: 0.0998\n",
      "Epoch 25/25\n",
      "300/300 [==============================] - 2s 6ms/step - loss: 2.3084 - accuracy: 0.1018\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 2.3054 - accuracy: 0.1000\n",
      "Fitting for combination 51\n",
      "(28, 28, 1)\n",
      "2\n",
      "[32, 128]\n",
      "[(2, 2), (3, 3)]\n",
      "2\n",
      "[64, 64, 10]\n",
      "10\n",
      "True\n",
      "['relu', 'tanh']\n",
      "['tanh', 'relu', 'softmax']\n",
      "Adam\n",
      "0.1\n",
      "0.01\n",
      "categorical_crossentropy\n",
      "150\n",
      "25\n",
      "0\n",
      "Epoch 1/25\n",
      "400/400 [==============================] - 2s 5ms/step - loss: 2.3555 - accuracy: 0.1005\n",
      "Epoch 2/25\n",
      "400/400 [==============================] - 2s 5ms/step - loss: 2.3035 - accuracy: 0.1003\n",
      "Epoch 3/25\n",
      "400/400 [==============================] - 2s 5ms/step - loss: 2.3031 - accuracy: 0.0987\n",
      "Epoch 4/25\n",
      "400/400 [==============================] - 2s 5ms/step - loss: 2.3031 - accuracy: 0.0994\n",
      "Epoch 5/25\n",
      "400/400 [==============================] - 2s 5ms/step - loss: 2.3030 - accuracy: 0.0993\n",
      "Epoch 6/25\n",
      "400/400 [==============================] - 2s 5ms/step - loss: 2.3030 - accuracy: 0.0979\n",
      "Epoch 7/25\n",
      "400/400 [==============================] - 2s 5ms/step - loss: 2.3029 - accuracy: 0.0994\n",
      "Epoch 8/25\n",
      "400/400 [==============================] - 2s 5ms/step - loss: 2.3029 - accuracy: 0.0979\n",
      "Epoch 9/25\n",
      "400/400 [==============================] - 2s 5ms/step - loss: 2.3028 - accuracy: 0.0995\n",
      "Epoch 10/25\n",
      "400/400 [==============================] - 2s 5ms/step - loss: 2.3028 - accuracy: 0.0990\n",
      "Epoch 11/25\n",
      "400/400 [==============================] - 2s 5ms/step - loss: 2.3028 - accuracy: 0.0991\n",
      "Epoch 12/25\n",
      "400/400 [==============================] - 2s 5ms/step - loss: 2.3028 - accuracy: 0.0989\n",
      "Epoch 13/25\n",
      "400/400 [==============================] - 2s 5ms/step - loss: 2.3028 - accuracy: 0.0965\n",
      "Epoch 14/25\n",
      "400/400 [==============================] - 2s 5ms/step - loss: 2.3027 - accuracy: 0.0992\n",
      "Epoch 15/25\n",
      "400/400 [==============================] - 2s 5ms/step - loss: 2.3027 - accuracy: 0.0978\n",
      "Epoch 16/25\n",
      "400/400 [==============================] - 2s 5ms/step - loss: 2.3027 - accuracy: 0.0990\n",
      "Epoch 17/25\n",
      "400/400 [==============================] - 2s 5ms/step - loss: 2.3028 - accuracy: 0.0971\n",
      "Epoch 18/25\n",
      "400/400 [==============================] - 2s 5ms/step - loss: 2.3027 - accuracy: 0.0980\n",
      "Epoch 19/25\n",
      "400/400 [==============================] - 2s 5ms/step - loss: 2.3027 - accuracy: 0.0963\n",
      "Epoch 20/25\n",
      "400/400 [==============================] - 2s 5ms/step - loss: 2.3027 - accuracy: 0.0981\n",
      "Epoch 21/25\n",
      "400/400 [==============================] - 2s 5ms/step - loss: 2.3027 - accuracy: 0.0997\n",
      "Epoch 22/25\n",
      "400/400 [==============================] - 2s 5ms/step - loss: 2.3028 - accuracy: 0.0973\n",
      "Epoch 23/25\n",
      "400/400 [==============================] - 2s 5ms/step - loss: 2.3027 - accuracy: 0.0975\n",
      "Epoch 24/25\n",
      "400/400 [==============================] - 2s 5ms/step - loss: 2.3027 - accuracy: 0.0953\n",
      "Epoch 25/25\n",
      "400/400 [==============================] - 2s 5ms/step - loss: 2.3027 - accuracy: 0.0994\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 2.3026 - accuracy: 0.1000\n",
      "Fitting for combination 52\n",
      "(28, 28, 1)\n",
      "2\n",
      "[32, 128]\n",
      "[(2, 2), (2, 2)]\n",
      "3\n",
      "[64, 32, 32, 10]\n",
      "10\n",
      "False\n",
      "['relu', 'tanh']\n",
      "['relu', 'tanh', 'tanh', 'softmax']\n",
      "Adam\n",
      "0.03\n",
      "0\n",
      "categorical_crossentropy\n",
      "200\n",
      "25\n",
      "0\n",
      "Epoch 1/25\n",
      "300/300 [==============================] - 2s 7ms/step - loss: 2.1695 - accuracy: 0.1614\n",
      "Epoch 2/25\n",
      "300/300 [==============================] - 2s 7ms/step - loss: 2.3396 - accuracy: 0.0973\n",
      "Epoch 3/25\n",
      "300/300 [==============================] - 2s 7ms/step - loss: 2.3387 - accuracy: 0.0992\n",
      "Epoch 4/25\n",
      "300/300 [==============================] - 2s 7ms/step - loss: 2.3313 - accuracy: 0.1010\n",
      "Epoch 5/25\n",
      "300/300 [==============================] - 2s 7ms/step - loss: 2.3391 - accuracy: 0.1007\n",
      "Epoch 6/25\n",
      "300/300 [==============================] - 2s 7ms/step - loss: 2.3359 - accuracy: 0.1014\n",
      "Epoch 7/25\n",
      "300/300 [==============================] - 2s 7ms/step - loss: 2.3411 - accuracy: 0.0994\n",
      "Epoch 8/25\n",
      "300/300 [==============================] - 2s 7ms/step - loss: 2.3397 - accuracy: 0.1019\n",
      "Epoch 9/25\n",
      "300/300 [==============================] - 2s 7ms/step - loss: 2.3414 - accuracy: 0.0990\n",
      "Epoch 10/25\n",
      "300/300 [==============================] - 2s 7ms/step - loss: 2.3357 - accuracy: 0.0996\n",
      "Epoch 11/25\n",
      "300/300 [==============================] - 2s 7ms/step - loss: 2.3366 - accuracy: 0.0990\n",
      "Epoch 12/25\n",
      "300/300 [==============================] - 2s 7ms/step - loss: 2.3390 - accuracy: 0.1003\n",
      "Epoch 13/25\n",
      "300/300 [==============================] - 2s 7ms/step - loss: 2.3424 - accuracy: 0.0984\n",
      "Epoch 14/25\n",
      "300/300 [==============================] - 2s 7ms/step - loss: 2.3379 - accuracy: 0.1018\n",
      "Epoch 15/25\n",
      "300/300 [==============================] - 2s 7ms/step - loss: 2.3340 - accuracy: 0.0989\n",
      "Epoch 16/25\n",
      "300/300 [==============================] - 2s 7ms/step - loss: 2.3339 - accuracy: 0.0993\n",
      "Epoch 17/25\n",
      "300/300 [==============================] - 2s 7ms/step - loss: 2.3367 - accuracy: 0.1020\n",
      "Epoch 18/25\n",
      "300/300 [==============================] - 2s 7ms/step - loss: 2.3380 - accuracy: 0.1000\n",
      "Epoch 19/25\n",
      "300/300 [==============================] - 2s 7ms/step - loss: 2.3413 - accuracy: 0.1013\n",
      "Epoch 20/25\n",
      "300/300 [==============================] - 2s 7ms/step - loss: 2.3374 - accuracy: 0.1009\n",
      "Epoch 21/25\n",
      "300/300 [==============================] - 2s 7ms/step - loss: 2.3370 - accuracy: 0.0989\n",
      "Epoch 22/25\n",
      "300/300 [==============================] - 2s 7ms/step - loss: 2.3375 - accuracy: 0.1010\n",
      "Epoch 23/25\n",
      "300/300 [==============================] - 2s 7ms/step - loss: 2.3399 - accuracy: 0.1007\n",
      "Epoch 24/25\n",
      "300/300 [==============================] - 2s 7ms/step - loss: 2.3463 - accuracy: 0.0993\n",
      "Epoch 25/25\n",
      "300/300 [==============================] - 2s 7ms/step - loss: 2.3389 - accuracy: 0.1016\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 2.3132 - accuracy: 0.1000\n",
      "Fitting for combination 53\n",
      "(28, 28, 1)\n",
      "2\n",
      "[32, 128]\n",
      "[(2, 2), (3, 3)]\n",
      "2\n",
      "[32, 128, 10]\n",
      "10\n",
      "False\n",
      "['relu', 'sigmoid']\n",
      "['sigmoid', 'relu', 'softmax']\n",
      "Adam\n",
      "0.03\n",
      "0.01\n",
      "categorical_crossentropy\n",
      "50\n",
      "25\n",
      "0\n",
      "Epoch 1/25\n",
      "1200/1200 [==============================] - 4s 3ms/step - loss: 2.3042 - accuracy: 0.1005\n",
      "Epoch 2/25\n",
      "1200/1200 [==============================] - 4s 3ms/step - loss: 2.3028 - accuracy: 0.0985\n",
      "Epoch 3/25\n",
      "1200/1200 [==============================] - 4s 3ms/step - loss: 2.3027 - accuracy: 0.1000\n",
      "Epoch 4/25\n",
      "1200/1200 [==============================] - 4s 3ms/step - loss: 2.3027 - accuracy: 0.0981\n",
      "Epoch 5/25\n",
      "1200/1200 [==============================] - 4s 3ms/step - loss: 2.3027 - accuracy: 0.0974\n",
      "Epoch 6/25\n",
      "1200/1200 [==============================] - 4s 3ms/step - loss: 2.3027 - accuracy: 0.0968\n",
      "Epoch 7/25\n",
      "1200/1200 [==============================] - 4s 3ms/step - loss: 2.3027 - accuracy: 0.0975\n",
      "Epoch 8/25\n",
      "1200/1200 [==============================] - 4s 3ms/step - loss: 2.3027 - accuracy: 0.0975\n",
      "Epoch 9/25\n",
      "1200/1200 [==============================] - 4s 3ms/step - loss: 2.3026 - accuracy: 0.0968\n",
      "Epoch 10/25\n",
      "1200/1200 [==============================] - 4s 3ms/step - loss: 2.3026 - accuracy: 0.0984\n",
      "Epoch 11/25\n",
      "1200/1200 [==============================] - 4s 3ms/step - loss: 2.3026 - accuracy: 0.0978\n",
      "Epoch 12/25\n",
      "1200/1200 [==============================] - 4s 3ms/step - loss: 2.3026 - accuracy: 0.0995\n",
      "Epoch 13/25\n",
      "1200/1200 [==============================] - 4s 3ms/step - loss: 2.3026 - accuracy: 0.0972\n",
      "Epoch 14/25\n",
      "1200/1200 [==============================] - 4s 3ms/step - loss: 2.3026 - accuracy: 0.0973\n",
      "Epoch 15/25\n",
      "1200/1200 [==============================] - 4s 3ms/step - loss: 2.3026 - accuracy: 0.0985\n",
      "Epoch 16/25\n",
      "1200/1200 [==============================] - 4s 3ms/step - loss: 2.3026 - accuracy: 0.0969\n",
      "Epoch 17/25\n",
      "1200/1200 [==============================] - 4s 3ms/step - loss: 2.3026 - accuracy: 0.0991\n",
      "Epoch 18/25\n",
      "1200/1200 [==============================] - 4s 3ms/step - loss: 2.3026 - accuracy: 0.0974\n",
      "Epoch 19/25\n",
      "1200/1200 [==============================] - 4s 3ms/step - loss: 2.3026 - accuracy: 0.0972\n",
      "Epoch 20/25\n",
      "1200/1200 [==============================] - 4s 3ms/step - loss: 2.3026 - accuracy: 0.0973\n",
      "Epoch 21/25\n",
      "1200/1200 [==============================] - 4s 3ms/step - loss: 2.3026 - accuracy: 0.0972\n",
      "Epoch 22/25\n",
      "1200/1200 [==============================] - 4s 3ms/step - loss: 2.3026 - accuracy: 0.0972\n",
      "Epoch 23/25\n",
      "1200/1200 [==============================] - 4s 3ms/step - loss: 2.3026 - accuracy: 0.0966\n",
      "Epoch 24/25\n",
      "1200/1200 [==============================] - 4s 3ms/step - loss: 2.3026 - accuracy: 0.0974\n",
      "Epoch 25/25\n",
      "1200/1200 [==============================] - 4s 3ms/step - loss: 2.3026 - accuracy: 0.0979\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 2.3026 - accuracy: 0.1000\n",
      "Fitting for combination 54\n",
      "(28, 28, 1)\n",
      "2\n",
      "[32, 128]\n",
      "[(2, 2), (2, 2)]\n",
      "3\n",
      "[128, 16, 128, 10]\n",
      "10\n",
      "True\n",
      "['relu', 'tanh']\n",
      "['relu', 'tanh', 'relu', 'softmax']\n",
      "Adam\n",
      "0.3\n",
      "0.01\n",
      "categorical_crossentropy\n",
      "50\n",
      "25\n",
      "0\n",
      "Epoch 1/25\n",
      "1200/1200 [==============================] - 4s 3ms/step - loss: 2.4194 - accuracy: 0.1011\n",
      "Epoch 2/25\n",
      "1200/1200 [==============================] - 4s 4ms/step - loss: 2.3179 - accuracy: 0.0979\n",
      "Epoch 3/25\n",
      "1200/1200 [==============================] - 4s 4ms/step - loss: 2.3146 - accuracy: 0.0994\n",
      "Epoch 4/25\n",
      "1200/1200 [==============================] - 4s 3ms/step - loss: 2.3063 - accuracy: 0.0992\n",
      "Epoch 5/25\n",
      "1200/1200 [==============================] - 4s 3ms/step - loss: 2.3063 - accuracy: 0.0992\n",
      "Epoch 6/25\n",
      "1200/1200 [==============================] - 4s 3ms/step - loss: 2.3077 - accuracy: 0.0988\n",
      "Epoch 7/25\n",
      "1200/1200 [==============================] - 4s 3ms/step - loss: 2.3041 - accuracy: 0.1003\n",
      "Epoch 8/25\n",
      "1200/1200 [==============================] - 4s 3ms/step - loss: 2.3052 - accuracy: 0.0991\n",
      "Epoch 9/25\n",
      "1200/1200 [==============================] - 4s 3ms/step - loss: 2.3133 - accuracy: 0.1016\n",
      "Epoch 10/25\n",
      "1200/1200 [==============================] - 4s 3ms/step - loss: 2.3086 - accuracy: 0.0982\n",
      "Epoch 11/25\n",
      "1200/1200 [==============================] - 4s 3ms/step - loss: 2.3046 - accuracy: 0.0993\n",
      "Epoch 12/25\n",
      "1200/1200 [==============================] - 4s 3ms/step - loss: 2.3047 - accuracy: 0.0999\n",
      "Epoch 13/25\n",
      "1200/1200 [==============================] - 4s 3ms/step - loss: 2.3043 - accuracy: 0.0969\n",
      "Epoch 14/25\n",
      "1200/1200 [==============================] - 4s 3ms/step - loss: 2.3038 - accuracy: 0.0987\n",
      "Epoch 15/25\n",
      "1200/1200 [==============================] - 4s 3ms/step - loss: 2.3099 - accuracy: 0.0991\n",
      "Epoch 16/25\n",
      "1200/1200 [==============================] - 4s 3ms/step - loss: 2.3051 - accuracy: 0.1004\n",
      "Epoch 17/25\n",
      "1200/1200 [==============================] - 4s 3ms/step - loss: 2.3049 - accuracy: 0.0989\n",
      "Epoch 18/25\n",
      "1200/1200 [==============================] - 4s 3ms/step - loss: 2.3044 - accuracy: 0.0966\n",
      "Epoch 19/25\n",
      "1200/1200 [==============================] - 4s 3ms/step - loss: 2.3052 - accuracy: 0.0962\n",
      "Epoch 20/25\n",
      "1200/1200 [==============================] - 4s 3ms/step - loss: 2.3048 - accuracy: 0.0981\n",
      "Epoch 21/25\n",
      "1200/1200 [==============================] - 4s 3ms/step - loss: 2.3031 - accuracy: 0.0986\n",
      "Epoch 22/25\n",
      "1200/1200 [==============================] - 4s 3ms/step - loss: 2.3080 - accuracy: 0.0966\n",
      "Epoch 23/25\n",
      "1200/1200 [==============================] - 4s 4ms/step - loss: 2.3052 - accuracy: 0.0983\n",
      "Epoch 24/25\n",
      "1200/1200 [==============================] - 4s 3ms/step - loss: 2.3046 - accuracy: 0.0970\n",
      "Epoch 25/25\n",
      "1200/1200 [==============================] - 4s 3ms/step - loss: 2.3043 - accuracy: 0.0998\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 2.3026 - accuracy: 0.1000\n",
      "Fitting for combination 55\n",
      "(28, 28, 1)\n",
      "2\n",
      "[32, 128]\n",
      "[(2, 2), (2, 2)]\n",
      "1\n",
      "[32, 10]\n",
      "10\n",
      "False\n",
      "['relu', 'sigmoid']\n",
      "['sigmoid', 'softmax']\n",
      "SGD\n",
      "0.01\n",
      "0\n",
      "categorical_crossentropy\n",
      "300\n",
      "25\n",
      "0\n",
      "Epoch 1/25\n",
      "200/200 [==============================] - 2s 8ms/step - loss: 2.3014 - accuracy: 0.1117\n",
      "Epoch 2/25\n",
      "200/200 [==============================] - 2s 8ms/step - loss: 2.2825 - accuracy: 0.1443\n",
      "Epoch 3/25\n",
      "200/200 [==============================] - 2s 8ms/step - loss: 2.2417 - accuracy: 0.2278\n",
      "Epoch 4/25\n",
      "200/200 [==============================] - 2s 8ms/step - loss: 2.1209 - accuracy: 0.3388\n",
      "Epoch 5/25\n",
      "200/200 [==============================] - 2s 8ms/step - loss: 1.8343 - accuracy: 0.5043\n",
      "Epoch 6/25\n",
      "200/200 [==============================] - 2s 8ms/step - loss: 1.5143 - accuracy: 0.6412\n",
      "Epoch 7/25\n",
      "200/200 [==============================] - 2s 8ms/step - loss: 1.2914 - accuracy: 0.6728\n",
      "Epoch 8/25\n",
      "200/200 [==============================] - 2s 8ms/step - loss: 1.1390 - accuracy: 0.6932\n",
      "Epoch 9/25\n",
      "200/200 [==============================] - 2s 8ms/step - loss: 1.0299 - accuracy: 0.7102\n",
      "Epoch 10/25\n",
      "200/200 [==============================] - 2s 8ms/step - loss: 0.9514 - accuracy: 0.7227\n",
      "Epoch 11/25\n",
      "200/200 [==============================] - 2s 8ms/step - loss: 0.8918 - accuracy: 0.7321\n",
      "Epoch 12/25\n",
      "200/200 [==============================] - 2s 8ms/step - loss: 0.8451 - accuracy: 0.7387\n",
      "Epoch 13/25\n",
      "200/200 [==============================] - 2s 8ms/step - loss: 0.8071 - accuracy: 0.7454\n",
      "Epoch 14/25\n",
      "200/200 [==============================] - 2s 8ms/step - loss: 0.7753 - accuracy: 0.7512\n",
      "Epoch 15/25\n",
      "200/200 [==============================] - 2s 8ms/step - loss: 0.7491 - accuracy: 0.7554\n",
      "Epoch 16/25\n",
      "200/200 [==============================] - 2s 8ms/step - loss: 0.7261 - accuracy: 0.7598\n",
      "Epoch 17/25\n",
      "200/200 [==============================] - 2s 8ms/step - loss: 0.7063 - accuracy: 0.7644\n",
      "Epoch 18/25\n",
      "200/200 [==============================] - 2s 8ms/step - loss: 0.6889 - accuracy: 0.7689\n",
      "Epoch 19/25\n",
      "200/200 [==============================] - 2s 8ms/step - loss: 0.6732 - accuracy: 0.7728\n",
      "Epoch 20/25\n",
      "200/200 [==============================] - 2s 8ms/step - loss: 0.6591 - accuracy: 0.7771\n",
      "Epoch 21/25\n",
      "200/200 [==============================] - 2s 8ms/step - loss: 0.6465 - accuracy: 0.7804\n",
      "Epoch 22/25\n",
      "200/200 [==============================] - 2s 8ms/step - loss: 0.6350 - accuracy: 0.7840\n",
      "Epoch 23/25\n",
      "200/200 [==============================] - 2s 8ms/step - loss: 0.6243 - accuracy: 0.7878\n",
      "Epoch 24/25\n",
      "200/200 [==============================] - 2s 8ms/step - loss: 0.6146 - accuracy: 0.7897\n",
      "Epoch 25/25\n",
      "200/200 [==============================] - 2s 8ms/step - loss: 0.6050 - accuracy: 0.7940\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 0.6196 - accuracy: 0.7841\n",
      "Fitting for combination 56\n",
      "(28, 28, 1)\n",
      "2\n",
      "[32, 128]\n",
      "[(2, 2), (5, 5)]\n",
      "2\n",
      "[32, 32, 10]\n",
      "10\n",
      "True\n",
      "['relu', 'tanh']\n",
      "['sigmoid', 'tanh', 'softmax']\n",
      "SGD\n",
      "0.3\n",
      "0.01\n",
      "categorical_crossentropy\n",
      "300\n",
      "25\n",
      "0\n",
      "Epoch 1/25\n",
      "200/200 [==============================] - 3s 10ms/step - loss: 1.5159 - accuracy: 0.4199\n",
      "Epoch 2/25\n",
      "200/200 [==============================] - 2s 10ms/step - loss: 1.0258 - accuracy: 0.6144\n",
      "Epoch 3/25\n",
      "200/200 [==============================] - 2s 10ms/step - loss: 0.9385 - accuracy: 0.6520\n",
      "Epoch 4/25\n",
      "200/200 [==============================] - 2s 10ms/step - loss: 0.8885 - accuracy: 0.6711\n",
      "Epoch 5/25\n",
      "200/200 [==============================] - 2s 10ms/step - loss: 0.8600 - accuracy: 0.6817\n",
      "Epoch 6/25\n",
      "200/200 [==============================] - 2s 10ms/step - loss: 0.8460 - accuracy: 0.6877\n",
      "Epoch 7/25\n",
      "200/200 [==============================] - 2s 10ms/step - loss: 0.8267 - accuracy: 0.6934\n",
      "Epoch 8/25\n",
      "200/200 [==============================] - 2s 10ms/step - loss: 0.8137 - accuracy: 0.7010\n",
      "Epoch 9/25\n",
      "200/200 [==============================] - 2s 10ms/step - loss: 0.8081 - accuracy: 0.7035\n",
      "Epoch 10/25\n",
      "200/200 [==============================] - 2s 10ms/step - loss: 0.8015 - accuracy: 0.7039\n",
      "Epoch 11/25\n",
      "200/200 [==============================] - 2s 10ms/step - loss: 0.7901 - accuracy: 0.7083\n",
      "Epoch 12/25\n",
      "200/200 [==============================] - 2s 10ms/step - loss: 0.7834 - accuracy: 0.7115\n",
      "Epoch 13/25\n",
      "200/200 [==============================] - 2s 10ms/step - loss: 0.7764 - accuracy: 0.7148\n",
      "Epoch 14/25\n",
      "200/200 [==============================] - 2s 10ms/step - loss: 0.7708 - accuracy: 0.7170\n",
      "Epoch 15/25\n",
      "200/200 [==============================] - 2s 10ms/step - loss: 0.7698 - accuracy: 0.7164\n",
      "Epoch 16/25\n",
      "200/200 [==============================] - 2s 10ms/step - loss: 0.7649 - accuracy: 0.7171\n",
      "Epoch 17/25\n",
      "200/200 [==============================] - 2s 10ms/step - loss: 0.7573 - accuracy: 0.7222\n",
      "Epoch 18/25\n",
      "200/200 [==============================] - 2s 10ms/step - loss: 0.7530 - accuracy: 0.7215\n",
      "Epoch 19/25\n",
      "200/200 [==============================] - 2s 10ms/step - loss: 0.7510 - accuracy: 0.7223\n",
      "Epoch 20/25\n",
      "200/200 [==============================] - 2s 10ms/step - loss: 0.7502 - accuracy: 0.7244\n",
      "Epoch 21/25\n",
      "200/200 [==============================] - 2s 10ms/step - loss: 0.7461 - accuracy: 0.7281\n",
      "Epoch 22/25\n",
      "200/200 [==============================] - 2s 10ms/step - loss: 0.7461 - accuracy: 0.7255\n",
      "Epoch 23/25\n",
      "200/200 [==============================] - 2s 10ms/step - loss: 0.7397 - accuracy: 0.7273\n",
      "Epoch 24/25\n",
      "200/200 [==============================] - 2s 10ms/step - loss: 0.7374 - accuracy: 0.7314\n",
      "Epoch 25/25\n",
      "200/200 [==============================] - 2s 10ms/step - loss: 0.7353 - accuracy: 0.7313\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5434 - accuracy: 0.7962\n",
      "Fitting for combination 57\n",
      "(28, 28, 1)\n",
      "2\n",
      "[32, 128]\n",
      "[(2, 2), (2, 2)]\n",
      "1\n",
      "[128, 10]\n",
      "10\n",
      "False\n",
      "['relu', 'relu']\n",
      "['tanh', 'softmax']\n",
      "SGD\n",
      "0.01\n",
      "0\n",
      "categorical_crossentropy\n",
      "200\n",
      "25\n",
      "0\n",
      "Epoch 1/25\n",
      "300/300 [==============================] - 2s 6ms/step - loss: 1.0827 - accuracy: 0.6620\n",
      "Epoch 2/25\n",
      "300/300 [==============================] - 2s 6ms/step - loss: 0.5819 - accuracy: 0.7912\n",
      "Epoch 3/25\n",
      "300/300 [==============================] - 2s 6ms/step - loss: 0.5222 - accuracy: 0.8093\n",
      "Epoch 4/25\n",
      "300/300 [==============================] - 2s 6ms/step - loss: 0.4890 - accuracy: 0.8225\n",
      "Epoch 5/25\n",
      "300/300 [==============================] - 2s 6ms/step - loss: 0.4671 - accuracy: 0.8314\n",
      "Epoch 6/25\n",
      "300/300 [==============================] - 2s 6ms/step - loss: 0.4490 - accuracy: 0.8383\n",
      "Epoch 7/25\n",
      "300/300 [==============================] - 2s 6ms/step - loss: 0.4374 - accuracy: 0.8420\n",
      "Epoch 8/25\n",
      "300/300 [==============================] - 2s 6ms/step - loss: 0.4237 - accuracy: 0.8485\n",
      "Epoch 9/25\n",
      "300/300 [==============================] - 2s 6ms/step - loss: 0.4115 - accuracy: 0.8515\n",
      "Epoch 10/25\n",
      "300/300 [==============================] - 2s 6ms/step - loss: 0.4050 - accuracy: 0.8552\n",
      "Epoch 11/25\n",
      "300/300 [==============================] - 2s 6ms/step - loss: 0.3965 - accuracy: 0.8570\n",
      "Epoch 12/25\n",
      "300/300 [==============================] - 2s 6ms/step - loss: 0.3878 - accuracy: 0.8604\n",
      "Epoch 13/25\n",
      "300/300 [==============================] - 2s 6ms/step - loss: 0.3820 - accuracy: 0.8616\n",
      "Epoch 14/25\n",
      "300/300 [==============================] - 2s 6ms/step - loss: 0.3731 - accuracy: 0.8662\n",
      "Epoch 15/25\n",
      "300/300 [==============================] - 2s 6ms/step - loss: 0.3690 - accuracy: 0.8680\n",
      "Epoch 16/25\n",
      "300/300 [==============================] - 2s 6ms/step - loss: 0.3624 - accuracy: 0.8701\n",
      "Epoch 17/25\n",
      "300/300 [==============================] - 2s 6ms/step - loss: 0.3555 - accuracy: 0.8714\n",
      "Epoch 18/25\n",
      "300/300 [==============================] - 2s 6ms/step - loss: 0.3508 - accuracy: 0.8744\n",
      "Epoch 19/25\n",
      "300/300 [==============================] - 2s 6ms/step - loss: 0.3474 - accuracy: 0.8747\n",
      "Epoch 20/25\n",
      "300/300 [==============================] - 2s 6ms/step - loss: 0.3418 - accuracy: 0.8759\n",
      "Epoch 21/25\n",
      "300/300 [==============================] - 2s 6ms/step - loss: 0.3377 - accuracy: 0.8783\n",
      "Epoch 22/25\n",
      "300/300 [==============================] - 2s 6ms/step - loss: 0.3316 - accuracy: 0.8805\n",
      "Epoch 23/25\n",
      "300/300 [==============================] - 2s 6ms/step - loss: 0.3288 - accuracy: 0.8818\n",
      "Epoch 24/25\n",
      "300/300 [==============================] - 2s 6ms/step - loss: 0.3271 - accuracy: 0.8817\n",
      "Epoch 25/25\n",
      "300/300 [==============================] - 2s 6ms/step - loss: 0.3225 - accuracy: 0.8843\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 0.3817 - accuracy: 0.8602\n",
      "Fitting for combination 58\n",
      "(28, 28, 1)\n",
      "2\n",
      "[32, 128]\n",
      "[(2, 2), (2, 2)]\n",
      "1\n",
      "[128, 10]\n",
      "10\n",
      "True\n",
      "['relu', 'sigmoid']\n",
      "['relu', 'softmax']\n",
      "SGD\n",
      "0.3\n",
      "0\n",
      "categorical_crossentropy\n",
      "150\n",
      "25\n",
      "0\n",
      "Epoch 1/25\n",
      "400/400 [==============================] - 2s 5ms/step - loss: 2.7415 - accuracy: 0.0991\n",
      "Epoch 2/25\n",
      "400/400 [==============================] - 2s 5ms/step - loss: 2.3031 - accuracy: 0.0979\n",
      "Epoch 3/25\n",
      "400/400 [==============================] - 2s 5ms/step - loss: 2.3031 - accuracy: 0.0993\n",
      "Epoch 4/25\n",
      "400/400 [==============================] - 2s 5ms/step - loss: 2.3031 - accuracy: 0.1002\n",
      "Epoch 5/25\n",
      "400/400 [==============================] - 2s 5ms/step - loss: 2.3032 - accuracy: 0.0997\n",
      "Epoch 6/25\n",
      "400/400 [==============================] - 2s 5ms/step - loss: 2.3031 - accuracy: 0.0992\n",
      "Epoch 7/25\n",
      "400/400 [==============================] - 2s 5ms/step - loss: 2.3031 - accuracy: 0.0993\n",
      "Epoch 8/25\n",
      "400/400 [==============================] - 2s 5ms/step - loss: 2.3032 - accuracy: 0.0983\n",
      "Epoch 9/25\n",
      "400/400 [==============================] - 2s 5ms/step - loss: 2.3031 - accuracy: 0.1013\n",
      "Epoch 10/25\n",
      "400/400 [==============================] - 2s 5ms/step - loss: 2.3031 - accuracy: 0.0990\n",
      "Epoch 11/25\n",
      "400/400 [==============================] - 2s 5ms/step - loss: 2.3031 - accuracy: 0.0988\n",
      "Epoch 12/25\n",
      "400/400 [==============================] - 2s 5ms/step - loss: 2.3031 - accuracy: 0.0994\n",
      "Epoch 13/25\n",
      "400/400 [==============================] - 2s 5ms/step - loss: 2.3032 - accuracy: 0.0975\n",
      "Epoch 14/25\n",
      "400/400 [==============================] - 2s 5ms/step - loss: 2.3031 - accuracy: 0.1002\n",
      "Epoch 15/25\n",
      "400/400 [==============================] - 2s 5ms/step - loss: 2.3031 - accuracy: 0.0999\n",
      "Epoch 16/25\n",
      "400/400 [==============================] - 2s 5ms/step - loss: 2.3031 - accuracy: 0.0999\n",
      "Epoch 17/25\n",
      "400/400 [==============================] - 2s 5ms/step - loss: 2.3031 - accuracy: 0.1007\n",
      "Epoch 18/25\n",
      "400/400 [==============================] - 2s 5ms/step - loss: 2.3032 - accuracy: 0.0987\n",
      "Epoch 19/25\n",
      "400/400 [==============================] - 2s 5ms/step - loss: 2.3032 - accuracy: 0.0971\n",
      "Epoch 20/25\n",
      "400/400 [==============================] - 2s 5ms/step - loss: 2.3032 - accuracy: 0.0982\n",
      "Epoch 21/25\n",
      "400/400 [==============================] - 2s 5ms/step - loss: 2.3032 - accuracy: 0.0972\n",
      "Epoch 22/25\n",
      "400/400 [==============================] - 2s 5ms/step - loss: 2.3031 - accuracy: 0.0977\n",
      "Epoch 23/25\n",
      "400/400 [==============================] - 2s 5ms/step - loss: 2.3031 - accuracy: 0.0980\n",
      "Epoch 24/25\n",
      "400/400 [==============================] - 2s 5ms/step - loss: 2.3032 - accuracy: 0.0992\n",
      "Epoch 25/25\n",
      "400/400 [==============================] - 2s 5ms/step - loss: 2.3030 - accuracy: 0.1000\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 2.3027 - accuracy: 0.1000\n",
      "Fitting for combination 59\n",
      "(28, 28, 1)\n",
      "2\n",
      "[32, 128]\n",
      "[(2, 2), (3, 3)]\n",
      "3\n",
      "[128, 128, 64, 10]\n",
      "10\n",
      "False\n",
      "['relu', 'tanh']\n",
      "['relu', 'sigmoid', 'sigmoid', 'softmax']\n",
      "SGD\n",
      "0.01\n",
      "0\n",
      "categorical_crossentropy\n",
      "150\n",
      "25\n",
      "0\n",
      "Epoch 1/25\n",
      "400/400 [==============================] - 2s 5ms/step - loss: 2.3257 - accuracy: 0.1248\n",
      "Epoch 2/25\n",
      "400/400 [==============================] - 2s 5ms/step - loss: 2.2889 - accuracy: 0.2242\n",
      "Epoch 3/25\n",
      "400/400 [==============================] - 2s 5ms/step - loss: 2.2513 - accuracy: 0.3870\n",
      "Epoch 4/25\n",
      "400/400 [==============================] - 2s 5ms/step - loss: 2.0573 - accuracy: 0.4777\n",
      "Epoch 5/25\n",
      "400/400 [==============================] - 2s 5ms/step - loss: 1.6525 - accuracy: 0.5463\n",
      "Epoch 6/25\n",
      "400/400 [==============================] - 2s 5ms/step - loss: 1.3607 - accuracy: 0.5805\n",
      "Epoch 7/25\n",
      "400/400 [==============================] - 2s 5ms/step - loss: 1.1862 - accuracy: 0.5958\n",
      "Epoch 8/25\n",
      "400/400 [==============================] - 2s 5ms/step - loss: 1.0700 - accuracy: 0.6253\n",
      "Epoch 9/25\n",
      "400/400 [==============================] - 2s 5ms/step - loss: 0.9802 - accuracy: 0.6707\n",
      "Epoch 10/25\n",
      "400/400 [==============================] - 2s 5ms/step - loss: 0.9031 - accuracy: 0.7076\n",
      "Epoch 11/25\n",
      "400/400 [==============================] - 2s 5ms/step - loss: 0.8361 - accuracy: 0.7324\n",
      "Epoch 12/25\n",
      "400/400 [==============================] - 2s 5ms/step - loss: 0.7787 - accuracy: 0.7442\n",
      "Epoch 13/25\n",
      "400/400 [==============================] - 2s 5ms/step - loss: 0.7308 - accuracy: 0.7528\n",
      "Epoch 14/25\n",
      "400/400 [==============================] - 2s 5ms/step - loss: 0.6917 - accuracy: 0.7583\n",
      "Epoch 15/25\n",
      "400/400 [==============================] - 2s 5ms/step - loss: 0.6602 - accuracy: 0.7632\n",
      "Epoch 16/25\n",
      "400/400 [==============================] - 2s 5ms/step - loss: 0.6337 - accuracy: 0.7686\n",
      "Epoch 17/25\n",
      "400/400 [==============================] - 2s 5ms/step - loss: 0.6104 - accuracy: 0.7735\n",
      "Epoch 18/25\n",
      "400/400 [==============================] - 2s 5ms/step - loss: 0.5900 - accuracy: 0.7797\n",
      "Epoch 19/25\n",
      "400/400 [==============================] - 2s 5ms/step - loss: 0.5720 - accuracy: 0.7857\n",
      "Epoch 20/25\n",
      "400/400 [==============================] - 2s 5ms/step - loss: 0.5559 - accuracy: 0.7912\n",
      "Epoch 21/25\n",
      "400/400 [==============================] - 2s 5ms/step - loss: 0.5413 - accuracy: 0.7972\n",
      "Epoch 22/25\n",
      "400/400 [==============================] - 2s 5ms/step - loss: 0.5281 - accuracy: 0.8038\n",
      "Epoch 23/25\n",
      "400/400 [==============================] - 2s 5ms/step - loss: 0.5165 - accuracy: 0.8105\n",
      "Epoch 24/25\n",
      "400/400 [==============================] - 2s 5ms/step - loss: 0.5060 - accuracy: 0.8139\n",
      "Epoch 25/25\n",
      "400/400 [==============================] - 2s 5ms/step - loss: 0.4960 - accuracy: 0.8199\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 0.5125 - accuracy: 0.8101\n",
      "Fitting for combination 60\n",
      "(28, 28, 1)\n",
      "3\n",
      "[32, 32, 32]\n",
      "[(2, 2), (2, 2), (5, 5)]\n",
      "2\n",
      "[16, 32, 10]\n",
      "10\n",
      "True\n",
      "['relu', 'sigmoid', 'tanh']\n",
      "['relu', 'sigmoid', 'softmax']\n",
      "Adam\n",
      "0.01\n",
      "0.01\n",
      "categorical_crossentropy\n",
      "300\n",
      "25\n",
      "0\n",
      "Epoch 1/25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-14 09:39:41.577987: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:954] layout failed: INVALID_ARGUMENT: Size of values 0 does not match size of permutation 4 @ fanin shape insequential_60/dropout_74/dropout/SelectV2-2-TransposeNHWCToNCHW-LayoutOptimizer\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200/200 [==============================] - 1s 4ms/step - loss: 2.3597 - accuracy: 0.1014\n",
      "Epoch 2/25\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 2.3062 - accuracy: 0.0998\n",
      "Epoch 3/25\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 2.3048 - accuracy: 0.1011\n",
      "Epoch 4/25\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 2.3043 - accuracy: 0.0994\n",
      "Epoch 5/25\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 2.3037 - accuracy: 0.0998\n",
      "Epoch 6/25\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 2.3037 - accuracy: 0.0987\n",
      "Epoch 7/25\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 2.3034 - accuracy: 0.1004\n",
      "Epoch 8/25\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 2.3033 - accuracy: 0.0998\n",
      "Epoch 9/25\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 2.3031 - accuracy: 0.0989\n",
      "Epoch 10/25\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 2.3033 - accuracy: 0.0989\n",
      "Epoch 11/25\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 2.3032 - accuracy: 0.1000\n",
      "Epoch 12/25\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 2.3030 - accuracy: 0.1006\n",
      "Epoch 13/25\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 2.3031 - accuracy: 0.0988\n",
      "Epoch 14/25\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 2.3031 - accuracy: 0.0977\n",
      "Epoch 15/25\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 2.3030 - accuracy: 0.1015\n",
      "Epoch 16/25\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 2.3030 - accuracy: 0.0983\n",
      "Epoch 17/25\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 2.3031 - accuracy: 0.1010\n",
      "Epoch 18/25\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 2.3029 - accuracy: 0.0999\n",
      "Epoch 19/25\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 2.3030 - accuracy: 0.0991\n",
      "Epoch 20/25\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 2.3029 - accuracy: 0.1034\n",
      "Epoch 21/25\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 2.3028 - accuracy: 0.1007\n",
      "Epoch 22/25\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 2.3028 - accuracy: 0.1006\n",
      "Epoch 23/25\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 2.3029 - accuracy: 0.1006\n",
      "Epoch 24/25\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 2.3030 - accuracy: 0.0986\n",
      "Epoch 25/25\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 2.3028 - accuracy: 0.0988\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 2.3026 - accuracy: 0.1000\n",
      "Fitting for combination 61\n",
      "(28, 28, 1)\n",
      "3\n",
      "[32, 32, 32]\n",
      "[(2, 2), (2, 2), (2, 2)]\n",
      "3\n",
      "[128, 128, 128, 10]\n",
      "10\n",
      "True\n",
      "['relu', 'sigmoid', 'sigmoid']\n",
      "['sigmoid', 'relu', 'tanh', 'softmax']\n",
      "Adam\n",
      "0.03\n",
      "0.01\n",
      "categorical_crossentropy\n",
      "150\n",
      "25\n",
      "0\n",
      "Epoch 1/25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-14 09:40:06.829016: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:954] layout failed: INVALID_ARGUMENT: Size of values 0 does not match size of permutation 4 @ fanin shape insequential_61/dropout_77/dropout/SelectV2-2-TransposeNHWCToNCHW-LayoutOptimizer\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "400/400 [==============================] - 2s 3ms/step - loss: 2.3393 - accuracy: 0.1027\n",
      "Epoch 2/25\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 2.3040 - accuracy: 0.0999\n",
      "Epoch 3/25\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 2.3034 - accuracy: 0.0984\n",
      "Epoch 4/25\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 2.3031 - accuracy: 0.0991\n",
      "Epoch 5/25\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 2.3030 - accuracy: 0.1000\n",
      "Epoch 6/25\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 2.3031 - accuracy: 0.0994\n",
      "Epoch 7/25\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 2.3029 - accuracy: 0.1005\n",
      "Epoch 8/25\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 2.3029 - accuracy: 0.0988\n",
      "Epoch 9/25\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 2.3028 - accuracy: 0.0989\n",
      "Epoch 10/25\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 2.3029 - accuracy: 0.0991\n",
      "Epoch 11/25\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 2.3028 - accuracy: 0.0994\n",
      "Epoch 12/25\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 2.3028 - accuracy: 0.0999\n",
      "Epoch 13/25\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 2.3028 - accuracy: 0.0978\n",
      "Epoch 14/25\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 2.3028 - accuracy: 0.0997\n",
      "Epoch 15/25\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 2.3028 - accuracy: 0.0978\n",
      "Epoch 16/25\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 2.3028 - accuracy: 0.1001\n",
      "Epoch 17/25\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 2.3028 - accuracy: 0.0988\n",
      "Epoch 18/25\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 2.3028 - accuracy: 0.0986\n",
      "Epoch 19/25\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 2.3027 - accuracy: 0.1001\n",
      "Epoch 20/25\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 2.3027 - accuracy: 0.0981\n",
      "Epoch 21/25\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 2.3027 - accuracy: 0.0993\n",
      "Epoch 22/25\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 2.3027 - accuracy: 0.0981\n",
      "Epoch 23/25\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 2.3027 - accuracy: 0.0994\n",
      "Epoch 24/25\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 2.3027 - accuracy: 0.0983\n",
      "Epoch 25/25\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 2.3027 - accuracy: 0.1000\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 2.3026 - accuracy: 0.1000\n",
      "Fitting for combination 62\n",
      "(28, 28, 1)\n",
      "3\n",
      "[32, 32, 32]\n",
      "[(2, 2), (2, 2), (5, 5)]\n",
      "2\n",
      "[128, 16, 10]\n",
      "10\n",
      "True\n",
      "['relu', 'sigmoid', 'sigmoid']\n",
      "['sigmoid', 'tanh', 'softmax']\n",
      "Adam\n",
      "0.03\n",
      "0.01\n",
      "categorical_crossentropy\n",
      "200\n",
      "25\n",
      "0\n",
      "Epoch 1/25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-14 09:40:40.676736: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:954] layout failed: INVALID_ARGUMENT: Size of values 0 does not match size of permutation 4 @ fanin shape insequential_62/dropout_81/dropout/SelectV2-2-TransposeNHWCToNCHW-LayoutOptimizer\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300/300 [==============================] - 2s 4ms/step - loss: 2.3478 - accuracy: 0.1019\n",
      "Epoch 2/25\n",
      "300/300 [==============================] - 1s 4ms/step - loss: 2.3088 - accuracy: 0.0994\n",
      "Epoch 3/25\n",
      "300/300 [==============================] - 1s 4ms/step - loss: 2.3064 - accuracy: 0.1000\n",
      "Epoch 4/25\n",
      "300/300 [==============================] - 1s 4ms/step - loss: 2.3056 - accuracy: 0.1010\n",
      "Epoch 5/25\n",
      "300/300 [==============================] - 1s 4ms/step - loss: 2.3049 - accuracy: 0.0992\n",
      "Epoch 6/25\n",
      "300/300 [==============================] - 1s 4ms/step - loss: 2.3045 - accuracy: 0.0978\n",
      "Epoch 7/25\n",
      "300/300 [==============================] - 1s 4ms/step - loss: 2.3043 - accuracy: 0.1002\n",
      "Epoch 8/25\n",
      "300/300 [==============================] - 1s 4ms/step - loss: 2.3042 - accuracy: 0.0986\n",
      "Epoch 9/25\n",
      "300/300 [==============================] - 1s 4ms/step - loss: 2.3037 - accuracy: 0.1008\n",
      "Epoch 10/25\n",
      "300/300 [==============================] - 1s 4ms/step - loss: 2.3040 - accuracy: 0.0983\n",
      "Epoch 11/25\n",
      "300/300 [==============================] - 1s 4ms/step - loss: 2.3038 - accuracy: 0.1002\n",
      "Epoch 12/25\n",
      "300/300 [==============================] - 1s 4ms/step - loss: 2.3034 - accuracy: 0.1012\n",
      "Epoch 13/25\n",
      "300/300 [==============================] - 1s 4ms/step - loss: 2.3036 - accuracy: 0.0979\n",
      "Epoch 14/25\n",
      "300/300 [==============================] - 1s 4ms/step - loss: 2.3032 - accuracy: 0.1011\n",
      "Epoch 15/25\n",
      "300/300 [==============================] - 1s 4ms/step - loss: 2.3032 - accuracy: 0.1008\n",
      "Epoch 16/25\n",
      "300/300 [==============================] - 1s 4ms/step - loss: 2.3034 - accuracy: 0.0998\n",
      "Epoch 17/25\n",
      "300/300 [==============================] - 1s 4ms/step - loss: 2.3035 - accuracy: 0.0989\n",
      "Epoch 18/25\n",
      "300/300 [==============================] - 1s 4ms/step - loss: 2.3033 - accuracy: 0.0973\n",
      "Epoch 19/25\n",
      "300/300 [==============================] - 1s 4ms/step - loss: 2.3032 - accuracy: 0.1006\n",
      "Epoch 20/25\n",
      "300/300 [==============================] - 1s 4ms/step - loss: 2.3033 - accuracy: 0.0987\n",
      "Epoch 21/25\n",
      "300/300 [==============================] - 1s 4ms/step - loss: 2.3033 - accuracy: 0.0972\n",
      "Epoch 22/25\n",
      "300/300 [==============================] - 1s 4ms/step - loss: 2.3031 - accuracy: 0.0999\n",
      "Epoch 23/25\n",
      "300/300 [==============================] - 1s 4ms/step - loss: 2.3031 - accuracy: 0.0997\n",
      "Epoch 24/25\n",
      "300/300 [==============================] - 1s 4ms/step - loss: 2.3030 - accuracy: 0.0995\n",
      "Epoch 25/25\n",
      "300/300 [==============================] - 1s 4ms/step - loss: 2.3030 - accuracy: 0.1001\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 2.3028 - accuracy: 0.1000\n",
      "Fitting for combination 63\n",
      "(28, 28, 1)\n",
      "3\n",
      "[32, 32, 32]\n",
      "[(2, 2), (2, 2), (2, 2)]\n",
      "2\n",
      "[128, 32, 10]\n",
      "10\n",
      "True\n",
      "['relu', 'sigmoid', 'tanh']\n",
      "['relu', 'sigmoid', 'softmax']\n",
      "Adam\n",
      "0.3\n",
      "0.03\n",
      "categorical_crossentropy\n",
      "300\n",
      "25\n",
      "0\n",
      "Epoch 1/25\n",
      " 13/200 [>.............................] - ETA: 0s - loss: 2.6249 - accuracy: 0.1097 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-14 09:41:09.734934: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:954] layout failed: INVALID_ARGUMENT: Size of values 0 does not match size of permutation 4 @ fanin shape insequential_63/dropout_84/dropout/SelectV2-2-TransposeNHWCToNCHW-LayoutOptimizer\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200/200 [==============================] - 1s 4ms/step - loss: 2.3433 - accuracy: 0.1016\n",
      "Epoch 2/25\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 2.3121 - accuracy: 0.0985\n",
      "Epoch 3/25\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 2.3106 - accuracy: 0.0979\n",
      "Epoch 4/25\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 2.3066 - accuracy: 0.1021\n",
      "Epoch 5/25\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 2.3059 - accuracy: 0.0996\n",
      "Epoch 6/25\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 2.3051 - accuracy: 0.0986\n",
      "Epoch 7/25\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 2.3045 - accuracy: 0.1018\n",
      "Epoch 8/25\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 2.3043 - accuracy: 0.0988\n",
      "Epoch 9/25\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 2.3038 - accuracy: 0.1017\n",
      "Epoch 10/25\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 2.3040 - accuracy: 0.0997\n",
      "Epoch 11/25\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 2.3039 - accuracy: 0.0997\n",
      "Epoch 12/25\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 2.3036 - accuracy: 0.1005\n",
      "Epoch 13/25\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 2.3037 - accuracy: 0.0998\n",
      "Epoch 14/25\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 2.3034 - accuracy: 0.1002\n",
      "Epoch 15/25\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 2.3035 - accuracy: 0.1012\n",
      "Epoch 16/25\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 2.3035 - accuracy: 0.1004\n",
      "Epoch 17/25\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 2.3034 - accuracy: 0.1011\n",
      "Epoch 18/25\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 2.3035 - accuracy: 0.0985\n",
      "Epoch 19/25\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 2.3034 - accuracy: 0.0984\n",
      "Epoch 20/25\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 2.3034 - accuracy: 0.0988\n",
      "Epoch 21/25\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 2.3031 - accuracy: 0.1010\n",
      "Epoch 22/25\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 2.3034 - accuracy: 0.0991\n",
      "Epoch 23/25\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 2.3034 - accuracy: 0.0992\n",
      "Epoch 24/25\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 2.3033 - accuracy: 0.0997\n",
      "Epoch 25/25\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 2.3031 - accuracy: 0.0992\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 2.3029 - accuracy: 0.1000\n",
      "Fitting for combination 64\n",
      "(28, 28, 1)\n",
      "3\n",
      "[32, 32, 32]\n",
      "[(2, 2), (2, 2), (4, 4)]\n",
      "3\n",
      "[64, 128, 16, 10]\n",
      "10\n",
      "True\n",
      "['relu', 'sigmoid', 'sigmoid']\n",
      "['tanh', 'relu', 'sigmoid', 'softmax']\n",
      "Adam\n",
      "0.01\n",
      "0\n",
      "categorical_crossentropy\n",
      "300\n",
      "25\n",
      "0\n",
      "Epoch 1/25\n",
      " 12/200 [>.............................] - ETA: 0s - loss: 2.3605 - accuracy: 0.0956  "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-14 09:41:32.363881: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:954] layout failed: INVALID_ARGUMENT: Size of values 0 does not match size of permutation 4 @ fanin shape insequential_64/dropout_87/dropout/SelectV2-2-TransposeNHWCToNCHW-LayoutOptimizer\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200/200 [==============================] - 1s 5ms/step - loss: 2.3067 - accuracy: 0.0983\n",
      "Epoch 2/25\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 2.3031 - accuracy: 0.0991\n",
      "Epoch 3/25\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 2.3030 - accuracy: 0.1006\n",
      "Epoch 4/25\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 2.3031 - accuracy: 0.1001\n",
      "Epoch 5/25\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 2.3030 - accuracy: 0.0997\n",
      "Epoch 6/25\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 2.3031 - accuracy: 0.0979\n",
      "Epoch 7/25\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 2.3031 - accuracy: 0.0989\n",
      "Epoch 8/25\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 2.3031 - accuracy: 0.0985\n",
      "Epoch 9/25\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 2.3031 - accuracy: 0.0996\n",
      "Epoch 10/25\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 2.3032 - accuracy: 0.0991\n",
      "Epoch 11/25\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 2.3038 - accuracy: 0.0988\n",
      "Epoch 12/25\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 2.3030 - accuracy: 0.1011\n",
      "Epoch 13/25\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 2.3031 - accuracy: 0.0979\n",
      "Epoch 14/25\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 2.3030 - accuracy: 0.1001\n",
      "Epoch 15/25\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 2.3030 - accuracy: 0.0994\n",
      "Epoch 16/25\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 2.3032 - accuracy: 0.1005\n",
      "Epoch 17/25\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 2.3032 - accuracy: 0.0975\n",
      "Epoch 18/25\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 2.3032 - accuracy: 0.0980\n",
      "Epoch 19/25\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 2.3031 - accuracy: 0.0979\n",
      "Epoch 20/25\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 2.3031 - accuracy: 0.0965\n",
      "Epoch 21/25\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 2.3031 - accuracy: 0.0979\n",
      "Epoch 22/25\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 2.3031 - accuracy: 0.0985\n",
      "Epoch 23/25\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 2.3030 - accuracy: 0.0987\n",
      "Epoch 24/25\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 2.3032 - accuracy: 0.0976\n",
      "Epoch 25/25\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 2.3030 - accuracy: 0.0992\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 2.3029 - accuracy: 0.1000\n",
      "Fitting for combination 65\n",
      "(28, 28, 1)\n",
      "3\n",
      "[32, 32, 32]\n",
      "[(2, 2), (2, 2), (3, 3)]\n",
      "3\n",
      "[16, 16, 32, 10]\n",
      "10\n",
      "False\n",
      "['relu', 'sigmoid', 'tanh']\n",
      "['tanh', 'sigmoid', 'tanh', 'softmax']\n",
      "SGD\n",
      "0.01\n",
      "0.03\n",
      "categorical_crossentropy\n",
      "50\n",
      "25\n",
      "0\n",
      "Epoch 1/25\n",
      "1200/1200 [==============================] - 3s 2ms/step - loss: 2.3084 - accuracy: 0.0983\n",
      "Epoch 2/25\n",
      "1200/1200 [==============================] - 3s 3ms/step - loss: 2.3040 - accuracy: 0.1000\n",
      "Epoch 3/25\n",
      "1200/1200 [==============================] - 3s 2ms/step - loss: 2.3032 - accuracy: 0.1000\n",
      "Epoch 4/25\n",
      "1200/1200 [==============================] - 3s 2ms/step - loss: 2.3027 - accuracy: 0.1000\n",
      "Epoch 5/25\n",
      "1200/1200 [==============================] - 3s 2ms/step - loss: 2.3024 - accuracy: 0.1000\n",
      "Epoch 6/25\n",
      "1200/1200 [==============================] - 3s 2ms/step - loss: 2.3022 - accuracy: 0.1000\n",
      "Epoch 7/25\n",
      "1200/1200 [==============================] - 3s 2ms/step - loss: 2.3020 - accuracy: 0.1000\n",
      "Epoch 8/25\n",
      "1200/1200 [==============================] - 3s 2ms/step - loss: 2.3019 - accuracy: 0.1000\n",
      "Epoch 9/25\n",
      "1200/1200 [==============================] - 3s 2ms/step - loss: 2.3018 - accuracy: 0.1000\n",
      "Epoch 10/25\n",
      "1200/1200 [==============================] - 3s 2ms/step - loss: 2.3017 - accuracy: 0.1000\n",
      "Epoch 11/25\n",
      "1200/1200 [==============================] - 3s 2ms/step - loss: 2.3016 - accuracy: 0.1000\n",
      "Epoch 12/25\n",
      "1200/1200 [==============================] - 3s 2ms/step - loss: 2.3015 - accuracy: 0.1000\n",
      "Epoch 13/25\n",
      "1200/1200 [==============================] - 3s 2ms/step - loss: 2.3014 - accuracy: 0.1000\n",
      "Epoch 14/25\n",
      "1200/1200 [==============================] - 3s 2ms/step - loss: 2.3014 - accuracy: 0.1000\n",
      "Epoch 15/25\n",
      "1200/1200 [==============================] - 3s 2ms/step - loss: 2.3013 - accuracy: 0.1000\n",
      "Epoch 16/25\n",
      "1200/1200 [==============================] - 3s 2ms/step - loss: 2.3012 - accuracy: 0.1000\n",
      "Epoch 17/25\n",
      "1200/1200 [==============================] - 3s 2ms/step - loss: 2.3012 - accuracy: 0.1000\n",
      "Epoch 18/25\n",
      "1200/1200 [==============================] - 3s 2ms/step - loss: 2.3011 - accuracy: 0.1000\n",
      "Epoch 19/25\n",
      "1200/1200 [==============================] - 3s 2ms/step - loss: 2.3011 - accuracy: 0.1000\n",
      "Epoch 20/25\n",
      "1200/1200 [==============================] - 3s 2ms/step - loss: 2.3011 - accuracy: 0.1000\n",
      "Epoch 21/25\n",
      "1200/1200 [==============================] - 3s 2ms/step - loss: 2.3010 - accuracy: 0.1000\n",
      "Epoch 22/25\n",
      "1200/1200 [==============================] - 3s 2ms/step - loss: 2.3010 - accuracy: 0.1000\n",
      "Epoch 23/25\n",
      "1200/1200 [==============================] - 3s 2ms/step - loss: 2.3009 - accuracy: 0.1000\n",
      "Epoch 24/25\n",
      "1200/1200 [==============================] - 3s 2ms/step - loss: 2.3009 - accuracy: 0.1000\n",
      "Epoch 25/25\n",
      "1200/1200 [==============================] - 3s 2ms/step - loss: 2.3009 - accuracy: 0.1000\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 2.3008 - accuracy: 0.1000\n",
      "Fitting for combination 66\n",
      "(28, 28, 1)\n",
      "3\n",
      "[32, 32, 32]\n",
      "[(2, 2), (2, 2), (4, 4)]\n",
      "3\n",
      "[128, 32, 64, 10]\n",
      "10\n",
      "True\n",
      "['relu', 'sigmoid', 'tanh']\n",
      "['relu', 'relu', 'tanh', 'softmax']\n",
      "SGD\n",
      "0.3\n",
      "0\n",
      "categorical_crossentropy\n",
      "300\n",
      "25\n",
      "0\n",
      "Epoch 1/25\n",
      " 35/200 [====>.........................] - ETA: 0s - loss: 2.3897 - accuracy: 0.1034"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-14 09:43:09.872724: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:954] layout failed: INVALID_ARGUMENT: Size of values 0 does not match size of permutation 4 @ fanin shape insequential_66/dropout_91/dropout/SelectV2-2-TransposeNHWCToNCHW-LayoutOptimizer\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200/200 [==============================] - 1s 4ms/step - loss: 2.3194 - accuracy: 0.1014\n",
      "Epoch 2/25\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 2.3038 - accuracy: 0.1007\n",
      "Epoch 3/25\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 2.3035 - accuracy: 0.0993\n",
      "Epoch 4/25\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 2.3034 - accuracy: 0.0981\n",
      "Epoch 5/25\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 2.3035 - accuracy: 0.0994\n",
      "Epoch 6/25\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 2.3034 - accuracy: 0.0991\n",
      "Epoch 7/25\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 2.3035 - accuracy: 0.0995\n",
      "Epoch 8/25\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 2.3036 - accuracy: 0.1006\n",
      "Epoch 9/25\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 2.3034 - accuracy: 0.1015\n",
      "Epoch 10/25\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 2.3037 - accuracy: 0.1007\n",
      "Epoch 11/25\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 2.3034 - accuracy: 0.0984\n",
      "Epoch 12/25\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 2.3035 - accuracy: 0.0992\n",
      "Epoch 13/25\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 2.3035 - accuracy: 0.0978\n",
      "Epoch 14/25\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 2.3035 - accuracy: 0.0988\n",
      "Epoch 15/25\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 2.3035 - accuracy: 0.1004\n",
      "Epoch 16/25\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 2.3034 - accuracy: 0.0992\n",
      "Epoch 17/25\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 2.3034 - accuracy: 0.1012\n",
      "Epoch 18/25\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 2.3036 - accuracy: 0.0991\n",
      "Epoch 19/25\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 2.3035 - accuracy: 0.0993\n",
      "Epoch 20/25\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 2.3035 - accuracy: 0.0964\n",
      "Epoch 21/25\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 2.3035 - accuracy: 0.0975\n",
      "Epoch 22/25\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 2.3034 - accuracy: 0.0979\n",
      "Epoch 23/25\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 2.3034 - accuracy: 0.0997\n",
      "Epoch 24/25\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 2.3036 - accuracy: 0.0978\n",
      "Epoch 25/25\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 2.3033 - accuracy: 0.0999\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 2.3027 - accuracy: 0.1000\n",
      "Fitting for combination 67\n",
      "(28, 28, 1)\n",
      "3\n",
      "[32, 32, 32]\n",
      "[(2, 2), (2, 2), (4, 4)]\n",
      "3\n",
      "[128, 64, 128, 10]\n",
      "10\n",
      "True\n",
      "['relu', 'sigmoid', 'relu']\n",
      "['tanh', 'sigmoid', 'tanh', 'softmax']\n",
      "SGD\n",
      "0.1\n",
      "0.03\n",
      "categorical_crossentropy\n",
      "100\n",
      "25\n",
      "0\n",
      "Epoch 1/25\n",
      "  1/600 [..............................] - ETA: 3:38 - loss: 2.5549 - accuracy: 0.1100"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-14 09:43:33.321016: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:954] layout failed: INVALID_ARGUMENT: Size of values 0 does not match size of permutation 4 @ fanin shape insequential_67/dropout_95/dropout/SelectV2-2-TransposeNHWCToNCHW-LayoutOptimizer\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "600/600 [==============================] - 2s 3ms/step - loss: 2.3620 - accuracy: 0.1006\n",
      "Epoch 2/25\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 2.3362 - accuracy: 0.1013\n",
      "Epoch 3/25\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 2.3312 - accuracy: 0.1014\n",
      "Epoch 4/25\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 2.3300 - accuracy: 0.1010\n",
      "Epoch 5/25\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 2.3275 - accuracy: 0.0993\n",
      "Epoch 6/25\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 2.3275 - accuracy: 0.0983\n",
      "Epoch 7/25\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 2.3242 - accuracy: 0.1015\n",
      "Epoch 8/25\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 2.3244 - accuracy: 0.1002\n",
      "Epoch 9/25\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 2.3246 - accuracy: 0.0993\n",
      "Epoch 10/25\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 2.3234 - accuracy: 0.0996\n",
      "Epoch 11/25\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 2.3238 - accuracy: 0.1000\n",
      "Epoch 12/25\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 2.3228 - accuracy: 0.1004\n",
      "Epoch 13/25\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 2.3221 - accuracy: 0.1017\n",
      "Epoch 14/25\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 2.3204 - accuracy: 0.1003\n",
      "Epoch 15/25\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 2.3206 - accuracy: 0.1005\n",
      "Epoch 16/25\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 2.3219 - accuracy: 0.1003\n",
      "Epoch 17/25\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 2.3200 - accuracy: 0.1006\n",
      "Epoch 18/25\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 2.3196 - accuracy: 0.1024\n",
      "Epoch 19/25\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 2.3213 - accuracy: 0.1018\n",
      "Epoch 20/25\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 2.3204 - accuracy: 0.1018\n",
      "Epoch 21/25\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 2.3205 - accuracy: 0.0976\n",
      "Epoch 22/25\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 2.3208 - accuracy: 0.1014\n",
      "Epoch 23/25\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 2.3197 - accuracy: 0.1004\n",
      "Epoch 24/25\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 2.3194 - accuracy: 0.1005\n",
      "Epoch 25/25\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 2.3194 - accuracy: 0.1007\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 2.3020 - accuracy: 0.1000\n",
      "Fitting for combination 68\n",
      "(28, 28, 1)\n",
      "3\n",
      "[32, 32, 32]\n",
      "[(2, 2), (2, 2), (4, 4)]\n",
      "1\n",
      "[128, 10]\n",
      "10\n",
      "False\n",
      "['relu', 'sigmoid', 'sigmoid']\n",
      "['tanh', 'softmax']\n",
      "SGD\n",
      "0.03\n",
      "0.01\n",
      "categorical_crossentropy\n",
      "150\n",
      "25\n",
      "0\n",
      "Epoch 1/25\n",
      "400/400 [==============================] - 2s 3ms/step - loss: 2.3147 - accuracy: 0.1034\n",
      "Epoch 2/25\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 2.3014 - accuracy: 0.1066\n",
      "Epoch 3/25\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 2.2996 - accuracy: 0.1162\n",
      "Epoch 4/25\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 2.2983 - accuracy: 0.1204\n",
      "Epoch 5/25\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 2.2970 - accuracy: 0.1270\n",
      "Epoch 6/25\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 2.2958 - accuracy: 0.1402\n",
      "Epoch 7/25\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 2.2948 - accuracy: 0.1513\n",
      "Epoch 8/25\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 2.2937 - accuracy: 0.1636\n",
      "Epoch 9/25\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 2.2925 - accuracy: 0.1548\n",
      "Epoch 10/25\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 2.2913 - accuracy: 0.1804\n",
      "Epoch 11/25\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 2.2901 - accuracy: 0.2128\n",
      "Epoch 12/25\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 2.2888 - accuracy: 0.2067\n",
      "Epoch 13/25\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 2.2876 - accuracy: 0.2362\n",
      "Epoch 14/25\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 2.2861 - accuracy: 0.2350\n",
      "Epoch 15/25\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 2.2847 - accuracy: 0.2440\n",
      "Epoch 16/25\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 2.2833 - accuracy: 0.2485\n",
      "Epoch 17/25\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 2.2818 - accuracy: 0.3050\n",
      "Epoch 18/25\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 2.2802 - accuracy: 0.2948\n",
      "Epoch 19/25\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 2.2785 - accuracy: 0.3370\n",
      "Epoch 20/25\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 2.2768 - accuracy: 0.3253\n",
      "Epoch 21/25\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 2.2749 - accuracy: 0.3624\n",
      "Epoch 22/25\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 2.2730 - accuracy: 0.3620\n",
      "Epoch 23/25\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 2.2710 - accuracy: 0.3840\n",
      "Epoch 24/25\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 2.2690 - accuracy: 0.4074\n",
      "Epoch 25/25\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 2.2666 - accuracy: 0.3736\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 2.2655 - accuracy: 0.4755\n",
      "Fitting for combination 69\n",
      "(28, 28, 1)\n",
      "3\n",
      "[32, 32, 32]\n",
      "[(2, 2), (2, 2), (3, 3)]\n",
      "2\n",
      "[16, 32, 10]\n",
      "10\n",
      "False\n",
      "['relu', 'sigmoid', 'relu']\n",
      "['sigmoid', 'tanh', 'softmax']\n",
      "SGD\n",
      "0.1\n",
      "0\n",
      "categorical_crossentropy\n",
      "50\n",
      "25\n",
      "0\n",
      "Epoch 1/25\n",
      "1200/1200 [==============================] - 3s 2ms/step - loss: 0.8342 - accuracy: 0.6921\n",
      "Epoch 2/25\n",
      "1200/1200 [==============================] - 3s 2ms/step - loss: 0.4708 - accuracy: 0.8280\n",
      "Epoch 3/25\n",
      "1200/1200 [==============================] - 3s 2ms/step - loss: 0.4048 - accuracy: 0.8525\n",
      "Epoch 4/25\n",
      "1200/1200 [==============================] - 3s 2ms/step - loss: 0.3687 - accuracy: 0.8655\n",
      "Epoch 5/25\n",
      "1200/1200 [==============================] - 3s 2ms/step - loss: 0.3459 - accuracy: 0.8737\n",
      "Epoch 6/25\n",
      "1200/1200 [==============================] - 3s 2ms/step - loss: 0.3250 - accuracy: 0.8800\n",
      "Epoch 7/25\n",
      "1200/1200 [==============================] - 3s 2ms/step - loss: 0.3052 - accuracy: 0.8877\n",
      "Epoch 8/25\n",
      "1200/1200 [==============================] - 3s 2ms/step - loss: 0.2924 - accuracy: 0.8924\n",
      "Epoch 9/25\n",
      "1200/1200 [==============================] - 3s 2ms/step - loss: 0.2792 - accuracy: 0.8975\n",
      "Epoch 10/25\n",
      "1200/1200 [==============================] - 3s 2ms/step - loss: 0.2681 - accuracy: 0.9012\n",
      "Epoch 11/25\n",
      "1200/1200 [==============================] - 3s 2ms/step - loss: 0.2573 - accuracy: 0.9050\n",
      "Epoch 12/25\n",
      "1200/1200 [==============================] - 3s 2ms/step - loss: 0.2490 - accuracy: 0.9092\n",
      "Epoch 13/25\n",
      "1200/1200 [==============================] - 3s 2ms/step - loss: 0.2392 - accuracy: 0.9119\n",
      "Epoch 14/25\n",
      "1200/1200 [==============================] - 3s 2ms/step - loss: 0.2299 - accuracy: 0.9153\n",
      "Epoch 15/25\n",
      "1200/1200 [==============================] - 3s 2ms/step - loss: 0.2217 - accuracy: 0.9172\n",
      "Epoch 16/25\n",
      "1200/1200 [==============================] - 3s 2ms/step - loss: 0.2150 - accuracy: 0.9200\n",
      "Epoch 17/25\n",
      "1200/1200 [==============================] - 3s 2ms/step - loss: 0.2078 - accuracy: 0.9233\n",
      "Epoch 18/25\n",
      "1200/1200 [==============================] - 3s 2ms/step - loss: 0.2022 - accuracy: 0.9250\n",
      "Epoch 19/25\n",
      "1200/1200 [==============================] - 3s 2ms/step - loss: 0.1948 - accuracy: 0.9277\n",
      "Epoch 20/25\n",
      "1200/1200 [==============================] - 3s 2ms/step - loss: 0.1897 - accuracy: 0.9302\n",
      "Epoch 21/25\n",
      "1200/1200 [==============================] - 3s 2ms/step - loss: 0.1847 - accuracy: 0.9321\n",
      "Epoch 22/25\n",
      "1200/1200 [==============================] - 3s 2ms/step - loss: 0.1775 - accuracy: 0.9344\n",
      "Epoch 23/25\n",
      "1200/1200 [==============================] - 3s 2ms/step - loss: 0.1723 - accuracy: 0.9374\n",
      "Epoch 24/25\n",
      "1200/1200 [==============================] - 3s 2ms/step - loss: 0.1674 - accuracy: 0.9394\n",
      "Epoch 25/25\n",
      "1200/1200 [==============================] - 3s 2ms/step - loss: 0.1643 - accuracy: 0.9407\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.2895 - accuracy: 0.9032\n",
      "Fitting for combination 70\n",
      "(28, 28, 1)\n",
      "3\n",
      "[32, 32, 64]\n",
      "[(2, 2), (2, 2), (3, 3)]\n",
      "1\n",
      "[64, 10]\n",
      "10\n",
      "True\n",
      "['relu', 'sigmoid', 'tanh']\n",
      "['sigmoid', 'softmax']\n",
      "Adam\n",
      "0.3\n",
      "0.03\n",
      "categorical_crossentropy\n",
      "150\n",
      "25\n",
      "0\n",
      "Epoch 1/25\n",
      " 28/400 [=>............................] - ETA: 1s - loss: 3.9280 - accuracy: 0.1012"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-14 09:46:06.088487: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:954] layout failed: INVALID_ARGUMENT: Size of values 0 does not match size of permutation 4 @ fanin shape insequential_70/dropout_99/dropout/SelectV2-2-TransposeNHWCToNCHW-LayoutOptimizer\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "400/400 [==============================] - 2s 3ms/step - loss: 2.4663 - accuracy: 0.1019\n",
      "Epoch 2/25\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 2.3220 - accuracy: 0.0998\n",
      "Epoch 3/25\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 2.3164 - accuracy: 0.0977\n",
      "Epoch 4/25\n",
      "400/400 [==============================] - 1s 4ms/step - loss: 2.3127 - accuracy: 0.0999\n",
      "Epoch 5/25\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 2.3107 - accuracy: 0.1008\n",
      "Epoch 6/25\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 2.3093 - accuracy: 0.0991\n",
      "Epoch 7/25\n",
      "400/400 [==============================] - 1s 4ms/step - loss: 2.3083 - accuracy: 0.1012\n",
      "Epoch 8/25\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 2.3084 - accuracy: 0.0994\n",
      "Epoch 9/25\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 2.3070 - accuracy: 0.0989\n",
      "Epoch 10/25\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 2.3072 - accuracy: 0.0993\n",
      "Epoch 11/25\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 2.3062 - accuracy: 0.1000\n",
      "Epoch 12/25\n",
      "400/400 [==============================] - 1s 4ms/step - loss: 2.3063 - accuracy: 0.0999\n",
      "Epoch 13/25\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 2.3061 - accuracy: 0.0995\n",
      "Epoch 14/25\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 2.3057 - accuracy: 0.1003\n",
      "Epoch 15/25\n",
      "400/400 [==============================] - 1s 4ms/step - loss: 2.3054 - accuracy: 0.0991\n",
      "Epoch 16/25\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 2.3052 - accuracy: 0.1022\n",
      "Epoch 17/25\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 2.3052 - accuracy: 0.0988\n",
      "Epoch 18/25\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 2.3050 - accuracy: 0.0984\n",
      "Epoch 19/25\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 2.3048 - accuracy: 0.0991\n",
      "Epoch 20/25\n",
      "400/400 [==============================] - 1s 4ms/step - loss: 2.3044 - accuracy: 0.0994\n",
      "Epoch 21/25\n",
      "400/400 [==============================] - 1s 4ms/step - loss: 2.3050 - accuracy: 0.0974\n",
      "Epoch 22/25\n",
      "400/400 [==============================] - 1s 4ms/step - loss: 2.3047 - accuracy: 0.0989\n",
      "Epoch 23/25\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 2.3042 - accuracy: 0.0988\n",
      "Epoch 24/25\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 2.3043 - accuracy: 0.1000\n",
      "Epoch 25/25\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 2.3045 - accuracy: 0.1019\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 2.3028 - accuracy: 0.1000\n",
      "Fitting for combination 71\n",
      "(28, 28, 1)\n",
      "3\n",
      "[32, 32, 64]\n",
      "[(2, 2), (2, 2), (5, 5)]\n",
      "3\n",
      "[128, 16, 32, 10]\n",
      "10\n",
      "False\n",
      "['relu', 'sigmoid', 'sigmoid']\n",
      "['relu', 'tanh', 'sigmoid', 'softmax']\n",
      "Adam\n",
      "0.03\n",
      "0.03\n",
      "categorical_crossentropy\n",
      "100\n",
      "25\n",
      "0\n",
      "Epoch 1/25\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 2.3059 - accuracy: 0.1002\n",
      "Epoch 2/25\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 2.3030 - accuracy: 0.0996\n",
      "Epoch 3/25\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 2.3028 - accuracy: 0.1003\n",
      "Epoch 4/25\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 2.3028 - accuracy: 0.0987\n",
      "Epoch 5/25\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 2.3028 - accuracy: 0.0975\n",
      "Epoch 6/25\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 2.3027 - accuracy: 0.0980\n",
      "Epoch 7/25\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 2.3027 - accuracy: 0.0984\n",
      "Epoch 8/25\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 2.3027 - accuracy: 0.0979\n",
      "Epoch 9/25\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 2.3027 - accuracy: 0.0983\n",
      "Epoch 10/25\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 2.3027 - accuracy: 0.0984\n",
      "Epoch 11/25\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 2.3027 - accuracy: 0.0983\n",
      "Epoch 12/25\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 2.3027 - accuracy: 0.0999\n",
      "Epoch 13/25\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 2.3027 - accuracy: 0.0971\n",
      "Epoch 14/25\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 2.3027 - accuracy: 0.0984\n",
      "Epoch 15/25\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 2.3026 - accuracy: 0.0986\n",
      "Epoch 16/25\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 2.3026 - accuracy: 0.0985\n",
      "Epoch 17/25\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 2.3026 - accuracy: 0.0987\n",
      "Epoch 18/25\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 2.3026 - accuracy: 0.0972\n",
      "Epoch 19/25\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 2.3026 - accuracy: 0.0974\n",
      "Epoch 20/25\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 2.3026 - accuracy: 0.0967\n",
      "Epoch 21/25\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 2.3026 - accuracy: 0.0958\n",
      "Epoch 22/25\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 2.3026 - accuracy: 0.0981\n",
      "Epoch 23/25\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 2.3026 - accuracy: 0.0967\n",
      "Epoch 24/25\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 2.3026 - accuracy: 0.0963\n",
      "Epoch 25/25\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 2.3026 - accuracy: 0.0985\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 2.3026 - accuracy: 0.1000\n",
      "Fitting for combination 72\n",
      "(28, 28, 1)\n",
      "3\n",
      "[32, 32, 64]\n",
      "[(2, 2), (2, 2), (4, 4)]\n",
      "1\n",
      "[32, 10]\n",
      "10\n",
      "False\n",
      "['relu', 'sigmoid', 'tanh']\n",
      "['sigmoid', 'softmax']\n",
      "Adam\n",
      "0.1\n",
      "0.01\n",
      "categorical_crossentropy\n",
      "200\n",
      "25\n",
      "0\n",
      "Epoch 1/25\n",
      "300/300 [==============================] - 1s 4ms/step - loss: 2.3487 - accuracy: 0.0992\n",
      "Epoch 2/25\n",
      "300/300 [==============================] - 1s 4ms/step - loss: 2.3161 - accuracy: 0.1009\n",
      "Epoch 3/25\n",
      "300/300 [==============================] - 1s 4ms/step - loss: 2.3125 - accuracy: 0.1003\n",
      "Epoch 4/25\n",
      "300/300 [==============================] - 1s 4ms/step - loss: 2.3112 - accuracy: 0.1004\n",
      "Epoch 5/25\n",
      "300/300 [==============================] - 1s 4ms/step - loss: 2.3088 - accuracy: 0.1004\n",
      "Epoch 6/25\n",
      "300/300 [==============================] - 1s 4ms/step - loss: 2.3086 - accuracy: 0.1001\n",
      "Epoch 7/25\n",
      "300/300 [==============================] - 1s 4ms/step - loss: 2.3076 - accuracy: 0.0985\n",
      "Epoch 8/25\n",
      "300/300 [==============================] - 1s 4ms/step - loss: 2.3078 - accuracy: 0.1007\n",
      "Epoch 9/25\n",
      "300/300 [==============================] - 1s 4ms/step - loss: 2.3063 - accuracy: 0.0993\n",
      "Epoch 10/25\n",
      "300/300 [==============================] - 1s 4ms/step - loss: 2.3067 - accuracy: 0.1005\n",
      "Epoch 11/25\n",
      "300/300 [==============================] - 1s 4ms/step - loss: 2.3062 - accuracy: 0.0996\n",
      "Epoch 12/25\n",
      "300/300 [==============================] - 1s 4ms/step - loss: 2.3057 - accuracy: 0.1001\n",
      "Epoch 13/25\n",
      "300/300 [==============================] - 1s 4ms/step - loss: 2.3055 - accuracy: 0.0994\n",
      "Epoch 14/25\n",
      "300/300 [==============================] - 1s 4ms/step - loss: 2.3053 - accuracy: 0.0998\n",
      "Epoch 15/25\n",
      "300/300 [==============================] - 1s 4ms/step - loss: 2.3051 - accuracy: 0.0983\n",
      "Epoch 16/25\n",
      "300/300 [==============================] - 1s 4ms/step - loss: 2.3049 - accuracy: 0.1007\n",
      "Epoch 17/25\n",
      "300/300 [==============================] - 1s 4ms/step - loss: 2.3048 - accuracy: 0.0999\n",
      "Epoch 18/25\n",
      "300/300 [==============================] - 1s 4ms/step - loss: 2.3051 - accuracy: 0.0993\n",
      "Epoch 19/25\n",
      "300/300 [==============================] - 1s 4ms/step - loss: 2.3047 - accuracy: 0.0987\n",
      "Epoch 20/25\n",
      "300/300 [==============================] - 1s 4ms/step - loss: 2.3045 - accuracy: 0.0987\n",
      "Epoch 21/25\n",
      "300/300 [==============================] - 1s 4ms/step - loss: 2.3045 - accuracy: 0.1000\n",
      "Epoch 22/25\n",
      "300/300 [==============================] - 1s 4ms/step - loss: 2.3045 - accuracy: 0.0994\n",
      "Epoch 23/25\n",
      "300/300 [==============================] - 1s 4ms/step - loss: 2.3041 - accuracy: 0.0982\n",
      "Epoch 24/25\n",
      "300/300 [==============================] - 1s 4ms/step - loss: 2.3044 - accuracy: 0.0976\n",
      "Epoch 25/25\n",
      "300/300 [==============================] - 1s 4ms/step - loss: 2.3042 - accuracy: 0.1012\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 2.3035 - accuracy: 0.1000\n",
      "Fitting for combination 73\n",
      "(28, 28, 1)\n",
      "3\n",
      "[32, 32, 64]\n",
      "[(2, 2), (2, 2), (5, 5)]\n",
      "3\n",
      "[128, 32, 128, 10]\n",
      "10\n",
      "False\n",
      "['relu', 'sigmoid', 'relu']\n",
      "['sigmoid', 'relu', 'sigmoid', 'softmax']\n",
      "Adam\n",
      "0.01\n",
      "0.03\n",
      "categorical_crossentropy\n",
      "300\n",
      "25\n",
      "0\n",
      "Epoch 1/25\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 2.3128 - accuracy: 0.1000\n",
      "Epoch 2/25\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 2.3051 - accuracy: 0.0997\n",
      "Epoch 3/25\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 2.3041 - accuracy: 0.0992\n",
      "Epoch 4/25\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 2.3038 - accuracy: 0.0997\n",
      "Epoch 5/25\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 2.3036 - accuracy: 0.1005\n",
      "Epoch 6/25\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 2.3036 - accuracy: 0.0984\n",
      "Epoch 7/25\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 2.3033 - accuracy: 0.1001\n",
      "Epoch 8/25\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 2.3032 - accuracy: 0.0988\n",
      "Epoch 9/25\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 2.3032 - accuracy: 0.1003\n",
      "Epoch 10/25\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 2.3032 - accuracy: 0.0997\n",
      "Epoch 11/25\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 2.3030 - accuracy: 0.0984\n",
      "Epoch 12/25\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 2.3030 - accuracy: 0.1014\n",
      "Epoch 13/25\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 2.3031 - accuracy: 0.0975\n",
      "Epoch 14/25\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 2.3029 - accuracy: 0.1002\n",
      "Epoch 15/25\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 2.3029 - accuracy: 0.0998\n",
      "Epoch 16/25\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 2.3030 - accuracy: 0.0991\n",
      "Epoch 17/25\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 2.3030 - accuracy: 0.0984\n",
      "Epoch 18/25\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 2.3030 - accuracy: 0.0962\n",
      "Epoch 19/25\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 2.3029 - accuracy: 0.0981\n",
      "Epoch 20/25\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 2.3029 - accuracy: 0.0987\n",
      "Epoch 21/25\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 2.3029 - accuracy: 0.0988\n",
      "Epoch 22/25\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 2.3029 - accuracy: 0.0980\n",
      "Epoch 23/25\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 2.3028 - accuracy: 0.0973\n",
      "Epoch 24/25\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 2.3029 - accuracy: 0.0971\n",
      "Epoch 25/25\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 2.3028 - accuracy: 0.0998\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 2.3027 - accuracy: 0.1000\n",
      "Fitting for combination 74\n",
      "(28, 28, 1)\n",
      "3\n",
      "[32, 32, 64]\n",
      "[(2, 2), (2, 2), (4, 4)]\n",
      "2\n",
      "[64, 64, 10]\n",
      "10\n",
      "True\n",
      "['relu', 'sigmoid', 'sigmoid']\n",
      "['sigmoid', 'sigmoid', 'softmax']\n",
      "Adam\n",
      "0.3\n",
      "0.01\n",
      "categorical_crossentropy\n",
      "50\n",
      "25\n",
      "0\n",
      "Epoch 1/25\n",
      "   1/1200 [..............................] - ETA: 13:29 - loss: 2.8389 - accuracy: 0.0600"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-14 09:48:27.124053: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:954] layout failed: INVALID_ARGUMENT: Size of values 0 does not match size of permutation 4 @ fanin shape insequential_74/dropout_101/dropout/SelectV2-2-TransposeNHWCToNCHW-LayoutOptimizer\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1200/1200 [==============================] - 4s 3ms/step - loss: 2.3977 - accuracy: 0.1006\n",
      "Epoch 2/25\n",
      "1200/1200 [==============================] - 4s 3ms/step - loss: 2.3260 - accuracy: 0.0988\n",
      "Epoch 3/25\n",
      "1200/1200 [==============================] - 4s 3ms/step - loss: 2.3168 - accuracy: 0.0998\n",
      "Epoch 4/25\n",
      "1200/1200 [==============================] - 3s 3ms/step - loss: 2.3126 - accuracy: 0.1012\n",
      "Epoch 5/25\n",
      "1200/1200 [==============================] - 3s 3ms/step - loss: 2.3108 - accuracy: 0.0996\n",
      "Epoch 6/25\n",
      "1200/1200 [==============================] - 3s 3ms/step - loss: 2.3084 - accuracy: 0.1015\n",
      "Epoch 7/25\n",
      "1200/1200 [==============================] - 3s 3ms/step - loss: 2.3084 - accuracy: 0.0991\n",
      "Epoch 8/25\n",
      "1200/1200 [==============================] - 3s 3ms/step - loss: 2.3077 - accuracy: 0.0981\n",
      "Epoch 9/25\n",
      "1200/1200 [==============================] - 3s 3ms/step - loss: 2.3069 - accuracy: 0.1005\n",
      "Epoch 10/25\n",
      "1200/1200 [==============================] - 3s 3ms/step - loss: 2.3062 - accuracy: 0.0980\n",
      "Epoch 11/25\n",
      "1200/1200 [==============================] - 3s 3ms/step - loss: 2.3062 - accuracy: 0.0998\n",
      "Epoch 12/25\n",
      "1200/1200 [==============================] - 3s 3ms/step - loss: 2.3057 - accuracy: 0.0996\n",
      "Epoch 13/25\n",
      "1200/1200 [==============================] - 3s 3ms/step - loss: 2.3055 - accuracy: 0.0986\n",
      "Epoch 14/25\n",
      "1200/1200 [==============================] - 3s 3ms/step - loss: 2.3056 - accuracy: 0.0979\n",
      "Epoch 15/25\n",
      "1200/1200 [==============================] - 3s 3ms/step - loss: 2.3050 - accuracy: 0.1002\n",
      "Epoch 16/25\n",
      "1200/1200 [==============================] - 3s 3ms/step - loss: 2.3049 - accuracy: 0.1010\n",
      "Epoch 17/25\n",
      "1200/1200 [==============================] - 4s 3ms/step - loss: 2.3048 - accuracy: 0.0996\n",
      "Epoch 18/25\n",
      "1200/1200 [==============================] - 3s 3ms/step - loss: 2.3048 - accuracy: 0.0981\n",
      "Epoch 19/25\n",
      "1200/1200 [==============================] - 3s 3ms/step - loss: 2.3047 - accuracy: 0.0979\n",
      "Epoch 20/25\n",
      "1200/1200 [==============================] - 3s 3ms/step - loss: 2.3046 - accuracy: 0.0987\n",
      "Epoch 21/25\n",
      "1200/1200 [==============================] - 3s 3ms/step - loss: 2.3045 - accuracy: 0.0997\n",
      "Epoch 22/25\n",
      "1200/1200 [==============================] - 3s 3ms/step - loss: 2.3044 - accuracy: 0.0991\n",
      "Epoch 23/25\n",
      "1200/1200 [==============================] - 3s 3ms/step - loss: 2.3044 - accuracy: 0.0977\n",
      "Epoch 24/25\n",
      "1200/1200 [==============================] - 4s 3ms/step - loss: 2.3043 - accuracy: 0.0985\n",
      "Epoch 25/25\n",
      "1200/1200 [==============================] - 3s 3ms/step - loss: 2.3041 - accuracy: 0.0996\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 2.3029 - accuracy: 0.1000\n",
      "Fitting for combination 75\n",
      "(28, 28, 1)\n",
      "3\n",
      "[32, 32, 64]\n",
      "[(2, 2), (2, 2), (5, 5)]\n",
      "3\n",
      "[128, 64, 64, 10]\n",
      "10\n",
      "True\n",
      "['relu', 'sigmoid', 'tanh']\n",
      "['sigmoid', 'sigmoid', 'sigmoid', 'softmax']\n",
      "SGD\n",
      "0.03\n",
      "0.01\n",
      "categorical_crossentropy\n",
      "50\n",
      "25\n",
      "0\n",
      "Epoch 1/25\n",
      "  36/1200 [..............................] - ETA: 3s - loss: 2.5253 - accuracy: 0.1056"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-14 09:49:54.837948: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:954] layout failed: INVALID_ARGUMENT: Size of values 0 does not match size of permutation 4 @ fanin shape insequential_75/dropout_104/dropout/SelectV2-2-TransposeNHWCToNCHW-LayoutOptimizer\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1200/1200 [==============================] - 4s 3ms/step - loss: 2.4312 - accuracy: 0.1037\n",
      "Epoch 2/25\n",
      "1200/1200 [==============================] - 3s 3ms/step - loss: 2.3991 - accuracy: 0.0996\n",
      "Epoch 3/25\n",
      "1200/1200 [==============================] - 3s 3ms/step - loss: 2.3892 - accuracy: 0.0997\n",
      "Epoch 4/25\n",
      "1200/1200 [==============================] - 3s 3ms/step - loss: 2.3844 - accuracy: 0.0992\n",
      "Epoch 5/25\n",
      "1200/1200 [==============================] - 3s 3ms/step - loss: 2.3809 - accuracy: 0.1005\n",
      "Epoch 6/25\n",
      "1200/1200 [==============================] - 3s 3ms/step - loss: 2.3756 - accuracy: 0.1020\n",
      "Epoch 7/25\n",
      "1200/1200 [==============================] - 3s 3ms/step - loss: 2.3751 - accuracy: 0.0990\n",
      "Epoch 8/25\n",
      "1200/1200 [==============================] - 3s 3ms/step - loss: 2.3737 - accuracy: 0.0990\n",
      "Epoch 9/25\n",
      "1200/1200 [==============================] - 3s 3ms/step - loss: 2.3723 - accuracy: 0.0991\n",
      "Epoch 10/25\n",
      "1200/1200 [==============================] - 3s 3ms/step - loss: 2.3706 - accuracy: 0.0994\n",
      "Epoch 11/25\n",
      "1200/1200 [==============================] - 3s 3ms/step - loss: 2.3677 - accuracy: 0.0998\n",
      "Epoch 12/25\n",
      "1200/1200 [==============================] - 3s 3ms/step - loss: 2.3678 - accuracy: 0.0992\n",
      "Epoch 13/25\n",
      "1200/1200 [==============================] - 3s 3ms/step - loss: 2.3683 - accuracy: 0.1004\n",
      "Epoch 14/25\n",
      "1200/1200 [==============================] - 3s 3ms/step - loss: 2.3669 - accuracy: 0.1007\n",
      "Epoch 15/25\n",
      "1200/1200 [==============================] - 3s 3ms/step - loss: 2.3645 - accuracy: 0.1016\n",
      "Epoch 16/25\n",
      "1200/1200 [==============================] - 3s 3ms/step - loss: 2.3634 - accuracy: 0.0997\n",
      "Epoch 17/25\n",
      "1200/1200 [==============================] - 3s 3ms/step - loss: 2.3645 - accuracy: 0.0984\n",
      "Epoch 18/25\n",
      "1200/1200 [==============================] - 3s 3ms/step - loss: 2.3621 - accuracy: 0.1012\n",
      "Epoch 19/25\n",
      "1200/1200 [==============================] - 3s 3ms/step - loss: 2.3630 - accuracy: 0.0995\n",
      "Epoch 20/25\n",
      "1200/1200 [==============================] - 3s 3ms/step - loss: 2.3621 - accuracy: 0.0988\n",
      "Epoch 21/25\n",
      "1200/1200 [==============================] - 3s 3ms/step - loss: 2.3619 - accuracy: 0.1001\n",
      "Epoch 22/25\n",
      "1200/1200 [==============================] - 3s 3ms/step - loss: 2.3616 - accuracy: 0.0994\n",
      "Epoch 23/25\n",
      "1200/1200 [==============================] - 3s 3ms/step - loss: 2.3596 - accuracy: 0.1020\n",
      "Epoch 24/25\n",
      "1200/1200 [==============================] - 3s 3ms/step - loss: 2.3606 - accuracy: 0.1012\n",
      "Epoch 25/25\n",
      "1200/1200 [==============================] - 3s 3ms/step - loss: 2.3599 - accuracy: 0.0997\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 2.3026 - accuracy: 0.1000\n",
      "Fitting for combination 76\n",
      "(28, 28, 1)\n",
      "3\n",
      "[32, 32, 64]\n",
      "[(2, 2), (2, 2), (4, 4)]\n",
      "1\n",
      "[64, 10]\n",
      "10\n",
      "True\n",
      "['relu', 'sigmoid', 'tanh']\n",
      "['relu', 'softmax']\n",
      "SGD\n",
      "0.01\n",
      "0.03\n",
      "categorical_crossentropy\n",
      "150\n",
      "25\n",
      "0\n",
      "Epoch 1/25\n",
      " 31/400 [=>............................] - ETA: 1s - loss: 2.3150 - accuracy: 0.1108"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-14 09:51:17.218837: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:954] layout failed: INVALID_ARGUMENT: Size of values 0 does not match size of permutation 4 @ fanin shape insequential_76/dropout_108/dropout/SelectV2-2-TransposeNHWCToNCHW-LayoutOptimizer\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "400/400 [==============================] - 2s 3ms/step - loss: 2.2983 - accuracy: 0.1150\n",
      "Epoch 2/25\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 2.2889 - accuracy: 0.1269\n",
      "Epoch 3/25\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 2.2831 - accuracy: 0.1325\n",
      "Epoch 4/25\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 2.2778 - accuracy: 0.1407\n",
      "Epoch 5/25\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 2.2725 - accuracy: 0.1448\n",
      "Epoch 6/25\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 2.2678 - accuracy: 0.1520\n",
      "Epoch 7/25\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 2.2629 - accuracy: 0.1548\n",
      "Epoch 8/25\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 2.2594 - accuracy: 0.1590\n",
      "Epoch 9/25\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 2.2550 - accuracy: 0.1633\n",
      "Epoch 10/25\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 2.2515 - accuracy: 0.1679\n",
      "Epoch 11/25\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 2.2457 - accuracy: 0.1749\n",
      "Epoch 12/25\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 2.2426 - accuracy: 0.1773\n",
      "Epoch 13/25\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 2.2385 - accuracy: 0.1798\n",
      "Epoch 14/25\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 2.2349 - accuracy: 0.1843\n",
      "Epoch 15/25\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 2.2314 - accuracy: 0.1880\n",
      "Epoch 16/25\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 2.2268 - accuracy: 0.1895\n",
      "Epoch 17/25\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 2.2238 - accuracy: 0.1947\n",
      "Epoch 18/25\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 2.2218 - accuracy: 0.1959\n",
      "Epoch 19/25\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 2.2180 - accuracy: 0.1998\n",
      "Epoch 20/25\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 2.2145 - accuracy: 0.2043\n",
      "Epoch 21/25\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 2.2109 - accuracy: 0.2095\n",
      "Epoch 22/25\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 2.2072 - accuracy: 0.2113\n",
      "Epoch 23/25\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 2.2044 - accuracy: 0.2137\n",
      "Epoch 24/25\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 2.2012 - accuracy: 0.2167\n",
      "Epoch 25/25\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 2.1966 - accuracy: 0.2222\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 2.1728 - accuracy: 0.3930\n",
      "Fitting for combination 77\n",
      "(28, 28, 1)\n",
      "3\n",
      "[32, 32, 64]\n",
      "[(2, 2), (2, 2), (3, 3)]\n",
      "3\n",
      "[128, 64, 32, 10]\n",
      "10\n",
      "True\n",
      "['relu', 'sigmoid', 'sigmoid']\n",
      "['relu', 'sigmoid', 'relu', 'softmax']\n",
      "SGD\n",
      "0.03\n",
      "0.03\n",
      "categorical_crossentropy\n",
      "50\n",
      "25\n",
      "0\n",
      "Epoch 1/25\n",
      "  39/1200 [..............................] - ETA: 3s - loss: 2.4457 - accuracy: 0.1092"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-14 09:51:52.002029: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:954] layout failed: INVALID_ARGUMENT: Size of values 0 does not match size of permutation 4 @ fanin shape insequential_77/dropout_110/dropout/SelectV2-2-TransposeNHWCToNCHW-LayoutOptimizer\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1200/1200 [==============================] - 3s 3ms/step - loss: 2.3414 - accuracy: 0.0997\n",
      "Epoch 2/25\n",
      "1200/1200 [==============================] - 3s 3ms/step - loss: 2.3267 - accuracy: 0.0996\n",
      "Epoch 3/25\n",
      "1200/1200 [==============================] - 3s 3ms/step - loss: 2.3239 - accuracy: 0.0991\n",
      "Epoch 4/25\n",
      "1200/1200 [==============================] - 3s 3ms/step - loss: 2.3224 - accuracy: 0.1010\n",
      "Epoch 5/25\n",
      "1200/1200 [==============================] - 3s 3ms/step - loss: 2.3231 - accuracy: 0.0988\n",
      "Epoch 6/25\n",
      "1200/1200 [==============================] - 3s 3ms/step - loss: 2.3202 - accuracy: 0.1008\n",
      "Epoch 7/25\n",
      "1200/1200 [==============================] - 3s 3ms/step - loss: 2.3210 - accuracy: 0.1006\n",
      "Epoch 8/25\n",
      "1200/1200 [==============================] - 3s 3ms/step - loss: 2.3216 - accuracy: 0.0980\n",
      "Epoch 9/25\n",
      "1200/1200 [==============================] - 3s 3ms/step - loss: 2.3197 - accuracy: 0.1000\n",
      "Epoch 10/25\n",
      "1200/1200 [==============================] - 3s 3ms/step - loss: 2.3188 - accuracy: 0.1014\n",
      "Epoch 11/25\n",
      "1200/1200 [==============================] - 3s 3ms/step - loss: 2.3183 - accuracy: 0.1016\n",
      "Epoch 12/25\n",
      "1200/1200 [==============================] - 3s 3ms/step - loss: 2.3193 - accuracy: 0.1009\n",
      "Epoch 13/25\n",
      "1200/1200 [==============================] - 3s 3ms/step - loss: 2.3201 - accuracy: 0.0976\n",
      "Epoch 14/25\n",
      "1200/1200 [==============================] - 3s 3ms/step - loss: 2.3198 - accuracy: 0.1008\n",
      "Epoch 15/25\n",
      "1200/1200 [==============================] - 3s 3ms/step - loss: 2.3173 - accuracy: 0.1021\n",
      "Epoch 16/25\n",
      "1200/1200 [==============================] - 3s 3ms/step - loss: 2.3185 - accuracy: 0.1003\n",
      "Epoch 17/25\n",
      "1200/1200 [==============================] - 3s 3ms/step - loss: 2.3190 - accuracy: 0.0987\n",
      "Epoch 18/25\n",
      "1200/1200 [==============================] - 3s 3ms/step - loss: 2.3176 - accuracy: 0.0998\n",
      "Epoch 19/25\n",
      "1200/1200 [==============================] - 3s 3ms/step - loss: 2.3184 - accuracy: 0.1000\n",
      "Epoch 20/25\n",
      "1200/1200 [==============================] - 3s 3ms/step - loss: 2.3173 - accuracy: 0.1009\n",
      "Epoch 21/25\n",
      "1200/1200 [==============================] - 3s 3ms/step - loss: 2.3168 - accuracy: 0.1022\n",
      "Epoch 22/25\n",
      "1200/1200 [==============================] - 3s 3ms/step - loss: 2.3173 - accuracy: 0.1002\n",
      "Epoch 23/25\n",
      "1200/1200 [==============================] - 3s 3ms/step - loss: 2.3177 - accuracy: 0.0994\n",
      "Epoch 24/25\n",
      "1200/1200 [==============================] - 3s 3ms/step - loss: 2.3183 - accuracy: 0.0986\n",
      "Epoch 25/25\n",
      "1200/1200 [==============================] - 3s 3ms/step - loss: 2.3168 - accuracy: 0.1006\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 2.3032 - accuracy: 0.1000\n",
      "Fitting for combination 78\n",
      "(28, 28, 1)\n",
      "3\n",
      "[32, 32, 64]\n",
      "[(2, 2), (2, 2), (4, 4)]\n",
      "2\n",
      "[32, 128, 10]\n",
      "10\n",
      "True\n",
      "['relu', 'sigmoid', 'relu']\n",
      "['tanh', 'tanh', 'softmax']\n",
      "SGD\n",
      "0.03\n",
      "0.03\n",
      "categorical_crossentropy\n",
      "300\n",
      "25\n",
      "0\n",
      "Epoch 1/25\n",
      " 11/200 [>.............................] - ETA: 0s - loss: 2.3465 - accuracy: 0.0988 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-14 09:53:12.966973: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:954] layout failed: INVALID_ARGUMENT: Size of values 0 does not match size of permutation 4 @ fanin shape insequential_78/dropout_114/dropout/SelectV2-2-TransposeNHWCToNCHW-LayoutOptimizer\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200/200 [==============================] - 1s 4ms/step - loss: 2.2927 - accuracy: 0.1486\n",
      "Epoch 2/25\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 2.2621 - accuracy: 0.2415\n",
      "Epoch 3/25\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 2.2247 - accuracy: 0.2851\n",
      "Epoch 4/25\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 2.1711 - accuracy: 0.3099\n",
      "Epoch 5/25\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 2.1019 - accuracy: 0.3327\n",
      "Epoch 6/25\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 2.0269 - accuracy: 0.3503\n",
      "Epoch 7/25\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 1.9502 - accuracy: 0.3638\n",
      "Epoch 8/25\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 1.8823 - accuracy: 0.3785\n",
      "Epoch 9/25\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 1.8223 - accuracy: 0.3923\n",
      "Epoch 10/25\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 1.7748 - accuracy: 0.3998\n",
      "Epoch 11/25\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 1.7354 - accuracy: 0.4063\n",
      "Epoch 12/25\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 1.6991 - accuracy: 0.4169\n",
      "Epoch 13/25\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 1.6703 - accuracy: 0.4207\n",
      "Epoch 14/25\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 1.6424 - accuracy: 0.4268\n",
      "Epoch 15/25\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 1.6191 - accuracy: 0.4325\n",
      "Epoch 16/25\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 1.6010 - accuracy: 0.4369\n",
      "Epoch 17/25\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 1.5768 - accuracy: 0.4457\n",
      "Epoch 18/25\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 1.5613 - accuracy: 0.4457\n",
      "Epoch 19/25\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 1.5470 - accuracy: 0.4517\n",
      "Epoch 20/25\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 1.5336 - accuracy: 0.4550\n",
      "Epoch 21/25\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 1.5190 - accuracy: 0.4582\n",
      "Epoch 22/25\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 1.5084 - accuracy: 0.4594\n",
      "Epoch 23/25\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 1.4971 - accuracy: 0.4647\n",
      "Epoch 24/25\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 1.4832 - accuracy: 0.4685\n",
      "Epoch 25/25\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 1.4734 - accuracy: 0.4725\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 1.2751 - accuracy: 0.6209\n",
      "Fitting for combination 79\n",
      "(28, 28, 1)\n",
      "3\n",
      "[32, 32, 64]\n",
      "[(2, 2), (2, 2), (4, 4)]\n",
      "1\n",
      "[16, 10]\n",
      "10\n",
      "True\n",
      "['relu', 'sigmoid', 'relu']\n",
      "['tanh', 'softmax']\n",
      "SGD\n",
      "0.03\n",
      "0.03\n",
      "categorical_crossentropy\n",
      "300\n",
      "25\n",
      "0\n",
      "Epoch 1/25\n",
      " 34/200 [====>.........................] - ETA: 0s - loss: 2.3171 - accuracy: 0.1121"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-14 09:53:37.237158: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:954] layout failed: INVALID_ARGUMENT: Size of values 0 does not match size of permutation 4 @ fanin shape insequential_79/dropout_117/dropout/SelectV2-2-TransposeNHWCToNCHW-LayoutOptimizer\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200/200 [==============================] - 1s 5ms/step - loss: 2.2992 - accuracy: 0.1438\n",
      "Epoch 2/25\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 2.2842 - accuracy: 0.2197\n",
      "Epoch 3/25\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 2.2703 - accuracy: 0.2662\n",
      "Epoch 4/25\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 2.2518 - accuracy: 0.2867\n",
      "Epoch 5/25\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 2.2285 - accuracy: 0.3057\n",
      "Epoch 6/25\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 2.1996 - accuracy: 0.3181\n",
      "Epoch 7/25\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 2.1652 - accuracy: 0.3252\n",
      "Epoch 8/25\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 2.1251 - accuracy: 0.3318\n",
      "Epoch 9/25\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 2.0801 - accuracy: 0.3394\n",
      "Epoch 10/25\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 2.0346 - accuracy: 0.3449\n",
      "Epoch 11/25\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 1.9917 - accuracy: 0.3460\n",
      "Epoch 12/25\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 1.9519 - accuracy: 0.3565\n",
      "Epoch 13/25\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 1.9121 - accuracy: 0.3618\n",
      "Epoch 14/25\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 1.8804 - accuracy: 0.3628\n",
      "Epoch 15/25\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 1.8532 - accuracy: 0.3700\n",
      "Epoch 16/25\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 1.8278 - accuracy: 0.3733\n",
      "Epoch 17/25\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 1.8078 - accuracy: 0.3810\n",
      "Epoch 18/25\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 1.7861 - accuracy: 0.3874\n",
      "Epoch 19/25\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 1.7719 - accuracy: 0.3875\n",
      "Epoch 20/25\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 1.7551 - accuracy: 0.3923\n",
      "Epoch 21/25\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 1.7425 - accuracy: 0.3952\n",
      "Epoch 22/25\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 1.7327 - accuracy: 0.3961\n",
      "Epoch 23/25\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 1.7155 - accuracy: 0.4028\n",
      "Epoch 24/25\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 1.7063 - accuracy: 0.4045\n",
      "Epoch 25/25\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 1.6940 - accuracy: 0.4097\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 1.5732 - accuracy: 0.5412\n",
      "Fitting for combination 80\n",
      "(28, 28, 1)\n",
      "3\n",
      "[32, 32, 128]\n",
      "[(2, 2), (2, 2), (2, 2)]\n",
      "2\n",
      "[32, 16, 10]\n",
      "10\n",
      "False\n",
      "['relu', 'sigmoid', 'tanh']\n",
      "['tanh', 'sigmoid', 'softmax']\n",
      "Adam\n",
      "0.1\n",
      "0.03\n",
      "categorical_crossentropy\n",
      "150\n",
      "25\n",
      "0\n",
      "Epoch 1/25\n",
      "400/400 [==============================] - 2s 3ms/step - loss: 2.3063 - accuracy: 0.0996\n",
      "Epoch 2/25\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 2.3030 - accuracy: 0.0996\n",
      "Epoch 3/25\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 2.3028 - accuracy: 0.0993\n",
      "Epoch 4/25\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 2.3028 - accuracy: 0.0994\n",
      "Epoch 5/25\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 2.3027 - accuracy: 0.0980\n",
      "Epoch 6/25\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 2.3027 - accuracy: 0.0976\n",
      "Epoch 7/25\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 2.3027 - accuracy: 0.0976\n",
      "Epoch 8/25\n",
      "400/400 [==============================] - 1s 4ms/step - loss: 2.3027 - accuracy: 0.0986\n",
      "Epoch 9/25\n",
      "400/400 [==============================] - 1s 4ms/step - loss: 2.3027 - accuracy: 0.0983\n",
      "Epoch 10/25\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 2.3027 - accuracy: 0.0980\n",
      "Epoch 11/25\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 2.3027 - accuracy: 0.0994\n",
      "Epoch 12/25\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 2.3027 - accuracy: 0.0988\n",
      "Epoch 13/25\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 2.3027 - accuracy: 0.0959\n",
      "Epoch 14/25\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 2.3026 - accuracy: 0.0979\n",
      "Epoch 15/25\n",
      "400/400 [==============================] - 1s 4ms/step - loss: 2.3026 - accuracy: 0.0979\n",
      "Epoch 16/25\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 2.3026 - accuracy: 0.0987\n",
      "Epoch 17/25\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 2.3026 - accuracy: 0.0975\n",
      "Epoch 18/25\n",
      "400/400 [==============================] - 1s 4ms/step - loss: 2.3026 - accuracy: 0.0975\n",
      "Epoch 19/25\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 2.3026 - accuracy: 0.0984\n",
      "Epoch 20/25\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 2.3026 - accuracy: 0.0967\n",
      "Epoch 21/25\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 2.3026 - accuracy: 0.0975\n",
      "Epoch 22/25\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 2.3026 - accuracy: 0.0972\n",
      "Epoch 23/25\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 2.3026 - accuracy: 0.0986\n",
      "Epoch 24/25\n",
      "400/400 [==============================] - 1s 4ms/step - loss: 2.3026 - accuracy: 0.0965\n",
      "Epoch 25/25\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 2.3026 - accuracy: 0.0987\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 2.3026 - accuracy: 0.1000\n",
      "Fitting for combination 81\n",
      "(28, 28, 1)\n",
      "3\n",
      "[32, 32, 128]\n",
      "[(2, 2), (2, 2), (4, 4)]\n",
      "3\n",
      "[32, 16, 128, 10]\n",
      "10\n",
      "False\n",
      "['relu', 'sigmoid', 'sigmoid']\n",
      "['relu', 'tanh', 'relu', 'softmax']\n",
      "Adam\n",
      "0.3\n",
      "0.03\n",
      "categorical_crossentropy\n",
      "200\n",
      "25\n",
      "0\n",
      "Epoch 1/25\n",
      "300/300 [==============================] - 1s 4ms/step - loss: 2.5506 - accuracy: 0.1000\n",
      "Epoch 2/25\n",
      "300/300 [==============================] - 1s 4ms/step - loss: 2.3035 - accuracy: 0.1005\n",
      "Epoch 3/25\n",
      "300/300 [==============================] - 1s 4ms/step - loss: 2.3031 - accuracy: 0.0994\n",
      "Epoch 4/25\n",
      "300/300 [==============================] - 1s 4ms/step - loss: 2.3031 - accuracy: 0.0988\n",
      "Epoch 5/25\n",
      "300/300 [==============================] - 1s 4ms/step - loss: 2.3030 - accuracy: 0.0990\n",
      "Epoch 6/25\n",
      "300/300 [==============================] - 1s 4ms/step - loss: 2.3030 - accuracy: 0.0979\n",
      "Epoch 7/25\n",
      "300/300 [==============================] - 1s 4ms/step - loss: 2.3029 - accuracy: 0.0989\n",
      "Epoch 8/25\n",
      "300/300 [==============================] - 1s 4ms/step - loss: 2.3029 - accuracy: 0.0986\n",
      "Epoch 9/25\n",
      "300/300 [==============================] - 1s 4ms/step - loss: 2.3028 - accuracy: 0.0990\n",
      "Epoch 10/25\n",
      "300/300 [==============================] - 1s 4ms/step - loss: 2.3029 - accuracy: 0.0987\n",
      "Epoch 11/25\n",
      "300/300 [==============================] - 1s 4ms/step - loss: 2.3028 - accuracy: 0.0991\n",
      "Epoch 12/25\n",
      "300/300 [==============================] - 1s 4ms/step - loss: 2.3028 - accuracy: 0.0989\n",
      "Epoch 13/25\n",
      "300/300 [==============================] - 1s 4ms/step - loss: 2.3028 - accuracy: 0.0952\n",
      "Epoch 14/25\n",
      "300/300 [==============================] - 1s 4ms/step - loss: 2.3027 - accuracy: 0.0993\n",
      "Epoch 15/25\n",
      "300/300 [==============================] - 1s 4ms/step - loss: 2.3027 - accuracy: 0.0983\n",
      "Epoch 16/25\n",
      "300/300 [==============================] - 1s 4ms/step - loss: 2.3028 - accuracy: 0.0993\n",
      "Epoch 17/25\n",
      "300/300 [==============================] - 1s 4ms/step - loss: 2.3028 - accuracy: 0.0982\n",
      "Epoch 18/25\n",
      "300/300 [==============================] - 1s 4ms/step - loss: 2.3027 - accuracy: 0.0975\n",
      "Epoch 19/25\n",
      "300/300 [==============================] - 1s 4ms/step - loss: 2.3027 - accuracy: 0.0969\n",
      "Epoch 20/25\n",
      "300/300 [==============================] - 1s 4ms/step - loss: 2.3027 - accuracy: 0.0977\n",
      "Epoch 21/25\n",
      "300/300 [==============================] - 1s 4ms/step - loss: 2.3027 - accuracy: 0.0998\n",
      "Epoch 22/25\n",
      "300/300 [==============================] - 1s 4ms/step - loss: 2.3027 - accuracy: 0.0974\n",
      "Epoch 23/25\n",
      "300/300 [==============================] - 1s 4ms/step - loss: 2.3027 - accuracy: 0.0972\n",
      "Epoch 24/25\n",
      "300/300 [==============================] - 1s 4ms/step - loss: 2.3027 - accuracy: 0.0956\n",
      "Epoch 25/25\n",
      "300/300 [==============================] - 1s 4ms/step - loss: 2.3027 - accuracy: 0.0992\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 2.3026 - accuracy: 0.1000\n",
      "Fitting for combination 82\n",
      "(28, 28, 1)\n",
      "3\n",
      "[32, 32, 128]\n",
      "[(2, 2), (2, 2), (2, 2)]\n",
      "1\n",
      "[128, 10]\n",
      "10\n",
      "False\n",
      "['relu', 'sigmoid', 'tanh']\n",
      "['sigmoid', 'softmax']\n",
      "Adam\n",
      "0.1\n",
      "0.03\n",
      "categorical_crossentropy\n",
      "300\n",
      "25\n",
      "0\n",
      "Epoch 1/25\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 2.6535 - accuracy: 0.0988\n",
      "Epoch 2/25\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 2.3108 - accuracy: 0.0993\n",
      "Epoch 3/25\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 2.3095 - accuracy: 0.0994\n",
      "Epoch 4/25\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 2.3079 - accuracy: 0.1018\n",
      "Epoch 5/25\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 2.3087 - accuracy: 0.0993\n",
      "Epoch 6/25\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 2.3075 - accuracy: 0.1005\n",
      "Epoch 7/25\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 2.3068 - accuracy: 0.1000\n",
      "Epoch 8/25\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 2.3074 - accuracy: 0.0992\n",
      "Epoch 9/25\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 2.3062 - accuracy: 0.0995\n",
      "Epoch 10/25\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 2.3062 - accuracy: 0.1018\n",
      "Epoch 11/25\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 2.3060 - accuracy: 0.0995\n",
      "Epoch 12/25\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 2.3056 - accuracy: 0.1004\n",
      "Epoch 13/25\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 2.3054 - accuracy: 0.1001\n",
      "Epoch 14/25\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 2.3052 - accuracy: 0.0997\n",
      "Epoch 15/25\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 2.3052 - accuracy: 0.0990\n",
      "Epoch 16/25\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 2.3052 - accuracy: 0.1008\n",
      "Epoch 17/25\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 2.3050 - accuracy: 0.1015\n",
      "Epoch 18/25\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 2.3054 - accuracy: 0.0998\n",
      "Epoch 19/25\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 2.3046 - accuracy: 0.0988\n",
      "Epoch 20/25\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 2.3046 - accuracy: 0.0979\n",
      "Epoch 21/25\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 2.3048 - accuracy: 0.1003\n",
      "Epoch 22/25\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 2.3045 - accuracy: 0.0995\n",
      "Epoch 23/25\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 2.3041 - accuracy: 0.0987\n",
      "Epoch 24/25\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 2.3045 - accuracy: 0.0979\n",
      "Epoch 25/25\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 2.3042 - accuracy: 0.1009\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 2.3031 - accuracy: 0.1000\n",
      "Fitting for combination 83\n",
      "(28, 28, 1)\n",
      "3\n",
      "[32, 32, 128]\n",
      "[(2, 2), (2, 2), (2, 2)]\n",
      "2\n",
      "[64, 64, 10]\n",
      "10\n",
      "True\n",
      "['relu', 'sigmoid', 'tanh']\n",
      "['relu', 'sigmoid', 'softmax']\n",
      "Adam\n",
      "0.1\n",
      "0\n",
      "categorical_crossentropy\n",
      "200\n",
      "25\n",
      "0\n",
      "Epoch 1/25\n",
      " 13/300 [>.............................] - ETA: 1s - loss: 2.8650 - accuracy: 0.1088  "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-14 09:55:34.258522: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:954] layout failed: INVALID_ARGUMENT: Size of values 0 does not match size of permutation 4 @ fanin shape insequential_83/dropout_119/dropout/SelectV2-2-TransposeNHWCToNCHW-LayoutOptimizer\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300/300 [==============================] - 2s 4ms/step - loss: 2.4846 - accuracy: 0.1002\n",
      "Epoch 2/25\n",
      "300/300 [==============================] - 1s 4ms/step - loss: 2.5051 - accuracy: 0.1000\n",
      "Epoch 3/25\n",
      "300/300 [==============================] - 1s 4ms/step - loss: 2.4774 - accuracy: 0.1004\n",
      "Epoch 4/25\n",
      "300/300 [==============================] - 1s 4ms/step - loss: 2.4656 - accuracy: 0.1016\n",
      "Epoch 5/25\n",
      "300/300 [==============================] - 1s 4ms/step - loss: 2.4688 - accuracy: 0.1009\n",
      "Epoch 6/25\n",
      "300/300 [==============================] - 1s 4ms/step - loss: 2.4616 - accuracy: 0.0998\n",
      "Epoch 7/25\n",
      "300/300 [==============================] - 1s 4ms/step - loss: 2.4566 - accuracy: 0.1015\n",
      "Epoch 8/25\n",
      "300/300 [==============================] - 1s 4ms/step - loss: 2.4653 - accuracy: 0.1001\n",
      "Epoch 9/25\n",
      "300/300 [==============================] - 1s 4ms/step - loss: 2.4725 - accuracy: 0.1002\n",
      "Epoch 10/25\n",
      "300/300 [==============================] - 1s 4ms/step - loss: 2.4734 - accuracy: 0.1019\n",
      "Epoch 11/25\n",
      "300/300 [==============================] - 1s 4ms/step - loss: 2.4666 - accuracy: 0.0974\n",
      "Epoch 12/25\n",
      "300/300 [==============================] - 1s 4ms/step - loss: 2.4697 - accuracy: 0.0983\n",
      "Epoch 13/25\n",
      "300/300 [==============================] - 1s 4ms/step - loss: 2.4602 - accuracy: 0.0999\n",
      "Epoch 14/25\n",
      "300/300 [==============================] - 1s 4ms/step - loss: 2.4625 - accuracy: 0.0993\n",
      "Epoch 15/25\n",
      "300/300 [==============================] - 1s 4ms/step - loss: 2.4655 - accuracy: 0.0993\n",
      "Epoch 16/25\n",
      "300/300 [==============================] - 1s 4ms/step - loss: 2.4691 - accuracy: 0.0993\n",
      "Epoch 17/25\n",
      "300/300 [==============================] - 1s 4ms/step - loss: 2.4649 - accuracy: 0.0978\n",
      "Epoch 18/25\n",
      "300/300 [==============================] - 1s 4ms/step - loss: 2.4631 - accuracy: 0.0999\n",
      "Epoch 19/25\n",
      "300/300 [==============================] - 1s 4ms/step - loss: 2.4671 - accuracy: 0.1016\n",
      "Epoch 20/25\n",
      "300/300 [==============================] - 1s 4ms/step - loss: 2.4637 - accuracy: 0.1018\n",
      "Epoch 21/25\n",
      "300/300 [==============================] - 1s 4ms/step - loss: 2.4670 - accuracy: 0.0979\n",
      "Epoch 22/25\n",
      "300/300 [==============================] - 1s 4ms/step - loss: 2.4707 - accuracy: 0.0993\n",
      "Epoch 23/25\n",
      "300/300 [==============================] - 1s 4ms/step - loss: 2.4712 - accuracy: 0.0994\n",
      "Epoch 24/25\n",
      "300/300 [==============================] - 1s 4ms/step - loss: 2.4745 - accuracy: 0.1001\n",
      "Epoch 25/25\n",
      "300/300 [==============================] - 1s 4ms/step - loss: 2.4624 - accuracy: 0.0997\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 2.3437 - accuracy: 0.1000\n",
      "Fitting for combination 84\n",
      "(28, 28, 1)\n",
      "3\n",
      "[32, 32, 128]\n",
      "[(2, 2), (2, 2), (4, 4)]\n",
      "1\n",
      "[16, 10]\n",
      "10\n",
      "True\n",
      "['relu', 'sigmoid', 'tanh']\n",
      "['tanh', 'softmax']\n",
      "Adam\n",
      "0.01\n",
      "0.01\n",
      "categorical_crossentropy\n",
      "100\n",
      "25\n",
      "0\n",
      "Epoch 1/25\n",
      " 31/600 [>.............................] - ETA: 1s - loss: 2.7418 - accuracy: 0.0987"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-14 09:56:06.045920: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:954] layout failed: INVALID_ARGUMENT: Size of values 0 does not match size of permutation 4 @ fanin shape insequential_84/dropout_122/dropout/SelectV2-2-TransposeNHWCToNCHW-LayoutOptimizer\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "600/600 [==============================] - 2s 3ms/step - loss: 2.3494 - accuracy: 0.0997\n",
      "Epoch 2/25\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 2.3045 - accuracy: 0.1007\n",
      "Epoch 3/25\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 2.3040 - accuracy: 0.1001\n",
      "Epoch 4/25\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 2.3037 - accuracy: 0.0984\n",
      "Epoch 5/25\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 2.3034 - accuracy: 0.0996\n",
      "Epoch 6/25\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 2.3033 - accuracy: 0.0983\n",
      "Epoch 7/25\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 2.3031 - accuracy: 0.0997\n",
      "Epoch 8/25\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 2.3031 - accuracy: 0.1010\n",
      "Epoch 9/25\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 2.3031 - accuracy: 0.0996\n",
      "Epoch 10/25\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 2.3031 - accuracy: 0.1007\n",
      "Epoch 11/25\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 2.3030 - accuracy: 0.0996\n",
      "Epoch 12/25\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 2.3029 - accuracy: 0.1016\n",
      "Epoch 13/25\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 2.3030 - accuracy: 0.0979\n",
      "Epoch 14/25\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 2.3030 - accuracy: 0.0991\n",
      "Epoch 15/25\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 2.3029 - accuracy: 0.1000\n",
      "Epoch 16/25\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 2.3028 - accuracy: 0.0995\n",
      "Epoch 17/25\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 2.3028 - accuracy: 0.1009\n",
      "Epoch 18/25\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 2.3029 - accuracy: 0.1006\n",
      "Epoch 19/25\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 2.3030 - accuracy: 0.0990\n",
      "Epoch 20/25\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 2.3028 - accuracy: 0.1002\n",
      "Epoch 21/25\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 2.3028 - accuracy: 0.0999\n",
      "Epoch 22/25\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 2.3028 - accuracy: 0.1002\n",
      "Epoch 23/25\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 2.3028 - accuracy: 0.0973\n",
      "Epoch 24/25\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 2.3028 - accuracy: 0.0996\n",
      "Epoch 25/25\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 2.3028 - accuracy: 0.1006\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 2.3026 - accuracy: 0.1000\n",
      "Fitting for combination 85\n",
      "(28, 28, 1)\n",
      "3\n",
      "[32, 32, 128]\n",
      "[(2, 2), (2, 2), (3, 3)]\n",
      "1\n",
      "[32, 10]\n",
      "10\n",
      "False\n",
      "['relu', 'sigmoid', 'relu']\n",
      "['sigmoid', 'softmax']\n",
      "SGD\n",
      "0.03\n",
      "0.03\n",
      "categorical_crossentropy\n",
      "100\n",
      "25\n",
      "0\n",
      "Epoch 1/25\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 2.3006 - accuracy: 0.1154\n",
      "Epoch 2/25\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 2.2948 - accuracy: 0.1458\n",
      "Epoch 3/25\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 2.2926 - accuracy: 0.1296\n",
      "Epoch 4/25\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 2.2909 - accuracy: 0.1650\n",
      "Epoch 5/25\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 2.2894 - accuracy: 0.1335\n",
      "Epoch 6/25\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 2.2880 - accuracy: 0.1405\n",
      "Epoch 7/25\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 2.2867 - accuracy: 0.1530\n",
      "Epoch 8/25\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 2.2855 - accuracy: 0.1806\n",
      "Epoch 9/25\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 2.2843 - accuracy: 0.1427\n",
      "Epoch 10/25\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 2.2833 - accuracy: 0.1623\n",
      "Epoch 11/25\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 2.2822 - accuracy: 0.2276\n",
      "Epoch 12/25\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 2.2812 - accuracy: 0.1545\n",
      "Epoch 13/25\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 2.2802 - accuracy: 0.2165\n",
      "Epoch 14/25\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 2.2792 - accuracy: 0.1748\n",
      "Epoch 15/25\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 2.2783 - accuracy: 0.2404\n",
      "Epoch 16/25\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 2.2774 - accuracy: 0.2361\n",
      "Epoch 17/25\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 2.2764 - accuracy: 0.2512\n",
      "Epoch 18/25\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 2.2755 - accuracy: 0.2338\n",
      "Epoch 19/25\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 2.2747 - accuracy: 0.2580\n",
      "Epoch 20/25\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 2.2738 - accuracy: 0.2426\n",
      "Epoch 21/25\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 2.2730 - accuracy: 0.2888\n",
      "Epoch 22/25\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 2.2721 - accuracy: 0.2723\n",
      "Epoch 23/25\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 2.2713 - accuracy: 0.2898\n",
      "Epoch 24/25\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 2.2705 - accuracy: 0.2740\n",
      "Epoch 25/25\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 2.2696 - accuracy: 0.2945\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 2.2694 - accuracy: 0.2975\n",
      "Fitting for combination 86\n",
      "(28, 28, 1)\n",
      "3\n",
      "[32, 32, 128]\n",
      "[(2, 2), (2, 2), (2, 2)]\n",
      "2\n",
      "[16, 32, 10]\n",
      "10\n",
      "False\n",
      "['relu', 'sigmoid', 'relu']\n",
      "['tanh', 'sigmoid', 'softmax']\n",
      "SGD\n",
      "0.1\n",
      "0\n",
      "categorical_crossentropy\n",
      "50\n",
      "25\n",
      "0\n",
      "Epoch 1/25\n",
      "1200/1200 [==============================] - 3s 2ms/step - loss: 1.4659 - accuracy: 0.4770\n",
      "Epoch 2/25\n",
      "1200/1200 [==============================] - 3s 2ms/step - loss: 0.5465 - accuracy: 0.8015\n",
      "Epoch 3/25\n",
      "1200/1200 [==============================] - 3s 2ms/step - loss: 0.4261 - accuracy: 0.8496\n",
      "Epoch 4/25\n",
      "1200/1200 [==============================] - 3s 2ms/step - loss: 0.3748 - accuracy: 0.8668\n",
      "Epoch 5/25\n",
      "1200/1200 [==============================] - 3s 2ms/step - loss: 0.3432 - accuracy: 0.8764\n",
      "Epoch 6/25\n",
      "1200/1200 [==============================] - 3s 2ms/step - loss: 0.3167 - accuracy: 0.8859\n",
      "Epoch 7/25\n",
      "1200/1200 [==============================] - 3s 2ms/step - loss: 0.2935 - accuracy: 0.8936\n",
      "Epoch 8/25\n",
      "1200/1200 [==============================] - 3s 2ms/step - loss: 0.2771 - accuracy: 0.8988\n",
      "Epoch 9/25\n",
      "1200/1200 [==============================] - 3s 2ms/step - loss: 0.2606 - accuracy: 0.9049\n",
      "Epoch 10/25\n",
      "1200/1200 [==============================] - 3s 2ms/step - loss: 0.2438 - accuracy: 0.9104\n",
      "Epoch 11/25\n",
      "1200/1200 [==============================] - 3s 2ms/step - loss: 0.2307 - accuracy: 0.9161\n",
      "Epoch 12/25\n",
      "1200/1200 [==============================] - 3s 2ms/step - loss: 0.2174 - accuracy: 0.9228\n",
      "Epoch 13/25\n",
      "1200/1200 [==============================] - 3s 2ms/step - loss: 0.2046 - accuracy: 0.9261\n",
      "Epoch 14/25\n",
      "1200/1200 [==============================] - 3s 2ms/step - loss: 0.1936 - accuracy: 0.9296\n",
      "Epoch 15/25\n",
      "1200/1200 [==============================] - 3s 2ms/step - loss: 0.1831 - accuracy: 0.9336\n",
      "Epoch 16/25\n",
      "1200/1200 [==============================] - 3s 2ms/step - loss: 0.1733 - accuracy: 0.9377\n",
      "Epoch 17/25\n",
      "1200/1200 [==============================] - 3s 2ms/step - loss: 0.1628 - accuracy: 0.9413\n",
      "Epoch 18/25\n",
      "1200/1200 [==============================] - 3s 3ms/step - loss: 0.1531 - accuracy: 0.9454\n",
      "Epoch 19/25\n",
      "1200/1200 [==============================] - 3s 2ms/step - loss: 0.1445 - accuracy: 0.9488\n",
      "Epoch 20/25\n",
      "1200/1200 [==============================] - 3s 2ms/step - loss: 0.1354 - accuracy: 0.9528\n",
      "Epoch 21/25\n",
      "1200/1200 [==============================] - 3s 2ms/step - loss: 0.1262 - accuracy: 0.9554\n",
      "Epoch 22/25\n",
      "1200/1200 [==============================] - 3s 2ms/step - loss: 0.1186 - accuracy: 0.9586\n",
      "Epoch 23/25\n",
      "1200/1200 [==============================] - 3s 2ms/step - loss: 0.1107 - accuracy: 0.9611\n",
      "Epoch 24/25\n",
      "1200/1200 [==============================] - 3s 2ms/step - loss: 0.1038 - accuracy: 0.9640\n",
      "Epoch 25/25\n",
      "1200/1200 [==============================] - 3s 2ms/step - loss: 0.0948 - accuracy: 0.9677\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 0.2978 - accuracy: 0.9074\n",
      "Fitting for combination 87\n",
      "(28, 28, 1)\n",
      "3\n",
      "[32, 32, 128]\n",
      "[(2, 2), (2, 2), (4, 4)]\n",
      "1\n",
      "[16, 10]\n",
      "10\n",
      "False\n",
      "['relu', 'sigmoid', 'tanh']\n",
      "['tanh', 'softmax']\n",
      "SGD\n",
      "0.03\n",
      "0.03\n",
      "categorical_crossentropy\n",
      "300\n",
      "25\n",
      "0\n",
      "Epoch 1/25\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 2.2994 - accuracy: 0.1673\n",
      "Epoch 2/25\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 2.1871 - accuracy: 0.3420\n",
      "Epoch 3/25\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 2.0174 - accuracy: 0.4325\n",
      "Epoch 4/25\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 1.7965 - accuracy: 0.4894\n",
      "Epoch 5/25\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 1.6057 - accuracy: 0.5292\n",
      "Epoch 6/25\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 1.4755 - accuracy: 0.5684\n",
      "Epoch 7/25\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 1.3892 - accuracy: 0.6014\n",
      "Epoch 8/25\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 1.3292 - accuracy: 0.6198\n",
      "Epoch 9/25\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 1.2850 - accuracy: 0.6302\n",
      "Epoch 10/25\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 1.2508 - accuracy: 0.6374\n",
      "Epoch 11/25\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 1.2235 - accuracy: 0.6442\n",
      "Epoch 12/25\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 1.2008 - accuracy: 0.6474\n",
      "Epoch 13/25\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 1.1817 - accuracy: 0.6521\n",
      "Epoch 14/25\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 1.1650 - accuracy: 0.6554\n",
      "Epoch 15/25\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 1.1507 - accuracy: 0.6595\n",
      "Epoch 16/25\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 1.1378 - accuracy: 0.6614\n",
      "Epoch 17/25\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 1.1264 - accuracy: 0.6646\n",
      "Epoch 18/25\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 1.1160 - accuracy: 0.6667\n",
      "Epoch 19/25\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 1.1066 - accuracy: 0.6693\n",
      "Epoch 20/25\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 1.0980 - accuracy: 0.6710\n",
      "Epoch 21/25\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 1.0901 - accuracy: 0.6737\n",
      "Epoch 22/25\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 1.0828 - accuracy: 0.6759\n",
      "Epoch 23/25\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 1.0760 - accuracy: 0.6772\n",
      "Epoch 24/25\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 1.0697 - accuracy: 0.6791\n",
      "Epoch 25/25\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 1.0637 - accuracy: 0.6802\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 1.0675 - accuracy: 0.6802\n",
      "Fitting for combination 88\n",
      "(28, 28, 1)\n",
      "3\n",
      "[32, 32, 128]\n",
      "[(2, 2), (2, 2), (5, 5)]\n",
      "2\n",
      "[128, 16, 10]\n",
      "10\n",
      "False\n",
      "['relu', 'sigmoid', 'tanh']\n",
      "['sigmoid', 'relu', 'softmax']\n",
      "SGD\n",
      "0.03\n",
      "0.03\n",
      "categorical_crossentropy\n",
      "300\n",
      "25\n",
      "0\n",
      "Epoch 1/25\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 2.3031 - accuracy: 0.0996\n",
      "Epoch 2/25\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 2.3026 - accuracy: 0.1000\n",
      "Epoch 3/25\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 2.3026 - accuracy: 0.1000\n",
      "Epoch 4/25\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 2.3026 - accuracy: 0.1000\n",
      "Epoch 5/25\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 2.3026 - accuracy: 0.1000\n",
      "Epoch 6/25\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 2.3026 - accuracy: 0.1000\n",
      "Epoch 7/25\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 2.3026 - accuracy: 0.1000\n",
      "Epoch 8/25\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 2.3026 - accuracy: 0.1000\n",
      "Epoch 9/25\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 2.3026 - accuracy: 0.1000\n",
      "Epoch 10/25\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 2.3026 - accuracy: 0.1000\n",
      "Epoch 11/25\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 2.3026 - accuracy: 0.1000\n",
      "Epoch 12/25\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 2.3026 - accuracy: 0.1000\n",
      "Epoch 13/25\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 2.3026 - accuracy: 0.1000\n",
      "Epoch 14/25\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 2.3026 - accuracy: 0.1000\n",
      "Epoch 15/25\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 2.3026 - accuracy: 0.1000\n",
      "Epoch 16/25\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 2.3026 - accuracy: 0.1000\n",
      "Epoch 17/25\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 2.3026 - accuracy: 0.1000\n",
      "Epoch 18/25\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 2.3026 - accuracy: 0.1000\n",
      "Epoch 19/25\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 2.3026 - accuracy: 0.1000\n",
      "Epoch 20/25\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 2.3026 - accuracy: 0.1000\n",
      "Epoch 21/25\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 2.3026 - accuracy: 0.1000\n",
      "Epoch 22/25\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 2.3026 - accuracy: 0.1000\n",
      "Epoch 23/25\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 2.3026 - accuracy: 0.1000\n",
      "Epoch 24/25\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 2.3026 - accuracy: 0.1000\n",
      "Epoch 25/25\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 2.3026 - accuracy: 0.1000\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 2.3026 - accuracy: 0.1000\n",
      "Fitting for combination 89\n",
      "(28, 28, 1)\n",
      "3\n",
      "[32, 32, 128]\n",
      "[(2, 2), (2, 2), (5, 5)]\n",
      "2\n",
      "[64, 32, 10]\n",
      "10\n",
      "True\n",
      "['relu', 'sigmoid', 'sigmoid']\n",
      "['sigmoid', 'relu', 'softmax']\n",
      "SGD\n",
      "0.3\n",
      "0.03\n",
      "categorical_crossentropy\n",
      "50\n",
      "25\n",
      "0\n",
      "Epoch 1/25\n",
      "  38/1200 [..............................] - ETA: 3s - loss: 2.3646 - accuracy: 0.0984"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-14 09:59:50.452479: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:954] layout failed: INVALID_ARGUMENT: Size of values 0 does not match size of permutation 4 @ fanin shape insequential_89/dropout_124/dropout/SelectV2-2-TransposeNHWCToNCHW-LayoutOptimizer\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1200/1200 [==============================] - 4s 3ms/step - loss: 2.3071 - accuracy: 0.0979\n",
      "Epoch 2/25\n",
      "1200/1200 [==============================] - 4s 3ms/step - loss: 2.3035 - accuracy: 0.1006\n",
      "Epoch 3/25\n",
      "1200/1200 [==============================] - 4s 3ms/step - loss: 2.3038 - accuracy: 0.1003\n",
      "Epoch 4/25\n",
      "1200/1200 [==============================] - 4s 3ms/step - loss: 2.3036 - accuracy: 0.1006\n",
      "Epoch 5/25\n",
      "1200/1200 [==============================] - 4s 3ms/step - loss: 2.3032 - accuracy: 0.0998\n",
      "Epoch 6/25\n",
      "1200/1200 [==============================] - 4s 3ms/step - loss: 2.3036 - accuracy: 0.0988\n",
      "Epoch 7/25\n",
      "1200/1200 [==============================] - 4s 3ms/step - loss: 2.3034 - accuracy: 0.1007\n",
      "Epoch 8/25\n",
      "1200/1200 [==============================] - 3s 3ms/step - loss: 2.3030 - accuracy: 0.1002\n",
      "Epoch 9/25\n",
      "1200/1200 [==============================] - 3s 3ms/step - loss: 2.3034 - accuracy: 0.0997\n",
      "Epoch 10/25\n",
      "1200/1200 [==============================] - 3s 3ms/step - loss: 2.3030 - accuracy: 0.0998\n",
      "Epoch 11/25\n",
      "1200/1200 [==============================] - 3s 3ms/step - loss: 2.3034 - accuracy: 0.0987\n",
      "Epoch 12/25\n",
      "1200/1200 [==============================] - 3s 3ms/step - loss: 2.3033 - accuracy: 0.1001\n",
      "Epoch 13/25\n",
      "1200/1200 [==============================] - 3s 3ms/step - loss: 2.3031 - accuracy: 0.0999\n",
      "Epoch 14/25\n",
      "1200/1200 [==============================] - 3s 3ms/step - loss: 2.3031 - accuracy: 0.0996\n",
      "Epoch 15/25\n",
      "1200/1200 [==============================] - 3s 3ms/step - loss: 2.3033 - accuracy: 0.0996\n",
      "Epoch 16/25\n",
      "1200/1200 [==============================] - 3s 3ms/step - loss: 2.3030 - accuracy: 0.1005\n",
      "Epoch 17/25\n",
      "1200/1200 [==============================] - 3s 3ms/step - loss: 2.3030 - accuracy: 0.1008\n",
      "Epoch 18/25\n",
      "1200/1200 [==============================] - 3s 3ms/step - loss: 2.3033 - accuracy: 0.0993\n",
      "Epoch 19/25\n",
      "1200/1200 [==============================] - 3s 3ms/step - loss: 2.3031 - accuracy: 0.0999\n",
      "Epoch 20/25\n",
      "1200/1200 [==============================] - 3s 3ms/step - loss: 2.3029 - accuracy: 0.1005\n",
      "Epoch 21/25\n",
      "1200/1200 [==============================] - 3s 3ms/step - loss: 2.3032 - accuracy: 0.1007\n",
      "Epoch 22/25\n",
      "1200/1200 [==============================] - 3s 3ms/step - loss: 2.3032 - accuracy: 0.0999\n",
      "Epoch 23/25\n",
      "1200/1200 [==============================] - 3s 3ms/step - loss: 2.3033 - accuracy: 0.1010\n",
      "Epoch 24/25\n",
      "1200/1200 [==============================] - 3s 3ms/step - loss: 2.3034 - accuracy: 0.0989\n",
      "Epoch 25/25\n",
      "1200/1200 [==============================] - 3s 3ms/step - loss: 2.3030 - accuracy: 0.1012\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 2.3026 - accuracy: 0.1000\n",
      "Fitting for combination 90\n",
      "(28, 28, 1)\n",
      "4\n",
      "[32, 32, 128, 32]\n",
      "[(2, 2), (2, 2), (2, 2), (3, 3)]\n",
      "2\n",
      "[32, 128, 10]\n",
      "10\n",
      "True\n",
      "['relu', 'sigmoid', 'relu', 'tanh']\n",
      "['tanh', 'relu', 'softmax']\n",
      "Adam\n",
      "0.1\n",
      "0.01\n",
      "categorical_crossentropy\n",
      "50\n",
      "25\n",
      "0\n",
      "Epoch 1/25\n",
      "  16/1200 [..............................] - ETA: 3s - loss: 3.5614 - accuracy: 0.1063  "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-14 10:01:13.640572: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:954] layout failed: INVALID_ARGUMENT: Size of values 0 does not match size of permutation 4 @ fanin shape insequential_90/dropout_127/dropout/SelectV2-2-TransposeNHWCToNCHW-LayoutOptimizer\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1200/1200 [==============================] - 4s 3ms/step - loss: 2.3215 - accuracy: 0.0993\n",
      "Epoch 2/25\n",
      "1200/1200 [==============================] - 3s 3ms/step - loss: 2.3033 - accuracy: 0.0988\n",
      "Epoch 3/25\n",
      "1200/1200 [==============================] - 3s 3ms/step - loss: 2.3030 - accuracy: 0.0998\n",
      "Epoch 4/25\n",
      "1200/1200 [==============================] - 3s 3ms/step - loss: 2.3029 - accuracy: 0.0979\n",
      "Epoch 5/25\n",
      "1200/1200 [==============================] - 3s 3ms/step - loss: 2.3028 - accuracy: 0.0984\n",
      "Epoch 6/25\n",
      "1200/1200 [==============================] - 4s 3ms/step - loss: 2.3028 - accuracy: 0.0989\n",
      "Epoch 7/25\n",
      "1200/1200 [==============================] - 3s 3ms/step - loss: 2.3028 - accuracy: 0.0987\n",
      "Epoch 8/25\n",
      "1200/1200 [==============================] - 3s 3ms/step - loss: 2.3028 - accuracy: 0.0966\n",
      "Epoch 9/25\n",
      "1200/1200 [==============================] - 3s 3ms/step - loss: 2.3028 - accuracy: 0.0986\n",
      "Epoch 10/25\n",
      "1200/1200 [==============================] - 3s 3ms/step - loss: 2.3027 - accuracy: 0.0981\n",
      "Epoch 11/25\n",
      "1200/1200 [==============================] - 4s 3ms/step - loss: 2.3027 - accuracy: 0.0995\n",
      "Epoch 12/25\n",
      "1200/1200 [==============================] - 3s 3ms/step - loss: 2.3028 - accuracy: 0.0989\n",
      "Epoch 13/25\n",
      "1200/1200 [==============================] - 3s 3ms/step - loss: 2.3027 - accuracy: 0.0963\n",
      "Epoch 14/25\n",
      "1200/1200 [==============================] - 3s 3ms/step - loss: 2.3027 - accuracy: 0.0979\n",
      "Epoch 15/25\n",
      "1200/1200 [==============================] - 3s 3ms/step - loss: 2.3027 - accuracy: 0.0981\n",
      "Epoch 16/25\n",
      "1200/1200 [==============================] - 3s 3ms/step - loss: 2.3027 - accuracy: 0.0982\n",
      "Epoch 17/25\n",
      "1200/1200 [==============================] - 3s 3ms/step - loss: 2.3027 - accuracy: 0.0981\n",
      "Epoch 18/25\n",
      "1200/1200 [==============================] - 4s 3ms/step - loss: 2.3027 - accuracy: 0.0975\n",
      "Epoch 19/25\n",
      "1200/1200 [==============================] - 4s 3ms/step - loss: 2.3027 - accuracy: 0.0968\n",
      "Epoch 20/25\n",
      "1200/1200 [==============================] - 3s 3ms/step - loss: 2.3027 - accuracy: 0.0975\n",
      "Epoch 21/25\n",
      "1200/1200 [==============================] - 4s 3ms/step - loss: 2.3027 - accuracy: 0.0978\n",
      "Epoch 22/25\n",
      "1200/1200 [==============================] - 4s 3ms/step - loss: 2.3027 - accuracy: 0.0968\n",
      "Epoch 23/25\n",
      "1200/1200 [==============================] - 4s 3ms/step - loss: 2.3027 - accuracy: 0.0977\n",
      "Epoch 24/25\n",
      "1200/1200 [==============================] - 4s 3ms/step - loss: 2.3027 - accuracy: 0.0971\n",
      "Epoch 25/25\n",
      "1200/1200 [==============================] - 4s 3ms/step - loss: 2.3026 - accuracy: 0.0997\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 2.3026 - accuracy: 0.1000\n",
      "Fitting for combination 91\n",
      "(28, 28, 1)\n",
      "4\n",
      "[32, 32, 128, 32]\n",
      "[(2, 2), (2, 2), (2, 2), (5, 5)]\n",
      "2\n",
      "[64, 128, 10]\n",
      "10\n",
      "True\n",
      "['relu', 'sigmoid', 'relu', 'tanh']\n",
      "['relu', 'relu', 'softmax']\n",
      "Adam\n",
      "0.01\n",
      "0.01\n",
      "categorical_crossentropy\n",
      "150\n",
      "25\n",
      "0\n",
      "Epoch 1/25\n",
      "  1/400 [..............................] - ETA: 2:30 - loss: 2.2924 - accuracy: 0.1200"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-14 10:02:42.525966: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:954] layout failed: INVALID_ARGUMENT: Size of values 0 does not match size of permutation 4 @ fanin shape insequential_91/dropout_130/dropout/SelectV2-2-TransposeNHWCToNCHW-LayoutOptimizer\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "400/400 [==============================] - 2s 4ms/step - loss: 2.3053 - accuracy: 0.0987\n",
      "Epoch 2/25\n",
      "400/400 [==============================] - 2s 4ms/step - loss: 2.3028 - accuracy: 0.0995\n",
      "Epoch 3/25\n",
      "400/400 [==============================] - 2s 4ms/step - loss: 2.3027 - accuracy: 0.1005\n",
      "Epoch 4/25\n",
      "400/400 [==============================] - 2s 4ms/step - loss: 2.3027 - accuracy: 0.0986\n",
      "Epoch 5/25\n",
      "400/400 [==============================] - 2s 4ms/step - loss: 2.3026 - accuracy: 0.0986\n",
      "Epoch 6/25\n",
      "400/400 [==============================] - 2s 4ms/step - loss: 2.3027 - accuracy: 0.0962\n",
      "Epoch 7/25\n",
      "400/400 [==============================] - 2s 4ms/step - loss: 2.3026 - accuracy: 0.0975\n",
      "Epoch 8/25\n",
      "400/400 [==============================] - 2s 4ms/step - loss: 2.3026 - accuracy: 0.1002\n",
      "Epoch 9/25\n",
      "400/400 [==============================] - 2s 4ms/step - loss: 2.3026 - accuracy: 0.1004\n",
      "Epoch 10/25\n",
      "400/400 [==============================] - 2s 4ms/step - loss: 2.3026 - accuracy: 0.0979\n",
      "Epoch 11/25\n",
      "400/400 [==============================] - 2s 4ms/step - loss: 2.3026 - accuracy: 0.0984\n",
      "Epoch 12/25\n",
      "400/400 [==============================] - 2s 4ms/step - loss: 2.3026 - accuracy: 0.1009\n",
      "Epoch 13/25\n",
      "400/400 [==============================] - 2s 4ms/step - loss: 2.3026 - accuracy: 0.0976\n",
      "Epoch 14/25\n",
      "400/400 [==============================] - 2s 4ms/step - loss: 2.3026 - accuracy: 0.0998\n",
      "Epoch 15/25\n",
      "400/400 [==============================] - 2s 4ms/step - loss: 2.3026 - accuracy: 0.0992\n",
      "Epoch 16/25\n",
      "400/400 [==============================] - 2s 4ms/step - loss: 2.3026 - accuracy: 0.0978\n",
      "Epoch 17/25\n",
      "400/400 [==============================] - 2s 4ms/step - loss: 2.3026 - accuracy: 0.0970\n",
      "Epoch 18/25\n",
      "400/400 [==============================] - 2s 4ms/step - loss: 2.3026 - accuracy: 0.0986\n",
      "Epoch 19/25\n",
      "400/400 [==============================] - 2s 4ms/step - loss: 2.3026 - accuracy: 0.0973\n",
      "Epoch 20/25\n",
      "400/400 [==============================] - 2s 4ms/step - loss: 2.3026 - accuracy: 0.0975\n",
      "Epoch 21/25\n",
      "400/400 [==============================] - 2s 4ms/step - loss: 2.3026 - accuracy: 0.0998\n",
      "Epoch 22/25\n",
      "400/400 [==============================] - 2s 4ms/step - loss: 2.3026 - accuracy: 0.0979\n",
      "Epoch 23/25\n",
      "400/400 [==============================] - 2s 4ms/step - loss: 2.3026 - accuracy: 0.1001\n",
      "Epoch 24/25\n",
      "400/400 [==============================] - 2s 4ms/step - loss: 2.3026 - accuracy: 0.0986\n",
      "Epoch 25/25\n",
      "400/400 [==============================] - 2s 4ms/step - loss: 2.3026 - accuracy: 0.1000\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 2.3026 - accuracy: 0.1000\n",
      "Fitting for combination 92\n",
      "(28, 28, 1)\n",
      "4\n",
      "[32, 32, 128, 32]\n",
      "[(2, 2), (2, 2), (2, 2), (2, 2)]\n",
      "1\n",
      "[16, 10]\n",
      "10\n",
      "False\n",
      "['relu', 'sigmoid', 'relu', 'tanh']\n",
      "['tanh', 'softmax']\n",
      "Adam\n",
      "0.1\n",
      "0\n",
      "categorical_crossentropy\n",
      "300\n",
      "25\n",
      "0\n",
      "Epoch 1/25\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 2.3590 - accuracy: 0.1002\n",
      "Epoch 2/25\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 2.3497 - accuracy: 0.0992\n",
      "Epoch 3/25\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 2.3438 - accuracy: 0.1002\n",
      "Epoch 4/25\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 2.3475 - accuracy: 0.1026\n",
      "Epoch 5/25\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 2.3458 - accuracy: 0.0997\n",
      "Epoch 6/25\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 2.3441 - accuracy: 0.1018\n",
      "Epoch 7/25\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 2.3462 - accuracy: 0.0993\n",
      "Epoch 8/25\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 2.3448 - accuracy: 0.1000\n",
      "Epoch 9/25\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 2.3448 - accuracy: 0.0993\n",
      "Epoch 10/25\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 2.3453 - accuracy: 0.1000\n",
      "Epoch 11/25\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 2.3496 - accuracy: 0.0996\n",
      "Epoch 12/25\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 2.3438 - accuracy: 0.0993\n",
      "Epoch 13/25\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 2.3483 - accuracy: 0.0990\n",
      "Epoch 14/25\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 2.3454 - accuracy: 0.0990\n",
      "Epoch 15/25\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 2.3412 - accuracy: 0.1016\n",
      "Epoch 16/25\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 2.3459 - accuracy: 0.1018\n",
      "Epoch 17/25\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 2.3491 - accuracy: 0.1001\n",
      "Epoch 18/25\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 2.3465 - accuracy: 0.0997\n",
      "Epoch 19/25\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 2.3478 - accuracy: 0.0987\n",
      "Epoch 20/25\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 2.3511 - accuracy: 0.1015\n",
      "Epoch 21/25\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 2.3439 - accuracy: 0.0988\n",
      "Epoch 22/25\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 2.3487 - accuracy: 0.1005\n",
      "Epoch 23/25\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 2.3496 - accuracy: 0.0978\n",
      "Epoch 24/25\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 2.3553 - accuracy: 0.0985\n",
      "Epoch 25/25\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 2.3448 - accuracy: 0.0996\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 2.3418 - accuracy: 0.1000\n",
      "Fitting for combination 93\n",
      "(28, 28, 1)\n",
      "4\n",
      "[32, 32, 128, 32]\n",
      "[(2, 2), (2, 2), (2, 2), (4, 4)]\n",
      "3\n",
      "[32, 128, 16, 10]\n",
      "10\n",
      "True\n",
      "['relu', 'sigmoid', 'relu', 'sigmoid']\n",
      "['relu', 'relu', 'tanh', 'softmax']\n",
      "Adam\n",
      "0.1\n",
      "0.03\n",
      "categorical_crossentropy\n",
      "150\n",
      "25\n",
      "0\n",
      "Epoch 1/25\n",
      " 13/400 [..............................] - ETA: 1s - loss: 2.6256 - accuracy: 0.1133  "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-14 10:03:50.690455: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:954] layout failed: INVALID_ARGUMENT: Size of values 0 does not match size of permutation 4 @ fanin shape insequential_93/dropout_133/dropout/SelectV2-2-TransposeNHWCToNCHW-LayoutOptimizer\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "400/400 [==============================] - 2s 4ms/step - loss: 2.3398 - accuracy: 0.0997\n",
      "Epoch 2/25\n",
      "400/400 [==============================] - 2s 4ms/step - loss: 2.3107 - accuracy: 0.0985\n",
      "Epoch 3/25\n",
      "400/400 [==============================] - 2s 4ms/step - loss: 2.3070 - accuracy: 0.1011\n",
      "Epoch 4/25\n",
      "400/400 [==============================] - 2s 4ms/step - loss: 2.3060 - accuracy: 0.1015\n",
      "Epoch 5/25\n",
      "400/400 [==============================] - 2s 4ms/step - loss: 2.3053 - accuracy: 0.1013\n",
      "Epoch 6/25\n",
      "400/400 [==============================] - 2s 4ms/step - loss: 2.3050 - accuracy: 0.0992\n",
      "Epoch 7/25\n",
      "400/400 [==============================] - 2s 4ms/step - loss: 2.3045 - accuracy: 0.1014\n",
      "Epoch 8/25\n",
      "400/400 [==============================] - 2s 4ms/step - loss: 2.3041 - accuracy: 0.1001\n",
      "Epoch 9/25\n",
      "400/400 [==============================] - 2s 4ms/step - loss: 2.3042 - accuracy: 0.0999\n",
      "Epoch 10/25\n",
      "400/400 [==============================] - 2s 4ms/step - loss: 2.3038 - accuracy: 0.1003\n",
      "Epoch 11/25\n",
      "400/400 [==============================] - 2s 4ms/step - loss: 2.3040 - accuracy: 0.0975\n",
      "Epoch 12/25\n",
      "400/400 [==============================] - 2s 4ms/step - loss: 2.3036 - accuracy: 0.1010\n",
      "Epoch 13/25\n",
      "400/400 [==============================] - 2s 4ms/step - loss: 2.3037 - accuracy: 0.0985\n",
      "Epoch 14/25\n",
      "400/400 [==============================] - 2s 4ms/step - loss: 2.3034 - accuracy: 0.0989\n",
      "Epoch 15/25\n",
      "400/400 [==============================] - 2s 4ms/step - loss: 2.3035 - accuracy: 0.1004\n",
      "Epoch 16/25\n",
      "400/400 [==============================] - 2s 4ms/step - loss: 2.3034 - accuracy: 0.0992\n",
      "Epoch 17/25\n",
      "400/400 [==============================] - 2s 4ms/step - loss: 2.3035 - accuracy: 0.0978\n",
      "Epoch 18/25\n",
      "400/400 [==============================] - 2s 4ms/step - loss: 2.3033 - accuracy: 0.0985\n",
      "Epoch 19/25\n",
      "400/400 [==============================] - 2s 4ms/step - loss: 2.3034 - accuracy: 0.0984\n",
      "Epoch 20/25\n",
      "400/400 [==============================] - 2s 4ms/step - loss: 2.3033 - accuracy: 0.0981\n",
      "Epoch 21/25\n",
      "400/400 [==============================] - 2s 4ms/step - loss: 2.3031 - accuracy: 0.0985\n",
      "Epoch 22/25\n",
      "400/400 [==============================] - 2s 4ms/step - loss: 2.3033 - accuracy: 0.0989\n",
      "Epoch 23/25\n",
      "400/400 [==============================] - 2s 4ms/step - loss: 2.3031 - accuracy: 0.0993\n",
      "Epoch 24/25\n",
      "400/400 [==============================] - 2s 4ms/step - loss: 2.3031 - accuracy: 0.1013\n",
      "Epoch 25/25\n",
      "400/400 [==============================] - 2s 4ms/step - loss: 2.3030 - accuracy: 0.1003\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 2.3028 - accuracy: 0.1000\n",
      "Fitting for combination 94\n",
      "(28, 28, 1)\n",
      "4\n",
      "[32, 32, 128, 32]\n",
      "[(2, 2), (2, 2), (2, 2), (4, 4)]\n",
      "1\n",
      "[16, 10]\n",
      "10\n",
      "True\n",
      "['relu', 'sigmoid', 'relu', 'sigmoid']\n",
      "['relu', 'softmax']\n",
      "Adam\n",
      "0.3\n",
      "0.03\n",
      "categorical_crossentropy\n",
      "300\n",
      "25\n",
      "0\n",
      "Epoch 1/25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-14 10:04:33.021932: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:954] layout failed: INVALID_ARGUMENT: Size of values 0 does not match size of permutation 4 @ fanin shape insequential_94/dropout_137/dropout/SelectV2-2-TransposeNHWCToNCHW-LayoutOptimizer\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200/200 [==============================] - 1s 5ms/step - loss: 2.3970 - accuracy: 0.1010\n",
      "Epoch 2/25\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 2.3037 - accuracy: 0.1008\n",
      "Epoch 3/25\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 2.3032 - accuracy: 0.1007\n",
      "Epoch 4/25\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 2.3032 - accuracy: 0.0982\n",
      "Epoch 5/25\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 2.3030 - accuracy: 0.0994\n",
      "Epoch 6/25\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 2.3031 - accuracy: 0.0977\n",
      "Epoch 7/25\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 2.3030 - accuracy: 0.0990\n",
      "Epoch 8/25\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 2.3029 - accuracy: 0.0993\n",
      "Epoch 9/25\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 2.3029 - accuracy: 0.0983\n",
      "Epoch 10/25\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 2.3029 - accuracy: 0.0977\n",
      "Epoch 11/25\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 2.3028 - accuracy: 0.0993\n",
      "Epoch 12/25\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 2.3028 - accuracy: 0.0994\n",
      "Epoch 13/25\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 2.3028 - accuracy: 0.0966\n",
      "Epoch 14/25\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 2.3027 - accuracy: 0.0989\n",
      "Epoch 15/25\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 2.3027 - accuracy: 0.0983\n",
      "Epoch 16/25\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 2.3028 - accuracy: 0.0993\n",
      "Epoch 17/25\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 2.3028 - accuracy: 0.0987\n",
      "Epoch 18/25\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 2.3028 - accuracy: 0.0968\n",
      "Epoch 19/25\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 2.3027 - accuracy: 0.0979\n",
      "Epoch 20/25\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 2.3027 - accuracy: 0.0979\n",
      "Epoch 21/25\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 2.3027 - accuracy: 0.1001\n",
      "Epoch 22/25\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 2.3027 - accuracy: 0.0975\n",
      "Epoch 23/25\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 2.3027 - accuracy: 0.0988\n",
      "Epoch 24/25\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 2.3027 - accuracy: 0.0960\n",
      "Epoch 25/25\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 2.3027 - accuracy: 0.0994\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 2.3026 - accuracy: 0.1000\n",
      "Fitting for combination 95\n",
      "(28, 28, 1)\n",
      "4\n",
      "[32, 32, 128, 32]\n",
      "[(2, 2), (2, 2), (2, 2), (4, 4)]\n",
      "3\n",
      "[16, 16, 32, 10]\n",
      "10\n",
      "True\n",
      "['relu', 'sigmoid', 'relu', 'tanh']\n",
      "['relu', 'relu', 'tanh', 'softmax']\n",
      "SGD\n",
      "0.01\n",
      "0\n",
      "categorical_crossentropy\n",
      "300\n",
      "25\n",
      "0\n",
      "Epoch 1/25\n",
      " 29/200 [===>..........................] - ETA: 0s - loss: 2.3043 - accuracy: 0.0991"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-14 10:05:00.601405: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:954] layout failed: INVALID_ARGUMENT: Size of values 0 does not match size of permutation 4 @ fanin shape insequential_95/dropout_139/dropout/SelectV2-2-TransposeNHWCToNCHW-LayoutOptimizer\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200/200 [==============================] - 1s 5ms/step - loss: 2.3029 - accuracy: 0.0986\n",
      "Epoch 2/25\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 2.3026 - accuracy: 0.0994\n",
      "Epoch 3/25\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 2.3026 - accuracy: 0.1013\n",
      "Epoch 4/25\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 2.3025 - accuracy: 0.1014\n",
      "Epoch 5/25\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 2.3025 - accuracy: 0.0991\n",
      "Epoch 6/25\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 2.3025 - accuracy: 0.1001\n",
      "Epoch 7/25\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 2.3024 - accuracy: 0.1020\n",
      "Epoch 8/25\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 2.3024 - accuracy: 0.1018\n",
      "Epoch 9/25\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 2.3023 - accuracy: 0.1024\n",
      "Epoch 10/25\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 2.3022 - accuracy: 0.1037\n",
      "Epoch 11/25\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 2.3018 - accuracy: 0.1041\n",
      "Epoch 12/25\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 2.3013 - accuracy: 0.1077\n",
      "Epoch 13/25\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 2.3012 - accuracy: 0.1058\n",
      "Epoch 14/25\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 2.3006 - accuracy: 0.1064\n",
      "Epoch 15/25\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 2.2997 - accuracy: 0.1095\n",
      "Epoch 16/25\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 2.2980 - accuracy: 0.1116\n",
      "Epoch 17/25\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 2.2951 - accuracy: 0.1194\n",
      "Epoch 18/25\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 2.2896 - accuracy: 0.1229\n",
      "Epoch 19/25\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 2.2823 - accuracy: 0.1322\n",
      "Epoch 20/25\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 2.2691 - accuracy: 0.1467\n",
      "Epoch 21/25\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 2.2499 - accuracy: 0.1581\n",
      "Epoch 22/25\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 2.2207 - accuracy: 0.1719\n",
      "Epoch 23/25\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 2.1747 - accuracy: 0.1904\n",
      "Epoch 24/25\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 2.1094 - accuracy: 0.2100\n",
      "Epoch 25/25\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 2.0325 - accuracy: 0.2296\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 1.7895 - accuracy: 0.4624\n",
      "Fitting for combination 96\n",
      "(28, 28, 1)\n",
      "4\n",
      "[32, 32, 128, 32]\n",
      "[(2, 2), (2, 2), (2, 2), (5, 5)]\n",
      "2\n",
      "[16, 32, 10]\n",
      "10\n",
      "True\n",
      "['relu', 'sigmoid', 'relu', 'relu']\n",
      "['relu', 'sigmoid', 'softmax']\n",
      "SGD\n",
      "0.1\n",
      "0.03\n",
      "categorical_crossentropy\n",
      "300\n",
      "25\n",
      "0\n",
      "Epoch 1/25\n",
      " 11/200 [>.............................] - ETA: 1s - loss: 2.5118 - accuracy: 0.1045 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-14 10:05:28.312304: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:954] layout failed: INVALID_ARGUMENT: Size of values 0 does not match size of permutation 4 @ fanin shape insequential_96/dropout_143/dropout/SelectV2-2-TransposeNHWCToNCHW-LayoutOptimizer\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200/200 [==============================] - 1s 5ms/step - loss: 2.4338 - accuracy: 0.1006\n",
      "Epoch 2/25\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 2.4076 - accuracy: 0.0993\n",
      "Epoch 3/25\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 2.3950 - accuracy: 0.1009\n",
      "Epoch 4/25\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 2.3896 - accuracy: 0.1020\n",
      "Epoch 5/25\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 2.3860 - accuracy: 0.1001\n",
      "Epoch 6/25\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 2.3840 - accuracy: 0.0968\n",
      "Epoch 7/25\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 2.3811 - accuracy: 0.1014\n",
      "Epoch 8/25\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 2.3792 - accuracy: 0.0981\n",
      "Epoch 9/25\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 2.3773 - accuracy: 0.0991\n",
      "Epoch 10/25\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 2.3734 - accuracy: 0.1035\n",
      "Epoch 11/25\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 2.3734 - accuracy: 0.1003\n",
      "Epoch 12/25\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 2.3723 - accuracy: 0.0974\n",
      "Epoch 13/25\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 2.3691 - accuracy: 0.1028\n",
      "Epoch 14/25\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 2.3695 - accuracy: 0.1019\n",
      "Epoch 15/25\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 2.3673 - accuracy: 0.1008\n",
      "Epoch 16/25\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 2.3652 - accuracy: 0.1015\n",
      "Epoch 17/25\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 2.3682 - accuracy: 0.0997\n",
      "Epoch 18/25\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 2.3673 - accuracy: 0.1003\n",
      "Epoch 19/25\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 2.3663 - accuracy: 0.1003\n",
      "Epoch 20/25\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 2.3653 - accuracy: 0.1012\n",
      "Epoch 21/25\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 2.3642 - accuracy: 0.1005\n",
      "Epoch 22/25\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 2.3645 - accuracy: 0.0986\n",
      "Epoch 23/25\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 2.3612 - accuracy: 0.1003\n",
      "Epoch 24/25\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 2.3624 - accuracy: 0.1006\n",
      "Epoch 25/25\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 2.3620 - accuracy: 0.1018\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 2.3024 - accuracy: 0.1000\n",
      "Fitting for combination 97\n",
      "(28, 28, 1)\n",
      "4\n",
      "[32, 32, 128, 32]\n",
      "[(2, 2), (2, 2), (2, 2), (2, 2)]\n",
      "3\n",
      "[32, 16, 128, 10]\n",
      "10\n",
      "False\n",
      "['relu', 'sigmoid', 'relu', 'tanh']\n",
      "['sigmoid', 'tanh', 'relu', 'softmax']\n",
      "SGD\n",
      "0.3\n",
      "0.01\n",
      "categorical_crossentropy\n",
      "200\n",
      "25\n",
      "0\n",
      "Epoch 1/25\n",
      "300/300 [==============================] - 2s 4ms/step - loss: 2.2785 - accuracy: 0.1571\n",
      "Epoch 2/25\n",
      "300/300 [==============================] - 1s 4ms/step - loss: 1.1944 - accuracy: 0.5899\n",
      "Epoch 3/25\n",
      "300/300 [==============================] - 1s 4ms/step - loss: 0.7994 - accuracy: 0.7160\n",
      "Epoch 4/25\n",
      "300/300 [==============================] - 1s 4ms/step - loss: 0.7042 - accuracy: 0.7384\n",
      "Epoch 5/25\n",
      "300/300 [==============================] - 1s 4ms/step - loss: 0.6562 - accuracy: 0.7520\n",
      "Epoch 6/25\n",
      "300/300 [==============================] - 1s 4ms/step - loss: 0.6248 - accuracy: 0.7612\n",
      "Epoch 7/25\n",
      "300/300 [==============================] - 1s 4ms/step - loss: 0.6025 - accuracy: 0.7683\n",
      "Epoch 8/25\n",
      "300/300 [==============================] - 1s 4ms/step - loss: 0.5850 - accuracy: 0.7746\n",
      "Epoch 9/25\n",
      "300/300 [==============================] - 1s 4ms/step - loss: 0.5709 - accuracy: 0.7805\n",
      "Epoch 10/25\n",
      "300/300 [==============================] - 1s 4ms/step - loss: 0.5599 - accuracy: 0.7842\n",
      "Epoch 11/25\n",
      "300/300 [==============================] - 1s 4ms/step - loss: 0.5504 - accuracy: 0.7883\n",
      "Epoch 12/25\n",
      "300/300 [==============================] - 1s 4ms/step - loss: 0.5423 - accuracy: 0.7909\n",
      "Epoch 13/25\n",
      "300/300 [==============================] - 1s 4ms/step - loss: 0.5354 - accuracy: 0.7946\n",
      "Epoch 14/25\n",
      "300/300 [==============================] - 1s 4ms/step - loss: 0.5292 - accuracy: 0.7970\n",
      "Epoch 15/25\n",
      "300/300 [==============================] - 1s 4ms/step - loss: 0.5240 - accuracy: 0.8003\n",
      "Epoch 16/25\n",
      "300/300 [==============================] - 1s 4ms/step - loss: 0.5193 - accuracy: 0.8030\n",
      "Epoch 17/25\n",
      "300/300 [==============================] - 1s 4ms/step - loss: 0.5145 - accuracy: 0.8054\n",
      "Epoch 18/25\n",
      "300/300 [==============================] - 1s 4ms/step - loss: 0.5107 - accuracy: 0.8068\n",
      "Epoch 19/25\n",
      "300/300 [==============================] - 1s 4ms/step - loss: 0.5074 - accuracy: 0.8090\n",
      "Epoch 20/25\n",
      "300/300 [==============================] - 1s 4ms/step - loss: 0.5038 - accuracy: 0.8104\n",
      "Epoch 21/25\n",
      "300/300 [==============================] - 1s 4ms/step - loss: 0.5007 - accuracy: 0.8116\n",
      "Epoch 22/25\n",
      "300/300 [==============================] - 1s 4ms/step - loss: 0.4977 - accuracy: 0.8139\n",
      "Epoch 23/25\n",
      "300/300 [==============================] - 1s 4ms/step - loss: 0.4953 - accuracy: 0.8146\n",
      "Epoch 24/25\n",
      "300/300 [==============================] - 1s 4ms/step - loss: 0.4928 - accuracy: 0.8156\n",
      "Epoch 25/25\n",
      "300/300 [==============================] - 1s 4ms/step - loss: 0.4904 - accuracy: 0.8167\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 0.5171 - accuracy: 0.8065\n",
      "Fitting for combination 98\n",
      "(28, 28, 1)\n",
      "4\n",
      "[32, 32, 128, 32]\n",
      "[(2, 2), (2, 2), (2, 2), (4, 4)]\n",
      "3\n",
      "[16, 64, 32, 10]\n",
      "10\n",
      "True\n",
      "['relu', 'sigmoid', 'relu', 'sigmoid']\n",
      "['relu', 'relu', 'tanh', 'softmax']\n",
      "SGD\n",
      "0.1\n",
      "0\n",
      "categorical_crossentropy\n",
      "200\n",
      "25\n",
      "0\n",
      "Epoch 1/25\n",
      " 13/300 [>.............................] - ETA: 1s - loss: 2.3163 - accuracy: 0.1077  "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-14 10:06:27.934030: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:954] layout failed: INVALID_ARGUMENT: Size of values 0 does not match size of permutation 4 @ fanin shape insequential_98/dropout_146/dropout/SelectV2-2-TransposeNHWCToNCHW-LayoutOptimizer\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300/300 [==============================] - 2s 4ms/step - loss: 2.3036 - accuracy: 0.1019\n",
      "Epoch 2/25\n",
      "300/300 [==============================] - 1s 4ms/step - loss: 2.3031 - accuracy: 0.0990\n",
      "Epoch 3/25\n",
      "300/300 [==============================] - 1s 4ms/step - loss: 2.3031 - accuracy: 0.0978\n",
      "Epoch 4/25\n",
      "300/300 [==============================] - 1s 4ms/step - loss: 2.3031 - accuracy: 0.0993\n",
      "Epoch 5/25\n",
      "300/300 [==============================] - 1s 4ms/step - loss: 2.3030 - accuracy: 0.1001\n",
      "Epoch 6/25\n",
      "300/300 [==============================] - 1s 4ms/step - loss: 2.3030 - accuracy: 0.1001\n",
      "Epoch 7/25\n",
      "300/300 [==============================] - 1s 4ms/step - loss: 2.3031 - accuracy: 0.0988\n",
      "Epoch 8/25\n",
      "300/300 [==============================] - 1s 4ms/step - loss: 2.3031 - accuracy: 0.0984\n",
      "Epoch 9/25\n",
      "300/300 [==============================] - 1s 4ms/step - loss: 2.3030 - accuracy: 0.0999\n",
      "Epoch 10/25\n",
      "300/300 [==============================] - 1s 4ms/step - loss: 2.3031 - accuracy: 0.0981\n",
      "Epoch 11/25\n",
      "300/300 [==============================] - 1s 4ms/step - loss: 2.3031 - accuracy: 0.0976\n",
      "Epoch 12/25\n",
      "300/300 [==============================] - 1s 4ms/step - loss: 2.3031 - accuracy: 0.0993\n",
      "Epoch 13/25\n",
      "300/300 [==============================] - 1s 4ms/step - loss: 2.3032 - accuracy: 0.0981\n",
      "Epoch 14/25\n",
      "300/300 [==============================] - 1s 4ms/step - loss: 2.3031 - accuracy: 0.0995\n",
      "Epoch 15/25\n",
      "300/300 [==============================] - 1s 4ms/step - loss: 2.3031 - accuracy: 0.0994\n",
      "Epoch 16/25\n",
      "300/300 [==============================] - 1s 4ms/step - loss: 2.3031 - accuracy: 0.0991\n",
      "Epoch 17/25\n",
      "300/300 [==============================] - 1s 4ms/step - loss: 2.3031 - accuracy: 0.0998\n",
      "Epoch 18/25\n",
      "300/300 [==============================] - 1s 4ms/step - loss: 2.3030 - accuracy: 0.0981\n",
      "Epoch 19/25\n",
      "300/300 [==============================] - 1s 4ms/step - loss: 2.3032 - accuracy: 0.0994\n",
      "Epoch 20/25\n",
      "300/300 [==============================] - 1s 4ms/step - loss: 2.3031 - accuracy: 0.0985\n",
      "Epoch 21/25\n",
      "300/300 [==============================] - 1s 4ms/step - loss: 2.3031 - accuracy: 0.0992\n",
      "Epoch 22/25\n",
      "300/300 [==============================] - 1s 4ms/step - loss: 2.3029 - accuracy: 0.1026\n",
      "Epoch 23/25\n",
      "300/300 [==============================] - 1s 4ms/step - loss: 2.3031 - accuracy: 0.1001\n",
      "Epoch 24/25\n",
      "300/300 [==============================] - 1s 4ms/step - loss: 2.3032 - accuracy: 0.0996\n",
      "Epoch 25/25\n",
      "300/300 [==============================] - 1s 4ms/step - loss: 2.3030 - accuracy: 0.0988\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 2.3027 - accuracy: 0.1000\n",
      "Fitting for combination 99\n",
      "(28, 28, 1)\n",
      "4\n",
      "[32, 32, 128, 32]\n",
      "[(2, 2), (2, 2), (2, 2), (4, 4)]\n",
      "2\n",
      "[64, 128, 10]\n",
      "10\n",
      "False\n",
      "['relu', 'sigmoid', 'relu', 'relu']\n",
      "['relu', 'sigmoid', 'softmax']\n",
      "SGD\n",
      "0.3\n",
      "0\n",
      "categorical_crossentropy\n",
      "200\n",
      "25\n",
      "0\n",
      "Epoch 1/25\n",
      "300/300 [==============================] - 1s 4ms/step - loss: 1.8006 - accuracy: 0.3099\n",
      "Epoch 2/25\n",
      "300/300 [==============================] - 1s 4ms/step - loss: 0.6803 - accuracy: 0.7379\n",
      "Epoch 3/25\n",
      "300/300 [==============================] - 1s 4ms/step - loss: 0.5682 - accuracy: 0.7790\n",
      "Epoch 4/25\n",
      "300/300 [==============================] - 1s 4ms/step - loss: 0.5099 - accuracy: 0.8039\n",
      "Epoch 5/25\n",
      "300/300 [==============================] - 1s 4ms/step - loss: 0.4741 - accuracy: 0.8169\n",
      "Epoch 6/25\n",
      "300/300 [==============================] - 1s 4ms/step - loss: 0.4446 - accuracy: 0.8292\n",
      "Epoch 7/25\n",
      "300/300 [==============================] - 1s 4ms/step - loss: 0.4255 - accuracy: 0.8365\n",
      "Epoch 8/25\n",
      "300/300 [==============================] - 1s 4ms/step - loss: 0.4056 - accuracy: 0.8464\n",
      "Epoch 9/25\n",
      "300/300 [==============================] - 1s 4ms/step - loss: 0.3856 - accuracy: 0.8540\n",
      "Epoch 10/25\n",
      "300/300 [==============================] - 1s 4ms/step - loss: 0.3776 - accuracy: 0.8570\n",
      "Epoch 11/25\n",
      "300/300 [==============================] - 1s 4ms/step - loss: 0.3644 - accuracy: 0.8616\n",
      "Epoch 12/25\n",
      "300/300 [==============================] - 1s 4ms/step - loss: 0.3506 - accuracy: 0.8681\n",
      "Epoch 13/25\n",
      "300/300 [==============================] - 1s 4ms/step - loss: 0.3457 - accuracy: 0.8690\n",
      "Epoch 14/25\n",
      "300/300 [==============================] - 1s 4ms/step - loss: 0.3367 - accuracy: 0.8720\n",
      "Epoch 15/25\n",
      "300/300 [==============================] - 1s 4ms/step - loss: 0.3260 - accuracy: 0.8766\n",
      "Epoch 16/25\n",
      "300/300 [==============================] - 1s 4ms/step - loss: 0.3205 - accuracy: 0.8782\n",
      "Epoch 17/25\n",
      "300/300 [==============================] - 1s 4ms/step - loss: 0.3104 - accuracy: 0.8823\n",
      "Epoch 18/25\n",
      "300/300 [==============================] - 1s 4ms/step - loss: 0.3048 - accuracy: 0.8836\n",
      "Epoch 19/25\n",
      "300/300 [==============================] - 1s 4ms/step - loss: 0.2995 - accuracy: 0.8877\n",
      "Epoch 20/25\n",
      "300/300 [==============================] - 1s 4ms/step - loss: 0.2935 - accuracy: 0.8885\n",
      "Epoch 21/25\n",
      "300/300 [==============================] - 1s 4ms/step - loss: 0.2868 - accuracy: 0.8916\n",
      "Epoch 22/25\n",
      "300/300 [==============================] - 1s 4ms/step - loss: 0.2803 - accuracy: 0.8948\n",
      "Epoch 23/25\n",
      "300/300 [==============================] - 1s 4ms/step - loss: 0.2765 - accuracy: 0.8963\n",
      "Epoch 24/25\n",
      "300/300 [==============================] - 1s 4ms/step - loss: 0.2737 - accuracy: 0.8963\n",
      "Epoch 25/25\n",
      "300/300 [==============================] - 1s 4ms/step - loss: 0.2653 - accuracy: 0.8996\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 0.3033 - accuracy: 0.8861\n",
      "Fitting for combination 100\n",
      "(28, 28, 1)\n",
      "4\n",
      "[32, 32, 128, 64]\n",
      "[(2, 2), (2, 2), (2, 2), (2, 2)]\n",
      "2\n",
      "[128, 16, 10]\n",
      "10\n",
      "False\n",
      "['relu', 'sigmoid', 'relu', 'tanh']\n",
      "['tanh', 'tanh', 'softmax']\n",
      "Adam\n",
      "0.1\n",
      "0.03\n",
      "categorical_crossentropy\n",
      "300\n",
      "25\n",
      "0\n",
      "Epoch 1/25\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 2.3326 - accuracy: 0.1005\n",
      "Epoch 2/25\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 2.3081 - accuracy: 0.0993\n",
      "Epoch 3/25\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 2.3057 - accuracy: 0.0991\n",
      "Epoch 4/25\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 2.3053 - accuracy: 0.1001\n",
      "Epoch 5/25\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 2.3051 - accuracy: 0.0998\n",
      "Epoch 6/25\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 2.3049 - accuracy: 0.0996\n",
      "Epoch 7/25\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 2.3042 - accuracy: 0.0986\n",
      "Epoch 8/25\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 2.3042 - accuracy: 0.0972\n",
      "Epoch 9/25\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 2.3040 - accuracy: 0.1001\n",
      "Epoch 10/25\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 2.3039 - accuracy: 0.0996\n",
      "Epoch 11/25\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 2.3038 - accuracy: 0.0991\n",
      "Epoch 12/25\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 2.3035 - accuracy: 0.1010\n",
      "Epoch 13/25\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 2.3036 - accuracy: 0.0987\n",
      "Epoch 14/25\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 2.3034 - accuracy: 0.1010\n",
      "Epoch 15/25\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 2.3033 - accuracy: 0.1013\n",
      "Epoch 16/25\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 2.3035 - accuracy: 0.1011\n",
      "Epoch 17/25\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 2.3035 - accuracy: 0.0983\n",
      "Epoch 18/25\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 2.3035 - accuracy: 0.0979\n",
      "Epoch 19/25\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 2.3033 - accuracy: 0.0973\n",
      "Epoch 20/25\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 2.3033 - accuracy: 0.0971\n",
      "Epoch 21/25\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 2.3032 - accuracy: 0.0975\n",
      "Epoch 22/25\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 2.3031 - accuracy: 0.0995\n",
      "Epoch 23/25\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 2.3031 - accuracy: 0.0984\n",
      "Epoch 24/25\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 2.3033 - accuracy: 0.0964\n",
      "Epoch 25/25\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 2.3030 - accuracy: 0.0990\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 2.3029 - accuracy: 0.1000\n",
      "Fitting for combination 101\n",
      "(28, 28, 1)\n",
      "4\n",
      "[32, 32, 128, 64]\n",
      "[(2, 2), (2, 2), (2, 2), (2, 2)]\n",
      "2\n",
      "[128, 128, 10]\n",
      "10\n",
      "False\n",
      "['relu', 'sigmoid', 'relu', 'sigmoid']\n",
      "['sigmoid', 'sigmoid', 'softmax']\n",
      "Adam\n",
      "0.1\n",
      "0\n",
      "categorical_crossentropy\n",
      "50\n",
      "25\n",
      "0\n",
      "Epoch 1/25\n",
      "1200/1200 [==============================] - 3s 3ms/step - loss: 2.6529 - accuracy: 0.0993\n",
      "Epoch 2/25\n",
      "1200/1200 [==============================] - 3s 3ms/step - loss: 2.6130 - accuracy: 0.1010\n",
      "Epoch 3/25\n",
      "1200/1200 [==============================] - 3s 3ms/step - loss: 2.5630 - accuracy: 0.1013\n",
      "Epoch 4/25\n",
      "1200/1200 [==============================] - 3s 3ms/step - loss: 2.5933 - accuracy: 0.0993\n",
      "Epoch 5/25\n",
      "1200/1200 [==============================] - 3s 3ms/step - loss: 2.5992 - accuracy: 0.1010\n",
      "Epoch 6/25\n",
      "1200/1200 [==============================] - 3s 2ms/step - loss: 2.5690 - accuracy: 0.1008\n",
      "Epoch 7/25\n",
      "1200/1200 [==============================] - 3s 2ms/step - loss: 2.5764 - accuracy: 0.1016\n",
      "Epoch 8/25\n",
      "1200/1200 [==============================] - 3s 2ms/step - loss: 2.6136 - accuracy: 0.1008\n",
      "Epoch 9/25\n",
      "1200/1200 [==============================] - 3s 2ms/step - loss: 2.6074 - accuracy: 0.1001\n",
      "Epoch 10/25\n",
      "1200/1200 [==============================] - 3s 3ms/step - loss: 2.5946 - accuracy: 0.1014\n",
      "Epoch 11/25\n",
      "1200/1200 [==============================] - 3s 3ms/step - loss: 2.5985 - accuracy: 0.1009\n",
      "Epoch 12/25\n",
      "1200/1200 [==============================] - 3s 3ms/step - loss: 2.5966 - accuracy: 0.0988\n",
      "Epoch 13/25\n",
      "1200/1200 [==============================] - 3s 2ms/step - loss: 2.5805 - accuracy: 0.0993\n",
      "Epoch 14/25\n",
      "1200/1200 [==============================] - 3s 2ms/step - loss: 2.5983 - accuracy: 0.0997\n",
      "Epoch 15/25\n",
      "1200/1200 [==============================] - 3s 3ms/step - loss: 2.6143 - accuracy: 0.0987\n",
      "Epoch 16/25\n",
      "1200/1200 [==============================] - 3s 3ms/step - loss: 2.6019 - accuracy: 0.0992\n",
      "Epoch 17/25\n",
      "1200/1200 [==============================] - 3s 2ms/step - loss: 2.5953 - accuracy: 0.0994\n",
      "Epoch 18/25\n",
      "1200/1200 [==============================] - 3s 2ms/step - loss: 2.5865 - accuracy: 0.1019\n",
      "Epoch 19/25\n",
      "1200/1200 [==============================] - 3s 2ms/step - loss: 2.6247 - accuracy: 0.1011\n",
      "Epoch 20/25\n",
      "1200/1200 [==============================] - 3s 2ms/step - loss: 2.6007 - accuracy: 0.0998\n",
      "Epoch 21/25\n",
      "1200/1200 [==============================] - 3s 2ms/step - loss: 2.5983 - accuracy: 0.1006\n",
      "Epoch 22/25\n",
      "1200/1200 [==============================] - 3s 2ms/step - loss: 2.5886 - accuracy: 0.1000\n",
      "Epoch 23/25\n",
      "1200/1200 [==============================] - 3s 3ms/step - loss: 2.6006 - accuracy: 0.1010\n",
      "Epoch 24/25\n",
      "1200/1200 [==============================] - 3s 3ms/step - loss: 2.5913 - accuracy: 0.1005\n",
      "Epoch 25/25\n",
      "1200/1200 [==============================] - 3s 3ms/step - loss: 2.6052 - accuracy: 0.1005\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 2.5692 - accuracy: 0.1000\n",
      "Fitting for combination 102\n",
      "(28, 28, 1)\n",
      "4\n",
      "[32, 32, 128, 64]\n",
      "[(2, 2), (2, 2), (2, 2), (2, 2)]\n",
      "2\n",
      "[32, 64, 10]\n",
      "10\n",
      "True\n",
      "['relu', 'sigmoid', 'relu', 'sigmoid']\n",
      "['tanh', 'sigmoid', 'softmax']\n",
      "Adam\n",
      "0.03\n",
      "0.01\n",
      "categorical_crossentropy\n",
      "50\n",
      "25\n",
      "0\n",
      "Epoch 1/25\n",
      "  36/1200 [..............................] - ETA: 3s - loss: 2.4035 - accuracy: 0.1028"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-14 10:09:18.070632: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:954] layout failed: INVALID_ARGUMENT: Size of values 0 does not match size of permutation 4 @ fanin shape insequential_102/dropout_150/dropout/SelectV2-2-TransposeNHWCToNCHW-LayoutOptimizer\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1200/1200 [==============================] - 4s 3ms/step - loss: 2.3067 - accuracy: 0.0990\n",
      "Epoch 2/25\n",
      "1200/1200 [==============================] - 3s 3ms/step - loss: 2.3030 - accuracy: 0.0978\n",
      "Epoch 3/25\n",
      "1200/1200 [==============================] - 3s 3ms/step - loss: 2.3028 - accuracy: 0.0989\n",
      "Epoch 4/25\n",
      "1200/1200 [==============================] - 3s 3ms/step - loss: 2.3028 - accuracy: 0.0972\n",
      "Epoch 5/25\n",
      "1200/1200 [==============================] - 3s 3ms/step - loss: 2.3028 - accuracy: 0.0992\n",
      "Epoch 6/25\n",
      "1200/1200 [==============================] - 3s 3ms/step - loss: 2.3028 - accuracy: 0.0960\n",
      "Epoch 7/25\n",
      "1200/1200 [==============================] - 3s 3ms/step - loss: 2.3026 - accuracy: 0.0992\n",
      "Epoch 8/25\n",
      "1200/1200 [==============================] - 3s 3ms/step - loss: 2.3027 - accuracy: 0.0961\n",
      "Epoch 9/25\n",
      "1200/1200 [==============================] - 3s 3ms/step - loss: 2.3027 - accuracy: 0.0986\n",
      "Epoch 10/25\n",
      "1200/1200 [==============================] - 3s 3ms/step - loss: 2.3027 - accuracy: 0.0989\n",
      "Epoch 11/25\n",
      "1200/1200 [==============================] - 3s 3ms/step - loss: 2.3027 - accuracy: 0.0971\n",
      "Epoch 12/25\n",
      "1200/1200 [==============================] - 3s 3ms/step - loss: 2.3027 - accuracy: 0.0991\n",
      "Epoch 13/25\n",
      "1200/1200 [==============================] - 3s 3ms/step - loss: 2.3027 - accuracy: 0.0970\n",
      "Epoch 14/25\n",
      "1200/1200 [==============================] - 3s 3ms/step - loss: 2.3026 - accuracy: 0.0992\n",
      "Epoch 15/25\n",
      "1200/1200 [==============================] - 3s 3ms/step - loss: 2.3026 - accuracy: 0.0991\n",
      "Epoch 16/25\n",
      "1200/1200 [==============================] - 3s 3ms/step - loss: 2.3026 - accuracy: 0.0970\n",
      "Epoch 17/25\n",
      "1200/1200 [==============================] - 3s 3ms/step - loss: 2.3027 - accuracy: 0.0972\n",
      "Epoch 18/25\n",
      "1200/1200 [==============================] - 3s 3ms/step - loss: 2.3027 - accuracy: 0.0985\n",
      "Epoch 19/25\n",
      "1200/1200 [==============================] - 3s 3ms/step - loss: 2.3026 - accuracy: 0.0980\n",
      "Epoch 20/25\n",
      "1200/1200 [==============================] - 3s 3ms/step - loss: 2.3026 - accuracy: 0.0979\n",
      "Epoch 21/25\n",
      "1200/1200 [==============================] - 3s 3ms/step - loss: 2.3026 - accuracy: 0.0979\n",
      "Epoch 22/25\n",
      "1200/1200 [==============================] - 3s 3ms/step - loss: 2.3026 - accuracy: 0.0979\n",
      "Epoch 23/25\n",
      "1200/1200 [==============================] - 3s 3ms/step - loss: 2.3026 - accuracy: 0.0971\n",
      "Epoch 24/25\n",
      "1200/1200 [==============================] - 3s 3ms/step - loss: 2.3026 - accuracy: 0.0985\n",
      "Epoch 25/25\n",
      "1200/1200 [==============================] - 3s 3ms/step - loss: 2.3027 - accuracy: 0.0989\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 2.3026 - accuracy: 0.1000\n",
      "Fitting for combination 103\n",
      "(28, 28, 1)\n",
      "4\n",
      "[32, 32, 128, 64]\n",
      "[(2, 2), (2, 2), (2, 2), (3, 3)]\n",
      "1\n",
      "[128, 10]\n",
      "10\n",
      "False\n",
      "['relu', 'sigmoid', 'relu', 'relu']\n",
      "['tanh', 'softmax']\n",
      "Adam\n",
      "0.03\n",
      "0\n",
      "categorical_crossentropy\n",
      "50\n",
      "25\n",
      "0\n",
      "Epoch 1/25\n",
      "1200/1200 [==============================] - 4s 3ms/step - loss: 2.5816 - accuracy: 0.0997\n",
      "Epoch 2/25\n",
      "1200/1200 [==============================] - 3s 3ms/step - loss: 2.5776 - accuracy: 0.0995\n",
      "Epoch 3/25\n",
      "1200/1200 [==============================] - 3s 3ms/step - loss: 2.5447 - accuracy: 0.1001\n",
      "Epoch 4/25\n",
      "1200/1200 [==============================] - 3s 3ms/step - loss: 2.5628 - accuracy: 0.0983\n",
      "Epoch 5/25\n",
      "1200/1200 [==============================] - 3s 3ms/step - loss: 2.5723 - accuracy: 0.1000\n",
      "Epoch 6/25\n",
      "1200/1200 [==============================] - 3s 3ms/step - loss: 2.5493 - accuracy: 0.1012\n",
      "Epoch 7/25\n",
      "1200/1200 [==============================] - 3s 3ms/step - loss: 2.5582 - accuracy: 0.1022\n",
      "Epoch 8/25\n",
      "1200/1200 [==============================] - 3s 3ms/step - loss: 2.5889 - accuracy: 0.1007\n",
      "Epoch 9/25\n",
      "1200/1200 [==============================] - 3s 3ms/step - loss: 2.5786 - accuracy: 0.1002\n",
      "Epoch 10/25\n",
      "1200/1200 [==============================] - 3s 3ms/step - loss: 2.5732 - accuracy: 0.0992\n",
      "Epoch 11/25\n",
      "1200/1200 [==============================] - 4s 3ms/step - loss: 2.5752 - accuracy: 0.1023\n",
      "Epoch 12/25\n",
      "1200/1200 [==============================] - 4s 3ms/step - loss: 2.5731 - accuracy: 0.0978\n",
      "Epoch 13/25\n",
      "1200/1200 [==============================] - 4s 3ms/step - loss: 2.5532 - accuracy: 0.0996\n",
      "Epoch 14/25\n",
      "1200/1200 [==============================] - 4s 3ms/step - loss: 2.5666 - accuracy: 0.0991\n",
      "Epoch 15/25\n",
      "1200/1200 [==============================] - 4s 3ms/step - loss: 2.5899 - accuracy: 0.0985\n",
      "Epoch 16/25\n",
      "1200/1200 [==============================] - 3s 3ms/step - loss: 2.5757 - accuracy: 0.0985\n",
      "Epoch 17/25\n",
      "1200/1200 [==============================] - 3s 3ms/step - loss: 2.5697 - accuracy: 0.0994\n",
      "Epoch 18/25\n",
      "1200/1200 [==============================] - 3s 3ms/step - loss: 2.5655 - accuracy: 0.1019\n",
      "Epoch 19/25\n",
      "1200/1200 [==============================] - 3s 3ms/step - loss: 2.5937 - accuracy: 0.1001\n",
      "Epoch 20/25\n",
      "1200/1200 [==============================] - 4s 3ms/step - loss: 2.5734 - accuracy: 0.0993\n",
      "Epoch 21/25\n",
      "1200/1200 [==============================] - 3s 3ms/step - loss: 2.5760 - accuracy: 0.1009\n",
      "Epoch 22/25\n",
      "1200/1200 [==============================] - 4s 3ms/step - loss: 2.5632 - accuracy: 0.0991\n",
      "Epoch 23/25\n",
      "1200/1200 [==============================] - 4s 3ms/step - loss: 2.5743 - accuracy: 0.0997\n",
      "Epoch 24/25\n",
      "1200/1200 [==============================] - 4s 3ms/step - loss: 2.5626 - accuracy: 0.1006\n",
      "Epoch 25/25\n",
      "1200/1200 [==============================] - 3s 3ms/step - loss: 2.5818 - accuracy: 0.0992\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 2.5142 - accuracy: 0.1000\n",
      "Fitting for combination 104\n",
      "(28, 28, 1)\n",
      "4\n",
      "[32, 32, 128, 64]\n",
      "[(2, 2), (2, 2), (2, 2), (5, 5)]\n",
      "3\n",
      "[64, 16, 16, 10]\n",
      "10\n",
      "True\n",
      "['relu', 'sigmoid', 'relu', 'sigmoid']\n",
      "['tanh', 'tanh', 'tanh', 'softmax']\n",
      "Adam\n",
      "0.03\n",
      "0.01\n",
      "categorical_crossentropy\n",
      "200\n",
      "25\n",
      "0\n",
      "Epoch 1/25\n",
      "  1/300 [..............................] - ETA: 1:50 - loss: 2.6451 - accuracy: 0.0800"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-14 10:12:08.687067: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:954] layout failed: INVALID_ARGUMENT: Size of values 0 does not match size of permutation 4 @ fanin shape insequential_104/dropout_153/dropout/SelectV2-2-TransposeNHWCToNCHW-LayoutOptimizer\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300/300 [==============================] - 2s 5ms/step - loss: 2.3217 - accuracy: 0.1021\n",
      "Epoch 2/25\n",
      "300/300 [==============================] - 1s 5ms/step - loss: 2.3060 - accuracy: 0.0983\n",
      "Epoch 3/25\n",
      "300/300 [==============================] - 1s 5ms/step - loss: 2.3043 - accuracy: 0.0984\n",
      "Epoch 4/25\n",
      "300/300 [==============================] - 1s 5ms/step - loss: 2.3039 - accuracy: 0.0988\n",
      "Epoch 5/25\n",
      "300/300 [==============================] - 1s 5ms/step - loss: 2.3037 - accuracy: 0.0994\n",
      "Epoch 6/25\n",
      "300/300 [==============================] - 1s 5ms/step - loss: 2.3033 - accuracy: 0.0996\n",
      "Epoch 7/25\n",
      "300/300 [==============================] - 1s 5ms/step - loss: 2.3033 - accuracy: 0.1000\n",
      "Epoch 8/25\n",
      "300/300 [==============================] - 1s 5ms/step - loss: 2.3032 - accuracy: 0.1002\n",
      "Epoch 9/25\n",
      "300/300 [==============================] - 1s 5ms/step - loss: 2.3032 - accuracy: 0.0991\n",
      "Epoch 10/25\n",
      "300/300 [==============================] - 1s 5ms/step - loss: 2.3031 - accuracy: 0.0982\n",
      "Epoch 11/25\n",
      "300/300 [==============================] - 1s 5ms/step - loss: 2.3030 - accuracy: 0.1000\n",
      "Epoch 12/25\n",
      "300/300 [==============================] - 1s 5ms/step - loss: 2.3029 - accuracy: 0.1008\n",
      "Epoch 13/25\n",
      "300/300 [==============================] - 1s 5ms/step - loss: 2.3029 - accuracy: 0.1014\n",
      "Epoch 14/25\n",
      "300/300 [==============================] - 1s 5ms/step - loss: 2.3028 - accuracy: 0.0997\n",
      "Epoch 15/25\n",
      "300/300 [==============================] - 1s 5ms/step - loss: 2.3029 - accuracy: 0.0995\n",
      "Epoch 16/25\n",
      "300/300 [==============================] - 1s 5ms/step - loss: 2.3030 - accuracy: 0.0979\n",
      "Epoch 17/25\n",
      "300/300 [==============================] - 1s 5ms/step - loss: 2.3028 - accuracy: 0.1004\n",
      "Epoch 18/25\n",
      "300/300 [==============================] - 1s 5ms/step - loss: 2.3028 - accuracy: 0.0988\n",
      "Epoch 19/25\n",
      "300/300 [==============================] - 1s 5ms/step - loss: 2.3029 - accuracy: 0.0993\n",
      "Epoch 20/25\n",
      "300/300 [==============================] - 1s 5ms/step - loss: 2.3028 - accuracy: 0.0984\n",
      "Epoch 21/25\n",
      "300/300 [==============================] - 1s 5ms/step - loss: 2.3027 - accuracy: 0.1018\n",
      "Epoch 22/25\n",
      "300/300 [==============================] - 1s 5ms/step - loss: 2.3027 - accuracy: 0.1002\n",
      "Epoch 23/25\n",
      "300/300 [==============================] - 1s 5ms/step - loss: 2.3028 - accuracy: 0.1007\n",
      "Epoch 24/25\n",
      "300/300 [==============================] - 1s 5ms/step - loss: 2.3027 - accuracy: 0.0982\n",
      "Epoch 25/25\n",
      "300/300 [==============================] - 1s 5ms/step - loss: 2.3027 - accuracy: 0.1001\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 2.3026 - accuracy: 0.1000\n",
      "Fitting for combination 105\n",
      "(28, 28, 1)\n",
      "4\n",
      "[32, 32, 128, 64]\n",
      "[(2, 2), (2, 2), (2, 2), (5, 5)]\n",
      "2\n",
      "[128, 16, 10]\n",
      "10\n",
      "False\n",
      "['relu', 'sigmoid', 'relu', 'relu']\n",
      "['sigmoid', 'tanh', 'softmax']\n",
      "SGD\n",
      "0.1\n",
      "0\n",
      "categorical_crossentropy\n",
      "300\n",
      "25\n",
      "0\n",
      "Epoch 1/25\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 2.3068 - accuracy: 0.1031\n",
      "Epoch 2/25\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 2.3036 - accuracy: 0.1033\n",
      "Epoch 3/25\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 2.2523 - accuracy: 0.1484\n",
      "Epoch 4/25\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 1.2334 - accuracy: 0.5465\n",
      "Epoch 5/25\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 0.8712 - accuracy: 0.6686\n",
      "Epoch 6/25\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 0.7636 - accuracy: 0.7125\n",
      "Epoch 7/25\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 0.6881 - accuracy: 0.7434\n",
      "Epoch 8/25\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 0.6511 - accuracy: 0.7559\n",
      "Epoch 9/25\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 0.6120 - accuracy: 0.7711\n",
      "Epoch 10/25\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 0.5907 - accuracy: 0.7794\n",
      "Epoch 11/25\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 0.5730 - accuracy: 0.7844\n",
      "Epoch 12/25\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 0.5595 - accuracy: 0.7896\n",
      "Epoch 13/25\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 0.5492 - accuracy: 0.7930\n",
      "Epoch 14/25\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 0.5349 - accuracy: 0.7988\n",
      "Epoch 15/25\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 0.5247 - accuracy: 0.8019\n",
      "Epoch 16/25\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 0.5160 - accuracy: 0.8044\n",
      "Epoch 17/25\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 0.5026 - accuracy: 0.8103\n",
      "Epoch 18/25\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 0.4981 - accuracy: 0.8118\n",
      "Epoch 19/25\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 0.4869 - accuracy: 0.8154\n",
      "Epoch 20/25\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 0.4796 - accuracy: 0.8195\n",
      "Epoch 21/25\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 0.4724 - accuracy: 0.8222\n",
      "Epoch 22/25\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 0.4625 - accuracy: 0.8258\n",
      "Epoch 23/25\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 0.4569 - accuracy: 0.8283\n",
      "Epoch 24/25\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 0.4545 - accuracy: 0.8288\n",
      "Epoch 25/25\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 0.4440 - accuracy: 0.8329\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 0.4914 - accuracy: 0.8103\n",
      "Fitting for combination 106\n",
      "(28, 28, 1)\n",
      "4\n",
      "[32, 32, 128, 64]\n",
      "[(2, 2), (2, 2), (2, 2), (3, 3)]\n",
      "2\n",
      "[32, 32, 10]\n",
      "10\n",
      "True\n",
      "['relu', 'sigmoid', 'relu', 'sigmoid']\n",
      "['relu', 'sigmoid', 'softmax']\n",
      "SGD\n",
      "0.01\n",
      "0.01\n",
      "categorical_crossentropy\n",
      "150\n",
      "25\n",
      "0\n",
      "Epoch 1/25\n",
      " 14/400 [>.............................] - ETA: 1s - loss: 2.5563 - accuracy: 0.1057  "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-14 10:13:16.187889: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:954] layout failed: INVALID_ARGUMENT: Size of values 0 does not match size of permutation 4 @ fanin shape insequential_106/dropout_157/dropout/SelectV2-2-TransposeNHWCToNCHW-LayoutOptimizer\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "400/400 [==============================] - 2s 4ms/step - loss: 2.4594 - accuracy: 0.0997\n",
      "Epoch 2/25\n",
      "400/400 [==============================] - 2s 4ms/step - loss: 2.4295 - accuracy: 0.1023\n",
      "Epoch 3/25\n",
      "400/400 [==============================] - 2s 4ms/step - loss: 2.4318 - accuracy: 0.0981\n",
      "Epoch 4/25\n",
      "400/400 [==============================] - 2s 4ms/step - loss: 2.4287 - accuracy: 0.0982\n",
      "Epoch 5/25\n",
      "400/400 [==============================] - 2s 4ms/step - loss: 2.4236 - accuracy: 0.1001\n",
      "Epoch 6/25\n",
      "400/400 [==============================] - 2s 4ms/step - loss: 2.4245 - accuracy: 0.0991\n",
      "Epoch 7/25\n",
      "400/400 [==============================] - 2s 4ms/step - loss: 2.4149 - accuracy: 0.1021\n",
      "Epoch 8/25\n",
      "400/400 [==============================] - 2s 4ms/step - loss: 2.4175 - accuracy: 0.1011\n",
      "Epoch 9/25\n",
      "400/400 [==============================] - 2s 4ms/step - loss: 2.4171 - accuracy: 0.1009\n",
      "Epoch 10/25\n",
      "400/400 [==============================] - 2s 4ms/step - loss: 2.4154 - accuracy: 0.1006\n",
      "Epoch 11/25\n",
      "400/400 [==============================] - 2s 4ms/step - loss: 2.4149 - accuracy: 0.0998\n",
      "Epoch 12/25\n",
      "400/400 [==============================] - 2s 4ms/step - loss: 2.4144 - accuracy: 0.0992\n",
      "Epoch 13/25\n",
      "400/400 [==============================] - 2s 4ms/step - loss: 2.4118 - accuracy: 0.1023\n",
      "Epoch 14/25\n",
      "400/400 [==============================] - 2s 4ms/step - loss: 2.4128 - accuracy: 0.0981\n",
      "Epoch 15/25\n",
      "400/400 [==============================] - 2s 4ms/step - loss: 2.4123 - accuracy: 0.0990\n",
      "Epoch 16/25\n",
      "400/400 [==============================] - 2s 4ms/step - loss: 2.4098 - accuracy: 0.0997\n",
      "Epoch 17/25\n",
      "400/400 [==============================] - 2s 4ms/step - loss: 2.4100 - accuracy: 0.1008\n",
      "Epoch 18/25\n",
      "400/400 [==============================] - 2s 4ms/step - loss: 2.4117 - accuracy: 0.1010\n",
      "Epoch 19/25\n",
      "400/400 [==============================] - 2s 4ms/step - loss: 2.4079 - accuracy: 0.1011\n",
      "Epoch 20/25\n",
      "400/400 [==============================] - 2s 4ms/step - loss: 2.4057 - accuracy: 0.1014\n",
      "Epoch 21/25\n",
      "400/400 [==============================] - 2s 4ms/step - loss: 2.4108 - accuracy: 0.0991\n",
      "Epoch 22/25\n",
      "400/400 [==============================] - 2s 4ms/step - loss: 2.4112 - accuracy: 0.0988\n",
      "Epoch 23/25\n",
      "400/400 [==============================] - 2s 4ms/step - loss: 2.4078 - accuracy: 0.0999\n",
      "Epoch 24/25\n",
      "400/400 [==============================] - 2s 4ms/step - loss: 2.4068 - accuracy: 0.1013\n",
      "Epoch 25/25\n",
      "400/400 [==============================] - 2s 4ms/step - loss: 2.4085 - accuracy: 0.0993\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 2.3028 - accuracy: 0.1000\n",
      "Fitting for combination 107\n",
      "(28, 28, 1)\n",
      "4\n",
      "[32, 32, 128, 64]\n",
      "[(2, 2), (2, 2), (2, 2), (5, 5)]\n",
      "2\n",
      "[64, 64, 10]\n",
      "10\n",
      "False\n",
      "['relu', 'sigmoid', 'relu', 'tanh']\n",
      "['relu', 'relu', 'softmax']\n",
      "SGD\n",
      "0.1\n",
      "0.01\n",
      "categorical_crossentropy\n",
      "150\n",
      "25\n",
      "0\n",
      "Epoch 1/25\n",
      "400/400 [==============================] - 2s 4ms/step - loss: 2.3019 - accuracy: 0.1101\n",
      "Epoch 2/25\n",
      "400/400 [==============================] - 2s 4ms/step - loss: 2.2911 - accuracy: 0.1716\n",
      "Epoch 3/25\n",
      "400/400 [==============================] - 2s 4ms/step - loss: 2.1237 - accuracy: 0.2869\n",
      "Epoch 4/25\n",
      "400/400 [==============================] - 2s 4ms/step - loss: 1.6046 - accuracy: 0.4318\n",
      "Epoch 5/25\n",
      "400/400 [==============================] - 2s 4ms/step - loss: 1.2396 - accuracy: 0.5479\n",
      "Epoch 6/25\n",
      "400/400 [==============================] - 2s 4ms/step - loss: 1.0469 - accuracy: 0.6177\n",
      "Epoch 7/25\n",
      "400/400 [==============================] - 2s 4ms/step - loss: 0.9593 - accuracy: 0.6477\n",
      "Epoch 8/25\n",
      "400/400 [==============================] - 2s 4ms/step - loss: 0.9013 - accuracy: 0.6668\n",
      "Epoch 9/25\n",
      "400/400 [==============================] - 2s 4ms/step - loss: 0.8567 - accuracy: 0.6836\n",
      "Epoch 10/25\n",
      "400/400 [==============================] - 2s 4ms/step - loss: 0.8224 - accuracy: 0.6953\n",
      "Epoch 11/25\n",
      "400/400 [==============================] - 2s 4ms/step - loss: 0.7959 - accuracy: 0.7030\n",
      "Epoch 12/25\n",
      "400/400 [==============================] - 2s 4ms/step - loss: 0.7750 - accuracy: 0.7097\n",
      "Epoch 13/25\n",
      "400/400 [==============================] - 2s 4ms/step - loss: 0.7587 - accuracy: 0.7146\n",
      "Epoch 14/25\n",
      "400/400 [==============================] - 2s 4ms/step - loss: 0.7450 - accuracy: 0.7203\n",
      "Epoch 15/25\n",
      "400/400 [==============================] - 2s 4ms/step - loss: 0.7337 - accuracy: 0.7239\n",
      "Epoch 16/25\n",
      "400/400 [==============================] - 2s 4ms/step - loss: 0.7240 - accuracy: 0.7266\n",
      "Epoch 17/25\n",
      "400/400 [==============================] - 2s 4ms/step - loss: 0.7154 - accuracy: 0.7295\n",
      "Epoch 18/25\n",
      "400/400 [==============================] - 2s 4ms/step - loss: 0.7079 - accuracy: 0.7331\n",
      "Epoch 19/25\n",
      "400/400 [==============================] - 2s 4ms/step - loss: 0.7011 - accuracy: 0.7355\n",
      "Epoch 20/25\n",
      "400/400 [==============================] - 2s 4ms/step - loss: 0.6950 - accuracy: 0.7371\n",
      "Epoch 21/25\n",
      "400/400 [==============================] - 2s 4ms/step - loss: 0.6891 - accuracy: 0.7395\n",
      "Epoch 22/25\n",
      "400/400 [==============================] - 2s 4ms/step - loss: 0.6839 - accuracy: 0.7419\n",
      "Epoch 23/25\n",
      "400/400 [==============================] - 2s 4ms/step - loss: 0.6793 - accuracy: 0.7438\n",
      "Epoch 24/25\n",
      "400/400 [==============================] - 2s 4ms/step - loss: 0.6747 - accuracy: 0.7461\n",
      "Epoch 25/25\n",
      "400/400 [==============================] - 2s 4ms/step - loss: 0.6705 - accuracy: 0.7468\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 0.6905 - accuracy: 0.7435\n",
      "Fitting for combination 108\n",
      "(28, 28, 1)\n",
      "4\n",
      "[32, 32, 128, 64]\n",
      "[(2, 2), (2, 2), (2, 2), (5, 5)]\n",
      "2\n",
      "[64, 128, 10]\n",
      "10\n",
      "True\n",
      "['relu', 'sigmoid', 'relu', 'sigmoid']\n",
      "['tanh', 'relu', 'softmax']\n",
      "SGD\n",
      "0.03\n",
      "0.01\n",
      "categorical_crossentropy\n",
      "300\n",
      "25\n",
      "0\n",
      "Epoch 1/25\n",
      "  1/200 [..............................] - ETA: 1:09 - loss: 2.4534 - accuracy: 0.1200"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-14 10:14:38.756887: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:954] layout failed: INVALID_ARGUMENT: Size of values 0 does not match size of permutation 4 @ fanin shape insequential_108/dropout_160/dropout/SelectV2-2-TransposeNHWCToNCHW-LayoutOptimizer\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200/200 [==============================] - 1s 6ms/step - loss: 2.3485 - accuracy: 0.1000\n",
      "Epoch 2/25\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 2.3151 - accuracy: 0.1014\n",
      "Epoch 3/25\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 2.3120 - accuracy: 0.1010\n",
      "Epoch 4/25\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 2.3096 - accuracy: 0.1008\n",
      "Epoch 5/25\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 2.3076 - accuracy: 0.1027\n",
      "Epoch 6/25\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 2.3081 - accuracy: 0.0988\n",
      "Epoch 7/25\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 2.3073 - accuracy: 0.0980\n",
      "Epoch 8/25\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 2.3066 - accuracy: 0.1003\n",
      "Epoch 9/25\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 2.3072 - accuracy: 0.1006\n",
      "Epoch 10/25\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 2.3065 - accuracy: 0.0994\n",
      "Epoch 11/25\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 2.3058 - accuracy: 0.1002\n",
      "Epoch 12/25\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 2.3050 - accuracy: 0.1037\n",
      "Epoch 13/25\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 2.3059 - accuracy: 0.0989\n",
      "Epoch 14/25\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 2.3052 - accuracy: 0.0988\n",
      "Epoch 15/25\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 2.3055 - accuracy: 0.0996\n",
      "Epoch 16/25\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 2.3048 - accuracy: 0.1006\n",
      "Epoch 17/25\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 2.3050 - accuracy: 0.1020\n",
      "Epoch 18/25\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 2.3050 - accuracy: 0.1003\n",
      "Epoch 19/25\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 2.3049 - accuracy: 0.1001\n",
      "Epoch 20/25\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 2.3053 - accuracy: 0.0991\n",
      "Epoch 21/25\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 2.3045 - accuracy: 0.1006\n",
      "Epoch 22/25\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 2.3049 - accuracy: 0.0990\n",
      "Epoch 23/25\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 2.3042 - accuracy: 0.0999\n",
      "Epoch 24/25\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 2.3046 - accuracy: 0.1020\n",
      "Epoch 25/25\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 2.3043 - accuracy: 0.1010\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 2.3024 - accuracy: 0.1428\n",
      "Fitting for combination 109\n",
      "(28, 28, 1)\n",
      "4\n",
      "[32, 32, 128, 64]\n",
      "[(2, 2), (2, 2), (2, 2), (2, 2)]\n",
      "2\n",
      "[16, 64, 10]\n",
      "10\n",
      "True\n",
      "['relu', 'sigmoid', 'relu', 'sigmoid']\n",
      "['tanh', 'relu', 'softmax']\n",
      "SGD\n",
      "0.03\n",
      "0.03\n",
      "categorical_crossentropy\n",
      "200\n",
      "25\n",
      "0\n",
      "Epoch 1/25\n",
      " 13/300 [>.............................] - ETA: 1s - loss: 2.3562 - accuracy: 0.0892  "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-14 10:15:08.532245: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:954] layout failed: INVALID_ARGUMENT: Size of values 0 does not match size of permutation 4 @ fanin shape insequential_109/dropout_163/dropout/SelectV2-2-TransposeNHWCToNCHW-LayoutOptimizer\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300/300 [==============================] - 2s 4ms/step - loss: 2.3091 - accuracy: 0.0985\n",
      "Epoch 2/25\n",
      "300/300 [==============================] - 1s 4ms/step - loss: 2.3036 - accuracy: 0.0997\n",
      "Epoch 3/25\n",
      "300/300 [==============================] - 1s 4ms/step - loss: 2.3032 - accuracy: 0.1005\n",
      "Epoch 4/25\n",
      "300/300 [==============================] - 1s 4ms/step - loss: 2.3029 - accuracy: 0.1009\n",
      "Epoch 5/25\n",
      "300/300 [==============================] - 1s 4ms/step - loss: 2.3026 - accuracy: 0.1030\n",
      "Epoch 6/25\n",
      "300/300 [==============================] - 1s 4ms/step - loss: 2.3028 - accuracy: 0.1014\n",
      "Epoch 7/25\n",
      "300/300 [==============================] - 1s 4ms/step - loss: 2.3026 - accuracy: 0.1044\n",
      "Epoch 8/25\n",
      "300/300 [==============================] - 1s 4ms/step - loss: 2.3025 - accuracy: 0.1038\n",
      "Epoch 9/25\n",
      "300/300 [==============================] - 1s 4ms/step - loss: 2.3025 - accuracy: 0.1031\n",
      "Epoch 10/25\n",
      "300/300 [==============================] - 1s 4ms/step - loss: 2.3025 - accuracy: 0.1037\n",
      "Epoch 11/25\n",
      "300/300 [==============================] - 1s 4ms/step - loss: 2.3024 - accuracy: 0.1062\n",
      "Epoch 12/25\n",
      "300/300 [==============================] - 1s 4ms/step - loss: 2.3023 - accuracy: 0.1047\n",
      "Epoch 13/25\n",
      "300/300 [==============================] - 1s 4ms/step - loss: 2.3025 - accuracy: 0.1037\n",
      "Epoch 14/25\n",
      "300/300 [==============================] - 1s 4ms/step - loss: 2.3024 - accuracy: 0.1030\n",
      "Epoch 15/25\n",
      "300/300 [==============================] - 1s 4ms/step - loss: 2.3024 - accuracy: 0.1059\n",
      "Epoch 16/25\n",
      "300/300 [==============================] - 1s 4ms/step - loss: 2.3023 - accuracy: 0.1038\n",
      "Epoch 17/25\n",
      "300/300 [==============================] - 1s 4ms/step - loss: 2.3023 - accuracy: 0.1033\n",
      "Epoch 18/25\n",
      "300/300 [==============================] - 1s 4ms/step - loss: 2.3023 - accuracy: 0.1049\n",
      "Epoch 19/25\n",
      "300/300 [==============================] - 1s 4ms/step - loss: 2.3023 - accuracy: 0.1038\n",
      "Epoch 20/25\n",
      "300/300 [==============================] - 1s 4ms/step - loss: 2.3023 - accuracy: 0.1058\n",
      "Epoch 21/25\n",
      "300/300 [==============================] - 1s 4ms/step - loss: 2.3022 - accuracy: 0.1071\n",
      "Epoch 22/25\n",
      "300/300 [==============================] - 1s 4ms/step - loss: 2.3022 - accuracy: 0.1045\n",
      "Epoch 23/25\n",
      "300/300 [==============================] - 1s 4ms/step - loss: 2.3023 - accuracy: 0.1059\n",
      "Epoch 24/25\n",
      "300/300 [==============================] - 1s 4ms/step - loss: 2.3022 - accuracy: 0.1067\n",
      "Epoch 25/25\n",
      "300/300 [==============================] - 1s 4ms/step - loss: 2.3022 - accuracy: 0.1063\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 2.3017 - accuracy: 0.1923\n",
      "Fitting for combination 110\n",
      "(28, 28, 1)\n",
      "4\n",
      "[32, 32, 128, 128]\n",
      "[(2, 2), (2, 2), (2, 2), (3, 3)]\n",
      "2\n",
      "[16, 64, 10]\n",
      "10\n",
      "True\n",
      "['relu', 'sigmoid', 'relu', 'relu']\n",
      "['sigmoid', 'sigmoid', 'softmax']\n",
      "Adam\n",
      "0.03\n",
      "0\n",
      "categorical_crossentropy\n",
      "150\n",
      "25\n",
      "0\n",
      "Epoch 1/25\n",
      " 13/400 [..............................] - ETA: 1s - loss: 2.4637 - accuracy: 0.1072  "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-14 10:15:41.926295: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:954] layout failed: INVALID_ARGUMENT: Size of values 0 does not match size of permutation 4 @ fanin shape insequential_110/dropout_166/dropout/SelectV2-2-TransposeNHWCToNCHW-LayoutOptimizer\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "400/400 [==============================] - 2s 4ms/step - loss: 2.3139 - accuracy: 0.1004\n",
      "Epoch 2/25\n",
      "400/400 [==============================] - 2s 4ms/step - loss: 2.3059 - accuracy: 0.0994\n",
      "Epoch 3/25\n",
      "400/400 [==============================] - 2s 4ms/step - loss: 2.3049 - accuracy: 0.1008\n",
      "Epoch 4/25\n",
      "400/400 [==============================] - 2s 4ms/step - loss: 2.3050 - accuracy: 0.1002\n",
      "Epoch 5/25\n",
      "400/400 [==============================] - 2s 4ms/step - loss: 2.3049 - accuracy: 0.0998\n",
      "Epoch 6/25\n",
      "400/400 [==============================] - 2s 4ms/step - loss: 2.3048 - accuracy: 0.0991\n",
      "Epoch 7/25\n",
      "400/400 [==============================] - 2s 4ms/step - loss: 2.3046 - accuracy: 0.0997\n",
      "Epoch 8/25\n",
      "400/400 [==============================] - 2s 4ms/step - loss: 2.3051 - accuracy: 0.0981\n",
      "Epoch 9/25\n",
      "400/400 [==============================] - 2s 4ms/step - loss: 2.3045 - accuracy: 0.0987\n",
      "Epoch 10/25\n",
      "400/400 [==============================] - 2s 4ms/step - loss: 2.3047 - accuracy: 0.1018\n",
      "Epoch 11/25\n",
      "400/400 [==============================] - 2s 4ms/step - loss: 2.3048 - accuracy: 0.1000\n",
      "Epoch 12/25\n",
      "400/400 [==============================] - 2s 4ms/step - loss: 2.3045 - accuracy: 0.1006\n",
      "Epoch 13/25\n",
      "400/400 [==============================] - 2s 4ms/step - loss: 2.3045 - accuracy: 0.0992\n",
      "Epoch 14/25\n",
      "400/400 [==============================] - 2s 4ms/step - loss: 2.3046 - accuracy: 0.1013\n",
      "Epoch 15/25\n",
      "400/400 [==============================] - 2s 4ms/step - loss: 2.3045 - accuracy: 0.0993\n",
      "Epoch 16/25\n",
      "400/400 [==============================] - 2s 4ms/step - loss: 2.3044 - accuracy: 0.0997\n",
      "Epoch 17/25\n",
      "400/400 [==============================] - 2s 4ms/step - loss: 2.3045 - accuracy: 0.1013\n",
      "Epoch 18/25\n",
      "400/400 [==============================] - 2s 4ms/step - loss: 2.3048 - accuracy: 0.0987\n",
      "Epoch 19/25\n",
      "400/400 [==============================] - 2s 4ms/step - loss: 2.3046 - accuracy: 0.0981\n",
      "Epoch 20/25\n",
      "400/400 [==============================] - 2s 4ms/step - loss: 2.3045 - accuracy: 0.0988\n",
      "Epoch 21/25\n",
      "400/400 [==============================] - 2s 4ms/step - loss: 2.3045 - accuracy: 0.0999\n",
      "Epoch 22/25\n",
      "400/400 [==============================] - 2s 4ms/step - loss: 2.3048 - accuracy: 0.0986\n",
      "Epoch 23/25\n",
      "400/400 [==============================] - 2s 4ms/step - loss: 2.3044 - accuracy: 0.0982\n",
      "Epoch 24/25\n",
      "400/400 [==============================] - 2s 4ms/step - loss: 2.3046 - accuracy: 0.0990\n",
      "Epoch 25/25\n",
      "400/400 [==============================] - 2s 4ms/step - loss: 2.3045 - accuracy: 0.1017\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 2.3041 - accuracy: 0.1000\n",
      "Fitting for combination 111\n",
      "(28, 28, 1)\n",
      "4\n",
      "[32, 32, 128, 128]\n",
      "[(2, 2), (2, 2), (2, 2), (3, 3)]\n",
      "3\n",
      "[128, 32, 64, 10]\n",
      "10\n",
      "True\n",
      "['relu', 'sigmoid', 'relu', 'relu']\n",
      "['tanh', 'tanh', 'sigmoid', 'softmax']\n",
      "Adam\n",
      "0.03\n",
      "0.03\n",
      "categorical_crossentropy\n",
      "200\n",
      "25\n",
      "0\n",
      "Epoch 1/25\n",
      " 11/300 [>.............................] - ETA: 1s - loss: 2.5090 - accuracy: 0.1018  "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-14 10:16:26.818205: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:954] layout failed: INVALID_ARGUMENT: Size of values 0 does not match size of permutation 4 @ fanin shape insequential_111/dropout_169/dropout/SelectV2-2-TransposeNHWCToNCHW-LayoutOptimizer\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300/300 [==============================] - 2s 5ms/step - loss: 2.3120 - accuracy: 0.0975\n",
      "Epoch 2/25\n",
      "300/300 [==============================] - 2s 5ms/step - loss: 2.3029 - accuracy: 0.0998\n",
      "Epoch 3/25\n",
      "300/300 [==============================] - 2s 5ms/step - loss: 2.3028 - accuracy: 0.1003\n",
      "Epoch 4/25\n",
      "300/300 [==============================] - 2s 5ms/step - loss: 2.3029 - accuracy: 0.0971\n",
      "Epoch 5/25\n",
      "300/300 [==============================] - 1s 5ms/step - loss: 2.3029 - accuracy: 0.0984\n",
      "Epoch 6/25\n",
      "300/300 [==============================] - 1s 5ms/step - loss: 2.3028 - accuracy: 0.0986\n",
      "Epoch 7/25\n",
      "300/300 [==============================] - 1s 5ms/step - loss: 2.3027 - accuracy: 0.0993\n",
      "Epoch 8/25\n",
      "300/300 [==============================] - 2s 5ms/step - loss: 2.3027 - accuracy: 0.0997\n",
      "Epoch 9/25\n",
      "300/300 [==============================] - 2s 5ms/step - loss: 2.3027 - accuracy: 0.1018\n",
      "Epoch 10/25\n",
      "300/300 [==============================] - 2s 5ms/step - loss: 2.3027 - accuracy: 0.0986\n",
      "Epoch 11/25\n",
      "300/300 [==============================] - 2s 5ms/step - loss: 2.3027 - accuracy: 0.0978\n",
      "Epoch 12/25\n",
      "300/300 [==============================] - 2s 5ms/step - loss: 2.3027 - accuracy: 0.0980\n",
      "Epoch 13/25\n",
      "300/300 [==============================] - 1s 5ms/step - loss: 2.3028 - accuracy: 0.0967\n",
      "Epoch 14/25\n",
      "300/300 [==============================] - 2s 5ms/step - loss: 2.3026 - accuracy: 0.1007\n",
      "Epoch 15/25\n",
      "300/300 [==============================] - 2s 5ms/step - loss: 2.3028 - accuracy: 0.0994\n",
      "Epoch 16/25\n",
      "300/300 [==============================] - 1s 5ms/step - loss: 2.3027 - accuracy: 0.0979\n",
      "Epoch 17/25\n",
      "300/300 [==============================] - 1s 5ms/step - loss: 2.3026 - accuracy: 0.0997\n",
      "Epoch 18/25\n",
      "300/300 [==============================] - 1s 5ms/step - loss: 2.3027 - accuracy: 0.0979\n",
      "Epoch 19/25\n",
      "300/300 [==============================] - 2s 5ms/step - loss: 2.3027 - accuracy: 0.0990\n",
      "Epoch 20/25\n",
      "300/300 [==============================] - 2s 5ms/step - loss: 2.3026 - accuracy: 0.0993\n",
      "Epoch 21/25\n",
      "300/300 [==============================] - 2s 5ms/step - loss: 2.3026 - accuracy: 0.0982\n",
      "Epoch 22/25\n",
      "300/300 [==============================] - 2s 5ms/step - loss: 2.3026 - accuracy: 0.0987\n",
      "Epoch 23/25\n",
      "300/300 [==============================] - 2s 5ms/step - loss: 2.3027 - accuracy: 0.1014\n",
      "Epoch 24/25\n",
      "300/300 [==============================] - 2s 5ms/step - loss: 2.3027 - accuracy: 0.0983\n",
      "Epoch 25/25\n",
      "300/300 [==============================] - 1s 5ms/step - loss: 2.3026 - accuracy: 0.0978\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 2.3026 - accuracy: 0.1000\n",
      "Fitting for combination 112\n",
      "(28, 28, 1)\n",
      "4\n",
      "[32, 32, 128, 128]\n",
      "[(2, 2), (2, 2), (2, 2), (5, 5)]\n",
      "1\n",
      "[128, 10]\n",
      "10\n",
      "False\n",
      "['relu', 'sigmoid', 'relu', 'tanh']\n",
      "['sigmoid', 'softmax']\n",
      "Adam\n",
      "0.03\n",
      "0.01\n",
      "categorical_crossentropy\n",
      "200\n",
      "25\n",
      "0\n",
      "Epoch 1/25\n",
      "300/300 [==============================] - 2s 5ms/step - loss: 2.3299 - accuracy: 0.1017\n",
      "Epoch 2/25\n",
      "300/300 [==============================] - 1s 5ms/step - loss: 2.3088 - accuracy: 0.0997\n",
      "Epoch 3/25\n",
      "300/300 [==============================] - 1s 5ms/step - loss: 2.3069 - accuracy: 0.0994\n",
      "Epoch 4/25\n",
      "300/300 [==============================] - 1s 5ms/step - loss: 2.3059 - accuracy: 0.1001\n",
      "Epoch 5/25\n",
      "300/300 [==============================] - 1s 5ms/step - loss: 2.3057 - accuracy: 0.1003\n",
      "Epoch 6/25\n",
      "300/300 [==============================] - 1s 5ms/step - loss: 2.3052 - accuracy: 0.0988\n",
      "Epoch 7/25\n",
      "300/300 [==============================] - 1s 5ms/step - loss: 2.3047 - accuracy: 0.0988\n",
      "Epoch 8/25\n",
      "300/300 [==============================] - 1s 5ms/step - loss: 2.3048 - accuracy: 0.0983\n",
      "Epoch 9/25\n",
      "300/300 [==============================] - 2s 5ms/step - loss: 2.3043 - accuracy: 0.0985\n",
      "Epoch 10/25\n",
      "300/300 [==============================] - 1s 5ms/step - loss: 2.3042 - accuracy: 0.1018\n",
      "Epoch 11/25\n",
      "300/300 [==============================] - 1s 5ms/step - loss: 2.3041 - accuracy: 0.0985\n",
      "Epoch 12/25\n",
      "300/300 [==============================] - 1s 5ms/step - loss: 2.3038 - accuracy: 0.1018\n",
      "Epoch 13/25\n",
      "300/300 [==============================] - 1s 5ms/step - loss: 2.3038 - accuracy: 0.0987\n",
      "Epoch 14/25\n",
      "300/300 [==============================] - 1s 5ms/step - loss: 2.3037 - accuracy: 0.1022\n",
      "Epoch 15/25\n",
      "300/300 [==============================] - 2s 5ms/step - loss: 2.3036 - accuracy: 0.0990\n",
      "Epoch 16/25\n",
      "300/300 [==============================] - 1s 5ms/step - loss: 2.3036 - accuracy: 0.1006\n",
      "Epoch 17/25\n",
      "300/300 [==============================] - 1s 5ms/step - loss: 2.3036 - accuracy: 0.1000\n",
      "Epoch 18/25\n",
      "300/300 [==============================] - 1s 5ms/step - loss: 2.3037 - accuracy: 0.0995\n",
      "Epoch 19/25\n",
      "300/300 [==============================] - 1s 5ms/step - loss: 2.3035 - accuracy: 0.0990\n",
      "Epoch 20/25\n",
      "300/300 [==============================] - 1s 5ms/step - loss: 2.3034 - accuracy: 0.0992\n",
      "Epoch 21/25\n",
      "300/300 [==============================] - 1s 5ms/step - loss: 2.3034 - accuracy: 0.0981\n",
      "Epoch 22/25\n",
      "300/300 [==============================] - 1s 5ms/step - loss: 2.3033 - accuracy: 0.0993\n",
      "Epoch 23/25\n",
      "300/300 [==============================] - 1s 5ms/step - loss: 2.3032 - accuracy: 0.0997\n",
      "Epoch 24/25\n",
      "300/300 [==============================] - 1s 5ms/step - loss: 2.3034 - accuracy: 0.0987\n",
      "Epoch 25/25\n",
      "300/300 [==============================] - 1s 5ms/step - loss: 2.3032 - accuracy: 0.0990\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 2.3028 - accuracy: 0.1000\n",
      "Fitting for combination 113\n",
      "(28, 28, 1)\n",
      "4\n",
      "[32, 32, 128, 128]\n",
      "[(2, 2), (2, 2), (2, 2), (5, 5)]\n",
      "1\n",
      "[16, 10]\n",
      "10\n",
      "False\n",
      "['relu', 'sigmoid', 'relu', 'sigmoid']\n",
      "['tanh', 'softmax']\n",
      "Adam\n",
      "0.1\n",
      "0\n",
      "categorical_crossentropy\n",
      "150\n",
      "25\n",
      "0\n",
      "Epoch 1/25\n",
      "400/400 [==============================] - 2s 4ms/step - loss: 2.3776 - accuracy: 0.0996\n",
      "Epoch 2/25\n",
      "400/400 [==============================] - 2s 4ms/step - loss: 2.3792 - accuracy: 0.0975\n",
      "Epoch 3/25\n",
      "400/400 [==============================] - 2s 4ms/step - loss: 2.3645 - accuracy: 0.0996\n",
      "Epoch 4/25\n",
      "400/400 [==============================] - 2s 4ms/step - loss: 2.3715 - accuracy: 0.1007\n",
      "Epoch 5/25\n",
      "400/400 [==============================] - 2s 4ms/step - loss: 2.3692 - accuracy: 0.1005\n",
      "Epoch 6/25\n",
      "400/400 [==============================] - 2s 4ms/step - loss: 2.3628 - accuracy: 0.1008\n",
      "Epoch 7/25\n",
      "400/400 [==============================] - 2s 4ms/step - loss: 2.3681 - accuracy: 0.1010\n",
      "Epoch 8/25\n",
      "400/400 [==============================] - 2s 4ms/step - loss: 2.3758 - accuracy: 0.0998\n",
      "Epoch 9/25\n",
      "400/400 [==============================] - 2s 4ms/step - loss: 2.3836 - accuracy: 0.1007\n",
      "Epoch 10/25\n",
      "400/400 [==============================] - 2s 4ms/step - loss: 2.3760 - accuracy: 0.1000\n",
      "Epoch 11/25\n",
      "400/400 [==============================] - 2s 4ms/step - loss: 2.3762 - accuracy: 0.1016\n",
      "Epoch 12/25\n",
      "400/400 [==============================] - 2s 4ms/step - loss: 2.3688 - accuracy: 0.0987\n",
      "Epoch 13/25\n",
      "400/400 [==============================] - 2s 4ms/step - loss: 2.3684 - accuracy: 0.1002\n",
      "Epoch 14/25\n",
      "400/400 [==============================] - 2s 4ms/step - loss: 2.3667 - accuracy: 0.1008\n",
      "Epoch 15/25\n",
      "400/400 [==============================] - 2s 4ms/step - loss: 2.3595 - accuracy: 0.1000\n",
      "Epoch 16/25\n",
      "400/400 [==============================] - 2s 4ms/step - loss: 2.3719 - accuracy: 0.1005\n",
      "Epoch 17/25\n",
      "400/400 [==============================] - 2s 4ms/step - loss: 2.3652 - accuracy: 0.0995\n",
      "Epoch 18/25\n",
      "400/400 [==============================] - 2s 4ms/step - loss: 2.3694 - accuracy: 0.0998\n",
      "Epoch 19/25\n",
      "400/400 [==============================] - 2s 4ms/step - loss: 2.3732 - accuracy: 0.1009\n",
      "Epoch 20/25\n",
      "400/400 [==============================] - 2s 4ms/step - loss: 2.3728 - accuracy: 0.1006\n",
      "Epoch 21/25\n",
      "400/400 [==============================] - 2s 4ms/step - loss: 2.3712 - accuracy: 0.0997\n",
      "Epoch 22/25\n",
      "400/400 [==============================] - 2s 4ms/step - loss: 2.3671 - accuracy: 0.0986\n",
      "Epoch 23/25\n",
      "400/400 [==============================] - 2s 4ms/step - loss: 2.3721 - accuracy: 0.0998\n",
      "Epoch 24/25\n",
      "400/400 [==============================] - 2s 4ms/step - loss: 2.3699 - accuracy: 0.1014\n",
      "Epoch 25/25\n",
      "400/400 [==============================] - 2s 4ms/step - loss: 2.3704 - accuracy: 0.0996\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 2.3279 - accuracy: 0.1000\n",
      "Fitting for combination 114\n",
      "(28, 28, 1)\n",
      "4\n",
      "[32, 32, 128, 128]\n",
      "[(2, 2), (2, 2), (2, 2), (2, 2)]\n",
      "1\n",
      "[64, 10]\n",
      "10\n",
      "True\n",
      "['relu', 'sigmoid', 'relu', 'tanh']\n",
      "['relu', 'softmax']\n",
      "Adam\n",
      "0.03\n",
      "0.01\n",
      "categorical_crossentropy\n",
      "100\n",
      "25\n",
      "0\n",
      "Epoch 1/25\n",
      "  1/600 [..............................] - ETA: 3:14 - loss: 2.3668 - accuracy: 0.1100"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-14 10:18:29.138994: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:954] layout failed: INVALID_ARGUMENT: Size of values 0 does not match size of permutation 4 @ fanin shape insequential_114/dropout_173/dropout/SelectV2-2-TransposeNHWCToNCHW-LayoutOptimizer\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "600/600 [==============================] - 2s 4ms/step - loss: 2.7523 - accuracy: 0.0995\n",
      "Epoch 2/25\n",
      "600/600 [==============================] - 2s 4ms/step - loss: 2.3029 - accuracy: 0.0991\n",
      "Epoch 3/25\n",
      "600/600 [==============================] - 2s 4ms/step - loss: 2.3027 - accuracy: 0.0999\n",
      "Epoch 4/25\n",
      "600/600 [==============================] - 2s 4ms/step - loss: 2.3028 - accuracy: 0.0984\n",
      "Epoch 5/25\n",
      "600/600 [==============================] - 2s 4ms/step - loss: 2.3027 - accuracy: 0.0983\n",
      "Epoch 6/25\n",
      "600/600 [==============================] - 2s 4ms/step - loss: 2.3027 - accuracy: 0.0971\n",
      "Epoch 7/25\n",
      "600/600 [==============================] - 2s 4ms/step - loss: 2.3027 - accuracy: 0.0977\n",
      "Epoch 8/25\n",
      "600/600 [==============================] - 2s 4ms/step - loss: 2.3027 - accuracy: 0.0988\n",
      "Epoch 9/25\n",
      "600/600 [==============================] - 2s 4ms/step - loss: 2.3027 - accuracy: 0.0985\n",
      "Epoch 10/25\n",
      "600/600 [==============================] - 2s 4ms/step - loss: 2.3027 - accuracy: 0.0981\n",
      "Epoch 11/25\n",
      "600/600 [==============================] - 2s 4ms/step - loss: 2.3026 - accuracy: 0.0981\n",
      "Epoch 12/25\n",
      "600/600 [==============================] - 2s 4ms/step - loss: 2.3026 - accuracy: 0.0997\n",
      "Epoch 13/25\n",
      "600/600 [==============================] - 2s 4ms/step - loss: 2.3026 - accuracy: 0.0960\n",
      "Epoch 14/25\n",
      "600/600 [==============================] - 2s 4ms/step - loss: 2.3026 - accuracy: 0.0984\n",
      "Epoch 15/25\n",
      "600/600 [==============================] - 2s 4ms/step - loss: 2.3026 - accuracy: 0.0985\n",
      "Epoch 16/25\n",
      "600/600 [==============================] - 2s 4ms/step - loss: 2.3026 - accuracy: 0.0977\n",
      "Epoch 17/25\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 2.3026 - accuracy: 0.0984\n",
      "Epoch 18/25\n",
      "600/600 [==============================] - 2s 4ms/step - loss: 2.3026 - accuracy: 0.0977\n",
      "Epoch 19/25\n",
      "600/600 [==============================] - 2s 4ms/step - loss: 2.3026 - accuracy: 0.0979\n",
      "Epoch 20/25\n",
      "600/600 [==============================] - 2s 4ms/step - loss: 2.3026 - accuracy: 0.0973\n",
      "Epoch 21/25\n",
      "600/600 [==============================] - 2s 4ms/step - loss: 2.3026 - accuracy: 0.0977\n",
      "Epoch 22/25\n",
      "600/600 [==============================] - 2s 4ms/step - loss: 2.3026 - accuracy: 0.0972\n",
      "Epoch 23/25\n",
      "600/600 [==============================] - 2s 4ms/step - loss: 2.3026 - accuracy: 0.0982\n",
      "Epoch 24/25\n",
      "600/600 [==============================] - 2s 4ms/step - loss: 2.3026 - accuracy: 0.0970\n",
      "Epoch 25/25\n",
      "600/600 [==============================] - 2s 4ms/step - loss: 2.3026 - accuracy: 0.0989\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 2.3026 - accuracy: 0.1000\n",
      "Fitting for combination 115\n",
      "(28, 28, 1)\n",
      "4\n",
      "[32, 32, 128, 128]\n",
      "[(2, 2), (2, 2), (2, 2), (5, 5)]\n",
      "1\n",
      "[16, 10]\n",
      "10\n",
      "True\n",
      "['relu', 'sigmoid', 'relu', 'sigmoid']\n",
      "['tanh', 'softmax']\n",
      "SGD\n",
      "0.03\n",
      "0.01\n",
      "categorical_crossentropy\n",
      "150\n",
      "25\n",
      "0\n",
      "Epoch 1/25\n",
      " 13/400 [..............................] - ETA: 1s - loss: 2.4343 - accuracy: 0.0918  "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-14 10:19:23.912307: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:954] layout failed: INVALID_ARGUMENT: Size of values 0 does not match size of permutation 4 @ fanin shape insequential_115/dropout_175/dropout/SelectV2-2-TransposeNHWCToNCHW-LayoutOptimizer\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "400/400 [==============================] - 2s 4ms/step - loss: 2.3072 - accuracy: 0.1052\n",
      "Epoch 2/25\n",
      "400/400 [==============================] - 2s 4ms/step - loss: 2.2987 - accuracy: 0.1195\n",
      "Epoch 3/25\n",
      "400/400 [==============================] - 2s 4ms/step - loss: 2.2962 - accuracy: 0.1324\n",
      "Epoch 4/25\n",
      "400/400 [==============================] - 2s 4ms/step - loss: 2.2945 - accuracy: 0.1360\n",
      "Epoch 5/25\n",
      "400/400 [==============================] - 2s 4ms/step - loss: 2.2926 - accuracy: 0.1476\n",
      "Epoch 6/25\n",
      "400/400 [==============================] - 2s 4ms/step - loss: 2.2909 - accuracy: 0.1639\n",
      "Epoch 7/25\n",
      "400/400 [==============================] - 2s 4ms/step - loss: 2.2892 - accuracy: 0.1763\n",
      "Epoch 8/25\n",
      "400/400 [==============================] - 2s 4ms/step - loss: 2.2875 - accuracy: 0.1814\n",
      "Epoch 9/25\n",
      "400/400 [==============================] - 2s 4ms/step - loss: 2.2855 - accuracy: 0.1821\n",
      "Epoch 10/25\n",
      "400/400 [==============================] - 2s 4ms/step - loss: 2.2839 - accuracy: 0.1877\n",
      "Epoch 11/25\n",
      "400/400 [==============================] - 2s 4ms/step - loss: 2.2822 - accuracy: 0.1938\n",
      "Epoch 12/25\n",
      "400/400 [==============================] - 2s 4ms/step - loss: 2.2807 - accuracy: 0.1998\n",
      "Epoch 13/25\n",
      "400/400 [==============================] - 2s 4ms/step - loss: 2.2786 - accuracy: 0.2081\n",
      "Epoch 14/25\n",
      "400/400 [==============================] - 2s 4ms/step - loss: 2.2767 - accuracy: 0.2107\n",
      "Epoch 15/25\n",
      "400/400 [==============================] - 2s 4ms/step - loss: 2.2746 - accuracy: 0.2180\n",
      "Epoch 16/25\n",
      "400/400 [==============================] - 2s 4ms/step - loss: 2.2729 - accuracy: 0.2180\n",
      "Epoch 17/25\n",
      "400/400 [==============================] - 2s 4ms/step - loss: 2.2711 - accuracy: 0.2177\n",
      "Epoch 18/25\n",
      "400/400 [==============================] - 2s 4ms/step - loss: 2.2688 - accuracy: 0.2224\n",
      "Epoch 19/25\n",
      "400/400 [==============================] - 2s 4ms/step - loss: 2.2667 - accuracy: 0.2241\n",
      "Epoch 20/25\n",
      "400/400 [==============================] - 2s 4ms/step - loss: 2.2645 - accuracy: 0.2254\n",
      "Epoch 21/25\n",
      "400/400 [==============================] - 2s 4ms/step - loss: 2.2623 - accuracy: 0.2230\n",
      "Epoch 22/25\n",
      "400/400 [==============================] - 2s 4ms/step - loss: 2.2598 - accuracy: 0.2309\n",
      "Epoch 23/25\n",
      "400/400 [==============================] - 2s 4ms/step - loss: 2.2576 - accuracy: 0.2298\n",
      "Epoch 24/25\n",
      "400/400 [==============================] - 2s 4ms/step - loss: 2.2553 - accuracy: 0.2382\n",
      "Epoch 25/25\n",
      "400/400 [==============================] - 2s 4ms/step - loss: 2.2531 - accuracy: 0.2404\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 2.2582 - accuracy: 0.2166\n",
      "Fitting for combination 116\n",
      "(28, 28, 1)\n",
      "4\n",
      "[32, 32, 128, 128]\n",
      "[(2, 2), (2, 2), (2, 2), (2, 2)]\n",
      "3\n",
      "[128, 16, 16, 10]\n",
      "10\n",
      "True\n",
      "['relu', 'sigmoid', 'relu', 'relu']\n",
      "['sigmoid', 'relu', 'tanh', 'softmax']\n",
      "SGD\n",
      "0.01\n",
      "0.01\n",
      "categorical_crossentropy\n",
      "100\n",
      "25\n",
      "0\n",
      "Epoch 1/25\n",
      " 42/600 [=>............................] - ETA: 2s - loss: 2.6257 - accuracy: 0.1021"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-14 10:20:09.527722: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:954] layout failed: INVALID_ARGUMENT: Size of values 0 does not match size of permutation 4 @ fanin shape insequential_116/dropout_177/dropout/SelectV2-2-TransposeNHWCToNCHW-LayoutOptimizer\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "600/600 [==============================] - 2s 3ms/step - loss: 2.3843 - accuracy: 0.0988\n",
      "Epoch 2/25\n",
      "600/600 [==============================] - 2s 4ms/step - loss: 2.3185 - accuracy: 0.0995\n",
      "Epoch 3/25\n",
      "600/600 [==============================] - 2s 4ms/step - loss: 2.3138 - accuracy: 0.1005\n",
      "Epoch 4/25\n",
      "600/600 [==============================] - 2s 4ms/step - loss: 2.3117 - accuracy: 0.1003\n",
      "Epoch 5/25\n",
      "600/600 [==============================] - 2s 4ms/step - loss: 2.3114 - accuracy: 0.0987\n",
      "Epoch 6/25\n",
      "600/600 [==============================] - 2s 4ms/step - loss: 2.3099 - accuracy: 0.1001\n",
      "Epoch 7/25\n",
      "600/600 [==============================] - 2s 4ms/step - loss: 2.3093 - accuracy: 0.0995\n",
      "Epoch 8/25\n",
      "600/600 [==============================] - 2s 4ms/step - loss: 2.3091 - accuracy: 0.1014\n",
      "Epoch 9/25\n",
      "600/600 [==============================] - 2s 4ms/step - loss: 2.3081 - accuracy: 0.0992\n",
      "Epoch 10/25\n",
      "600/600 [==============================] - 2s 4ms/step - loss: 2.3086 - accuracy: 0.0991\n",
      "Epoch 11/25\n",
      "600/600 [==============================] - 2s 4ms/step - loss: 2.3074 - accuracy: 0.1006\n",
      "Epoch 12/25\n",
      "600/600 [==============================] - 2s 4ms/step - loss: 2.3077 - accuracy: 0.0996\n",
      "Epoch 13/25\n",
      "600/600 [==============================] - 2s 4ms/step - loss: 2.3075 - accuracy: 0.1014\n",
      "Epoch 14/25\n",
      "600/600 [==============================] - 2s 4ms/step - loss: 2.3071 - accuracy: 0.1001\n",
      "Epoch 15/25\n",
      "600/600 [==============================] - 2s 4ms/step - loss: 2.3074 - accuracy: 0.0997\n",
      "Epoch 16/25\n",
      "600/600 [==============================] - 2s 4ms/step - loss: 2.3078 - accuracy: 0.1002\n",
      "Epoch 17/25\n",
      "600/600 [==============================] - 2s 4ms/step - loss: 2.3070 - accuracy: 0.0999\n",
      "Epoch 18/25\n",
      "600/600 [==============================] - 2s 4ms/step - loss: 2.3070 - accuracy: 0.0999\n",
      "Epoch 19/25\n",
      "600/600 [==============================] - 2s 4ms/step - loss: 2.3067 - accuracy: 0.1003\n",
      "Epoch 20/25\n",
      "600/600 [==============================] - 2s 4ms/step - loss: 2.3072 - accuracy: 0.0987\n",
      "Epoch 21/25\n",
      "600/600 [==============================] - 2s 4ms/step - loss: 2.3071 - accuracy: 0.0993\n",
      "Epoch 22/25\n",
      "600/600 [==============================] - 2s 4ms/step - loss: 2.3064 - accuracy: 0.1017\n",
      "Epoch 23/25\n",
      "600/600 [==============================] - 2s 4ms/step - loss: 2.3070 - accuracy: 0.0973\n",
      "Epoch 24/25\n",
      "600/600 [==============================] - 2s 4ms/step - loss: 2.3065 - accuracy: 0.1000\n",
      "Epoch 25/25\n",
      "600/600 [==============================] - 2s 4ms/step - loss: 2.3061 - accuracy: 0.0977\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 2.3027 - accuracy: 0.1000\n",
      "Fitting for combination 117\n",
      "(28, 28, 1)\n",
      "4\n",
      "[32, 32, 128, 128]\n",
      "[(2, 2), (2, 2), (2, 2), (3, 3)]\n",
      "1\n",
      "[128, 10]\n",
      "10\n",
      "True\n",
      "['relu', 'sigmoid', 'relu', 'relu']\n",
      "['sigmoid', 'softmax']\n",
      "SGD\n",
      "0.03\n",
      "0.01\n",
      "categorical_crossentropy\n",
      "50\n",
      "25\n",
      "0\n",
      "Epoch 1/25\n",
      "  32/1200 [..............................] - ETA: 3s - loss: 2.5139 - accuracy: 0.1000"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-14 10:21:06.842439: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:954] layout failed: INVALID_ARGUMENT: Size of values 0 does not match size of permutation 4 @ fanin shape insequential_117/dropout_181/dropout/SelectV2-2-TransposeNHWCToNCHW-LayoutOptimizer\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1200/1200 [==============================] - 4s 3ms/step - loss: 2.3320 - accuracy: 0.0978\n",
      "Epoch 2/25\n",
      "1200/1200 [==============================] - 4s 3ms/step - loss: 2.3048 - accuracy: 0.1003\n",
      "Epoch 3/25\n",
      "1200/1200 [==============================] - 4s 3ms/step - loss: 2.3038 - accuracy: 0.1019\n",
      "Epoch 4/25\n",
      "1200/1200 [==============================] - 4s 3ms/step - loss: 2.3033 - accuracy: 0.1016\n",
      "Epoch 5/25\n",
      "1200/1200 [==============================] - 4s 3ms/step - loss: 2.3036 - accuracy: 0.1009\n",
      "Epoch 6/25\n",
      "1200/1200 [==============================] - 4s 3ms/step - loss: 2.3033 - accuracy: 0.1023\n",
      "Epoch 7/25\n",
      "1200/1200 [==============================] - 4s 3ms/step - loss: 2.3030 - accuracy: 0.1032\n",
      "Epoch 8/25\n",
      "1200/1200 [==============================] - 4s 3ms/step - loss: 2.3030 - accuracy: 0.1033\n",
      "Epoch 9/25\n",
      "1200/1200 [==============================] - 4s 3ms/step - loss: 2.3030 - accuracy: 0.1018\n",
      "Epoch 10/25\n",
      "1200/1200 [==============================] - 4s 3ms/step - loss: 2.3031 - accuracy: 0.1032\n",
      "Epoch 11/25\n",
      "1200/1200 [==============================] - 4s 3ms/step - loss: 2.3032 - accuracy: 0.0993\n",
      "Epoch 12/25\n",
      "1200/1200 [==============================] - 4s 3ms/step - loss: 2.3030 - accuracy: 0.1030\n",
      "Epoch 13/25\n",
      "1200/1200 [==============================] - 4s 3ms/step - loss: 2.3031 - accuracy: 0.1034\n",
      "Epoch 14/25\n",
      "1200/1200 [==============================] - 4s 3ms/step - loss: 2.3031 - accuracy: 0.1015\n",
      "Epoch 15/25\n",
      "1200/1200 [==============================] - 4s 3ms/step - loss: 2.3028 - accuracy: 0.1024\n",
      "Epoch 16/25\n",
      "1200/1200 [==============================] - 4s 3ms/step - loss: 2.3031 - accuracy: 0.0993\n",
      "Epoch 17/25\n",
      "1200/1200 [==============================] - 4s 3ms/step - loss: 2.3028 - accuracy: 0.1010\n",
      "Epoch 18/25\n",
      "1200/1200 [==============================] - 4s 3ms/step - loss: 2.3030 - accuracy: 0.1029\n",
      "Epoch 19/25\n",
      "1200/1200 [==============================] - 4s 3ms/step - loss: 2.3031 - accuracy: 0.1025\n",
      "Epoch 20/25\n",
      "1200/1200 [==============================] - 4s 3ms/step - loss: 2.3026 - accuracy: 0.1032\n",
      "Epoch 21/25\n",
      "1200/1200 [==============================] - 4s 3ms/step - loss: 2.3029 - accuracy: 0.1023\n",
      "Epoch 22/25\n",
      "1200/1200 [==============================] - 4s 3ms/step - loss: 2.3027 - accuracy: 0.1034\n",
      "Epoch 23/25\n",
      "1200/1200 [==============================] - 4s 3ms/step - loss: 2.3029 - accuracy: 0.1011\n",
      "Epoch 24/25\n",
      "1200/1200 [==============================] - 4s 3ms/step - loss: 2.3026 - accuracy: 0.1037\n",
      "Epoch 25/25\n",
      "1200/1200 [==============================] - 4s 3ms/step - loss: 2.3026 - accuracy: 0.1036\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 2.3020 - accuracy: 0.1000\n",
      "Fitting for combination 118\n",
      "(28, 28, 1)\n",
      "4\n",
      "[32, 32, 128, 128]\n",
      "[(2, 2), (2, 2), (2, 2), (5, 5)]\n",
      "1\n",
      "[128, 10]\n",
      "10\n",
      "False\n",
      "['relu', 'sigmoid', 'relu', 'tanh']\n",
      "['relu', 'softmax']\n",
      "SGD\n",
      "0.01\n",
      "0.03\n",
      "categorical_crossentropy\n",
      "100\n",
      "25\n",
      "0\n",
      "Epoch 1/25\n",
      "600/600 [==============================] - 3s 4ms/step - loss: 2.3012 - accuracy: 0.1177\n",
      "Epoch 2/25\n",
      "600/600 [==============================] - 2s 4ms/step - loss: 2.2997 - accuracy: 0.1477\n",
      "Epoch 3/25\n",
      "600/600 [==============================] - 2s 4ms/step - loss: 2.2992 - accuracy: 0.1602\n",
      "Epoch 4/25\n",
      "600/600 [==============================] - 2s 4ms/step - loss: 2.2989 - accuracy: 0.2034\n",
      "Epoch 5/25\n",
      "600/600 [==============================] - 2s 4ms/step - loss: 2.2987 - accuracy: 0.1947\n",
      "Epoch 6/25\n",
      "600/600 [==============================] - 2s 4ms/step - loss: 2.2985 - accuracy: 0.2715\n",
      "Epoch 7/25\n",
      "600/600 [==============================] - 2s 4ms/step - loss: 2.2983 - accuracy: 0.2782\n",
      "Epoch 8/25\n",
      "600/600 [==============================] - 2s 4ms/step - loss: 2.2982 - accuracy: 0.2760\n",
      "Epoch 9/25\n",
      "600/600 [==============================] - 2s 4ms/step - loss: 2.2981 - accuracy: 0.3215\n",
      "Epoch 10/25\n",
      "600/600 [==============================] - 2s 4ms/step - loss: 2.2980 - accuracy: 0.2839\n",
      "Epoch 11/25\n",
      "600/600 [==============================] - 2s 4ms/step - loss: 2.2979 - accuracy: 0.3206\n",
      "Epoch 12/25\n",
      "600/600 [==============================] - 2s 4ms/step - loss: 2.2978 - accuracy: 0.2867\n",
      "Epoch 13/25\n",
      "600/600 [==============================] - 2s 4ms/step - loss: 2.2977 - accuracy: 0.3189\n",
      "Epoch 14/25\n",
      "600/600 [==============================] - 2s 4ms/step - loss: 2.2976 - accuracy: 0.2833\n",
      "Epoch 15/25\n",
      "600/600 [==============================] - 2s 4ms/step - loss: 2.2976 - accuracy: 0.3146\n",
      "Epoch 16/25\n",
      "600/600 [==============================] - 2s 4ms/step - loss: 2.2975 - accuracy: 0.3462\n",
      "Epoch 17/25\n",
      "600/600 [==============================] - 2s 4ms/step - loss: 2.2975 - accuracy: 0.3438\n",
      "Epoch 18/25\n",
      "600/600 [==============================] - 2s 4ms/step - loss: 2.2974 - accuracy: 0.3615\n",
      "Epoch 19/25\n",
      "600/600 [==============================] - 2s 4ms/step - loss: 2.2973 - accuracy: 0.3558\n",
      "Epoch 20/25\n",
      "600/600 [==============================] - 2s 4ms/step - loss: 2.2973 - accuracy: 0.3483\n",
      "Epoch 21/25\n",
      "600/600 [==============================] - 2s 4ms/step - loss: 2.2972 - accuracy: 0.3511\n",
      "Epoch 22/25\n",
      "600/600 [==============================] - 2s 4ms/step - loss: 2.2972 - accuracy: 0.3653\n",
      "Epoch 23/25\n",
      "600/600 [==============================] - 2s 4ms/step - loss: 2.2972 - accuracy: 0.3579\n",
      "Epoch 24/25\n",
      "600/600 [==============================] - 2s 4ms/step - loss: 2.2971 - accuracy: 0.3662\n",
      "Epoch 25/25\n",
      "600/600 [==============================] - 2s 4ms/step - loss: 2.2971 - accuracy: 0.3803\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 2.2971 - accuracy: 0.3679\n",
      "Fitting for combination 119\n",
      "(28, 28, 1)\n",
      "4\n",
      "[32, 32, 128, 128]\n",
      "[(2, 2), (2, 2), (2, 2), (4, 4)]\n",
      "1\n",
      "[128, 10]\n",
      "10\n",
      "True\n",
      "['relu', 'sigmoid', 'relu', 'relu']\n",
      "['tanh', 'softmax']\n",
      "SGD\n",
      "0.01\n",
      "0.03\n",
      "categorical_crossentropy\n",
      "50\n",
      "25\n",
      "0\n",
      "Epoch 1/25\n",
      "  31/1200 [..............................] - ETA: 4s - loss: 2.3649 - accuracy: 0.0787"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-14 10:23:43.005002: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:954] layout failed: INVALID_ARGUMENT: Size of values 0 does not match size of permutation 4 @ fanin shape insequential_119/dropout_183/dropout/SelectV2-2-TransposeNHWCToNCHW-LayoutOptimizer\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1200/1200 [==============================] - 4s 3ms/step - loss: 2.3062 - accuracy: 0.1054\n",
      "Epoch 2/25\n",
      "1200/1200 [==============================] - 4s 3ms/step - loss: 2.3015 - accuracy: 0.1069\n",
      "Epoch 3/25\n",
      "1200/1200 [==============================] - 4s 3ms/step - loss: 2.3002 - accuracy: 0.1108\n",
      "Epoch 4/25\n",
      "1200/1200 [==============================] - 4s 4ms/step - loss: 2.2993 - accuracy: 0.1117\n",
      "Epoch 5/25\n",
      "1200/1200 [==============================] - 4s 4ms/step - loss: 2.2990 - accuracy: 0.1144\n",
      "Epoch 6/25\n",
      "1200/1200 [==============================] - 4s 3ms/step - loss: 2.2985 - accuracy: 0.1154\n",
      "Epoch 7/25\n",
      "1200/1200 [==============================] - 4s 3ms/step - loss: 2.2984 - accuracy: 0.1167\n",
      "Epoch 8/25\n",
      "1200/1200 [==============================] - 4s 3ms/step - loss: 2.2982 - accuracy: 0.1161\n",
      "Epoch 9/25\n",
      "1200/1200 [==============================] - 4s 3ms/step - loss: 2.2980 - accuracy: 0.1153\n",
      "Epoch 10/25\n",
      "1200/1200 [==============================] - 4s 4ms/step - loss: 2.2978 - accuracy: 0.1180\n",
      "Epoch 11/25\n",
      "1200/1200 [==============================] - 4s 3ms/step - loss: 2.2974 - accuracy: 0.1184\n",
      "Epoch 12/25\n",
      "1200/1200 [==============================] - 4s 4ms/step - loss: 2.2974 - accuracy: 0.1172\n",
      "Epoch 13/25\n",
      "1200/1200 [==============================] - 4s 3ms/step - loss: 2.2971 - accuracy: 0.1172\n",
      "Epoch 14/25\n",
      "1200/1200 [==============================] - 4s 4ms/step - loss: 2.2973 - accuracy: 0.1194\n",
      "Epoch 15/25\n",
      "1200/1200 [==============================] - 4s 3ms/step - loss: 2.2969 - accuracy: 0.1208\n",
      "Epoch 16/25\n",
      "1200/1200 [==============================] - 4s 3ms/step - loss: 2.2968 - accuracy: 0.1203\n",
      "Epoch 17/25\n",
      "1200/1200 [==============================] - 4s 3ms/step - loss: 2.2967 - accuracy: 0.1212\n",
      "Epoch 18/25\n",
      "1200/1200 [==============================] - 4s 3ms/step - loss: 2.2960 - accuracy: 0.1211\n",
      "Epoch 19/25\n",
      "1200/1200 [==============================] - 4s 4ms/step - loss: 2.2965 - accuracy: 0.1192\n",
      "Epoch 20/25\n",
      "1200/1200 [==============================] - 4s 3ms/step - loss: 2.2965 - accuracy: 0.1191\n",
      "Epoch 21/25\n",
      "1200/1200 [==============================] - 4s 3ms/step - loss: 2.2963 - accuracy: 0.1220\n",
      "Epoch 22/25\n",
      "1200/1200 [==============================] - 4s 3ms/step - loss: 2.2963 - accuracy: 0.1225\n",
      "Epoch 23/25\n",
      "1200/1200 [==============================] - 4s 3ms/step - loss: 2.2959 - accuracy: 0.1230\n",
      "Epoch 24/25\n",
      "1200/1200 [==============================] - 4s 3ms/step - loss: 2.2960 - accuracy: 0.1239\n",
      "Epoch 25/25\n",
      "1200/1200 [==============================] - 4s 3ms/step - loss: 2.2959 - accuracy: 0.1210\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 2.3012 - accuracy: 0.1620\n",
      "Best parameters: {'network_inputs': (28, 28, 1), 'conv2d_layers': 3, 'conv2d_outputs': [32, 32, 128, 32], 'kernels': [(2, 2), (2, 2), (2, 2), (3, 3)], 'dense_layers': 2, 'dense_outputs': [16, 32, 10], 'network_output': 10, 'dropout_layers': False, 'conv2d_activations': ['relu', 'sigmoid', 'relu', 'tanh'], 'dense_activations': ['tanh', 'sigmoid', 'softmax'], 'optimizers': 'SGD', 'learning_rates': 0.1, 'weight_decays': 0, 'loss_functions': 'categorical_crossentropy', 'batches': 50, 'epochs': 25, 'accuracy': 0.9074000120162964}\n"
     ]
    }
   ],
   "source": [
    "# Much more reasonable Randomized Greedy grid search that only brute forces the number of layers, neurons per layer, and optimization functions,\n",
    "# which calculates ~40 different combination (~3.5 hours). All other parameters are chosen randomly during each iteration\n",
    "for conv2d_layer in parameters['conv2d_layers']:\n",
    "    for conv2d_output in parameters['conv2d_outputs']:\n",
    "        #for dense_layer in parameters['dense_layers']:\n",
    "            #for dense_output in parameters['dense_outputs']:\n",
    "        for optimizer in parameters['optimizers']:\n",
    "            for batch in parameters['batches']:\n",
    "\n",
    "                # Randomly select all other hyperparameters\n",
    "                network_input = parameters['network_inputs']\n",
    "                network_output = parameters['network_output']\n",
    "                kernel = random.choice(parameters['kernels'])\n",
    "                dense_layer = random.choice(parameters['dense_layers'])\n",
    "                dropout_layer = random.choice(parameters['dropout_layers'])\n",
    "                conv2d_activation = random.choice(parameters['conv2d_activations'])\n",
    "                dense_activation = random.choice(parameters['dense_activations'])\n",
    "                learning_rate = random.choice(parameters['learning_rates'])\n",
    "                weight_decay = random.choice(parameters['weight_decays'])\n",
    "                loss_function = random.choice(parameters['loss_functions']) # I'm aware that there is only a single loss function, I'm just treating it the same as everything else\n",
    "                batch = random.choice(parameters['batches'])\n",
    "                epochs = random.choice(parameters['epochs'])\n",
    "                                         \n",
    "                # Define the convolutional layer architecture\n",
    "                conv2d_outputs = []\n",
    "                conv2d_activations = []\n",
    "                kernels = []\n",
    "                if len(local_layers['conv2d_outputs']) < conv2d_layer: # This works like the above two parts\n",
    "                    local_layers['conv2d_outputs'].append(conv2d_output)\n",
    "                    local_layers['conv2d_activations'].append(conv2d_activation)\n",
    "                    local_layers['kernels'].append(kernel)\n",
    "                for layer in range(0,conv2d_layer-1):\n",
    "                    conv2d_outputs.append(local_layers['conv2d_outputs'][layer])\n",
    "                    conv2d_activations.append(local_layers['conv2d_activations'][layer])\n",
    "                    kernels.append(local_layers['kernels'][layer])\n",
    "                conv2d_outputs.append(conv2d_output)\n",
    "                conv2d_activations.append(conv2d_activation)\n",
    "                kernels.append(kernel)\n",
    "                        \n",
    "                    #conv2d_outputs += local_layers['conv2d_outputs']\n",
    "                    #conv2d_outputs.append(conv2d_output)\n",
    "                    #conv2d_activations += local_layers['conv2d_activations']\n",
    "                    #conv2d_activations.append(conv2d_activation)\n",
    "                    #kernels += local_layers['kernels']\n",
    "                    #kernels.append(kernel)\n",
    "\n",
    "                # Define the dense layer architecture\n",
    "                dense_outputs = []\n",
    "                dense_activations = []\n",
    "                for layer in range(0,dense_layer): # Unlike all of the above, this is always purely random\n",
    "                    dense_outputs.append(random.choice(parameters['dense_outputs']))\n",
    "                    dense_activations.append(random.choice(parameters['dense_activations']))                \n",
    "                dense_outputs.append(network_output) # Except for the final output layer, that needs to about 10 features\n",
    "                dense_activations.append('softmax') # And it needs softmax\n",
    "        \n",
    "                print(f\"Fitting for combination {combination}\")\n",
    "\n",
    "                # Save the current hyperparameters to results\n",
    "                results['conv2d_layers'].append(conv2d_layer)\n",
    "                results['conv2d_outputs'].append(conv2d_outputs)\n",
    "                results['dense_layers'].append(dense_layer)\n",
    "                results['dense_outputs'].append(dense_outputs)\n",
    "                results['kernels'].append(kernels)\n",
    "                results['dropout_layers'].append(dropout_layer)\n",
    "                results['conv2d_activations'].append(conv2d_activations)\n",
    "                results['dense_activations'].append(dense_activations)\n",
    "                results['optimizers'].append(optimizer)\n",
    "                results['learning_rates'].append(learning_rate)\n",
    "                results['weight_decays'].append(weight_decay)\n",
    "                results['loss_functions'].append(loss_function)\n",
    "                results['batches'].append(batch)\n",
    "                results['epochs'].append(epochs)\n",
    "                results['network_inputs'].append(network_input)\n",
    "                results['network_output'].append(network_output)\n",
    "                results['accuracy'].append(0)\n",
    "\n",
    "    \n",
    "                for parameter in results:\n",
    "                    print(results[parameter][combination])\n",
    "\n",
    "                # Build the model\n",
    "                model = cnn_model(network_input, conv2d_outputs, kernels, dense_outputs, conv2d_activations, dense_activations, dropout_layer)\n",
    "\n",
    "                # Since this architecture requires compile statements, I'm just doing those here with an if/then\n",
    "                if optimizer == 'SGD':\n",
    "                    model.compile(optimizer=SGD(learning_rate=learning_rate,\n",
    "                          decay=weight_decay),\n",
    "                          loss=loss_function,\n",
    "                          metrics=['accuracy'])\n",
    "                elif optimizer == 'Adam':\n",
    "                    model.compile(optimizer=Adam(learning_rate=learning_rate,\n",
    "                          decay=weight_decay),\n",
    "                          loss=loss_function,\n",
    "                          metrics=['accuracy'])\n",
    "\n",
    "                # Train the model\n",
    "                model.fit(x_train, y_train, epochs=epochs, batch_size=batch, verbose=1)\n",
    "                \n",
    "                # Evaluate the model on the validation data\n",
    "                loss, accuracy = model.evaluate(x_test, y_test)\n",
    "                results['accuracy'][combination] = accuracy\n",
    "                \n",
    "                # If the current score is better than the best score, update the best score, best parameters, and local_layers\n",
    "                if accuracy > best_parameters['accuracy']:\n",
    "                    for parameter in results:\n",
    "                        best_parameters[parameter] = results[parameter][combination]\n",
    "                    local_layers['conv2d_outputs'] = conv2d_outputs\n",
    "                    local_layers['conv2d_activations'] = conv2d_activations\n",
    "                    local_layers['kernels'] = kernels\n",
    "                elif accuracy < worst_parameters['accuracy']:\n",
    "                    for parameter in results:\n",
    "                        worst_parameters[parameter] = results[parameter][combination]\n",
    "                \n",
    "                combination += 1\n",
    "\n",
    "\n",
    "print('Best parameters:', best_parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'network_inputs': (28, 28, 1), 'conv2d_layers': 3, 'conv2d_outputs': [32, 32, 128, 32], 'kernels': [(2, 2), (2, 2), (2, 2), (3, 3)], 'dense_layers': 2, 'dense_outputs': [16, 32, 10], 'network_output': 10, 'dropout_layers': False, 'conv2d_activations': ['relu', 'sigmoid', 'relu', 'tanh'], 'dense_activations': ['tanh', 'sigmoid', 'softmax'], 'optimizers': 'SGD', 'learning_rates': 0.1, 'weight_decays': 0, 'loss_functions': 'categorical_crossentropy', 'batches': 50, 'epochs': 25, 'accuracy': 0.9074000120162964}\n"
     ]
    }
   ],
   "source": [
    "print(best_parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please note that like with all the others, the above is not exactly correct. These are the true best parameters:\n",
    "\n",
    "(28, 28, 1)\n",
    "3\n",
    "[32, 32, 128]\n",
    "[(2, 2), (2, 2), (2, 2)]\n",
    "2\n",
    "[16, 32, 10]\n",
    "10\n",
    "False\n",
    "['relu', 'sigmoid', 'relu']\n",
    "['tanh', 'sigmoid', 'softmax']\n",
    "SGD\n",
    "0.1\n",
    "0\n",
    "categorical_crossentropy\n",
    "50\n",
    "25\n",
    "0\n",
    "\n",
    "I ESPECIALLY don't want to re-run this one, as it takes like 4 hours to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"fmnist_cnn_without_augments.csv\", \"w\", newline=\"\") as file:\n",
    "    writer = csv.DictWriter(file, results.keys())\n",
    "    writer.writeheader()\n",
    "    writer.writerow(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "1200/1200 [==============================] - 3s 2ms/step - loss: 1.4111 - accuracy: 0.4955\n",
      "Epoch 2/25\n",
      "1200/1200 [==============================] - 3s 2ms/step - loss: 0.5504 - accuracy: 0.8000\n",
      "Epoch 3/25\n",
      "1200/1200 [==============================] - 3s 2ms/step - loss: 0.4244 - accuracy: 0.8492\n",
      "Epoch 4/25\n",
      "1200/1200 [==============================] - 3s 2ms/step - loss: 0.3716 - accuracy: 0.8671\n",
      "Epoch 5/25\n",
      "1200/1200 [==============================] - 3s 2ms/step - loss: 0.3403 - accuracy: 0.8771\n",
      "Epoch 6/25\n",
      "1200/1200 [==============================] - 3s 2ms/step - loss: 0.3134 - accuracy: 0.8854\n",
      "Epoch 7/25\n",
      "1200/1200 [==============================] - 3s 2ms/step - loss: 0.2909 - accuracy: 0.8945\n",
      "Epoch 8/25\n",
      "1200/1200 [==============================] - 3s 2ms/step - loss: 0.2744 - accuracy: 0.8994\n",
      "Epoch 9/25\n",
      "1200/1200 [==============================] - 3s 2ms/step - loss: 0.2563 - accuracy: 0.9065\n",
      "Epoch 10/25\n",
      "1200/1200 [==============================] - 3s 2ms/step - loss: 0.2402 - accuracy: 0.9128\n",
      "Epoch 11/25\n",
      "1200/1200 [==============================] - 3s 2ms/step - loss: 0.2261 - accuracy: 0.9181\n",
      "Epoch 12/25\n",
      "1200/1200 [==============================] - 3s 2ms/step - loss: 0.2132 - accuracy: 0.9229\n",
      "Epoch 13/25\n",
      "1200/1200 [==============================] - 3s 2ms/step - loss: 0.2008 - accuracy: 0.9269\n",
      "Epoch 14/25\n",
      "1200/1200 [==============================] - 3s 2ms/step - loss: 0.1875 - accuracy: 0.9324\n",
      "Epoch 15/25\n",
      "1200/1200 [==============================] - 3s 2ms/step - loss: 0.1765 - accuracy: 0.9368\n",
      "Epoch 16/25\n",
      "1200/1200 [==============================] - 3s 2ms/step - loss: 0.1643 - accuracy: 0.9405\n",
      "Epoch 17/25\n",
      "1200/1200 [==============================] - 3s 2ms/step - loss: 0.1546 - accuracy: 0.9441\n",
      "Epoch 18/25\n",
      "1200/1200 [==============================] - 3s 2ms/step - loss: 0.1448 - accuracy: 0.9478\n",
      "Epoch 19/25\n",
      "1200/1200 [==============================] - 3s 2ms/step - loss: 0.1365 - accuracy: 0.9517\n",
      "Epoch 20/25\n",
      "1200/1200 [==============================] - 3s 2ms/step - loss: 0.1280 - accuracy: 0.9558\n",
      "Epoch 21/25\n",
      "1200/1200 [==============================] - 3s 2ms/step - loss: 0.1192 - accuracy: 0.9581\n",
      "Epoch 22/25\n",
      "1200/1200 [==============================] - 3s 2ms/step - loss: 0.1127 - accuracy: 0.9611\n",
      "Epoch 23/25\n",
      "1200/1200 [==============================] - 3s 2ms/step - loss: 0.1042 - accuracy: 0.9640\n",
      "Epoch 24/25\n",
      "1200/1200 [==============================] - 3s 2ms/step - loss: 0.0982 - accuracy: 0.9667\n",
      "Epoch 25/25\n",
      "1200/1200 [==============================] - 3s 2ms/step - loss: 0.0908 - accuracy: 0.9699\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 0.2961 - accuracy: 0.9083\n",
      "Test accuracy: 0.90829998254776\n"
     ]
    }
   ],
   "source": [
    "model = cnn_model((28, 28, 1), [32, 32, 128], [(2, 2), (2, 2), (2, 2)], [16, 32, 10], ['relu', 'sigmoid', 'relu'], ['tanh', 'sigmoid', 'softmax'], False)\n",
    "\n",
    "model.compile(optimizer=SGD(learning_rate=0.1,\n",
    "            decay=0),\n",
    "            loss='categorical_crossentropy',\n",
    "            metrics=['accuracy'])\n",
    "\n",
    "model.fit(x_train, y_train, epochs=25, batch_size=50, verbose=1)\n",
    "\n",
    "loss, accuracy = model.evaluate(x_test, y_test)\n",
    "print('Test accuracy:', accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<keras.layers.convolutional.conv2d.Conv2D at 0x7f76d4be9a50>,\n",
       " <keras.layers.convolutional.conv2d.Conv2D at 0x7f76d4be8b50>,\n",
       " <keras.layers.pooling.max_pooling2d.MaxPooling2D at 0x7f76f30f7ad0>,\n",
       " <keras.layers.convolutional.conv2d.Conv2D at 0x7f76f4639550>,\n",
       " <keras.layers.reshaping.flatten.Flatten at 0x7f76f472d690>,\n",
       " <keras.layers.core.dense.Dense at 0x7f76f4775710>,\n",
       " <keras.layers.core.dense.Dense at 0x7f76f4774e10>,\n",
       " <keras.layers.core.dense.Dense at 0x7f76d4b76b50>]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_3 (Conv2D)           (None, 27, 27, 32)        160       \n",
      "                                                                 \n",
      " conv2d_4 (Conv2D)           (None, 26, 26, 32)        4128      \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPooling  (None, 13, 13, 32)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_5 (Conv2D)           (None, 12, 12, 128)       16512     \n",
      "                                                                 \n",
      " flatten_1 (Flatten)         (None, 18432)             0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 16)                294928    \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 32)                544       \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 10)                330       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 316,602\n",
      "Trainable params: 316,602\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 445
    },
    "id": "zJa4Lf76KZDM",
    "outputId": "f88fd412-5a2c-45c5-9514-63e8dcde4ece"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 97ms/step\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "image_index = 27\n",
    "\n",
    "# Extract the test image and label\n",
    "test_image = x_test[image_index]\n",
    "test_label = np.argmax(y_test[image_index])\n",
    "\n",
    "# Reshape the test image for prediction (Keras expects a batch dimension)\n",
    "test_image_reshaped = np.expand_dims(test_image, axis=0)\n",
    "\n",
    "# Make predictions on the test image\n",
    "predicted_label = np.argmax(model.predict(test_image_reshaped), axis=-1)\n",
    "\n",
    "# Plot the test image with predicted and actual labels\n",
    "plt.imshow(test_image, cmap='gray')\n",
    "plt.title(f'Predicted Label: {predicted_label[0]}, Actual Label: {test_label}')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Evaluations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The below is basically just loading the data from the saved CSVs and fixing how they're displayed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please note that Pandas was fighting me so much that I just manually fixed these lines in the saved .csv files before loading them:\n",
    "\n",
    "784 1 10 [30, 10] False ['tanh'] Adam 0.001 0.01 CrossEntropyLoss 100 25 0 = 0.9315\n",
    "\n",
    "784 1 10 [40, 10] True+ ['relu'] SGD 0.03 0 CrossEntropyLoss 100 25 0 = 0.8762\n",
    "\n",
    "(28, 28, 1) 3 [32, 32, 128] [(2, 2), (2, 2), (2, 2)] 2 [16, 32, 10] 10 False ['relu', 'sigmoid', 'relu'] ['tanh', 'sigmoid', 'softmax'] SGD 0.1 0 categorical_crossentropy 50 25 0 = 0.9074"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_fc_df = pd.read_csv('./mnist_without_augments.csv', sep=',')\n",
    "fmnist_fc_df = pd.read_csv('./fmnist_without_augments.csv', sep=',')\n",
    "fmnist_cnn_df = pd.read_csv('./fmnist_cnn_without_augments.csv', sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>inputs</th>\n",
       "      <th>number_of_layers</th>\n",
       "      <th>outputs</th>\n",
       "      <th>neurons_per_layer</th>\n",
       "      <th>dropout_layers</th>\n",
       "      <th>activation_functions</th>\n",
       "      <th>optimizers</th>\n",
       "      <th>learning_rates</th>\n",
       "      <th>weight_decays</th>\n",
       "      <th>loss_functions</th>\n",
       "      <th>batches</th>\n",
       "      <th>epochs</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>784, 784, 784, 784, 784, 784, 784, 784, 784, 7...</td>\n",
       "      <td>1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1...</td>\n",
       "      <td>10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10...</td>\n",
       "      <td>[10, 10], [10, 10], [10, 10], [10, 10], [20, 1...</td>\n",
       "      <td>True, True, False, False, False, False, True, ...</td>\n",
       "      <td>['relu'], ['tanh'], ['sigmoid'], ['relu'], ['s...</td>\n",
       "      <td>'Adam', 'SGD', 'Adam', 'SGD', 'Adam', 'SGD', '...</td>\n",
       "      <td>0.1, 0.01, 0.03, 0.1, 0.1, 0.03, 0.1, 0.3, 0.1...</td>\n",
       "      <td>0.1, 0, 0.3, 0.03, 0, 0.03, 0.3, 0.03, 0.01, 0...</td>\n",
       "      <td>'CrossEntropyLoss', 'CrossEntropyLoss', 'Cross...</td>\n",
       "      <td>200, 200, 50, 100, 100, 200, 100, 50, 200, 100...</td>\n",
       "      <td>25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25...</td>\n",
       "      <td>0.1502, 0.8457, 0.2163, 0.7733, 0.6983, 0.7625...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              inputs  \\\n",
       "0  784, 784, 784, 784, 784, 784, 784, 784, 784, 7...   \n",
       "\n",
       "                                    number_of_layers  \\\n",
       "0  1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1...   \n",
       "\n",
       "                                             outputs  \\\n",
       "0  10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10...   \n",
       "\n",
       "                                   neurons_per_layer  \\\n",
       "0  [10, 10], [10, 10], [10, 10], [10, 10], [20, 1...   \n",
       "\n",
       "                                      dropout_layers  \\\n",
       "0  True, True, False, False, False, False, True, ...   \n",
       "\n",
       "                                activation_functions  \\\n",
       "0  ['relu'], ['tanh'], ['sigmoid'], ['relu'], ['s...   \n",
       "\n",
       "                                          optimizers  \\\n",
       "0  'Adam', 'SGD', 'Adam', 'SGD', 'Adam', 'SGD', '...   \n",
       "\n",
       "                                      learning_rates  \\\n",
       "0  0.1, 0.01, 0.03, 0.1, 0.1, 0.03, 0.1, 0.3, 0.1...   \n",
       "\n",
       "                                       weight_decays  \\\n",
       "0  0.1, 0, 0.3, 0.03, 0, 0.03, 0.3, 0.03, 0.01, 0...   \n",
       "\n",
       "                                      loss_functions  \\\n",
       "0  'CrossEntropyLoss', 'CrossEntropyLoss', 'Cross...   \n",
       "\n",
       "                                             batches  \\\n",
       "0  200, 200, 50, 100, 100, 200, 100, 50, 200, 100...   \n",
       "\n",
       "                                              epochs  \\\n",
       "0  25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25...   \n",
       "\n",
       "                                               score  \n",
       "0  0.1502, 0.8457, 0.2163, 0.7733, 0.6983, 0.7625...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fmnist_fc_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in mnist_fc_df:\n",
    "    mnist_fc_df[column] = mnist_fc_df[column].apply(literal_eval)\n",
    "\n",
    "for column in fmnist_fc_df:\n",
    "    fmnist_fc_df[column] = fmnist_fc_df[column].apply(literal_eval)\n",
    "\n",
    "for column in fmnist_cnn_df:\n",
    "    fmnist_cnn_df[column] = fmnist_cnn_df[column].apply(literal_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_dict = {}\n",
    "fmnist_dict = {}\n",
    "fmnist_cnn_dict = {}\n",
    "for column in mnist_fc_df:\n",
    "    mnist_dict[column] = []\n",
    "    for row in mnist_fc_df[column][0]:\n",
    "        mnist_dict[column].append(row)\n",
    "\n",
    "for column in fmnist_fc_df:\n",
    "    fmnist_dict[column] = []\n",
    "    for row in fmnist_fc_df[column][0]:\n",
    "        fmnist_dict[column].append(row)\n",
    "\n",
    "for column in fmnist_cnn_df:\n",
    "    fmnist_cnn_dict[column] = []\n",
    "    for row in fmnist_cnn_df[column][0]:\n",
    "        fmnist_cnn_dict[column].append(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_fixed_fc_df = pd.DataFrame.from_dict(mnist_dict)\n",
    "fmnist_fixed_fc_df = pd.DataFrame.from_dict(fmnist_dict)\n",
    "fmnist_fixed_cnn_df = pd.DataFrame.from_dict(fmnist_cnn_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>inputs</th>\n",
       "      <th>number_of_layers</th>\n",
       "      <th>outputs</th>\n",
       "      <th>neurons_per_layer</th>\n",
       "      <th>dropout_layers</th>\n",
       "      <th>activation_functions</th>\n",
       "      <th>optimizers</th>\n",
       "      <th>learning_rates</th>\n",
       "      <th>weight_decays</th>\n",
       "      <th>loss_functions</th>\n",
       "      <th>batches</th>\n",
       "      <th>epochs</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>784</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>[10, 10]</td>\n",
       "      <td>False</td>\n",
       "      <td>[sigmoid]</td>\n",
       "      <td>Adam</td>\n",
       "      <td>0.300</td>\n",
       "      <td>1.00</td>\n",
       "      <td>CrossEntropyLoss</td>\n",
       "      <td>300</td>\n",
       "      <td>25</td>\n",
       "      <td>0.0980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>784</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>[10, 10]</td>\n",
       "      <td>False</td>\n",
       "      <td>[relu]</td>\n",
       "      <td>SGD</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.03</td>\n",
       "      <td>CrossEntropyLoss</td>\n",
       "      <td>300</td>\n",
       "      <td>25</td>\n",
       "      <td>0.8959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>784</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>[20, 10]</td>\n",
       "      <td>False</td>\n",
       "      <td>[sigmoid]</td>\n",
       "      <td>Adam</td>\n",
       "      <td>0.300</td>\n",
       "      <td>1.00</td>\n",
       "      <td>CrossEntropyLoss</td>\n",
       "      <td>100</td>\n",
       "      <td>25</td>\n",
       "      <td>0.0892</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>784</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>[20, 10]</td>\n",
       "      <td>True</td>\n",
       "      <td>[relu]</td>\n",
       "      <td>SGD</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.30</td>\n",
       "      <td>CrossEntropyLoss</td>\n",
       "      <td>200</td>\n",
       "      <td>25</td>\n",
       "      <td>0.8162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>784</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>[30, 10]</td>\n",
       "      <td>False</td>\n",
       "      <td>[tanh]</td>\n",
       "      <td>Adam</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.01</td>\n",
       "      <td>CrossEntropyLoss</td>\n",
       "      <td>100</td>\n",
       "      <td>25</td>\n",
       "      <td>0.9315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>784</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>[30, 10]</td>\n",
       "      <td>True</td>\n",
       "      <td>[sigmoid]</td>\n",
       "      <td>SGD</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.01</td>\n",
       "      <td>CrossEntropyLoss</td>\n",
       "      <td>300</td>\n",
       "      <td>25</td>\n",
       "      <td>0.9059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>784</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>[40, 10]</td>\n",
       "      <td>True</td>\n",
       "      <td>[relu]</td>\n",
       "      <td>Adam</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.10</td>\n",
       "      <td>CrossEntropyLoss</td>\n",
       "      <td>300</td>\n",
       "      <td>25</td>\n",
       "      <td>0.2036</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>784</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>[40, 10]</td>\n",
       "      <td>False</td>\n",
       "      <td>[relu]</td>\n",
       "      <td>SGD</td>\n",
       "      <td>0.300</td>\n",
       "      <td>0.03</td>\n",
       "      <td>CrossEntropyLoss</td>\n",
       "      <td>100</td>\n",
       "      <td>25</td>\n",
       "      <td>0.1135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>784</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>[50, 10]</td>\n",
       "      <td>False</td>\n",
       "      <td>[tanh]</td>\n",
       "      <td>Adam</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.30</td>\n",
       "      <td>CrossEntropyLoss</td>\n",
       "      <td>100</td>\n",
       "      <td>25</td>\n",
       "      <td>0.5220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>784</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>[50, 10]</td>\n",
       "      <td>True</td>\n",
       "      <td>[relu]</td>\n",
       "      <td>SGD</td>\n",
       "      <td>0.300</td>\n",
       "      <td>0.10</td>\n",
       "      <td>CrossEntropyLoss</td>\n",
       "      <td>300</td>\n",
       "      <td>25</td>\n",
       "      <td>0.1010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>784</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>[30, 10, 10]</td>\n",
       "      <td>False</td>\n",
       "      <td>[tanh, relu]</td>\n",
       "      <td>Adam</td>\n",
       "      <td>0.030</td>\n",
       "      <td>0.30</td>\n",
       "      <td>CrossEntropyLoss</td>\n",
       "      <td>300</td>\n",
       "      <td>25</td>\n",
       "      <td>0.1028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>784</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>[30, 10, 10]</td>\n",
       "      <td>True</td>\n",
       "      <td>[tanh, relu]</td>\n",
       "      <td>SGD</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.01</td>\n",
       "      <td>CrossEntropyLoss</td>\n",
       "      <td>200</td>\n",
       "      <td>25</td>\n",
       "      <td>0.9252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>784</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>[30, 20, 10]</td>\n",
       "      <td>False</td>\n",
       "      <td>[tanh, relu]</td>\n",
       "      <td>Adam</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.10</td>\n",
       "      <td>CrossEntropyLoss</td>\n",
       "      <td>300</td>\n",
       "      <td>25</td>\n",
       "      <td>0.0974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>784</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>[30, 20, 10]</td>\n",
       "      <td>True</td>\n",
       "      <td>[tanh, sigmoid]</td>\n",
       "      <td>SGD</td>\n",
       "      <td>0.030</td>\n",
       "      <td>0.03</td>\n",
       "      <td>CrossEntropyLoss</td>\n",
       "      <td>100</td>\n",
       "      <td>25</td>\n",
       "      <td>0.7916</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>784</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>[30, 30, 10]</td>\n",
       "      <td>True</td>\n",
       "      <td>[tanh, sigmoid]</td>\n",
       "      <td>Adam</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.30</td>\n",
       "      <td>CrossEntropyLoss</td>\n",
       "      <td>200</td>\n",
       "      <td>25</td>\n",
       "      <td>0.0892</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>784</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>[30, 30, 10]</td>\n",
       "      <td>False</td>\n",
       "      <td>[tanh, sigmoid]</td>\n",
       "      <td>SGD</td>\n",
       "      <td>0.030</td>\n",
       "      <td>0.01</td>\n",
       "      <td>CrossEntropyLoss</td>\n",
       "      <td>100</td>\n",
       "      <td>25</td>\n",
       "      <td>0.9102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>784</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>[30, 40, 10]</td>\n",
       "      <td>False</td>\n",
       "      <td>[tanh, sigmoid]</td>\n",
       "      <td>Adam</td>\n",
       "      <td>0.030</td>\n",
       "      <td>1.00</td>\n",
       "      <td>CrossEntropyLoss</td>\n",
       "      <td>200</td>\n",
       "      <td>25</td>\n",
       "      <td>0.1032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>784</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>[30, 40, 10]</td>\n",
       "      <td>True</td>\n",
       "      <td>[tanh, sigmoid]</td>\n",
       "      <td>SGD</td>\n",
       "      <td>0.010</td>\n",
       "      <td>1.00</td>\n",
       "      <td>CrossEntropyLoss</td>\n",
       "      <td>300</td>\n",
       "      <td>25</td>\n",
       "      <td>0.1135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>784</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>[30, 50, 10]</td>\n",
       "      <td>True</td>\n",
       "      <td>[tanh, sigmoid]</td>\n",
       "      <td>Adam</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.01</td>\n",
       "      <td>CrossEntropyLoss</td>\n",
       "      <td>200</td>\n",
       "      <td>25</td>\n",
       "      <td>0.8334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>784</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>[30, 50, 10]</td>\n",
       "      <td>True</td>\n",
       "      <td>[tanh, relu]</td>\n",
       "      <td>SGD</td>\n",
       "      <td>0.300</td>\n",
       "      <td>0.10</td>\n",
       "      <td>CrossEntropyLoss</td>\n",
       "      <td>300</td>\n",
       "      <td>25</td>\n",
       "      <td>0.1930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>784</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>[30, 10, 10, 10]</td>\n",
       "      <td>True</td>\n",
       "      <td>[tanh, sigmoid, sigmoid]</td>\n",
       "      <td>Adam</td>\n",
       "      <td>0.100</td>\n",
       "      <td>1.00</td>\n",
       "      <td>CrossEntropyLoss</td>\n",
       "      <td>100</td>\n",
       "      <td>25</td>\n",
       "      <td>0.1135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>784</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>[30, 10, 10, 10]</td>\n",
       "      <td>False</td>\n",
       "      <td>[tanh, sigmoid, sigmoid]</td>\n",
       "      <td>SGD</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.01</td>\n",
       "      <td>CrossEntropyLoss</td>\n",
       "      <td>200</td>\n",
       "      <td>25</td>\n",
       "      <td>0.3557</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>784</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>[30, 10, 20, 10]</td>\n",
       "      <td>False</td>\n",
       "      <td>[tanh, sigmoid, tanh]</td>\n",
       "      <td>Adam</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.03</td>\n",
       "      <td>CrossEntropyLoss</td>\n",
       "      <td>100</td>\n",
       "      <td>25</td>\n",
       "      <td>0.1028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>784</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>[30, 10, 20, 10]</td>\n",
       "      <td>False</td>\n",
       "      <td>[tanh, sigmoid, sigmoid]</td>\n",
       "      <td>SGD</td>\n",
       "      <td>0.030</td>\n",
       "      <td>0.01</td>\n",
       "      <td>CrossEntropyLoss</td>\n",
       "      <td>100</td>\n",
       "      <td>25</td>\n",
       "      <td>0.3742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>784</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>[30, 10, 30, 10]</td>\n",
       "      <td>False</td>\n",
       "      <td>[tanh, sigmoid, tanh]</td>\n",
       "      <td>Adam</td>\n",
       "      <td>0.100</td>\n",
       "      <td>1.00</td>\n",
       "      <td>CrossEntropyLoss</td>\n",
       "      <td>100</td>\n",
       "      <td>25</td>\n",
       "      <td>0.0980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>784</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>[30, 10, 30, 10]</td>\n",
       "      <td>True</td>\n",
       "      <td>[tanh, sigmoid, tanh]</td>\n",
       "      <td>SGD</td>\n",
       "      <td>0.300</td>\n",
       "      <td>0.10</td>\n",
       "      <td>CrossEntropyLoss</td>\n",
       "      <td>300</td>\n",
       "      <td>25</td>\n",
       "      <td>0.1135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>784</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>[30, 10, 40, 10]</td>\n",
       "      <td>False</td>\n",
       "      <td>[tanh, sigmoid, relu]</td>\n",
       "      <td>Adam</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.01</td>\n",
       "      <td>CrossEntropyLoss</td>\n",
       "      <td>100</td>\n",
       "      <td>25</td>\n",
       "      <td>0.9038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>784</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>[30, 10, 40, 10]</td>\n",
       "      <td>False</td>\n",
       "      <td>[tanh, sigmoid, relu]</td>\n",
       "      <td>SGD</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.01</td>\n",
       "      <td>CrossEntropyLoss</td>\n",
       "      <td>100</td>\n",
       "      <td>25</td>\n",
       "      <td>0.4582</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>784</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>[30, 10, 50, 10]</td>\n",
       "      <td>True</td>\n",
       "      <td>[tanh, sigmoid, relu]</td>\n",
       "      <td>Adam</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.03</td>\n",
       "      <td>CrossEntropyLoss</td>\n",
       "      <td>300</td>\n",
       "      <td>25</td>\n",
       "      <td>0.1009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>784</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>[30, 10, 50, 10]</td>\n",
       "      <td>False</td>\n",
       "      <td>[tanh, sigmoid, sigmoid]</td>\n",
       "      <td>SGD</td>\n",
       "      <td>0.003</td>\n",
       "      <td>1.00</td>\n",
       "      <td>CrossEntropyLoss</td>\n",
       "      <td>100</td>\n",
       "      <td>25</td>\n",
       "      <td>0.1135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>784</td>\n",
       "      <td>4</td>\n",
       "      <td>10</td>\n",
       "      <td>[30, 10, 10, 10, 10]</td>\n",
       "      <td>True</td>\n",
       "      <td>[tanh, sigmoid, relu, relu]</td>\n",
       "      <td>Adam</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.01</td>\n",
       "      <td>CrossEntropyLoss</td>\n",
       "      <td>200</td>\n",
       "      <td>25</td>\n",
       "      <td>0.8953</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>784</td>\n",
       "      <td>4</td>\n",
       "      <td>10</td>\n",
       "      <td>[30, 10, 10, 10, 10]</td>\n",
       "      <td>True</td>\n",
       "      <td>[tanh, sigmoid, relu, tanh]</td>\n",
       "      <td>SGD</td>\n",
       "      <td>0.300</td>\n",
       "      <td>0.01</td>\n",
       "      <td>CrossEntropyLoss</td>\n",
       "      <td>100</td>\n",
       "      <td>25</td>\n",
       "      <td>0.4651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>784</td>\n",
       "      <td>4</td>\n",
       "      <td>10</td>\n",
       "      <td>[30, 10, 10, 20, 10]</td>\n",
       "      <td>True</td>\n",
       "      <td>[tanh, sigmoid, relu, relu]</td>\n",
       "      <td>Adam</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.03</td>\n",
       "      <td>CrossEntropyLoss</td>\n",
       "      <td>300</td>\n",
       "      <td>25</td>\n",
       "      <td>0.1135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>784</td>\n",
       "      <td>4</td>\n",
       "      <td>10</td>\n",
       "      <td>[30, 10, 10, 20, 10]</td>\n",
       "      <td>False</td>\n",
       "      <td>[tanh, sigmoid, relu, tanh]</td>\n",
       "      <td>SGD</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.10</td>\n",
       "      <td>CrossEntropyLoss</td>\n",
       "      <td>100</td>\n",
       "      <td>25</td>\n",
       "      <td>0.1135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>784</td>\n",
       "      <td>4</td>\n",
       "      <td>10</td>\n",
       "      <td>[30, 10, 10, 30, 10]</td>\n",
       "      <td>True</td>\n",
       "      <td>[tanh, sigmoid, relu, sigmoid]</td>\n",
       "      <td>Adam</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.01</td>\n",
       "      <td>CrossEntropyLoss</td>\n",
       "      <td>100</td>\n",
       "      <td>25</td>\n",
       "      <td>0.1135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>784</td>\n",
       "      <td>4</td>\n",
       "      <td>10</td>\n",
       "      <td>[30, 10, 10, 30, 10]</td>\n",
       "      <td>False</td>\n",
       "      <td>[tanh, sigmoid, relu, tanh]</td>\n",
       "      <td>SGD</td>\n",
       "      <td>0.030</td>\n",
       "      <td>0.30</td>\n",
       "      <td>CrossEntropyLoss</td>\n",
       "      <td>200</td>\n",
       "      <td>25</td>\n",
       "      <td>0.1135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>784</td>\n",
       "      <td>4</td>\n",
       "      <td>10</td>\n",
       "      <td>[30, 10, 10, 40, 10]</td>\n",
       "      <td>False</td>\n",
       "      <td>[tanh, sigmoid, relu, tanh]</td>\n",
       "      <td>Adam</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.30</td>\n",
       "      <td>CrossEntropyLoss</td>\n",
       "      <td>200</td>\n",
       "      <td>25</td>\n",
       "      <td>0.1135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>784</td>\n",
       "      <td>4</td>\n",
       "      <td>10</td>\n",
       "      <td>[30, 10, 10, 40, 10]</td>\n",
       "      <td>True</td>\n",
       "      <td>[tanh, sigmoid, relu, sigmoid]</td>\n",
       "      <td>SGD</td>\n",
       "      <td>0.300</td>\n",
       "      <td>1.00</td>\n",
       "      <td>CrossEntropyLoss</td>\n",
       "      <td>100</td>\n",
       "      <td>25</td>\n",
       "      <td>0.1032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>784</td>\n",
       "      <td>4</td>\n",
       "      <td>10</td>\n",
       "      <td>[30, 10, 10, 50, 10]</td>\n",
       "      <td>True</td>\n",
       "      <td>[tanh, sigmoid, relu, relu]</td>\n",
       "      <td>Adam</td>\n",
       "      <td>0.001</td>\n",
       "      <td>1.00</td>\n",
       "      <td>CrossEntropyLoss</td>\n",
       "      <td>300</td>\n",
       "      <td>25</td>\n",
       "      <td>0.1135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>784</td>\n",
       "      <td>4</td>\n",
       "      <td>10</td>\n",
       "      <td>[30, 10, 10, 50, 10]</td>\n",
       "      <td>False</td>\n",
       "      <td>[tanh, sigmoid, relu, relu]</td>\n",
       "      <td>SGD</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.30</td>\n",
       "      <td>CrossEntropyLoss</td>\n",
       "      <td>100</td>\n",
       "      <td>25</td>\n",
       "      <td>0.1135</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    inputs  number_of_layers  outputs     neurons_per_layer  dropout_layers  \\\n",
       "0      784                 1       10              [10, 10]           False   \n",
       "1      784                 1       10              [10, 10]           False   \n",
       "2      784                 1       10              [20, 10]           False   \n",
       "3      784                 1       10              [20, 10]            True   \n",
       "4      784                 1       10              [30, 10]           False   \n",
       "5      784                 1       10              [30, 10]            True   \n",
       "6      784                 1       10              [40, 10]            True   \n",
       "7      784                 1       10              [40, 10]           False   \n",
       "8      784                 1       10              [50, 10]           False   \n",
       "9      784                 1       10              [50, 10]            True   \n",
       "10     784                 2       10          [30, 10, 10]           False   \n",
       "11     784                 2       10          [30, 10, 10]            True   \n",
       "12     784                 2       10          [30, 20, 10]           False   \n",
       "13     784                 2       10          [30, 20, 10]            True   \n",
       "14     784                 2       10          [30, 30, 10]            True   \n",
       "15     784                 2       10          [30, 30, 10]           False   \n",
       "16     784                 2       10          [30, 40, 10]           False   \n",
       "17     784                 2       10          [30, 40, 10]            True   \n",
       "18     784                 2       10          [30, 50, 10]            True   \n",
       "19     784                 2       10          [30, 50, 10]            True   \n",
       "20     784                 3       10      [30, 10, 10, 10]            True   \n",
       "21     784                 3       10      [30, 10, 10, 10]           False   \n",
       "22     784                 3       10      [30, 10, 20, 10]           False   \n",
       "23     784                 3       10      [30, 10, 20, 10]           False   \n",
       "24     784                 3       10      [30, 10, 30, 10]           False   \n",
       "25     784                 3       10      [30, 10, 30, 10]            True   \n",
       "26     784                 3       10      [30, 10, 40, 10]           False   \n",
       "27     784                 3       10      [30, 10, 40, 10]           False   \n",
       "28     784                 3       10      [30, 10, 50, 10]            True   \n",
       "29     784                 3       10      [30, 10, 50, 10]           False   \n",
       "30     784                 4       10  [30, 10, 10, 10, 10]            True   \n",
       "31     784                 4       10  [30, 10, 10, 10, 10]            True   \n",
       "32     784                 4       10  [30, 10, 10, 20, 10]            True   \n",
       "33     784                 4       10  [30, 10, 10, 20, 10]           False   \n",
       "34     784                 4       10  [30, 10, 10, 30, 10]            True   \n",
       "35     784                 4       10  [30, 10, 10, 30, 10]           False   \n",
       "36     784                 4       10  [30, 10, 10, 40, 10]           False   \n",
       "37     784                 4       10  [30, 10, 10, 40, 10]            True   \n",
       "38     784                 4       10  [30, 10, 10, 50, 10]            True   \n",
       "39     784                 4       10  [30, 10, 10, 50, 10]           False   \n",
       "\n",
       "              activation_functions optimizers  learning_rates  weight_decays  \\\n",
       "0                        [sigmoid]       Adam           0.300           1.00   \n",
       "1                           [relu]        SGD           0.001           0.03   \n",
       "2                        [sigmoid]       Adam           0.300           1.00   \n",
       "3                           [relu]        SGD           0.003           0.30   \n",
       "4                           [tanh]       Adam           0.001           0.01   \n",
       "5                        [sigmoid]        SGD           0.010           0.01   \n",
       "6                           [relu]       Adam           0.100           0.10   \n",
       "7                           [relu]        SGD           0.300           0.03   \n",
       "8                           [tanh]       Adam           0.010           0.30   \n",
       "9                           [relu]        SGD           0.300           0.10   \n",
       "10                    [tanh, relu]       Adam           0.030           0.30   \n",
       "11                    [tanh, relu]        SGD           0.003           0.01   \n",
       "12                    [tanh, relu]       Adam           0.100           0.10   \n",
       "13                 [tanh, sigmoid]        SGD           0.030           0.03   \n",
       "14                 [tanh, sigmoid]       Adam           0.100           0.30   \n",
       "15                 [tanh, sigmoid]        SGD           0.030           0.01   \n",
       "16                 [tanh, sigmoid]       Adam           0.030           1.00   \n",
       "17                 [tanh, sigmoid]        SGD           0.010           1.00   \n",
       "18                 [tanh, sigmoid]       Adam           0.010           0.01   \n",
       "19                    [tanh, relu]        SGD           0.300           0.10   \n",
       "20        [tanh, sigmoid, sigmoid]       Adam           0.100           1.00   \n",
       "21        [tanh, sigmoid, sigmoid]        SGD           0.100           0.01   \n",
       "22           [tanh, sigmoid, tanh]       Adam           0.010           0.03   \n",
       "23        [tanh, sigmoid, sigmoid]        SGD           0.030           0.01   \n",
       "24           [tanh, sigmoid, tanh]       Adam           0.100           1.00   \n",
       "25           [tanh, sigmoid, tanh]        SGD           0.300           0.10   \n",
       "26           [tanh, sigmoid, relu]       Adam           0.003           0.01   \n",
       "27           [tanh, sigmoid, relu]        SGD           0.001           0.01   \n",
       "28           [tanh, sigmoid, relu]       Adam           0.100           0.03   \n",
       "29        [tanh, sigmoid, sigmoid]        SGD           0.003           1.00   \n",
       "30     [tanh, sigmoid, relu, relu]       Adam           0.001           0.01   \n",
       "31     [tanh, sigmoid, relu, tanh]        SGD           0.300           0.01   \n",
       "32     [tanh, sigmoid, relu, relu]       Adam           0.003           0.03   \n",
       "33     [tanh, sigmoid, relu, tanh]        SGD           0.003           0.10   \n",
       "34  [tanh, sigmoid, relu, sigmoid]       Adam           0.001           0.01   \n",
       "35     [tanh, sigmoid, relu, tanh]        SGD           0.030           0.30   \n",
       "36     [tanh, sigmoid, relu, tanh]       Adam           0.001           0.30   \n",
       "37  [tanh, sigmoid, relu, sigmoid]        SGD           0.300           1.00   \n",
       "38     [tanh, sigmoid, relu, relu]       Adam           0.001           1.00   \n",
       "39     [tanh, sigmoid, relu, relu]        SGD           0.100           0.30   \n",
       "\n",
       "      loss_functions  batches  epochs   score  \n",
       "0   CrossEntropyLoss      300      25  0.0980  \n",
       "1   CrossEntropyLoss      300      25  0.8959  \n",
       "2   CrossEntropyLoss      100      25  0.0892  \n",
       "3   CrossEntropyLoss      200      25  0.8162  \n",
       "4   CrossEntropyLoss      100      25  0.9315  \n",
       "5   CrossEntropyLoss      300      25  0.9059  \n",
       "6   CrossEntropyLoss      300      25  0.2036  \n",
       "7   CrossEntropyLoss      100      25  0.1135  \n",
       "8   CrossEntropyLoss      100      25  0.5220  \n",
       "9   CrossEntropyLoss      300      25  0.1010  \n",
       "10  CrossEntropyLoss      300      25  0.1028  \n",
       "11  CrossEntropyLoss      200      25  0.9252  \n",
       "12  CrossEntropyLoss      300      25  0.0974  \n",
       "13  CrossEntropyLoss      100      25  0.7916  \n",
       "14  CrossEntropyLoss      200      25  0.0892  \n",
       "15  CrossEntropyLoss      100      25  0.9102  \n",
       "16  CrossEntropyLoss      200      25  0.1032  \n",
       "17  CrossEntropyLoss      300      25  0.1135  \n",
       "18  CrossEntropyLoss      200      25  0.8334  \n",
       "19  CrossEntropyLoss      300      25  0.1930  \n",
       "20  CrossEntropyLoss      100      25  0.1135  \n",
       "21  CrossEntropyLoss      200      25  0.3557  \n",
       "22  CrossEntropyLoss      100      25  0.1028  \n",
       "23  CrossEntropyLoss      100      25  0.3742  \n",
       "24  CrossEntropyLoss      100      25  0.0980  \n",
       "25  CrossEntropyLoss      300      25  0.1135  \n",
       "26  CrossEntropyLoss      100      25  0.9038  \n",
       "27  CrossEntropyLoss      100      25  0.4582  \n",
       "28  CrossEntropyLoss      300      25  0.1009  \n",
       "29  CrossEntropyLoss      100      25  0.1135  \n",
       "30  CrossEntropyLoss      200      25  0.8953  \n",
       "31  CrossEntropyLoss      100      25  0.4651  \n",
       "32  CrossEntropyLoss      300      25  0.1135  \n",
       "33  CrossEntropyLoss      100      25  0.1135  \n",
       "34  CrossEntropyLoss      100      25  0.1135  \n",
       "35  CrossEntropyLoss      200      25  0.1135  \n",
       "36  CrossEntropyLoss      200      25  0.1135  \n",
       "37  CrossEntropyLoss      100      25  0.1032  \n",
       "38  CrossEntropyLoss      300      25  0.1135  \n",
       "39  CrossEntropyLoss      100      25  0.1135  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist_fixed_fc_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can get into actual evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best FC MNIST Score: 0.9315\n",
      "Average FC MNIST Score: 0.34571250000000003\n",
      "Worst FC MNIST Score: 0.0892\n",
      "Best FC FMNIST Score: 0.8762\n",
      "Average FC FMNIST Score: 0.41558375000000003\n",
      "Worst FC FMNIST Score: 0.0902\n",
      "Best FC MNIST Score: 0.9074000120162964\n",
      "Average FC MNIST Score: 0.33313416708260774\n",
      "Worst FC MNIST Score: 0.10000000149011612\n"
     ]
    }
   ],
   "source": [
    "mnist_fc_score_df = mnist_fixed_fc_df['score']\n",
    "fmnist_fc_score_df = fmnist_fixed_fc_df['score']\n",
    "fmnist_cnn_score_df = fmnist_fixed_cnn_df['accuracy']\n",
    "\n",
    "mnist_fc_scores = {'Worst':mnist_fc_score_df.min(),'Average':mnist_fc_score_df.mean(),'Best':mnist_fc_score_df.max()}\n",
    "fmnist_fc_scores = {'Worst':fmnist_fc_score_df.min(),'Average':fmnist_fc_score_df.mean(), 'Best':fmnist_fc_score_df.max()}\n",
    "fmnist_cnn_scores = {'Worst':fmnist_cnn_score_df.min(),'Average':fmnist_cnn_score_df.mean(),'Best':fmnist_cnn_score_df.max(),}\n",
    "\n",
    "overall_scores = {'MNIST FC':mnist_fc_scores,'FMNIST FC':fmnist_fc_scores,'FMNIST CNN':fmnist_cnn_scores}\n",
    "\n",
    "print(f'Best FC MNIST Score: {mnist_fc_scores[\"Best\"]}')\n",
    "print(f'Average FC MNIST Score: {mnist_fc_scores[\"Average\"]}')\n",
    "print(f'Worst FC MNIST Score: {mnist_fc_scores[\"Worst\"]}')\n",
    "print(f'Best FC FMNIST Score: {fmnist_fc_scores[\"Best\"]}')\n",
    "print(f'Average FC FMNIST Score: {fmnist_fc_scores[\"Average\"]}')\n",
    "print(f'Worst FC FMNIST Score: {fmnist_fc_scores[\"Worst\"]}')\n",
    "print(f'Best FC MNIST Score: {fmnist_cnn_scores[\"Best\"]}')\n",
    "print(f'Average FC MNIST Score: {fmnist_cnn_scores[\"Average\"]}')\n",
    "print(f'Worst FC MNIST Score: {fmnist_cnn_scores[\"Worst\"]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 2000x900 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x = np.arange(len(overall_scores.keys()))\n",
    "width = 0.25  # the width of the bars\n",
    "multiplier = 0\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(20,9),layout='constrained')\n",
    "\n",
    "for model in overall_scores:\n",
    "    for performance, score in overall_scores[model].items():\n",
    "        offset = width * multiplier\n",
    "        rects = ax.bar(offset, score, width, label=performance)\n",
    "        ax.bar_label(rects, padding=3)\n",
    "        multiplier += 1  \n",
    "    multiplier += 1\n",
    "\n",
    "# Add some text for labels, title and custom x-axis tick labels, etc.\n",
    "ax.set_ylabel('Percentage Scores')\n",
    "ax.set_title(f'Overall Model Scores')\n",
    "ax.set_xticks(x + width, overall_scores.keys())\n",
    "ax.legend(loc='upper left', ncols=3)\n",
    "ax.set_ylim(0, 1)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_fc_complexity_by_scores = mnist_fixed_fc_df.groupby('number_of_layers')['score'].mean()\n",
    "fmnist_fc_complexity_by_scores = fmnist_fixed_fc_df.groupby('number_of_layers')['score'].mean()\n",
    "fmnist_cnn_complexity_by_scores = fmnist_fixed_cnn_df.groupby('conv2d_layers')['accuracy'].mean()\n",
    "complexity_scores = {'MNIST FC':mnist_fc_complexity_by_scores,'FMNIST FC':fmnist_fc_complexity_by_scores,'FMNIST CNN':fmnist_cnn_complexity_by_scores}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 2000x900 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x = np.arange(len(complexity_scores.keys()))\n",
    "width = 0.25  # the width of the bars\n",
    "multiplier = 0\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(20,9),layout='constrained')\n",
    "\n",
    "for model in complexity_scores:\n",
    "    for performance, score in complexity_scores[model].items():\n",
    "        offset = width * multiplier\n",
    "        rects = ax.bar(offset, score, width, label=performance)\n",
    "        ax.bar_label(rects, padding=3)\n",
    "        multiplier += 1  \n",
    "    multiplier += 1\n",
    "\n",
    "# Add some text for labels, title and custom x-axis tick labels, etc.\n",
    "ax.set_ylabel('Average Percentage Scores')\n",
    "ax.set_title(f'Layer Depth Model Scores')\n",
    "ax.set_xticks(x + width+0.25, complexity_scores.keys())\n",
    "ax.legend(loc='upper right', ncols=3)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_fc_width_by_scores = mnist_fixed_fc_df.groupby(['number_of_layers',mnist_fixed_fc_df['neurons_per_layer'].map(tuple)])['score'].mean()\n",
    "fmnist_fc_width_by_scores = fmnist_fixed_fc_df.groupby(['number_of_layers',fmnist_fixed_fc_df['neurons_per_layer'].map(tuple)])['score'].mean()\n",
    "fmnist_cnn_width_by_scores = fmnist_fixed_cnn_df.groupby(['conv2d_layers',fmnist_fixed_cnn_df['conv2d_outputs'].map(tuple)])['accuracy'].mean()\n",
    "width_scores = {'MNIST FC':mnist_fc_width_by_scores,'FMNIST FC':fmnist_fc_width_by_scores,'FMNIST CNN':fmnist_cnn_width_by_scores}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "number_of_layers  neurons_per_layer   \n",
       "1                 (10, 10)                0.49695\n",
       "                  (20, 10)                0.45270\n",
       "                  (30, 10)                0.91870\n",
       "                  (40, 10)                0.15855\n",
       "                  (50, 10)                0.31150\n",
       "2                 (30, 10, 10)            0.51400\n",
       "                  (30, 20, 10)            0.44450\n",
       "                  (30, 30, 10)            0.49970\n",
       "                  (30, 40, 10)            0.10835\n",
       "                  (30, 50, 10)            0.51320\n",
       "3                 (30, 10, 10, 10)        0.23460\n",
       "                  (30, 10, 20, 10)        0.23850\n",
       "                  (30, 10, 30, 10)        0.10575\n",
       "                  (30, 10, 40, 10)        0.68100\n",
       "                  (30, 10, 50, 10)        0.10720\n",
       "4                 (30, 10, 10, 10, 10)    0.68020\n",
       "                  (30, 10, 10, 20, 10)    0.11350\n",
       "                  (30, 10, 10, 30, 10)    0.11350\n",
       "                  (30, 10, 10, 40, 10)    0.10835\n",
       "                  (30, 10, 10, 50, 10)    0.11350\n",
       "Name: score, dtype: float64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist_fc_width_by_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "number_of_layers  neurons_per_layer   \n",
       "1                 (10, 10)                0.496375\n",
       "                  (20, 10)                0.642175\n",
       "                  (30, 10)                0.722475\n",
       "                  (40, 10)                0.664500\n",
       "                  (50, 10)                0.563950\n",
       "2                 (40, 10, 10)            0.254775\n",
       "                  (40, 20, 10)            0.447050\n",
       "                  (40, 30, 10)            0.312375\n",
       "                  (40, 40, 10)            0.623675\n",
       "                  (40, 50, 10)            0.272725\n",
       "3                 (40, 10, 10, 10)        0.468875\n",
       "                  (40, 10, 20, 10)        0.588550\n",
       "                  (40, 10, 30, 10)        0.293400\n",
       "                  (40, 10, 40, 10)        0.258775\n",
       "                  (40, 10, 50, 10)        0.435775\n",
       "4                 (40, 10, 10, 10, 10)    0.253225\n",
       "                  (40, 10, 10, 20, 10)    0.348050\n",
       "                  (40, 10, 10, 30, 10)    0.464950\n",
       "                  (40, 10, 10, 40, 10)    0.100000\n",
       "                  (40, 10, 10, 50, 10)    0.100000\n",
       "Name: score, dtype: float64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fmnist_fc_width_by_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "conv2d_layers  conv2d_outputs    \n",
       "1              (32,)                 0.49282\n",
       "               (64,)                 0.40767\n",
       "               (128,)                0.45406\n",
       "2              (32, 32)              0.39972\n",
       "               (32, 64)              0.47735\n",
       "               (32, 128)             0.38506\n",
       "3              (32, 32, 32)          0.21787\n",
       "               (32, 32, 64)          0.22551\n",
       "               (32, 32, 128)         0.25851\n",
       "4              (32, 32, 128, 32)     0.28550\n",
       "               (32, 32, 128, 64)     0.24889\n",
       "               (32, 32, 128, 128)    0.14465\n",
       "Name: accuracy, dtype: float64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fmnist_cnn_width_by_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAB9sAAAOPCAYAAABrT6G/AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8pXeV/AAAACXBIWXMAAA9hAAAPYQGoP6dpAAEAAElEQVR4nOzde1hVZf7//xeKbtEUyRPiCXScIo+MZIEVmIlpKWU2JpaSh/GYB7LxVGrMKKWFliaamugntbLEMUdNHUXxq46okSBOmMLHEkkHEUwBTdfvj37sT1s2uLduZCvPx3Xt62rf973W+753683i8s29tothGIYAAAAAAAAAAAAAAIDNKpX3BAAAAAAAAAAAAAAAuNtQbAcAAAAAAAAAAAAAwE4U2wEAAAAAAAAAAAAAsBPFdgAAAAAAAAAAAAAA7ESxHQAAAAAAAAAAAAAAO1FsBwAAAAAAAAAAAADAThTbAQAAAAAAAAAAAACwE8V2AAAAAAAAAAAAAADsRLEdAAAAAAAAAAAAAAA7UWwHAAAAADhcbGysXFxcdPDgwfKeikOEh4fLxcXF/KpRo4a8vb3Vq1cvLV++XIWFhXdkHrNmzdL69euLtd/u5z1jxgy5uLioUqVKOnnyZLH+S5cuqVatWnJxcVF4ePgtxbAmIyNDLi4uio2NtfvY+Ph4ubi4KD4+/qZjjx07pldeeUXNmzdXtWrVVLduXf3pT3/S6NGjlZeXZ//EAQAAAAAQxXYAAAAAAGzi5uamffv2ad++fdq4caMiIyNVo0YNDR06VB06dNBPP/1U5nMoqdjuKPfdd5+WL19erH3t2rW6evWqqlSpUmaxy8q3336rDh06KDU1VdOmTdOWLVu0aNEiPfPMM/rmm290/vz58p4iAAAAAOAu5VreEwAAAAAAwBnk5+fLzc2txP5KlSrp0UcftWgbMGCAXn31VT377LPq06eP9u/fX9bTLFN9+/bVihUr9Pbbb6tSpf/7+/xly5bp+eef14YNG8pxdrdm3rx5qlSpkuLj41WzZk1ze58+ffS3v/1NhmHcsblcvnxZ1atXv2PxAAAAAABli53tAAAAAIByUVBQoNdff13t27eXu7u77r//fgUEBOgf//iHxbguXbrowQcfLFYUNQxDf/jDH/TMM8+Y265cuaK///3vevDBB2UymVSvXj29+uqrOnfunMWx3t7eevbZZ7Vu3Tr5+fmpWrVqevvtt29pHSEhIRo6dKj+/e9/a/fu3RZ9n3/+uQICAlSjRg3dd9996tatm7799luLMeHh4brvvvt09OhRdenSRTVq1FC9evU0evRoXb582TzOxcVFly5d0ooVK8yPsw8ODrY418WLFzVixAjVrVtXderUUe/evZWZmWnzWgYNGqQff/xR27ZtM7elpaVpz549GjRokNVjTp06pZdffln169eXyWSSr6+v3n//fV2/ft1iXGZmpv785z+rZs2acnd3V9++fZWVlWX1nAcPHlSvXr10//33q1q1avLz89MXX3xh8zp+Lzs7W7Vq1dJ9991ntd/FxcXi/ZYtW9SlSxe5u7urevXq8vX1VVRUlMWYDRs2KCAgQNWrV1fNmjXVtWtX7du3z2JM0aP5Dx8+rD59+sjDw0MtWrSQ9Nu1u3DhQrVv315ubm7y8PBQnz59ij3C/9tvv9Wzzz5r/my9vLz0zDPP3JGnKAAAAAAAbo5iOwAAAACgXBQWFur8+fOaMGGC1q9frzVr1uixxx5T7969tXLlSvO4sWPH6vvvv9e//vUvi+M3b96sEydOaNSoUZKk69evKzQ0VO+8847CwsL0z3/+U++88462bdum4OBg5efnWxx/+PBhvfHGGxozZoy2bNmiF1544ZbX0qtXL0myKLbPmjVL/fr100MPPaQvvvhC//M//6OLFy/q8ccfV2pqqsXxV69eVY8ePdSlSxetX79eo0eP1uLFi9W3b1/zmH379snNzU09evQwP85+4cKFFucZMmSIqlSpotWrV2v27NmKj4/Xyy+/bPM6WrZsqccff1yffPKJue2TTz6Rt7e3unTpUmz8uXPnFBgYqK1bt+pvf/ubNmzYoKeeekoTJkzQ6NGjzePy8/P11FNPaevWrYqKitLatWvl6elpsb4iO3fuVKdOnXThwgUtWrRI//jHP9S+fXv17dv3lr7bPSAgQGfOnFH//v21a9euYtfB7y1btkw9evTQ9evXtWjRIn399dcaM2aMRXF79erVCg0NVa1atbRmzRotW7ZMOTk5Cg4O1p49e4qds3fv3vrDH/6gtWvXatGiRZKkYcOGady4cXrqqae0fv16LVy4UEePHlVgYKB+/vlnSdKlS5fUtWtX/fzzz/roo4+0bds2zZs3T02bNtXFixft/hwAAAAAAGXAAAAAAADAwZYvX25IMhITE20+5tdffzWuXr1qDB482PDz8zO3X7t2zWjevLkRGhpqMb579+5GixYtjOvXrxuGYRhr1qwxJBlfffWVxbjExERDkrFw4UJzW7NmzYzKlSsb33//vU1zGzhwoFGjRo0S+48dO2ZIMkaMGGEYhmGcOnXKcHV1NV577TWLcRcvXjQ8PT2NP//5zxbnlmR88MEHFmNnzpxpSDL27NljbqtRo4YxcODAYvGLPu+RI0datM+ePduQZJw5c6bU9U2fPt2QZJw7d85Yvny5YTKZjOzsbOPXX381GjZsaMyYMcNq/EmTJhmSjH//+98W5xsxYoTh4uJi/nxjYmIMScY//vEPi3FDhw41JBnLly83tz344IOGn5+fcfXqVYuxzz77rNGwYUPj2rVrhmEYxs6dOw1Jxs6dO0tdW0FBgfHcc88ZkgxJRuXKlQ0/Pz9j6tSpxtmzZ83jLl68aNSqVct47LHHzNfUja5du2Z4eXkZbdq0Mc+j6Nj69esbgYGBxT7TadOmWZxj3759hiTj/ffft2j/8ccfDTc3N+Ovf/2rYRiGcfDgQUOSsX79+lLXBwAAAAAoP+xsBwAAAACUm7Vr16pTp06677775OrqqipVqmjZsmU6duyYeUylSpU0evRobdy4UadOnZIknThxQlu2bNHIkSPNjwHfuHGjateurZ49e+rXX381v9q3by9PT0/Fx8dbxG7btq3++Mc/OmQdxg2PuP/mm2/066+/asCAARZzqVatmoKCgorNRZL69+9v8T4sLEzSbzu9bVW0w75I27ZtJUn/+7//a/M5XnzxRVWtWlWrVq3Spk2blJWVpfDwcKtjd+zYoYceekgdO3a0aA8PD5dhGNqxY4d5DTVr1iw2v6I1Fvnhhx/0n//8x/xZ/P6z69Gjh86cOaPvv//e5rVIkslkUlxcnFJTUzV37ly99NJLOnfunGbOnClfX1/z+fbu3au8vDyLa+pG33//vTIzM/XKK69YfKf9fffdpxdeeEH79++3ePS/pGJPTNi4caNcXFz08ssvW6zP09NT7dq1M18bf/jDH+Th4aGJEydq0aJFxZ6GAAAAAAAofxTbAQAAAADlYt26dfrzn/+sRo0a6dNPP9W+ffuUmJioQYMGqaCgwGLsoEGD5ObmZn4M90cffSQ3NzeL7xH/+eefdeHCBVWtWlVVqlSxeGVlZem///2vxTkbNmzosLUUFbO9vLzMc5Gkhx9+uNhcPv/882JzcXV1VZ06dSzaPD09Jf32neO2uvEcJpNJkkp9dPqNatSoob59++qTTz7RsmXL9NRTT6lZs2ZWx2ZnZ1v9HIs+h6K5Z2dnq0GDBsXGFa2xSNHnNmHChGKf28iRIyWp2GdnK19fX40bN06ffvqpTp06pejoaGVnZ+utt96S9Nsj8SWpcePGJZ6jaD0lrfn69evKycmxaL9x7M8//yzDMNSgQYNia9y/f795fe7u7tq1a5fat2+vKVOmqFWrVvLy8tL06dN19erVW/oMAAAAAACO5VreEwAAAAAAVEyffvqpfHx89Pnnn1vsJC4sLCw21t3dXQMHDtTSpUs1YcIELV++XGFhYapdu7Z5TN26dVWnTh1t2bLFaryaNWtavC9p9/Kt2LBhgyQpODjYPBdJ+vLLL0ssVP/er7/+quzsbItieVZWlqTiBfQ7YdCgQVq6dKmOHDmiVatWlTiuTp06OnPmTLH2zMxMSf/3OdSpU0cHDhwoNq5ojUWKxk+ePFm9e/e2GvOBBx6wbRGlcHFx0fjx4xUZGamUlBRJUr169STJ4vvZb1T0/6KkNVeqVEkeHh7FYv1e3bp15eLiooSEBPMfQ/ze79vatGmjzz77TIZh6MiRI4qNjVVkZKTc3Nw0adIkG1cLAAAAACgr7GwHAAAAAJQLFxcXVa1a1aIYmZWVpX/84x9Wx48ZM0b//e9/1adPH124cEGjR4+26H/22WeVnZ2ta9euyd/fv9jLEUVaa7Zt26alS5cqMDBQjz32mCSpW7ducnV11YkTJ6zOxd/fv9h5bixqr169WtL/FfCl3wqx9uxSv1UBAQEaNGiQnn/+eT3//PMljuvSpYtSU1N1+PBhi/aVK1fKxcVFnTt3liR17txZFy9eNP9RQpGiNRZ54IEH1LJlS3333Xclfm43/tHEzVgrjEu/Fcfz8vLMu/ADAwPl7u6uRYsWFftagN/Pr1GjRlq9erXFmEuXLumrr75SQECAqlevXup8nn32WRmGodOnT1tdX5s2bYod4+Lionbt2mnu3LmqXbt2sc8bAAAAAFA+2NkOAAAAACgzO3bsUEZGRrH2Hj166Nlnn9W6des0cuRI9enTRz/++KP+9re/qWHDhjp+/HixY/74xz/q6aef1ubNm/XYY4+pXbt2Fv0vvfSSVq1apR49emjs2LHq2LGjqlSpop9++kk7d+5UaGhoqYXjm7l+/br2798v6bfd96dOndLmzZv1xRdfyNfXV1988YV5rLe3tyIjIzV16lSdPHlSTz/9tDw8PPTzzz/rwIEDqlGjht5++23z+KpVq+r999/XL7/8oocfflh79+7V3//+d3Xv3t1cwJd+2+kcHx+vr7/+Wg0bNlTNmjXL7I8Ili1bdtMx48eP18qVK/XMM88oMjJSzZo10z//+U8tXLhQI0aM0B//+EdJ0oABAzR37lwNGDBAM2fOVMuWLbVp0yZ98803xc65ePFide/eXd26dVN4eLgaNWqk8+fP69ixYzp8+LDWrl1r1zr+8pe/6MKFC3rhhRfUunVrVa5cWf/5z380d+5cVapUSRMnTpT02/euv//++xoyZIieeuopDR06VA0aNNAPP/yg7777TgsWLFClSpU0e/Zs9e/fX88++6yGDRumwsJCzZkzRxcuXNA777xz0/l06tRJf/nLX/Tqq6/q4MGDeuKJJ1SjRg2dOXNGe/bsUZs2bTRixAht3LhRCxcu1HPPPafmzZvLMAytW7dOFy5cUNeuXe36DAAAAAAAZYNiOwAAAACgzBQVMm+Unp6uV199VWfPntWiRYv0ySefqHnz5po0aZJ++ukni0L07/Xt21ebN28utqtdkipXrqwNGzbogw8+0P/8z/8oKipKrq6uaty4sYKCgqzuGLZHfn6+AgICJElubm6qV6+e2rVrpyVLlqh///6qWrWqxfjJkyfroYce0gcffKA1a9aosLBQnp6eevjhhzV8+HCLsVWqVNHGjRs1ZswY/f3vf5ebm5uGDh2qOXPmWIz74IMPNGrUKL300ku6fPmygoKCFB8ff1vruh316tXT3r17NXnyZE2ePFl5eXlq3ry5Zs+erYiICPO46tWra8eOHRo7dqwmTZokFxcXhYSE6LPPPlNgYKDFOTt37qwDBw5o5syZGjdunHJyclSnTh099NBD+vOf/2z3HF977TV9/vnnWrJkiU6fPq1Lly6pXr16CggI0MqVK/Xoo4+axw4ePFheXl569913NWTIEBmGIW9vbw0cONA8JiwsTDVq1FBUVJT69u2rypUr69FHH9XOnTuLraUkixcv1qOPPqrFixdr4cKFun79ury8vNSpUyd17NhRktSyZUvVrl1bs2fPVmZmpqpWraoHHnhAsbGxFvMBAAAAAJQfF6OkZ6MBAAAAAOBkXnjhBe3fv18ZGRmqUqVKeU/HIcLDw/Xll1/ql19+Ke+pAAAAAAAAO7CzHQAAAADg1AoLC3X48GEdOHBAcXFxio6OvmcK7QAAAAAA4O5FsR0AAAAA4NTOnDmjwMBA1apVS8OGDdNrr71W3lMCAAAAAADgMfIAAAAAAAAAAAAAANirUnlPAAAAAAAAAAAAAACAuw3FdgAAAAAAAAAAAAAA7ESxHQAAAAAAAAAAAAAAO7mW9wTutOvXryszM1M1a9aUi4tLeU8HAAAAAAAAAAAAAOBEDMPQxYsX5eXlpUqVSt6/XuGK7ZmZmWrSpEl5TwMAAAAAAAAAAAAA4MR+/PFHNW7cuMT+Cldsr1mzpqTfPphatWqV82wAAAAAAAAAAAAAAM4kLy9PTZo0MdeWS1Lhiu1Fj46vVasWxXYAAAAAAAAAAAAAgFU3+1rykh8wDwAAAAAAAAAAAAAArKLYDgAAAAAAAAAAAACAnSi2AwAAAAAAAAAAAABgpwr3ne0AAAAAAAAAUJFcu3ZNV69eLe9pAAAAOI3KlSvL1dX1pt/JfjMU2wEAAAAAAADgHvXLL7/op59+kmEY5T0VAAAAp1K9enU1bNhQVatWveVzUGwHAAAAAAAAgHvQtWvX9NNPP6l69eqqV6/ebe/cAgAAuBcYhqErV67o3LlzSk9PV8uWLVWp0q19+zrFdgAAAAAAAAC4B129elWGYahevXpyc3Mr7+kAAAA4DTc3N1WpUkX/+7//qytXrqhatWq3dJ5bK9EDAAAAAAAAAO4K7GgHAAAo7lZ3s1ucwwHzAAAAAAAAAAAAAACgQqHYDgAAAAAAAAAAAACAnfjOdgAAAAAAAACoQLwn/fOOxst45xm7j8nOzpavr68OHDggb29vx0+qjBUWFqply5aKi4tThw4dyns6sNO/drS4o/G6PHnC7mPu9hzB7VmwYIG2bt2qDRs2lNsc3u/77B2L9frnG2/pOPKkYrtTecLOdgAAAAAAAACAU4mKilLPnj0tiiNjx45Vhw4dZDKZ1L59e6vHJScnKygoSG5ubmrUqJEiIyNlGIZdsXfv3q2ePXvKy8tLLi4uWr9+fbExhmFoxowZ8vLykpubm4KDg3X06FFzv8lk0oQJEzRx4kS7YgO2ujFHsrOz9fTTT8vLy0smk0lNmjTR6NGjlZeXZ3Gcs+SIre5U3v9eVFSUXFxcNG7cOIt2Z1rT0KFDlZiYqD179tgdvyKxdi8pkp2drcaNG8vFxUUXLlyw6CNP7o013ak8odgOAAAAAAAAAHAa+fn5WrZsmYYMGWLRbhiGBg0apL59+1o9Li8vT127dpWXl5cSExM1f/58vffee4qOjrYr/qVLl9SuXTstWLCgxDGzZ89WdHS0FixYoMTERHl6eqpr1666ePGieUz//v2VkJCgY8eO2RUfuBlrOVKpUiWFhoZqw4YNSktLU2xsrLZv367hw4ebxzhbjtjiTuV9kcTERH388cdq27atU6/JZDIpLCxM8+fPt2+BFUhJ95IigwcPtvr/mTy5d9Z0p/KEx8gDAAAAAAAAAJzG5s2b5erqqoCAAIv2Dz/8UJJ07tw5HTlypNhxq1atUkFBgWJjY2UymdS6dWulpaUpOjpaERERcnFxsSl+9+7d1b179xL7DcPQvHnzNHXqVPXu3VuStGLFCjVo0ECrV6/WsGHDJEl16tRRYGCg1qxZo8jISJtiA7awliMeHh4aMWKE+X2zZs00cuRIzZkzx9zmbDliizuV95L0yy+/qH///lqyZIn+/ve/O/2aevXqpZCQEOXn58vNzc3m+BVFSfcSSYqJidGFCxc0bdo0bd682aKPPLm31nQn8oSd7QAAAAAAAAAAp7F79275+/vbfdy+ffsUFBQkk8lkbuvWrZsyMzOVkZHhsPmlp6crKytLISEh5jaTyaSgoCDt3bvXYmzHjh2VkJDgsNiAZFuOZGZmat26dQoKCjK3OWOO3C5HrmnUqFF65pln9NRTTxXrc8Y1+fv76+rVqzpw4IBD498rSsqT1NRURUZGauXKlapUqXiZlDy5dc64pjuRJxTbAQAAAAAAAABOIyMjQ15eXnYfl5WVpQYNGli0Fb3PyspyyNx+fy5rsW6M06hRI4cWMgCp9Bzp16+fqlevrkaNGqlWrVpaunSpuc8Zc8QRsRyxps8++0yHDx9WVFRUiXF+f+7fxyqvNdWoUUO1a9fmZ0wJrOVJYWGh+vXrpzlz5qhp06ZWjyNPbi/O78/9+1j3cp5QbAcAAAAAAAAAOI38/HxVq1btlo698VG4hmFYbXcEa7FubHNzc9Ply5cdHhsVW2k5MnfuXB0+fFjr16/XiRMnFBERYdHvbDlSVnGstZfkxx9/1NixY/Xpp5/e9GePs62JnzEls5YnkydPlq+vr15++eVSjyVPHB/rXs4Tiu0AAAAAAAAAAKdRt25d5eTk2H2cp6dnsZ1zZ8+elVR8l93t8PT0lFR8N+DZs2eLxTl//rzq1avnsNiAVHqOeHp66sEHH1RoaKgWL16smJgYnTlzxtznbDniiFi3u6ZDhw7p7Nmz6tChg1xdXeXq6qpdu3bpww8/lKurq65du+a0a+JnTMms5cmOHTu0du1a8//nLl26mMdOnz5dEnlyu3Ek51tTWecJxXYAAAAAAAAAgNPw8/NTamqq3ccFBARo9+7dunLlirlt69at8vLykre3t8Pm5+PjI09PT23bts3cduXKFe3atUuBgYEWY1NSUuTn5+ew2IBke44U7fIsLCyU5Jw5crscsaYuXbooOTlZSUlJ5pe/v7/69++vpKQkVa5c2SnXdOLECRUUFPAzpgTW8uSrr77Sd999Z/7/XPQ1CwkJCRo1apQk8uR2OOOa7kSeUGwHAAAAAAAAADiNbt266ejRo8V2JP7www9KSkpSVlaW8vPzzcWSon9oDwsLk8lkUnh4uFJSUhQXF6dZs2YpIiLCrsfX/vLLL+ZzS1J6erqSkpJ06tQpSb89nnbcuHGaNWuW4uLilJKSovDwcFWvXl1hYWEW50pISFBISMhtfBpAcdZyZNOmTVq+fLlSUlKUkZGhTZs2acSIEerUqZO58OSMOXIzdyLva9asqdatW1u8atSooTp16qh169ZOu6aEhAQ1b95cLVq0sCt+RWEtT1q0aGHx/9nHx0eS5Ovrq/r160siT+61Nd2RPDEqmNzcXEOSkZubW95TAQAAAAAAAIAyk5+fb6Smphr5+fnlPRW7Pfroo8aiRYss2oKCggxJxV7p6enmMUeOHDEef/xxw2QyGZ6ensaMGTOM69evm/vT09MNScbOnTtLjL1z506rcQYOHGgec/36dWP69OmGp6enYTKZjCeeeMJITk62OM/evXuN2rVrG5cvX76tzwKw5sYc2bFjhxEQEGC4u7sb1apVM1q2bGlMnDjRyMnJsTjOmXJk4MCBRlBQUKnrvFN5by3u2LFjLdqcaU2GYRghISFGVFSUzWuqiKzdS36v6FomT+7NNRnGzfOktN+VbK0puxjG//8ckQoiLy9P7u7uys3NVa1atcp7OrhHLFy4UHPmzNGZM2fUqlUrzZs3T48//niJ4z/66CMtWLBAGRkZatq0qaZOnaoBAwaY+48ePapp06bp0KFD+t///V/NnTtX48aNszjHr7/+qhkzZmjVqlXKyspSw4YNFR4erjfffFOVKv320IqS/iJp9uzZeuONN25/4QAAAAAAAHBaBQUFSk9Pl4+Pj6pVq1be07HLpk2bNGHCBKWkpJj/rcsR4uPj9fzzz+vkyZPy8PBw2HmtefHFF+Xn56cpU6aUaRxUTPdCjgQHBys4OFgzZswo0zj34ppSUlLUpUsXpaWlyd3dvUxj3c3IE9vdi2uyJU9K+13J1pqyq0NnDVRAn3/+ucaNG6eFCxeqU6dOWrx4sbp3767U1FQ1bdq02PiYmBhNnjxZS5Ys0cMPP6wDBw5o6NCh8vDwUM+ePSVJly9fVvPmzfXiiy9q/PjxVuO+++67WrRokVasWKFWrVrp4MGDevXVV+Xu7q6xY8dKks6cOWNxzObNmzV48GC98MILDv4UAAAAAAAAAMfp0aOHjh8/rtOnT6tJkyYOO++WLVs0ZcqUMi8kFBYWql27diX+2x5wu+72HLl48aJOnDihjRs3lmkc6d5cU2ZmplauXEmh/SbIE9vdi2u6U3nCznbgNj3yyCP605/+pJiYGHObr6+vnnvuOUVFRRUbHxgYqE6dOmnOnDnmtnHjxungwYPas2dPsfHe3t4aN25csZ3tzz77rBo0aKBly5aZ21544QVVr15d//M//2N1rs8995wuXryof/3rX/YuEwAAAAAAAHeZu3lnOwAAQFlzxM52xz0zAaiArly5okOHDikkJMSiPSQkRHv37rV6TGFhYbGEdXNz04EDB3T16lWbYz/22GP617/+pbS0NEnSd999pz179qhHjx5Wx//888/65z//qcGDB9scAwAAAAAAAAAAAIB1PEYeuA3//e9/de3aNTVo0MCivUGDBsrKyrJ6TLdu3bR06VI999xz+tOf/qRDhw7pk08+0dWrV/Xf//5XDRs2tCn2xIkTlZubqwcffFCVK1fWtWvXNHPmTPXr18/q+BUrVqhmzZrq3bu3fYsEAAAAAAAAAAAAUAzFdsABXFxcLN4bhlGsrchbb72lrKwsPfroozIMQw0aNFB4eLhmz56typUr2xzz888/16effqrVq1erVatWSkpK0rhx4+Tl5aWBAwcWG//JJ5+of//+PDIMAAAAAAAAAAAAcAAeIw/chrp166py5crFdrGfPXu22G73Im5ubvrkk090+fJlZWRk6NSpU/L29lbNmjVVt25dm2O/8cYbmjRpkl566SW1adNGr7zyisaPH2/1e+ITEhL0/fffa8iQIfYtEAAAAAAAAAAAAIBVFNuB21C1alV16NBB27Zts2jftm2bAgMDSz22SpUqaty4sSpXrqzPPvtMzz77rCpVsj0lL1++XGx85cqVdf369WJjly1bpg4dOqhdu3Y2nx8AAAAAAAAAAABAyXiMPHCbIiIi9Morr8jf318BAQH6+OOPderUKQ0fPlySNHnyZJ0+fVorV66UJKWlpenAgQN65JFHlJOTo+joaKWkpGjFihXmc165ckWpqanm/z59+rSSkpJ033336Q9/+IMkqWfPnpo5c6aaNm2qVq1a6dtvv1V0dLQGDRpkMb+8vDytXbtW77///p34OAAAAAAAAAAAAIAKgWI7cJv69u2r7OxsRUZG6syZM2rdurU2bdqkZs2aSZLOnDmjU6dOmcdfu3ZN77//vr7//ntVqVJFnTt31t69e+Xt7W0ek5mZKT8/P/P79957T++9956CgoIUHx8vSZo/f77eeustjRw5UmfPnpWXl5eGDRumadOmWczvs88+k2EY6tevX9l9CAAAAAAAAAAAAEAF42IYhlHek7iT8vLy5O7urtzcXNWqVau8pwMAAAAAAAAAZaKgoEDp6eny8fFRtWrV/q9jhvudnciMXLsPyc7Olq+vrw4cOGCxSeVuUVhYqJYtWyouLk4dOnQo7+nATp47k+5ovKzO7e0+5m7PEdyeBQsWaOvWrdqwYUO5zeGnSQl3LFbjdx6/pePIk4rNljwp8Xcl2V5T5jvbAQAAAAAAAABOJSoqSj179jQXR7777jv169dPTZo0kZubm3x9ffXBBx8UOy45OVlBQUFyc3NTo0aNFBkZKXv3m+3evVs9e/aUl5eXXFxctH79+mJjDMPQjBkz5OXlJTc3NwUHB+vo0aPmfpPJpAkTJmjixIl2xQZsdWOOZGdn6+mnn5aXl5dMJpOaNGmi0aNHKy8vz+I4Z8kRW40dO1YdOnSQyWRS+/btrY5xxJpmzJghFxcXi5enp6fTrmno0KFKTEzUnj177I5fkdyYJ7+XnZ2txo0by8XFRRcuXLDoI0+si4qK0sMPP6yaNWuqfv36eu655/T999877ZruVJ5QbAcAAAAAAAAAOI38/HwtW7ZMQ4YMMbcdOnRI9erV06effqqjR49q6tSpmjx5shYsWGAek5eXp65du8rLy0uJiYmaP3++3nvvPUVHR9sV/9KlS2rXrp3FuW80e/ZsRUdHa8GCBUpMTJSnp6e6du2qixcvmsf0799fCQkJOnbsmF3xgZuxliOVKlVSaGioNmzYoLS0NMXGxmr79u0aPny4eYyz5YgtDMPQoEGD1LdvX6v9jlqTJLVq1Upnzpwxv5KTk512TSaTSWFhYZo/f77d66worOXJ7w0ePFht27Yt1k6elGzXrl0aNWqU9u/fr23btunXX39VSEiILl265JRrulN5wne2AwAAAAAAAACcxubNm+Xq6qqAgABz26BBgyzGNG/eXPv27dO6des0evRoSdKqVatUUFCg2NhYmUwmtW7dWmlpaYqOjlZERIRcXFxsit+9e3d17969xH7DMDRv3jxNnTpVvXv3liStWLFCDRo00OrVqzVs2DBJUp06dRQYGKg1a9YoMjLSrs8AKI21HPHw8NCIESPM75s1a6aRI0dqzpw55jZnyxFbfPjhh5Kkc+fO6ciRI8X6HbUmSXJ1dS22m92Z19SrVy+FhIQoPz9fbm5uNsevKKzlSZGYmBhduHBB06ZN0+bNmy36yJOSbdmyxeL98uXLVb9+fR06dEhPPPGEU67pTuQJO9sBAAAAAAAAAE5j9+7d8vf3v+m43Nxc3X///eb3+/btU1BQkEwmk7mtW7duyszMVEZGhsPml56erqysLIWEhJjbTCaTgoKCtHfvXouxHTt2VELCnfteY1QMtuRIZmam1q1bp6CgIHObM+bI7XLkmo4fPy4vLy/5+PjopZde0smTJ819zrgmf39/Xb16VQcOHHBo/HtFSXmSmpqqyMhIrVy5UpUqFS+Tkie2y83NlSTzvdgZ13Qn8oRiOwAAAAAAAADAaWRkZMjLy6vUMfv27dMXX3xhsUsuKytLDRo0sBhX9D4rK8th8ys6l7VYN8Zp1KiRQ4szgFR6jvTr10/Vq1dXo0aNVKtWLS1dutTc54w54ohYjljTI488opUrV+qbb77RkiVLlJWVpcDAQGVnZ1ucy5nWVKNGDdWuXZufMSWwlieFhYXq16+f5syZo6ZNm1o9jjyxjWEYioiI0GOPPabWrVtbnMuZ1nQn8oTHyANlpM2KNuU9BSUPTL75IAAAAAAAAMCJ5Ofnq1q1aiX2Hz16VKGhoZo2bZq6du1q0Xfjo3ANw7Da7gjWYt3Y5ubmpsuXLzs8Niq20nJk7ty5mj59ur7//ntNmTJFERERWrhwobnf2XKkrOJYay/N7x/33aZNGwUEBKhFixZasWKFIiIiSo1VnmviZ0zJrOXJ5MmT5evrq5dffrnUY8mTmxs9erSOHDmiPXv22BTrXs4TdrYDAAAAAAAAAJxG3bp1lZOTY7UvNTVVTz75pIYOHao333zTos/T07PYzrmzZ89KKr7L7nYUfaeztVg3xjl//rzq1avnsNiAVHqOeHp66sEHH1RoaKgWL16smJgYnTlzxtznbDniiFhlsaYaNWqoTZs2On78uDmO5Hxr4mdMyazlyY4dO7R27Vq5urrK1dVVXbp0MY+dPn26JPLEFq+99po2bNignTt3qnHjxhZxJOdbU1nnCcV2AAAAAAAAAIDT8PPzU2pqarH2o0ePqnPnzho4cKBmzpxZrD8gIEC7d+/WlStXzG1bt26Vl5eXvL29HTY/Hx8feXp6atu2bea2K1euaNeuXQoMDLQYm5KSIj8/P4fFBqSSc+RGRbs8CwsLJTlnjtyuslpTYWGhjh07poYNG0pyzjWdOHFCBQUF/IwpgbU8+eqrr/Tdd98pKSlJSUlJ5q9ZSEhI0KhRoySRJ6UxDEOjR4/WunXrtGPHDvn4+Fj0O+Oa7kSeUGwHAAAAAAAAADiNbt266ejRoxY7EosK7V27dlVERISysrKUlZWlc+fOmceEhYXJZDIpPDxcKSkpiouL06xZsxQREWHX42t/+eUXcyFGktLT05WUlKRTp05J+u3xtOPGjdOsWbMUFxenlJQUhYeHq3r16goLC7M4V0JCgkJCQm7j0wCKs5YjmzZt0vLly5WSkqKMjAxt2rRJI0aMUKdOncyFJ2fMkZv54YcflJSUpKysLOXn55vjFhXYHLWmCRMmaNeuXUpPT9e///1v9enTR3l5eRo4cKDTrikhIUHNmzdXixYt7IpfUVjLkxYtWqh169bmV1Gx2NfXV/Xr15dEnpRm1KhR+vTTT7V69WrVrFnTfC/Oz8932jXdkTwxKpjc3FxDkpGbm1veU8E9rnVs63J/AQAAAAAAoOLKz883UlNTjfz8/PKeit0effRRY9GiReb306dPNyQVezVr1sziuCNHjhiPP/64YTKZDE9PT2PGjBnG9evXzf3p6emGJGPnzp0lxt65c6fVWAMHDjSPuX79ujF9+nTD09PTMJlMxhNPPGEkJydbnGfv3r1G7dq1jcuXL9/WZwFYc2OO7NixwwgICDDc3d2NatWqGS1btjQmTpxo5OTkWBznTDkycOBAIygoqNR1BgUFWY2Vnp7u0DX17dvXaNiwoVGlShXDy8vL6N27t3H06FGLMc60JsMwjJCQECMqKqrUWBXdjXlyo6JrmTyxbU3WYkgyli9f7pRrMoyb50lpvyvZWlN2MYz//zkiFUReXp7c3d2Vm5urWrVqlfd0cA9rs6JNeU9ByQOTy3sKAAAAAAAAKCcFBQVKT0+Xj4+PqlWrVt7TscumTZs0YcIEpaSkqFIlxz2gNT4+Xs8//7xOnjwpDw8Ph53XmhdffFF+fn6aMmVKmcZBxXQv5EhwcLCCg4M1Y8aMMo1zL64pJSVFXbp0UVpamtzd3cs01t2MPLHdvbgmW/KktN+VbK0puzp01gAAAAAAAAAA3KYePXro+PHjOn36tJo0aeKw827ZskVTpkwp80JCYWGh2rVrp/Hjx5dpHFRcd3uOXLx4USdOnNDGjRvLNI50b64pMzNTK1eupNB+E+SJ7e7FNd2pPGFnO1BG2NkOAAAAAACA8nQ372wHAAAoa47Y2e64ZyYAAAAAAAAAAAAAAFBBUGwHAAAAAAAAAAAAAMBOFNsBAAAAAAAAAAAAALATxXYAAAAAAAAAAAAAAOxEsR0AAAAAAAAAAAAAADtRbAcAAAAAAAAAAAAAwE4U2wEAAAAAAAAAAAAAsJNreU8AAAAAAAAAAHDntFnR5o7GSx6YbPcx2dnZ8vX11YEDB+Tt7e34SZWxwsJCtWzZUnFxcerQoUN5Twd28p70zzsaL+OdZ+w+hhxBeZsxY4bTx7rb8wS3Z8GCBdq6das2bNhQpnHY2Q4AAAAAAAAAcCpRUVHq2bOnuTiSnZ2tp59+Wl5eXjKZTGrSpIlGjx6tvLw8i+OSk5MVFBQkNzc3NWrUSJGRkTIMw67Yu3fvVs+ePeXl5SUXFxetX7++2BjDMDRjxgx5eXnJzc1NwcHBOnr0qLnfZDJpwoQJmjhxot1rB2xxY45I0tixY9WhQweZTCa1b9/e6nHkCCoSa3lSJDs7W40bN5aLi4suXLhg0ecseWKrO5X7MTExatu2rWrVqqVatWopICBAmzdvdto1DR06VImJidqzZ4/d8e1BsR0AAAAAAAAA4DTy8/O1bNkyDRkyxNxWqVIlhYaGasOGDUpLS1NsbKy2b9+u4cOHm8fk5eWpa9eu8vLyUmJioubPn6/33ntP0dHRdsW/dOmS2rVrpwULFpQ4Zvbs2YqOjtaCBQuUmJgoT09Pde3aVRcvXjSP6d+/vxISEnTs2DG74gM3Yy1HpN+KXIMGDVLfvn2tHkeOoCIpKU+KDB48WG3bti3W7mx5Yos7lfuNGzfWO++8o4MHD+rgwYN68sknFRoaalFMd6Y1mUwmhYWFaf78+XbFthePkQcAAAAAAAAAOI3NmzfL1dVVAQEB5jYPDw+NGDHC/L5Zs2YaOXKk5syZY25btWqVCgoKFBsbK5PJpNatWystLU3R0dGKiIiQi4uLTfG7d++u7t27l9hvGIbmzZunqVOnqnfv3pKkFStWqEGDBlq9erWGDRsmSapTp44CAwO1Zs0aRUZG2vUZAKWxliOS9OGHH0qSzp07pyNHjhQ7jhxBRVJSnki/7dC+cOGCpk2bVmxntrPliS3uVO737NnT4v3MmTMVExOj/fv3q1WrVk65pl69eikkJET5+flyc3OzOb492NkOAAAAAAAAAHAau3fvlr+/f6ljMjMztW7dOgUFBZnb9u3bp6CgIJlMJnNbt27dlJmZqYyMDIfNLz09XVlZWQoJCTG3mUwmBQUFae/evRZjO3bsqISEBIfFBiTbcsQacgQVSUl5kpqaqsjISK1cuVKVKhUvkzpjntyusljTtWvX9Nlnn+nSpUvmP2hwxjX5+/vr6tWrOnDggEPj/x7FdgAAAAAAAACA08jIyJCXl5fVvn79+ql69epq1KiRatWqpaVLl5r7srKy1KBBA4vxRe+zsrIcNr+ic1mLdWOcRo0aObQ4A0il50hpyBFUJNbypLCwUP369dOcOXPUtGlTq8c5Y544Ipaj1pScnKz77rtPJpNJw4cPV1xcnB566CGLcznTmmrUqKHatWuX6c8Ziu0AAAAAAAAAAKeRn5+vatWqWe2bO3euDh8+rPXr1+vEiROKiIiw6L/xUbiGYVhtdwRrsW5sc3Nz0+XLlx0eGxVbaTlyM+QIKgpreTJ58mT5+vrq5ZdfLvVYZ8uTsopjrf1mHnjgASUlJWn//v0aMWKEBg4cqNTU1JvGKs81lfXPGYrtAAAAAAAAAACnUbduXeXk5Fjt8/T01IMPPqjQ0FAtXrxYMTExOnPmjLnvxp1zZ8+elVR8l93t8PT0lFR8N+DZs2eLxTl//rzq1avnsNiAVHqOlIYcQUViLU927NihtWvXytXVVa6ururSpYt57PTp0yU5Z544Ipaj1lS1alX94Q9/kL+/v6KiotSuXTt98MEH5jiS862prH/OUGwHAAAAAAAAADgNPz+/YrvkrCnawVZYWChJCggI0O7du3XlyhXzmK1bt8rLy0ve3t4Om5+Pj488PT21bds2c9uVK1e0a9cuBQYGWoxNSUmRn5+fw2IDku05ciNyBBWJtTz56quv9N133ykpKUlJSUnmryJJSEjQqFGjJDlnntyuslyTYRjm+7AzrunEiRMqKCgo058zFNsBAAAAAAAAAE6jW7duOnr0qMWOxE2bNmn58uVKSUlRRkaGNm3apBEjRqhTp07mf1QPCwuTyWRSeHi4UlJSFBcXp1mzZikiIsKux9f+8ssv5kKMJKWnpyspKUmnTp2S9NvjaceNG6dZs2YpLi5OKSkpCg8PV/Xq1RUWFmZxroSEBIWEhNzeBwLcwFqOSNIPP/ygpKQkZWVlKT8/33wdFxWjyBFUJNbypEWLFmrdurX55ePjI0ny9fVV/fr1JTlnntzMncr9KVOmKCEhQRkZGUpOTtbUqVMVHx+v/v37O+2aEhIS1Lx5c7Vo0cKu+PZwMYr+/K+CyMvLk7u7u3Jzc1WrVq3yng7uYW1WtCnvKSh5YHJ5TwEAAAAAAADlpKCgQOnp6fLx8bnl73cuLwEBAQoPD9ewYcMkSTt37tTUqVOVmpqqwsJCNWnSRL1799akSZNUu3Zt83HJyckaNWqUDhw4IA8PDw0fPlzTpk0z/8N7RkaGfHx8tHPnTgUHB1uNHR8fr86dOxdrHzhwoGJjYyX9tpPv7bff1uLFi5WTk6NHHnlEH330kVq3bm0ev2/fPvXo0UOZmZlyc3NzzAcD/P9uzBFJCg4O1q5du4qNTU9PN/9RCjmCisRanvxe0bWck5PjtPeS8PBwZWRkKD4+vsR13qncHzx4sP71r3/pzJkzcnd3V9u2bTVx4kR17drVPMaZ1iT99kcXnTt31qRJk6zGKe13JVtryhTbgTJCsR0AAAAAAADl6W4utm/atEkTJkxQSkqKKlVy3ANa4+Pj9fzzz+vkyZPy8PBw2HmtefHFF+Xn56cpU6aUaRxUTOQIcHP3Qp4EBwcrODhYM2bMKNM49+KaUlJS1KVLF6Wlpcnd3d3qGEcU210dOmsAAAAAAAAAAG5Tjx49dPz4cZ0+fVpNmjRx2Hm3bNmiKVOmlHkhobCwUO3atdP48ePLNA4qLnIEuLm7PU8uXryoEydOaOPGjWUaR7o315SZmamVK1eWWGh3FHa2A2WEne0AAAAAAAAoT3fzznYAAICy5oid7Y57ZgIAAAAAAAAAAAAAABUExXYAAAAAAAAAAAAAAOxEsR0AAAAAAAAAAAAAADtRbAcAAAAAAAAAAAAAwE4U2wEAAAAAAAAAAAAAsBPFdgAAAAAAAAAAAAAA7ESxHQAAAAAAAAAAAAAAO7mW9wQAAAAAAAAAAHfOsQd972g83/8cs/uY7Oxs+fr66sCBA/L29nb8pMpYYWGhWrZsqbi4OHXo0KG8pwN7zXC/w/Fy7T6EHEF5+9eOFncsVpcnT9zScXd7nuD2LFiwQFu3btWGDRvKNA472wEAAAAAAAAATiUqKko9e/a0WhzJzs5W48aN5eLiogsXLlj0JScnKygoSG5ubmrUqJEiIyNlGIZdsXfv3q2ePXvKy8tLLi4uWr9+fbExhmFoxowZ8vLykpubm4KDg3X06FFzv8lk0oQJEzRx4kS7YgO2ujFHvvvuO/Xr109NmjSRm5ubfH199cEHHxQ7jhxBRXK330tsNXbsWHXo0EEmk0nt27e3OsYRa/q9qKgoubi4aNy4cRbtzrSmoUOHKjExUXv27LE7vj0otgMAAAAAAAAAnEZ+fr6WLVumIUOGWO0fPHiw2rZtW6w9Ly9PXbt2lZeXlxITEzV//ny99957io6Otiv+pUuX1K5dOy1YsKDEMbNnz1Z0dLQWLFigxMREeXp6qmvXrrp48aJ5TP/+/ZWQkKBjx+zf2Q+UxlqOHDp0SPXq1dOnn36qo0ePaurUqZo8ebLFdUyOoCK5V+4ltjAMQ4MGDVLfvn2t9jtqTUUSExP18ccfW/38nGlNJpNJYWFhmj9/vn0LtBOPkQcAAAAAAAAAOI3NmzfL1dVVAQEBxfpiYmJ04cIFTZs2TZs3b7boW7VqlQoKChQbGyuTyaTWrVsrLS1N0dHRioiIkIuLi03xu3fvru7du5fYbxiG5s2bp6lTp6p3796SpBUrVqhBgwZavXq1hg0bJkmqU6eOAgMDtWbNGkVGRtq6fOCmrOXIoEGDLMY0b95c+/bt07p16zR69GhJ5AgqlnvlXmKLDz/8UJJ07tw5HTlypFi/o9YkSb/88ov69++vJUuW6O9//7vTr6lXr14KCQlRfn6+3NzcbI5vD3a2AwAAAAAAAACcxu7du+Xv71+sPTU1VZGRkVq5cqUqVSr+T9v79u1TUFCQTCaTua1bt27KzMxURkaGw+aXnp6urKwshYSEmNtMJpOCgoK0d+9ei7EdO3ZUQkKCw2IDUsk5cqPc3Fzdf//95vfkCCqSe+lecrscuaZRo0bpmWee0VNPPVWszxnX5O/vr6tXr+rAgQMOjf97FNsBAAAAAAAAAE4jIyNDXl5eFm2FhYXq16+f5syZo6ZNm1o9LisrSw0aNLBoK3qflZXlsPkVnctarBvjNGrUyKHFGUCyniM32rdvn7744guLnaTkCCqSe+le4ohYjljTZ599psOHDysqKqrEOL8/9+9jldeaatSoodq1a5fpzxkeIw8AAAAAAAAAcBr5+fmqVq2aRdvkyZPl6+url19+udRjb3wUrmEYVtsdwVqsG9vc3Nx0+fJlh8dGxWYtR37v6NGjCg0N1bRp09S1a1eLPnIEFcW9dC8pqzjW2kvy448/auzYsdq6dWupP39KilWeayrrnzPsbAcAAAAAAAAAOI26desqJyfHom3Hjh1au3atXF1d5erqqi5dupjHTp8+XZLk6elZbOfc2bNnJRXfZXc7PD09JRXfDXj27Nlicc6fP6969eo5LDYgWc+RIqmpqXryySc1dOhQvfnmmxZ95AgqknvpXuKIWLe7pkOHDuns2bPq0KGD+fPbtWuXPvzwQ7m6uuratWtOu6ay/jlDsR0AAAAAAAAA4DT8/PyUmppq0fbVV1/pu+++U1JSkpKSkrR06VJJUkJCgkaNGiVJCggI0O7du3XlyhXzcVu3bpWXl5e8vb0dNj8fHx95enpq27Zt5rYrV65o165dCgwMtBibkpIiPz8/h8UGJOs5Iv22o71z584aOHCgZs6cWayfHEFFci/dS26XI9bUpUsXJScnmz+7pKQk+fv7q3///kpKSlLlypWdck0nTpxQQUFBmf6codgOAAAAAAAAAHAa3bp109GjRy12JLZo0UKtW7c2v3x8fCRJvr6+ql+/viQpLCxMJpNJ4eHhSklJUVxcnGbNmqWIiAi7Hl/7yy+/mAsJkpSenq6kpCSdOnVK0m+Ppx03bpxmzZqluLg4paSkKDw8XNWrV1dYWJjFuRISEhQSEnI7HwdQjLUcKSq0d+3aVREREcrKylJWVpbOnTtnHkOOoCK5l+4lN/PDDz8oKSlJWVlZys/PN8ctKkQ7Yk01a9a0+Oxat26tGjVqqE6dOmrdurXTrikhIUHNmzdXixYt7IpvD76zHQAAAAAAAAAqEN//HCvvKZSqTZs28vf31xdffKFhw4bZfJy7u7u2bdumUaNGyd/fXx4eHoqIiFBERIR5TEZGhnx8fLRz504FBwdbPc/BgwfVuXNn8/ui4wcOHKjY2FhJ0l//+lfl5+dr5MiRysnJ0SOPPKKtW7eqZs2a5uP27dun3Nxc9enTx47VwynMyC3vGZTKWo6sXbtW586d06pVq7Rq1Srz2GbNmikjI0MSOQLH6vLkifKeQqnulXtJeHi4MjIyFB8fX+KchwwZol27dpnfF+3iTk9Pl7e3t8PWZAtnWpMkrVmzRkOHDr3l9djCxSj6tvgKIi8vT+7u7srNzVWtWrXKezq4h7VZ0aa8p6DkgcnlPQUAAAAAAACUk4KCAqWnp8vHx0fVqlUr7+nYZdOmTZowYYJSUlJUqZLjHtAaHx+v559/XidPnpSHh4fDzmvNiy++KD8/P02ZMqVM46BiIkeAm7sX8iQ4OFjBwcGaMWNGmca5F9eUkpKiLl26KC0tTe7u7lbHlPa7kq01ZXa2AwAAAAAAAACcSo8ePXT8+HGdPn1aTZo0cdh5t2zZoilTppR5IaGwsFDt2rXT+PHjyzQOKi5yBLi5uz1PLl68qBMnTmjjxo1lGke6N9eUmZmplStXllhodxR2tgNlhJ3tAAAAAAAAKE938852AACAsuaIne2Oe2YCAAAAAAAAAAAAAAAVBMV2AAAAAAAAAAAAAADsRLEdAAAAAAAAAAAAAAA7UWwHAAAAAAAAAAAAAMBOFNsBAAAAAAAAAAAAALATxXYAAAAAAAAAAAAAAOxEsR0AAAAAAAAAAAAAADu5lvcEAAAAAAAAAAB3zkfDd9zReKMWPWn3MdnZ2fL19dWBAwfk7e3t+EmVscLCQrVs2VJxcXHq0KFDeU8Hdmqzos0djZc8MNnuY8gRlDfPnUl3LFZW5/a3dNzdnie4PQsWLNDWrVu1YcOGMo3DznYAAAAAAAAAgFOJiopSz549LYojLi4uxV6LFi2yOC45OVlBQUFyc3NTo0aNFBkZKcMw7Iq9e/du9ezZU15eXnJxcdH69euLjTEMQzNmzJCXl5fc3NwUHByso0ePmvtNJpMmTJigiRMn2hUbsNWNOZKdna2nn35aXl5eMplMatKkiUaPHq28vDyL48gRVCTW7iVFsrOz1bhxY7m4uOjChQsWfc6SJ7YaO3asOnToIJPJpPbt21sd44g1zZgxo9h92NPT02nXNHToUCUmJmrPnj12x7cHxXYAAAAAAAAAgNPIz8/XsmXLNGTIkGJ9y5cv15kzZ8yvgQMHmvvy8vLUtWtXeXl5KTExUfPnz9d7772n6Ohou+JfunRJ7dq104IFC0ocM3v2bEVHR2vBggVKTEyUp6enunbtqosXL5rH9O/fXwkJCTp27Jhd8YGbsZYjlSpVUmhoqDZs2KC0tDTFxsZq+/btGj58uHkMOYKKpLR7iSQNHjxYbdu2LdbubHliC8MwNGjQIPXt29dqv6PWJEmtWrWyuA8nJ1s+mcOZ1mQymRQWFqb58+fbvU578Bh5AAAAAAAAAIDT2Lx5s1xdXRUQEFCsr3bt2sV20RVZtWqVCgoKFBsbK5PJpNatWystLU3R0dGKiIiQi4uLTfG7d++u7t27l9hvGIbmzZunqVOnqnfv3pKkFStWqEGDBlq9erWGDRsmSapTp44CAwO1Zs0aRUZG2hQbsIW1HPHw8NCIESPM75s1a6aRI0dqzpw55jZyBBVJafeSmJgYXbhwQdOmTdPmzZst+pwtT2zx4YcfSpLOnTunI0eOFOt31JokydXVtcT7sDOuqVevXgoJCVF+fr7c3Nxsjm8PdrYDAAAAAAAAAJzG7t275e/vb7Vv9OjRqlu3rh5++GEtWrRI169fN/ft27dPQUFBMplM5rZu3bopMzNTGRkZDptfenq6srKyFBISYm4zmUwKCgrS3r17LcZ27NhRCQkJDosNSKXnSJHMzEytW7dOQUFB5jZyBBVJSXmSmpqqyMhIrVy5UpUqFS+TOmOe3C5Hrun48ePy8vKSj4+PXnrpJZ08edLc54xr8vf319WrV3XgwAGHxv89iu0AAAAAAAAAAKeRkZEhLy+vYu1/+9vftHbtWm3fvl0vvfSSXn/9dc2aNcvcn5WVpQYNGlgcU/Q+KyvLYfMrOpe1WDfGadSokUOLM4BUco5IUr9+/VS9enU1atRItWrV0tKlS8195AgqEmt5UlhYqH79+mnOnDlq2rSp1eOcMU8cEcsRa3rkkUe0cuVKffPNN1qyZImysrIUGBio7Oxsi3M505pq1Kih2rVrl+nPGYrtAAAAAAAAAACnkZ+fr2rVqhVrf/PNNxUQEKD27dvr9ddfV2RkpMUjsiUVexSuYRhW2x3BWqwb29zc3HT58mWHx0bFVlKOSNLcuXN1+PBhrV+/XidOnFBERIRFPzmCisJankyePFm+vr56+eWXSz3W2fKkrOJYay9N9+7d9cILL6hNmzZ66qmn9M9//lPSb4+Kv1ms8lxTWf+codgOAAAAAAAAAHAadevWVU5Ozk3HPfroo8rLy9PPP/8sSfL09Cy2c+7s2bOSiu+yux1F31VrLdaNcc6fP6969eo5LDYglZ4jnp6eevDBBxUaGqrFixcrJiZGZ86cMfeRI6gorOXJjh07tHbtWrm6usrV1VVdunQxj50+fbok58wTR8QqizXVqFFDbdq00fHjx81xJOdbU1n/nKHYDgAAAAAAAABwGn5+fkpNTb3puG+//VbVqlVT7dq1JUkBAQHavXu3rly5Yh6zdetWeXl5ydvb22Hz8/Hxkaenp7Zt22Zuu3Llinbt2qXAwECLsSkpKfLz83NYbECyPUeKdnkWFhZKIkdQsVjLk6+++krfffedkpKSlJSUZP6ahYSEBI0aNUqSc+bJ7SqrNRUWFurYsWNq2LChJOdc04kTJ1RQUFCmP2cotgMAAAAAAAAAnEa3bt109OhRix2JX3/9tZYsWaKUlBSdOHFCS5cu1dSpU/WXv/xFJpNJkhQWFiaTyaTw8HClpKQoLi5Os2bNUkREhF2Pr/3ll1/MhRhJSk9PV1JSkk6dOiXpt8fTjhs3TrNmzVJcXJxSUlIUHh6u6tWrKywszOJcCQkJCgkJuc1PBLBkLUc2bdqk5cuXKyUlRRkZGdq0aZNGjBihTp06mQtP5AgqEmt50qJFC7Vu3dr88vHxkST5+vqqfv36kpwzT27mhx9+UFJSkrKyspSfn2+OW1SIdtSaJkyYoF27dik9PV3//ve/1adPH+Xl5WngwIFOu6aEhAQ1b95cLVq0sCu+PVzL7MwAAAAAAAAAAKczatGT5T2FUrVp00b+/v764osvNGzYMElSlSpVtHDhQkVEROj69etq3ry5IiMjzTsRJcnd3V3btm3TqFGj5O/vLw8PD0VERFh8Z3VGRoZ8fHy0c+dOBQcHW41/8OBBde7c2fy+6PiBAwcqNjZWkvTXv/5V+fn5GjlypHJycvTII49o69atqlmzpvm4ffv2KTc3V3369HHUR4M7JHlgcnlPoVTWcsTNzU1LlizR+PHjVVhYqCZNmqh3796aNGmS+ThyBI6U1bl9eU+hVNbyxBbOlifh4eHKyMhQfHx8iXMeMmSIdu3aZX5ftIs7PT1d3t7eDlvTTz/9pH79+um///2v6tWrp0cffVT79+9Xs2bNzGOcaU2StGbNGg0dOrTEOI7gYhQ9R6SCyMvLk7u7u3Jzc1WrVq3yng7uYW1WtCnvKTj9L4UAAAAAAAAoOwUFBUpPT5ePj4+qVatW3tOxy6ZNmzRhwgSlpKSoUiXHPaA1Pj5ezz//vE6ePCkPDw+HndeaF198UX5+fpoyZUqZxkHFRI4AN3cv5ElwcLCCg4M1Y8aMMo1zL64pJSVFXbp0UVpamtzd3a2OKe13JVtryuxsBwAAAAAAAAA4lR49euj48eM6ffq0mjRp4rDzbtmyRVOmTCnzQkJhYaHatWun8ePHl2kcVFzkCHBzd3ueXLx4USdOnNDGjRvLNI50b64pMzNTK1euLLHQ7ijsbAfKCDvbAQAAAAAAUJ7u5p3tAAAAZc0RO9sd98wEAAAAAAAAAAAAAAAqCIrtAAAAAAAAAAAAAADYiWI7AAAAAAAAAAAAAAB2otgOAAAAAAAAAAAAAICdKLYDAAAAAAAAAAAAAGAniu0AAAAAAAAAAAAAANiJYjsAAAAAAAAAAAAAAHZyLe8JAAAAAAAAAADunPf7PntH473++Ua7j8nOzpavr68OHDggb29vx0/qHrJgwQJt3bpVGzZsKO+p3DOOPeh7R+P5/ueY3cfc7TlSWFioli1bKi4uTh06dCjv6eAWeE/65x2LlfHOM7d03J3Ik7Nnz6pVq1ZKSkpSo0aNyiQGnBs72wEAAAAAAAAATiUqKko9e/Y0F0eys7P19NNPy8vLSyaTSU2aNNHo0aOVl5dncVxycrKCgoLk5uamRo0aKTIyUoZh2BV79+7d6tmzp7y8vOTi4qL169cXG2MYhmbMmCEvLy+5ubkpODhYR48etXudY8eOVYcOHWQymdS+fXurY262pqFDhyoxMVF79uyxOz7uXjfmyO9lZ2ercePGcnFx0YULFyz6nCVHTCaTJkyYoIkTJ9oVG7DH7/PElvtIfHy8QkND1bBhQ9WoUUPt27fXqlWrSo1Rv359vfLKK5o+fXpZLwdOimI7AAAAAAAAAMBp5Ofna9myZRoyZIi5rVKlSgoNDdWGDRuUlpam2NhYbd++XcOHDzePycvLU9euXeXl5aXExETNnz9f7733nqKjo+2Kf+nSJbVr104LFiwocczs2bMVHR2tBQsWKDExUZ6enuratasuXrxoVyzDMDRo0CD17dvXar8tazKZTAoLC9P8+fPtio27l7Uc+b3Bgwerbdu2xdqdLUf69++vhIQEHTtm/85+4GZuzBNb7iN79+5V27Zt9dVXX+nIkSMaNGiQBgwYoK+//rrUWK+++qpWrVqlnJycMl0TnBOPkQcAAAAAAAAAOI3NmzfL1dVVAQEB5jYPDw+NGDHC/L5Zs2YaOXKk5syZY25btWqVCgoKFBsbK5PJpNatWystLU3R0dGKiIiQi4uLTfG7d++u7t27l9hvGIbmzZunqVOnqnfv3pKkFStWqEGDBlq9erWGDRtm81o//PBDSdK5c+d05MiRYv22rqlXr14KCQlRfn6+3NzcbI6Pu5O1HCkSExOjCxcuaNq0adq8ebNFn7PlSJ06dRQYGKg1a9YoMjLS1uUDNrkxT2y5j0yZMsXiHGPGjNE333yjuLg49ezZs8RYbdq0kaenp+Li4jRo0CAHrwTOjp3tAAAAAAAAAACnsXv3bvn7+5c6JjMzU+vWrVNQUJC5bd++fQoKCpLJZDK3devWTZmZmcrIyHDY/NLT05WVlaWQkBBzm8lkUlBQkPbu3euwOJLta/L399fVq1d14MABh8aHcyopR1JTUxUZGamVK1eqUqXi5R9nzJGOHTsqISHBYbGBIje7l1i7j1iTm5ur+++//6bxuJYrLortAAAAAAAAAACnkZGRIS8vL6t9/fr1U/Xq1dWoUSPVqlVLS5cuNfdlZWWpQYMGFuOL3mdlZTlsfkXnshbLkXGKYtmypho1aqh27doOLZjCeVnLkcLCQvXr109z5sxR06ZNrR7njDnSqFEjrluUiZLuJaXdR2705ZdfKjExUa+++upN43EtV1wU2wEAAAAAAAAATiM/P1/VqlWz2jd37lwdPnxY69ev14kTJxQREWHRf+NjsA3DsNruCNZi3ak41trd3Nx0+fJlh8eH87GWI5MnT5avr69efvnlUo91thzhukVZKelecrP7SJH4+HiFh4dryZIlatWq1U3jcS1XXBTbAQAAAAAAAABOo27dusrJybHa5+npqQcffFChoaFavHixYmJidObMGXPfjbtmz549K6n4Dtvb4enpKan4TuCzZ886NE5RLFvXdP78edWrV8+h8eGcrOXIjh07tHbtWrm6usrV1VVdunQxj50+fbok58wRrluUlZLuJaXdR4rs2rVLPXv2VHR0tAYMGGBTPK7liotiOwAAAAAAAADAafj5+Sk1NfWm44p25BYWFkqSAgICtHv3bl25csU8ZuvWrfLy8pK3t7fD5ufj4yNPT09t27bN3HblyhXt2rVLgYGBDosj2b6mEydOqKCgQH5+fg6ND+dkLUe++uorfffdd0pKSlJSUpL50dgJCQkaNWqUJOfMkZSUFK5blAlb7iU33kek33a0P/PMM3rnnXf0l7/8xeZ4XMsVF8V2AAAAAAAAAIDT6Natm44ePWqxI3HTpk1avny5UlJSlJGRoU2bNmnEiBHq1KmTuUgYFhYmk8mk8PBwpaSkKC4uTrNmzVJERIRdj8j+5ZdfzAVLSUpPT1dSUpJOnTol6bdHY48bN06zZs1SXFycUlJSFB4erurVqyssLMyutf7www9KSkpSVlaW8vPzzXGLiqG2rikhIUHNmzdXixYt7IqPu5O1HGnRooVat25tfvn4+EiSfH19Vb9+fUnOmSMJCQkKCQm5nY8DsOrGPLHlPlJUaB8zZoxeeOEFZWVlKSsrS+fPny811uXLl3Xo0CGu5QrKtbwnAAAAAAAAAAC4c17/fGN5T6FUbdq0kb+/v7744gsNGzZM0m/fhbtkyRKNHz9ehYWFatKkiXr37q1JkyaZj3N3d9e2bds0atQo+fv7y8PDQxERERbfx5uRkSEfHx/t3LlTwcHBVuMfPHhQnTt3Nr8vOn7gwIGKjY2VJP31r39Vfn6+Ro4cqZycHD3yyCPaunWratasaT4uPDxcGRkZio+PL3GtQ4YM0a5du8zvi3ZFpqeny9vb26Y1SdKaNWs0dOjQUj5V2MP3P8fKewqlspYjtnC2HNm3b59yc3PVp08fO1YPZ5HxzjPlPYVS3ZgnttxHYmNjdfnyZUVFRSkqKsrcHhQUZP5Zbi1H/vGPf6hp06Z6/PHH7+QS4SRcjKJnJFQQeXl5cnd3V25urmrVqlXe08E9rM2KNuU9BSUPTC7vKQAAAAAAAKCcFBQUKD09XT4+PqpWrVp5T8cumzZt0oQJE5SSkqJKlRz3gNb4+Hg9//zzOnnypDw8PBx2XmuCg4MVHBysGTNmlGmclJQUdenSRWlpaXJ3dy/TWHAe90KOvPjii/Lz89OUKVPKNA4qrrLIE2s50rFjR40bN87up5ug/JX2u5KtNWV2tgMAAAAAAAAAnEqPHj10/PhxnT59Wk2aNHHYebds2aIpU6aUeRHx4sWLOnHihDZuLPunCGRmZmrlypUU2iuYuz1HCgsL1a5dO40fP75M46BiK4s8uTFHzp49qz59+qhfv34OOT/uPuxsB8oIO9sBAAAAAABQnu7mne0AAABlzRE72x33bBEAAAAAAAAAAAAAACoIiu0AAAAAAAAAAAAAANiJYjsAAAAAAAAAAAAAAHai2A4AAAAAAAAAAAAAgJ0otgMAAAAAAAAAAAAAYCeK7QAAAAAAAAAAAAAA2IliOwAAAAAAAADAqWRnZ6t+/frKyMgo76k4vQULFqhXr17lPQ3cYXd7jhQWFqpp06Y6dOhQeU8F97A7kSdnz55VvXr1dPr06TKLAefmWt4TAAAAAAAAAADcOT9NSrij8Rq/87jdx0RFRalnz57y9vaW9FvBpH///jpy5Ii5eBIaGqpZs2apVq1a5uOSk5M1evRoHThwQPfff7+GDRumt956Sy4uLnbFXrdunf7zn//Izc1NgYGBevfdd/XAAw+YxxiGobffflsff/yxcnJy9Mgjj+ijjz5Sq1at7Frn2LFjtWfPHqWkpMjX11dJSUnFxtxsTUOHDtXMmTO1Z88ePfbYY3bFh3UfDd9xR+ONWvSk3cfcmCOSrF7nMTExGj58uPm9I3Jk9+7dmjNnjg4dOqQzZ84oLi5Ozz33nMWYm+WIyWTShAkTNHHiRG3fvt2+xcM5zHC/g7Fyb+kwa3kSGxur6OhopaWlqXbt2urTp48WLFhQ7NgffvhBfn5+qly5si5cuFBijPr16+uVV17R9OnTtXTp0luaJ+5u5b6zfeHChfLx8VG1atXUoUMHJSSU/oveqlWr1K5dO1WvXl0NGzbUq6++quzs7Ds0WwAAAAAAAABAWcrPz9eyZcs0ZMgQc1ulSpUUGhqqDRs2KC0tTbGxsdq+fbtFETEvL09du3aVl5eXEhMTNX/+fL333nuKjo62K/6uXbs0atQo7d+/X9u2bdOvv/6qkJAQXbp0yTxm9uzZio6O1oIFC5SYmChPT0917dpVFy9etCuWYRgaNGiQ+vbta7XfljWZTCaFhYVp/vz5dsXG3ctajhRZvny5zpw5Y34NHDjQ3OeoHLl06ZLatWtntUBZxJYc6d+/vxISEnTs2DG74gO2sJYn0dHRmjp1qiZNmqSjR4/qX//6l7p161bs2KtXr6pfv356/HHb/ljs1Vdf1apVq5STk+Ow+ePuUa472z///HONGzdOCxcuVKdOnbR48WJ1795dqampatq0abHxe/bs0YABAzR37lz17NlTp0+f1vDhwzVkyBDFxcWVwwoAAAAAAAAAAI60efNmubq6KiAgwNzm4eGhESNGmN83a9ZMI0eO1Jw5c8xtq1atUkFBgWJjY2UymdS6dWulpaUpOjpaERERNu/c3bJli8X75cuXq379+jp06JCeeOIJGYahefPmaerUqerdu7ckacWKFWrQoIFWr16tYcOG2bzWDz/8UJJ07tw5HTlypFi/rWvq1auXQkJClJ+fLzc3N5vj4+5kLUeK1K5dW56enlaPc1SOdO/eXd27dy+x39YcqVOnjgIDA7VmzRpFRkbaFBuw1Y15kpOTozfffFNff/21unTpYh5n7Ykkb775ph588EF16dJFe/fuvWmsNm3ayNPTU3FxcRo0aJDjFoG7QrnubI+OjtbgwYM1ZMgQ+fr6at68eWrSpIliYmKsjt+/f7+8vb01ZswY+fj46LHHHtOwYcN08ODBOzxzAAAAAAAAAEBZ2L17t/z9/Usdk5mZqXXr1ikoKMjctm/fPgUFBclkMpnbunXrpszMzNv6vt7c3N8eX3z//fdLktLT05WVlaWQkBDzGJPJpKCgIJuKMvawdU3+/v66evWqDhw44ND4cE6l5cjo0aNVt25dPfzww1q0aJGuX79u7iurHLmRPTnSsWPHmz7xGLgVN+bJtm3bdP36dZ0+fVq+vr5q3Lix/vznP+vHH3+0OG7Hjh1au3atPvroI7vicS1XXOVWbL9y5YoOHTpk8cNWkkJCQkr8hSQwMFA//fSTNm3aJMMw9PPPP+vLL7/UM888U2KcwsJC5eXlWbwAAAAAAAAAAM4pIyNDXl5eVvv69eun6tWrq1GjRqpVq5bF9+NmZWWpQYMGFuOL3mdlZd3SXAzDUEREhB577DG1bt3a4lzWYt1qnJLYuqYaNWqodu3aDi2YwnmVlCN/+9vftHbtWm3fvl0vvfSSXn/9dc2aNcvcXxY5Yo09OdKoUSOuW5SJG/Pk5MmTun79umbNmqV58+bpyy+/1Pnz59W1a1dduXJFkpSdna3w8HDFxsaqVq1adsXjWq64yq3Y/t///lfXrl2z6xeSwMBArVq1Sn379lXVqlXl6emp2rVrl/pdNFFRUXJ3dze/mjRp4tB1AAAAAAAAAAAcJz8/X9WqVbPaN3fuXB0+fFjr16/XiRMnFBERYdF/42OwDcOw2m6r0aNH68iRI1qzZk2xPmuxbjVOaWxdk5ubmy5fvuzw+HA+JeXIm2++qYCAALVv316vv/66IiMjLb5qQXJ8jpTGlhzhukVZuTFPrl+/rqtXr+rDDz9Ut27d9Oijj2rNmjU6fvy4du7cKUkaOnSowsLC9MQTT9gdj2u54irXx8hL9v1CkpqaqjFjxmjatGk6dOiQtmzZovT0dA0fPrzE80+ePFm5ubnm142PgwAAAAAAAAAAOI+6desqJyfHap+np6cefPBBhYaGavHixYqJidGZM2fMfTdu5Dp79qyk4jtsbfHaa69pw4YN2rlzpxo3bmwxB6n4TuCzZ8/eUpzS2LOm8+fPq169eg6ND+dUWo783qOPPqq8vDz9/PPPkhyfIyWxJ0e4blFWbsyThg0bSpIeeughc1u9evVUt25dnTp1StJvj5B/77335OrqKldXVw0ePFi5ublydXXVJ598Umo8ruWKq9yK7XXr1lXlypXt+oUkKipKnTp10htvvKG2bduqW7duWrhwoT755BPzL1Q3MplMqlWrlsULAAAAAAAAAOCc/Pz8lJqaetNxRTtyCwsLJUkBAQHavXu3+XHAkrR161Z5eXnJ29vb5viGYWj06NFat26dduzYIR8fH4t+Hx8feXp6atu2bea2K1euaNeuXQoMDLQ5ji1sXdOJEydUUFAgPz8/h8aHc7I1R7799ltVq1ZNtWvXluS4HLkZe3IkJSWF6xZl4sY86dSpkyTp+++/N7edP39e//3vf9WsWTNJ0r59+5SUlGR+RUZGqmbNmkpKStLzzz9fajyu5Yqr3IrtVatWVYcOHSx+2ErStm3bSvyF5PLly6pUyXLKlStXlvR/v1gBAAAAAAAAAO5e3bp109GjRy12JG7atEnLly9XSkqKMjIytGnTJo0YMUKdOnUyFwnDwsJkMpkUHh6ulJQUxcXFadasWYqIiLDrEdmjRo3Sp59+qtWrV6tmzZrKyspSVlaW8vPzJf32tNZx48Zp1qxZiouLU0pKisLDw1W9enWFhYXZtdYffvhBSUlJ5vMXFXiKiqG2rikhIUHNmzdXixYt7IqPu5O1HPn666+1ZMkSpaSk6MSJE1q6dKmmTp2qv/zlLzKZTJIclyO//PKL+VqVpPT0dCUlJZl3B9uTIwkJCQoJCbnNTwQo7sY8+eMf/6jQ0FCNHTtWe/fuVUpKigYOHKgHH3xQnTt3liT5+vqqdevW5lejRo1UqVIltW7dWh4eHiXGunz5sg4dOsS1XEGV62PkIyIitHTpUn3yySc6duyYxo8fr1OnTpkfCz958mQNGDDAPL5nz55at26dYmJidPLkSf2///f/NGbMGHXs2FFeXl7ltQwAAAAAAAAAgIO0adNG/v7++uKLL8xtbm5uWrJkiR577DH5+vpq3LhxevbZZ7Vx40bzGHd3d23btk0//fST/P39NXLkSEVERFh8r3tGRoZcXFwUHx9fYvyYmBjl5uYqODhYDRs2NL8+//xz85i//vWvGjdunEaOHCl/f3+dPn1aW7duVc2aNc1jwsPDFRwcXOpahwwZIj8/Py1evFhpaWny8/OTn5+fMjMzbV6TJK1Zs0ZDhw4tNRbuHdZypEqVKlq4cKECAgLUtm1bffDBB4qMjNT7779vHuOoHDl48KD5WpV+q/X4+flp2rRp5jG25Mi+ffuUm5urPn36OOJjASxYy5OVK1fqkUce0TPPPKOgoCBVqVJFW7ZsUZUqVWw+r7Uc+cc//qGmTZvq8ccfd+QScJdwMcp5S/jChQs1e/ZsnTlzRq1bt9bcuXP1xBNPSPrtl5GMjAyLC3b+/PlatGiR0tPTVbt2bT355JN699131ahRI5vi5eXlyd3dXbm5uTxSHmWqzYo25T0FJQ9MLu8pAAAAAAAAoJwUFBQoPT1dPj4+qlatWnlPxy6bNm3ShAkTlJKSUuxpp7cjPj5ezz//vE6ePFnqLkVHCA4OVnBwsGbMmFGmcVJSUtSlSxelpaXJ3d29TGPBedwLOfLiiy/Kz89PU6ZMKdM4qLjKIk+s5UjHjh01btw4u59ugvJX2u9KttaUXct6kjczcuRIjRw50mpfbGxssbbXXntNr732WhnPCgAAAAAAAABQXnr06KHjx4/r9OnTatKkicPOu2XLFk2ZMqXMi4gXL17UiRMnLHbel5XMzEytXLmSQnsFc7fnSGFhodq1a6fx48eXaRxUbGWRJzfmyNmzZ9WnTx/169fPIefH3afcd7bfaexsx53CznYAAAAAAACUp7t5ZzsAAEBZc8TO9nL9znYAAAAAAAAAAAAAAO5GFNsBAAAAAAAAAAAAALATxXYAAAAAAAAAAAAAAOxEsR0AAAAAAAAAAAAAADtRbAcAAAAAAAAAAAAAwE4U2wEAAAAAAAAAAAAAsBPFdgAAAAAAAAAAAAAA7ESxHQAAAAAAAADgVLKzs1W/fn1lZGSU91Sc3oIFC9SrV6/yngbuMHLEduRIxeUseZKcnKzGjRvr0qVL5ToPlA3X8p4AAAAAAAAAAODOmTFjhtPHi4qKUs+ePeXt7S3pt4JJ//79deTIEXPxJDQ0VLNmzVKtWrXMxyUnJ2v06NE6cOCA7r//fg0bNkxvvfWWXFxcbI4dExOjmJgYc3GmVatWmjZtmrp3724eYxiG3n77bX388cfKycnRI488oo8++kitWrWya51jx47Vnj17lJKSIl9fXyUlJRUbc7M1DR06VDNnztSePXv02GOP2RUf1r3f99k7Gu/1zzfafcyNOfJ72dnZateunU6fPq2cnBzVrl3b3OeIHNm9e7fmzJmjQ4cO6cyZM4qLi9Nzzz1nMYYcufe1WdHmjsVKHph8S8fdmCc3u57i4+M1d+5cHThwQHl5eWrZsqXeeOMN9e/f32LcqlWrNHv2bB0/flzu7u56+umn9d5776lOnTpW59GmTRt17NhRc+fO1ZtvvnlLa4HzYmc7AAAAAAAAAMBp5Ofna9myZRoyZIi5rVKlSgoNDdWGDRuUlpam2NhYbd++XcOHDzePycvLU9euXeXl5aXExETNnz9f7733nqKjo+2K37hxY73zzjs6ePCgDh48qCeffFKhoaE6evSoeczs2bMVHR2tBQsWKDExUZ6enuratasuXrxoVyzDMDRo0CD17dvXar8tazKZTAoLC9P8+fPtio27l7Uc+b3Bgwerbdu2xdodlSOXLl1Su3bttGDBghLHkCMob9by5GbX0969e9W2bVt99dVXOnLkiAYNGqQBAwbo66+/No/Zs2ePBgwYoMGDB+vo0aNau3atEhMTS8zHIq+++qpiYmJ07do1xywQToOd7QAAAAAAAAAAp7F582a5uroqICDA3Obh4aERI0aY3zdr1kwjR47UnDlzzG2rVq1SQUGBYmNjZTKZ1Lp1a6WlpSk6OloRERE279zt2bOnxfuZM2cqJiZG+/fvV6tWrWQYhubNm6epU6eqd+/ekqQVK1aoQYMGWr16tYYNG2bzWj/88ENJ0rlz53TkyJFi/bauqVevXgoJCVF+fr7c3Nxsjo+7k7UcKRITE6MLFy5o2rRp2rx5s0Wfo3Kke/fuFk96uBE5AmdgLU9udj1NmTLF4v2YMWP0zTffKC4uznxv2L9/v7y9vTVmzBhJko+Pj4YNG6bZs2eXOp9u3bopOztbu3bt0pNPPnlba4NzYWc7AAAAAAAAAMBp7N69W/7+/qWOyczM1Lp16xQUFGRu27dvn4KCgmQymcxt3bp1U2Zm5i1/X++1a9f02Wef6dKlS+aCTXp6urKyshQSEmIeZzKZFBQUpL17995SnJLYuiZ/f39dvXpVBw4ccGh8OKeSciQ1NVWRkZFauXKlKlUqXv4pixyxhhyBM7DlXmKL3Nxc3X///eb3gYGB+umnn7Rp0yYZhqGff/5ZX375pZ555plSz1O1alW1a9dOCQkJtz0nOBeK7QAAAAAAAAAAp5GRkSEvLy+rff369VP16tXVqFEj1apVS0uXLjX3ZWVlqUGDBhbji95nZWXZNYfk5GTdd999MplMGj58uOLi4vTQQw9ZnMtaLHvj3Iyta6pRo4Zq167t0IIpnJe1HCksLFS/fv00Z84cNW3a1OpxjsyR0pAjcAal3Uts9eWXXyoxMVGvvvqquS0wMFCrVq1S3759VbVqVXl6eqp27do2fU1Bo0aNuAbvQRTbAQAAAAAAAABOIz8/X9WqVbPaN3fuXB0+fFjr16/XiRMnFBERYdF/42OwDcOw2n4zDzzwgJKSkrR//36NGDFCAwcOVGpq6k1j2RvHFrauyc3NTZcvX3Z4fDgfazkyefJk+fr66uWXXy71WEfliC3IEZSn0u4ltoiPj1d4eLiWLFmiVq1amdtTU1M1ZswYTZs2TYcOHdKWLVuUnp6u4cOH3/ScXIP3JortAAAAAAAAAACnUbduXeXk5Fjt8/T01IMPPqjQ0FAtXrxYMTExOnPmjLnvxl2zZ8+elVR8h+3NVK1aVX/4wx/k7++vqKgotWvXTh988IE5jlR8J/DZs2ftjnMz9qzp/PnzqlevnkPjwzlZy5EdO3Zo7dq1cnV1laurq7p06WIeO336dEmOzZHSkCNwBqXdS25m165d6tmzp6KjozVgwACLvqioKHXq1ElvvPGG2rZtq27dumnhwoX65JNPzPejknAN3psotgMAAAAAAAAAnIafn1+xXeTWFO1eLSwslCQFBARo9+7dunLlinnM1q1b5eXlJW9v79uak2EY5jg+Pj7y9PTUtm3bzP1XrlzRrl27FBgYeFtxbmTrmk6cOKGCggL5+fk5ND6ck7Uc+eqrr/Tdd98pKSlJSUlJ5q9YSEhI0KhRoySVbY78HjkCZ2DrveRG8fHxeuaZZ/TOO+/oL3/5S7H+y5cvq1Ily/Jq5cqVJf3ffakkKSkpXIP3IIrtAAAAAAAAAACn0a1bNx09etRiR+KmTZu0fPlypaSkKCMjQ5s2bdKIESPUqVMnc0EtLCxMJpNJ4eHhSklJUVxcnGbNmqWIiAi7Hl09ZcoUJSQkKCMjQ8nJyZo6dari4+PVv39/Sb89mnrcuHGaNWuW4uLilJKSovDwcFWvXl1hYWF2rfWHH35QUlKSsrKylJ+fby6UFhUObV1TQkKCmjdvrhYtWtgVH3cnaznSokULtW7d2vzy8fGRJPn6+qp+/fqSHJcjv/zyi/lalaT09HQlJSXp1KlTksgROAdreXKz66mo0D5mzBi98MILysrKUlZWls6fP28+R8+ePbVu3TrFxMTo5MmT+n//7/9pzJgx6tixY6nfEZ+RkaHTp0/rqaeeKrtFo1y4lvcEAAAAAAAAAAAo0qZNG/n7++uLL77QsGHDJP32PbdLlizR+PHjVVhYqCZNmqh3796aNGmS+Th3d3dt27ZNo0aNkr+/vzw8PBQREWHxve4ZGRny8fHRzp07FRwcbDX+zz//rFdeeUVnzpyRu7u72rZtqy1btqhr167mMX/961+Vn5+vkSNHKicnR4888oi2bt2qmjVrmseEh4crIyND8fHxJa51yJAh2rVrl/l90Y7H9PR0eXt727QmSVqzZo2GDh168w8X9wRrOWILR+XIwYMH1blzZ/P7ouMHDhyo2NhYSeQIyp+1PLnZ9RQbG6vLly8rKipKUVFR5nFBQUHm6zQ8PFwXL17UggUL9Prrr6t27dp68skn9e6775rHx8fHq3PnzubzSr9dgyEhIWrWrFkZrxx3motxs2ca3GPy8vLk7u6u3Nxc1apVq7yng3tYmxVtynsKSh6YXN5TAAAAAAAAQDkpKChQenq6fHx8VK1atfKejl02bdqkCRMmKCUlpdjjem9HfHy8nn/+eZ08eVIeHh4OO681wcHBCg4O1owZM8o0TkpKirp06aK0tDS5u7uXaSw4D3LEduRIxVVWeXIzsbGxmjlzplJTU1WlShUVFhaqZcuWWrNmjTp16nTH5oGbK+13JVtryuxsBwAAAAAAAAA4lR49euj48eM6ffq0mjRp4rDzbtmyRVOmTCnzIuLFixd14sQJbdy4sUzjSFJmZqZWrlxJEbGCIUdsR45UXGWVJzezZcsWzZo1S1WqVJEk/e///q+mTp1Kof0exc52oIywsx0AAAAAAADl6W7e2Q4AAFDWHLGz/c49MwEAAAAAAAAAAAAAgHsExXYAAAAAAAAAAAAAAOxEsR0AAAAAAAAAAAAAADtRbAcAAAAAAAAAAAAAwE4U2wEAAAAAAAAAAAAAsBPFdgAAAAAAAAAAAAAA7ESxHQAAAAAAAAAAAAAAO1FsBwAAAAAAAAA4lezsbNWvX18ZGRnlPRWnt2DBAvXq1au8p4E7jByxHTlScd3teVJYWKimTZvq0KFD5T0VlMK1vCcAAAAAAAAAALhz/rWjxR2N1+XJE3YfExUVpZ49e8rb21vSbwWT/v3768iRI+biSWhoqGbNmqVatWqZj0tOTtbo0aN14MAB3X///Ro2bJjeeustubi43NLco6KiNGXKFI0dO1bz5s0ztxuGobffflsff/yxcnJy9Mgjj+ijjz5Sq1at7Dr/2LFjtWfPHqWkpMjX11dJSUnFxtxsTUOHDtXMmTO1Z88ePfbYY7e0Tlj6aVLCHY3X+J3H7T7mxhz5vezsbLVr106nT59WTk6Oateube5zRI5ERUVp3bp1+s9//iM3NzcFBgbq3Xff1QMPPGAeQ47c+4496HvHYvn+59gtHXcr95L4+HjNnTtXBw4cUF5enlq2bKk33nhD/fv3tyt2TEyMYmJizIX+Vq1aadq0aerevbsk6erVq3rzzTe1adMmnTx5Uu7u7nrqqaf0zjvvyMvLS5JkMpk0YcIETZw4Udu3b7+lzwBlj53tAAAAAAAAAACnkZ+fr2XLlmnIkCHmtkqVKik0NFQbNmxQWlqaYmNjtX37dg0fPtw8Ji8vT127dpWXl5cSExM1f/58vffee4qOjr6leSQmJurjjz9W27Zti/XNnj1b0dHRWrBggRITE+Xp6amuXbvq4sWLdsUwDEODBg1S3759rfbbsiaTyaSwsDDNnz/fvgXirmUtR35v8ODBVq9bR+XIrl27NGrUKO3fv1/btm3Tr7/+qpCQEF26dMk8hhxBebvVe8nevXvVtm1bffXVVzpy5IgGDRqkAQMG6Ouvv7YrfuPGjfXOO+/o4MGDOnjwoJ588kmFhobq6NGjkqTLly/r8OHDeuutt3T48GGtW7dOaWlpxZ7C0L9/fyUkJOjYsVv7gwOUPXa2AwAAAAAAAACcxubNm+Xq6qqAgABzm4eHh0aMGGF+36xZM40cOVJz5swxt61atUoFBQWKjY2VyWRS69atlZaWpujoaEVERNi1c/eXX35R//79tWTJEv3973+36DMMQ/PmzdPUqVPVu3dvSdKKFSvUoEEDrV69WsOGDbM5zocffihJOnfunI4cOVKs39Y19erVSyEhIcrPz5ebm5vN8XF3spYjRWJiYnThwgVNmzZNmzdvtuhzVI5s2bLF4v3y5ctVv359HTp0SE888QQ5Aqdwq/eSKVOmWJxnzJgx+uabbxQXF6eePXvaHP/GsTNnzlRMTIz279+vVq1ayd3dXdu2bbMYM3/+fHXs2FGnTp1S06ZNJUl16tRRYGCg1qxZo8jISJvj485hZzsAAAAAAAAAwGns3r1b/v7+pY7JzMzUunXrFBQUZG7bt2+fgoKCZDKZzG3dunVTZmam3d/XO2rUKD3zzDN66qmnivWlp6crKytLISEh5jaTyaSgoCDt3bvXrjg3Y+ua/P39dfXqVR04cMCh8eGcSsqR1NRURUZGauXKlapUqXj5x5E58nu5ubmSpPvvv18SOQLncKv3Emtyc3PN1/etuHbtmj777DNdunTJ6h/J/D6Oi4uLxVc/SFLHjh2VkHBnv94CtqPYDgAAAAAAAABwGhkZGebvq71Rv379VL16dTVq1Ei1atXS0qVLzX1ZWVlq0KCBxfii91lZWTbH/+yzz3T48GFFRUVZ7S86l7VY9sSxha1rqlGjhmrXrn1bBVPcPazlSGFhofr166c5c+aYd8TeyFE58nuGYSgiIkKPPfaYWrdubXEucgTl6VbvJTf68ssvlZiYqFdffdXuOSQnJ+u+++6TyWTS8OHDFRcXp4ceesjq2IKCAk2aNElhYWHm748v0qhRI65dJ0axHQAAAAAAAADgNPLz81WtWjWrfXPnztXhw4e1fv16nThxQhERERb9Nz4G2zAMq+0l+fHHHzV27Fh9+umnJc6htFj2PKreVrauyc3NTZcvX3Z4fDgfazkyefJk+fr66uWXXy712NvNkRuNHj1aR44c0Zo1a2yKRY7gTrmde0mR+Ph4hYeHa8mSJWrVqpXdc3jggQeUlJSk/fv3a8SIERo4cKBSU1OLjbt69apeeuklXb9+XQsXLizWz7Xr3Ci2AwAAAAAAAACcRt26dZWTk2O1z9PTUw8++KBCQ0O1ePFixcTE6MyZM+a+G3fNnj17VlLxHbYlOXTokM6ePasOHTrI1dVVrq6u2rVrlz788EO5urrq2rVr8vT0lFR8J/DZs2dtjmMre9Z0/vx51atXz6Hx4Zys5ciOHTu0du1a83XbpUsX89jp06dLckyO/N5rr72mDRs2aOfOnWrcuLG5nRyBM7jVe0mRXbt2qWfPnoqOjtaAAQNuaQ5Vq1bVH/7wB/n7+ysqKkrt2rXTBx98YDHm6tWr+vOf/6z09HRt27at2K52iWvX2VFsBwAAAAAAAAA4DT8/P6s7/25UtHu1sLBQkhQQEKDdu3frypUr5jFbt26Vl5eXvL29bYrdpUsXJScnKykpyfzy9/dX//79lZSUpMqVK8vHx0eenp7atm2b+bgrV65o165dCgwMtGOlN2frmk6cOKGCggL5+fk5ND6ck7Uc+eqrr/Tdd9+Zr9uix2InJCRo1KhRkhyTI9JvuTd69GitW7dOO3bskI+Pj0U/OQJncKv3Eum3He3PPPOM3nnnHf3lL39x2JwMw7CIU1RoP378uLZv3646depYPS4lJYVr14lRbAcAAAAAAAAAOI1u3brp6NGjFjsSN23apOXLlyslJUUZGRnatGmTRowYoU6dOpkLamFhYTKZTAoPD1dKSori4uI0a9YsRURE2Pzo6po1a6p169YWrxo1aqhOnTrm76N2cXHRuHHjNGvWLMXFxSklJUXh4eGqXr26wsLC7FrrDz/8oKSkJGVlZSk/P99cKC0qHNq6poSEBDVv3lwtWrSwKz7uTtZypEWLFhbXbVEB3NfXV/Xr15fkmByRpFGjRunTTz/V6tWrVbNmTWVlZZmvYYkcgXO41XtJUaF9zJgxeuGFF8zX9/nz5+2KP2XKFCUkJCgjI0PJycmaOnWq4uPj1b9/f0nSr7/+qj59+ujgwYNatWqVrl27Zo71+z8ekX67fkNCQm7vA0GZodgOAAAAAAAAAHAabdq0kb+/v7744gtzm5ubm5YsWaLHHntMvr6+GjdunJ599llt3LjRPMbd3V3btm3TTz/9JH9/f40cOVIREREW38WbkZEhFxcXxcfH39Yc//rXv2rcuHEaOXKk/P39dfr0aW3dulU1a9Y0jwkPD1dwcHCp5xkyZIj8/Py0ePFipaWlyc/PT35+fsrMzLR5TZK0Zs0aDR069LbWhLuHtRyxhaNyJCYmRrm5uQoODlbDhg3Nr88//9w8hhxBebvVe0lsbKwuX76sqKgoi+u7d+/e5jG25MnPP/+sV155RQ888IC6dOmif//739qyZYu6du0qSfrpp5+0YcMG/fTTT2rfvr1FrL1795rPs2/fPuXm5qpPnz4O/HTgSC5G0fMRKoi8vDy5u7srNzfX6vceAI7SZkWb8p6Ckgcml/cUAAAAAAAAUE4KCgqUnp4uHx8fVatWrbynY5dNmzZpwoQJSklJUaVKjtszFh8fr+eff14nT56Uh4eHw85rTXBwsIKDgzVjxowyjZOSkqIuXbooLS1N7u7uZRoLzoMcsR05UnHdC3ny4osvys/PT1OmTCnTOBVVab8r2VpTdi3rSQIAAAAAAAAAYI8ePXro+PHjOn36tJo0aeKw827ZskVTpkwp8+LIxYsXdeLECYvdkmUlMzNTK1eupIhYwZAjtiNHKq67PU8KCwvVrl07jR8/vkzj4Pawsx0oI+xsBwAAAAAAQHm6m3e2AwAAlDVH7GznO9sBAAAAAAAAAAAAALATxXYAAAAAAAAAAAAAAOxEsR0AAAAA7gELFy40P/asQ4cOSkhIKHFseHi4XFxcir1atWplHrNkyRI9/vjj8vDwkIeHh5566ikdOHDA4jxRUVF6+OGHVbNmTdWvX1/PPfecvv/++xLjDhs2TC4uLpo3b565LSMjw+pcXFxctHbtWvM4b2/vYv2TJk26hU8KAAAAAADAMSi2AwAAAMBd7vPPP9e4ceM0depUffvtt3r88cfVvXt3nTp1yur4Dz74QGfOnDG/fvzxR91///168cUXzWPi4+PVr18/7dy5U/v27VPTpk0VEhKi06dPm8fs2rVLo0aN0v79+7Vt2zb9+uuvCgkJ0aVLl4rFXL9+vf7973/Ly8vLor1JkyYWczlz5ozefvtt1ahRQ927d7cYGxkZaTHuzTffvJ2PDQAAAAAA4La4lvcEAAAAAAC3Jzo6WoMHD9aQIUMkSfPmzdM333yjmJgYRUVFFRvv7u4ud3d38/v169crJydHr776qrlt1apVFscsWbJEX375pf71r39pwIABkqQtW7ZYjFm+fLnq16+vQ4cO6YknnjC3nz59WqNHj9Y333yjZ555xuKYypUry9PT06ItLi5Offv21X333WfRXrNmzWJjAQAAAAAAygs72wEAAADgLnblyhUdOnRIISEhFu0hISHau3evTedYtmyZnnrqKTVr1qzEMZcvX9bVq1d1//33lzgmNzdXkizGXL9+Xa+88oreeOMNi8fUl+TQoUNKSkrS4MGDi/W9++67qlOnjtq3b6+ZM2fqypUrNz0fAAAAAABAWWFnOwAAAADcxf773//q2rVratCggUV7gwYNlJWVddPjz5w5o82bN2v16tWljps0aZIaNWqkp556ymq/YRiKiIjQY489ptatW5vb3333Xbm6umrMmDE2rOa3wr+vr68CAwMt2seOHas//elP8vDw0IEDBzR58mSlp6dr6dKlNp0XAAAAAADA0djZDgAAAAD3ABcXF4v3hmEUa7MmNjZWtWvX1nPPPVfimNmzZ2vNmjVat26dqlWrZnXM6NGjdeTIEa1Zs8bcdujQIX3wwQeKjY21aS75+flavXq11V3t48ePV1BQkNq2bashQ4Zo0aJFWrZsmbKzs296XgAAcPfJzs5W/fr1lZGRUd5TcXoLFixQr169ynsauMPIEduRIxXX3Z4nhYWFatq0qQ4dOlTeU0Ep2NkOAAAAAHexunXrqnLlysV2sZ89e7bYbvcbGYahTz75RK+88oqqVq1qdcx7772nWbNmafv27Wrbtq3VMa+99po2bNig3bt3q3Hjxub2hIQEnT17Vk2bNjW3Xbt2Ta+//rrmzZtX7B88vvzyS12+fNn8nfClefTRRyVJP/zwg+rUqXPT8QAA4P947ky6o/GyOre3+5ioqCj17NlT3t7ekn4rmPTv319HjhwxF09CQ0M1a9Ys1apVy3xccnKyRo8erQMHDuj+++/XsGHD9NZbb9n0h39FZsyYobffftui7canBhmGobffflsff/yxcnJy9Mgjj+ijjz6y6Wtzfm/s2LHas2ePUlJS5Ovrq6SkpGJjbramoUOHaubMmdqzZ48ee+wxu+LDuhkzZjh9vBtz5Peys7PVrl07nT59Wjk5Oapdu7a5zxE5EhMTo5iYGPPv861atdK0adPUvXt38xhy5N730fAddyzWqEVP3tJxt3IviY+P19y5c3XgwAHl5eWpZcuWeuONN9S/f3+7458+fVoTJ07U5s2blZ+frz/+8Y9atmyZOnToUGzssGHD9PHHH2vu3LkaN26cJMlkMmnChAmaOHGitm/ffkufAcoeO9sBAAAA4C5WtWpVdejQQdu2bbNo37ZtW7FHsd9o165d+uGHH6zuJJekOXPm6G9/+5u2bNkif3//Yv2GYWj06NFat26dduzYIR8fH4v+V155RUeOHFFSUpL55eXlpTfeeEPffPNNsfMtW7ZMvXr1Ur169W62bH377beSpIYNG950LAAAuLvk5+dr2bJlGjJkiLmtUqVKCg0N1YYNG5SWlqbY2Fht375dw4cPN4/Jy8tT165d5eXlpcTERM2fP1/vvfeeoqOj7Z5Dq1atdObMGfMrOTnZon/27NmKjo7WggULlJiYKE9PT3Xt2lUXL160K45hGBo0aJD69u1rtd+WNZlMJoWFhWn+/Pl2rxN3J2s58nuDBw+2+oeyjsqRxo0b65133tHBgwd18OBBPfnkkwoNDdXRo0fNY8gRlLdbvZfs3btXbdu21VdffaUjR45o0KBBGjBggL7++mu74ufk5KhTp06qUqWKNm/erNTUVL3//vsWf/xSZP369fr3v/8tLy+vYn39+/dXQkKCjh07Zld83DnsbAcAAACAu1xERIReeeUV+fv7KyAgQB9//LFOnTpl/geDyZMn6/Tp01q5cqXFccuWLdMjjzxi8R3rRWbPnq233npLq1evlre3t3kn13333af77rtPkjRq1CitXr1a//jHP1SzZk3zGHd3d7m5ualOnTrFdp1XqVJFnp6eeuCBByzaf/jhB+3evVubNm0qNpd9+/Zp//796ty5s9zd3ZWYmKjx48erV69eFrvmAQDAvWHz5s1ydXVVQECAuc3Dw0MjRowwv2/WrJlGjhypOXPmmNtWrVqlgoICxcbGymQyqXXr1kpLS1N0dLQiIiLs2rnr6uoqT09Pq32GYWjevHmaOnWqevfuLUlasWKFGjRooNWrV2vYsGE2x/nw/2Pv7uNqvP8/gL8iJXehpBul5jYhTcuEb7kpi6Uxc1NDwpCNNIYoZCryTRvTMBZbcy8za01mJcMkO3RjSjcz3Yik0K06vz/6dX0dnTgnJ514PR+PHg/nuj7X9f58zuN6n+s4n+vz+Xz5JQDgzp07uHr1aq39srZp3LhxsLe3R0lJCTQ0NGSOT02TtBypERISgvv378PHxwe//PKLxD5F5Yijo6PE6/Xr1yMkJAQXLlyAmZkZc4SUQn3vJV5eXhLnWbhwIX799VeEh4fXuvafZcOGDTA0NMS3334rbJM2E0VWVhY+/vhj/Prrrxg7dmyt/VpaWrC2tsa+ffvg6+src3x6eTiynYiIiIiIqImbPHkygoOD4evriwEDBgid1l27dgUA5OTk4ObNmxLHFBYW4siRI3WOat+2bRvKy8sxceJE6OnpCX+bNm0SyoSEhKCwsBC2trYSZQ4cOCB3G3bv3g0DAwPY29vX2qeuro4DBw7A1tYWffr0gY+PD+bMmSOxPjwRERG9Os6cOSN1Vp0nZWdn4+jRo7CxsRG2nT9/HjY2NlBXVxe2jR49GtnZ2XKv15uamgp9fX2YmJhgypQpSE9PF/ZlZGQgNzdX4nuLuro6bGxscO7cObniPI+sbbK0tERFRQUuXryo0PiknOrKkeTkZPj6+mLv3r1o1qx2948ic6RGZWUl9u/fj0ePHgmdmswRUgb1vZdIU1hYiI4dO8oV//jx47C0tMQHH3wAHR0dWFhYYOfOnRJlqqqqMG3aNCxduvSZSyxYWVkhNjZWrvj08nBkOxERERER0SvA3d0d7u7uUveFhobW2qapqYni4uI6zyfLj21isVjW6j33vH5+fvDz85O6780338SFCxfkjkVERERNU2ZmptSpdAFg6tSp+PHHH1FSUgJHR0d88803wr7c3NxaowY7d+4s7Ht6yZu6DBo0CHv37kXPnj1x+/ZtfP7557C2tkZSUhK0tLSE2Xxqzv1krH/++UfWZspE1ja1bt0a7du3R2Zm5nM7jajpk5YjZWVlmDp1KgIDA2FkZCTxgEgNReUIUL1O+uDBg1FaWoo2bdogPDwcffr0Ec715LmfjMUcoZelvveSpx0+fBhxcXHYvn27XPHT09MREhICT09PeHl54eLFi1i4cCHU1dUxffp0ANWj31VVVbFw4cJnnsvAwKDeD8RQw+PIdiIiIiIiIiIiIiJSGiUlJWjZsqXUfZs3b8bly5dx7NgxpKWlwdPTU2L/09Ng1zwcKM8U8g4ODnj//ffRr18/jBo1Cj///DOA6mmwnxdLnjiykrVNGhoaz3yYkl4d0nJkxYoVMDU1xYcffvjMYxWRIwDQq1cviEQiXLhwAfPnz8eMGTOQnJz83FjMEXpZXuReUiM6Ohqurq7YuXPnM0eeS1NVVYU333wTfn5+sLCwwNy5czFnzhyEhIQAAOLj4/HFF18gNDT0uXnBa1e5sbOdiIiIiIiIiIiIiJSGtrY2CgoKpO7T1dVF79694eTkhO3btyMkJAQ5OTnCvpoRtTXy8vIA1B5hK4/WrVujX79+SE1NFeIAkBrrReJII0+b7t27h06dOik0PiknaTly+vRpHDp0CKqqqlBVVcXIkSOFsqtXrwag2BxRU1ND9+7dYWlpCX9/f5ibm+OLL74Q4gDMEWpc9b2X1IiJiYGjoyOCgoKEkejy0NPTE2Z7qGFqaios8RYbG4u8vDwYGRkJefvPP//g008/rTVbA69d5cbOdiIiIiIiIiIiIiJSGhYWFrVGyEpTM3q1rKwMADB48GCcOXMG5eXlQpmTJ09CX1+/VseFPMrKynDt2jXo6ekBAExMTKCrq4uoqCihTHl5OWJiYmBtbV3vONLI2qa0tDSUlpbCwsJCofFJOUnLkSNHjuDKlSsQiUQQiUTCtNixsbFYsGABgIbLEaA6H2tykTlCyqC+9xKgekT72LFjERAQgI8++qhe8YcMGYLr169LbEtJSUHXrl0BANOmTcPVq1eFnBWJRNDX18fSpUvx66+/ShyXmJjIa1eJsbOdiIiIiIiIiIiIiJTG6NGjkZSUJDEiMSIiAt9++y0SExORmZmJiIgIzJ8/H0OGDBE61JydnaGurg5XV1ckJiYiPDwcfn5+8PT0lGvq6iVLliAmJgYZGRn4888/MXHiRBQVFWHGjBkAqqem9vDwgJ+fH8LDw5GYmAhXV1e0atUKzs7OcrX1xo0bEIlEyM3NRUlJidDhUtNxKGubYmNj8cYbb6Bbt25yxaemSVqOdOvWDX379hX+atYqNzU1hY6ODgDF5YiXlxdiY2ORmZmJhIQErFy5EtHR0XBxcQHAHCHlUN97SU1H+8KFC/H+++8jNzcXubm5uHfvnlzxFy9ejAsXLsDPzw83btzADz/8gB07dggPv2hpaUnkbN++fdGiRQvo6uqiV69eEueKjY2Fvb39i70h1GDY2U5ERERERERERERESqNfv36wtLTEwYMHhW0aGhrYuXMnhg4dClNTU3h4eODdd9/FiRMnhDKampqIiorCrVu3YGlpCXd3d3h6ekqsxZuZmQkVFRVER0fXGf/WrVuYOnUqevXqhQkTJkBNTQ0XLlwQRiMCwGeffQYPDw+4u7vD0tISWVlZOHnyJNq2bSuUcXV1ha2t7TPbOnv2bFhYWGD79u1ISUmBhYUFLCwskJ2dLXObAGDfvn2YM2fOM2PRq0NajshCUTly+/ZtTJs2Db169cLIkSPx559/IjIyEnZ2dkIZ5gg1tvreS0JDQ1FcXAx/f3/o6ekJfxMmTBDKyJInb731FsLDw7Fv3z707dsX69atQ3BwsPBQiqzOnz+PwsJCTJw4Ua7j6OVREdfMj/CaKCoqgqamJgoLC9GuXbvGrg69wvrt6dfYVUDCjITGrgIRERERKTl+byUiInp1lZaWIiMjAyYmJmjZsmVjV0cuERERWLJkCRITE9GsmeLGjEVHR2P8+PFIT09Hhw4dFHZeaWxtbWFra4s1a9Y0aJzExESMHDkSKSkp0NTUbNBYpDyYI7Jjjry+XoU8+eCDD2BhYQEvL68GjfO6etZ3JVn7lFUbupJERERERERERERERPIYM2YMUlNTkZWVBUNDQ4WdNzIyEl5eXg3eOfLgwQOkpaVJjJZsKNnZ2di7dy87EV8zzBHZMUdeX009T8rKymBubo7Fixc3aBx6MRzZTtRAOEKIiIiIiJoCfm8lIiJ6dTXlke1EREREDU0RI9u5ZjsREREREREREREREREREZGc2NlOREREREREREREREREREQkJ3a2ExERERERERERERERERERyYmd7URERERERERERERERERERHJiZzsREREREREREREREREREZGc2NlOREREREREREREREREREQkJ3a2ExERERERERERERERERERyYmd7URERERERERERESkVPLz86Gjo4PMzMzGrgr9v7KyMhgZGSE+Pr6xq0Jgjshj69atGDduXGNXgxrB65AneXl56NSpE7Kyshq7Kq8t1cauABERERERERERERG9PMbLf36p8TIDxsp9jL+/PxwdHWFsbAygusPExcUFV69eFTpPnJyc4Ofnh3bt2gnHJSQk4OOPP8bFixfRsWNHzJ07F97e3lBRUZE59pkzZxAYGIj4+Hjk5OQgPDwc7733nkQZsViMtWvXYseOHSgoKMCgQYPw1VdfwczMTK52Llq0CGfPnkViYiJMTU0hEolqlVGWNqmrq2PJkiVYtmwZTp06JVc7m5rfTnd7qfFGjkiT+5inc+RJ+fn5MDc3R1ZWFgoKCtC+fXthnyKup6fr4eXlhUWLFiE4OFjYrkw5MmfOHKxfvx5nz57F0KFD69VOqu2/k999abE+PXCiXsfV514SHR2NzZs34+LFiygqKkKPHj2wdOlSuLi4yBX7eZ+7FRUVWLVqFSIiIpCeng5NTU2MGjUKAQEB0NfXF8rl5uZi6dKliIqKwoMHD9CrVy94eXlh4sSJAAAdHR1MmzYNq1evxjfffFOv94leDEe2ExEREREREREREZHSKCkpwa5duzB79mxhW7NmzeDk5ITjx48jJSUFoaGhOHXqFObNmyeUKSoqgp2dHfT19REXF4ctW7Zg06ZNCAoKkiv+o0ePYG5ujq1bt9ZZZuPGjQgKCsLWrVsRFxcHXV1d2NnZ4cGDB3LFEovFcHNzw+TJk6XuV7Y2ubi4IDY2FteuXZMrPimWtBx50qxZs9C/f/9a2xV1PdWIi4vDjh07pMZSphxRV1eHs7MztmzZIl8DqUmr773k3Llz6N+/P44cOYKrV6/Czc0N06dPx08//SRX/Od97hYXF+Py5cvw9vbG5cuXcfToUaSkpNSahWHatGm4fv06jh8/joSEBEyYMAGTJ0/GX3/9JZSZOXMmwsLCUFBQIFcdSTE4sp2IiIiIiIiIiIiIlMYvv/wCVVVVDB48WNjWoUMHzJ8/X3jdtWtXuLu7IzAwUNgWFhaG0tJShIaGQl1dHX379kVKSgqCgoLg6ekp88hdBwcHODg41LlfLBYjODgYK1euxIQJEwAAe/bsQefOnfHDDz9g7ty5Mrf1yy+/BADcuXMHV69erbVf2dqkpaUFa2tr7Nu3D76+vjK3kxRLWo7UCAkJwf379+Hj44NffvlFYp+iricAePjwIVxcXLBz5058/vnnEvuUMUfGjRsHe3t7lJSUQENDQ+b41HTV917i5eUlcZ6FCxfi119/RXh4OBwdHWWO/7zPXU1NTURFRUls27JlC6ysrHDz5k0YGRkBAM6fP4+QkBBYWVkBAFatWoXNmzfj8uXLsLCwAAD069cPurq6CA8Ph5ubm8x1JMXgyHYiIiIiIiIiIiIiUhpnzpyBpaXlM8tkZ2fj6NGjsLGxEbadP38eNjY2UFdXF7aNHj0a2dnZCl2vNyMjA7m5ubC3txe2qaurw8bGBufOnVNYHEA522RlZYXY2FiFxSb51ZUjycnJ8PX1xd69e9GsWe3uH0VeTwsWLMDYsWMxatSoWvuUMUcsLS1RUVGBixcvKjQ+Ka/63kukKSwsRMeOHRVZvTrjqKioSCz9MHToUBw4cAD37t1DVVUV9u/fj7KyMtja2kocy8/mxsPOdiIiIiIiIiIiIiJSGpmZmRLr1T5p6tSpaNWqFQwMDNCuXTuJ9Wlzc3PRuXNnifI1r3NzcxVWv5pzSYulyDg1sZStTQYGBgrt6Cf5ScuRsrIyTJ06FYGBgcKI2Kcp6nrav38/Ll++DH9//zrjPHnuJ2M1Vo60bt0a7du357X7GqnvveRphw8fRlxcHGbOnNlQVQUAlJaWYvny5XB2dhbWjweAAwcO4PHjx9DS0oK6ujrmzp2L8PBwdOvWTeJ4fjY3Hna2ExEREREREREREZHSKCkpQcuWLaXuq5k699ixY0hLS4Onp6fE/qenwRaLxVK3K4K0WC8rjrTtDRXr6W0aGhooLi5WeGySnbQcWbFiBUxNTfHhhx8+89gXvZ7+/fdfLFq0CN9//32defqsWI2ZI7x2Xy8vci+pER0dDVdXV+zcuRNmZmYNVteKigpMmTIFVVVV2LZtm8S+VatWoaCgAKdOncKlS5fg6emJDz74AAkJCRLleH03Hq7ZTkRERERERERERERKQ1tbGwUFBVL36erqQldXF71794aWlhaGDRsGb29v6OnpQVdXt9ao2by8PAC1R9i+CF1dXQDVo2b19PQkYikyTk0sZWvTvXv30KlTJ4XFJvlJy5HTp08jISEBhw8fBvC/DmdtbW2sXLkSa9euVcj1FB8fj7y8PAwcOFDYVllZiTNnzmDr1q0oKytT2hzhtft6qe+9pEZMTAwcHR0RFBSE6dOnN1g9KyoqMGnSJGRkZOD06dMSo9rT0tKwdetWJCYmCp395ubmiI2NxVdffYWvv/5aKMvru/FwZDsRERERERERERERKQ0LCwskJyc/t1xNZ2JZWRkAYPDgwThz5gzKy8uFMidPnoS+vj6MjY0VVj8TExPo6uoiKipK2FZeXo6YmBhYW1srLA6gnG1KTEyEhYWFwmKT/KTlyJEjR3DlyhWIRCKIRCJhWuzY2FgsWLAAgGKup5EjRyIhIUGIIxKJYGlpCRcXF4hEIjRv3lwpcyQtLQ2lpaW8dl8j9b2XANUj2seOHYuAgAB89NFHDVbHmo721NRUnDp1ClpaWhL7a0aqN2sm2Z3bvHlzVFVVSWzjZ3PjYWc7ERERERERERERESmN0aNHIykpSWJEYkREBL799lskJiYiMzMTERERmD9/PoYMGSJ0qDk7O0NdXR2urq5ITExEeHg4/Pz84OnpKdfU1Q8fPhQ6EQEgIyMDIpEIN2/eBFA9NbWHhwf8/PwQHh6OxMREuLq6olWrVnB2dparrTdu3IBIJEJubi5KSkqEuDUdh8rYptjYWNjb28vVTlIsaTnSrVs39O3bV/gzMTEBAJiamkJHRweAYq6ntm3bSsTp27cvWrduDS0tLfTt2xeAcuZIbGws3njjjVrrXNOrq773kpqO9oULF+L9999Hbm4ucnNzce/ePbniP+9z9/Hjx5g4cSIuXbqEsLAwVFZWCrFqru/evXuje/fumDt3Li5evIi0tDT897//RVRUFN577z0hVnFxMeLj4/nZ3EjY2U5ERESvjW3btsHExAQtW7bEwIEDERsb+8zyZWVlWLlyJbp27Qp1dXV069YNu3fvligTHByMXr16QUNDA4aGhli8eDFKS0uF/WfOnIGjoyP09fWhoqKCY8eONUTTiIiIiIiIXhn9+vWDpaUlDh48KGzT0NDAzp07MXToUJiamsLDwwPvvvsuTpw4IZTR1NREVFQUbt26BUtLS7i7u8PT01NiLd7MzEyoqKggOjq6zviXLl2ChYWFMELQ09MTFhYW8PHxEcp89tln8PDwgLu7OywtLZGVlYWTJ0+ibdu2QhlXV1fY2to+s62zZ8+GhYUFtm/fjpSUFCFudna2Urbp/PnzKCwsxMSJE5/ZLmpY0nJEFoq6nmShTDkCAPv27cOcOXNeqE3UtNT3XhIaGori4mL4+/tDT09P+JswYYJQRhGfu7du3cLx48dx69YtDBgwQCLWuXPnAAAtWrRAREQEOnXqBEdHR/Tv3x979+7Fnj17MGbMGCHWjz/+CCMjIwwbNkwh7x3JR0VcMz/Ca6KoqAiampooLCyUWPeASNH67enX2FVAwoyExq4CEZHSOHDgAKZNm4Zt27ZhyJAh2L59O7755hskJyfDyMhI6jFOTk64ffs2Pv/8c3Tv3h15eXl4/PixMOVZWFgYZs2ahd27d8Pa2hopKSlwdXXF5MmTsXnzZgDAL7/8gj/++ANvvvkm3n//fYSHh0s8edqQtm3bhsDAQOTk5MDMzAzBwcHP/NJdVlYGX19ffP/998jNzUWXLl2wcuVKuLm5CWWCg4MREhKCmzdvQltbGxMnToS/vz9atmwpU9yKigqsWrUKERERSE9Ph6amJkaNGoWAgADo6+s33JtBRHXi91YiIqJXV2lpKTIyMoSHjpuSiIgILFmyBImJibWmz30R0dHRGD9+PNLT09GhQweFnVcaW1tb2NraYs2aNQ0a52W26YMPPoCFhQW8vLwaNA49H3NEdomJiRg5ciRSUlKgqanZoLFIubwKeSILKysreHh4yD1zBD37u5KsfcqqDV1JIiIiImUQFBSEWbNmYfbs2QCqO41//fVXhISEwN/fv1b5yMhIxMTEID09HR07dgSAWuuXnT9/HkOGDBG+yBobG2Pq1Km4ePGiUMbBwQEODg4N1Kq6HThwAB4eHhIPFzg4ODzz4YJJkybh9u3b2LVrl8TDBTXCwsKwfPnyWg8XABAeLnhe3OLiYly+fBne3t4wNzdHQUEBPDw8MG7cOFy6dKnB3xciIiIiImoaxowZg9TUVGRlZcHQ0FBh542MjISXl1eDd448ePAAaWlpEqMlG8rLalNZWRnMzc2xePHiBo1DsmGOyC47Oxt79+5lR/trqKnniSzy8vIwceJETJ06tbGr8triyHaiBsIRQkREyqO8vBytWrXCoUOHMH78eGH7okWLIBKJEBMTU+sYd3d3pKSkwNLSEt999x1at26NcePGYd26ddDQ0AAA7N+/H/PmzcPJkydhZWWF9PR0jB07FjNmzMDy5ctrnVNFReWljWwfNGgQ3nzzTYSEhAjbTE1N8d5779X5cMGUKVMkHi542scff4xr167ht99+E7Z9+umnuHjxojAlv7xxASAuLg5WVlb4559/6nwQgIgaDr+3EhERvbqa8sh2IiIiooamiJHtXLOdiIiIXnl3795FZWUlOnfuLLG9c+fOyM3NlXpMeno6zp49i8TERISHhyM4OBiHDx/GggULhDJTpkzBunXrMHToULRo0QLdunXD8OHDpXa0v0zl5eWIj4+Hvb29xHZ7e3thzaenHT9+HJaWlti4cSMMDAzQs2dPLFmyBCUlJUKZoUOHIj4+Xhi5n56ejoiICIwdO7becQGgsLAQKioqaN++fX2aS0RERERERERERNQoOI08ERERvTZUVFQkXovF4lrbalRVVUFFRQVhYWHCNGNBQUGYOHEivvrqK2hoaCA6Ohrr16/Htm3bMGjQINy4cQOLFi2Cnp4evL29G7w9dXmRhwtatmyJ8PBw3L17F+7u7rh37x52794NoPrhgjt37mDo0KEQi8V4/Pgx5s+fLzxcUJ+4paWlWL58OZydnTnrEBERERERERERETUp7GwnIiKiV562tjaaN29eq8M3Ly+vVsdwDT09PRgYGEis52VqagqxWIxbt26hR48e8Pb2xrRp04R14Pv164dHjx7ho48+wsqVK9GsWeNOItRYDxfIGreiogJTpkxBVVUVtm3b9qLNJSIiIiIiIiIiInqpOI08ERERvfLU1NQwcOBAREVFSWyPioqCtbW11GOGDBmC7OxsPHz4UNiWkpKCZs2aoUuXLgCA4uLiWh3qzZs3h1gshlgsVnArZNcQDxcAkHi4oF+/fhg/fjz8/Pzg7++PqqoqueJWVFRg0qRJyMjIQFRUFEe1ExERERERERERUZPDznYiIiJ6LXh6euKbb77B7t27ce3aNSxevBg3b97EvHnzAAArVqzA9OnThfLOzs7Q0tLCzJkzkZycjDNnzmDp0qVwc3ODhoYGAMDR0REhISHYv3+/0Gns7e2NcePGoXnz5gCAhw8fQiQSQSQSAQAyMjIgEolw8+bNBmtrYz1cIGvcmo721NRUnDp1ClpaWi/UXiIiIiIiIiIiIqLGwGnkiYiI6LUwefJk5Ofnw9fXFzk5Oejbty8iIiLQtWtXAEBOTo5EB3ibNm0QFRWFTz75BJaWltDS0sKkSZPw+eefC2VWrVoFFRUVrFq1CllZWejUqRMcHR2xfv16ocylS5cwfPhw4bWnpycAYMaMGQgNDW2w9np6emLatGmwtLTE4MGDsWPHjloPF2RlZWHv3r0Aqh8uWLduHWbOnIm1a9fi7t27Uh8uCAoKgoWFhTCN/NMPFzwv7uPHjzFx4kRcvnwZJ06cQGVlpTASvmPHjlBTU2uw94SIiIiIiIiIiIhIkVTEjTnHaSMoKiqCpqYmCgsLOV0pNah+e/o1dhWQMCOhsatARESNaNu2bdi4caPwcMHmzZvxn//8BwDg6uqKzMxMREdHC+X//vtvfPLJJ/jjjz8kHi6o6Wx//Pgx1q9fj++++67WwwXt27eXKW5mZiZMTEyk1vf333+Hra1tg7wXRFQ3fm8lIiJ6dZWWliIjIwMmJiZo2bJlY1eHiIiISKk867uSrH3K7GwnaiD80ZKIiIiImgJ+byUiInp1NeXO9vz8fJiamuLixYswNjZu7OoQgLKyMvTo0QPh4eEYOHBgY1fntccckd3WrVtx8uRJHD9+vLGrQi8Z80R2r2ueKKKzndPIExEREREREREREb1O1mi+5HiFch/i7+8PR0dHoXMkPz8fLi4uuHr1KvLz86GjowMnJyf4+flJ/ACekJCAjz/+GBcvXkTHjh0xd+5ceHt7Q0VFRebYZ86cQWBgIOLj45GTk4Pw8HC89957EmXEYjHWrl2LHTt2oKCgAIMGDcJXX30FMzMzmeNcuXIFAQEBOHv2LO7evQtjY2PMmzcPixYtkiinLG1SV1fHkiVLsGzZMpw6dUrm2E2R7u+ilxovd/gAuY95OkeelJ+fD3Nzc2RlZaGgoEBiNjpFXE9r1qzB2rVrJbZ17txZWCYOUEyOAMCiRYtw9uxZJCYmwtTUFCKRqFaZ57Vpzpw5WL9+Pc6ePYuhQ4fKFZ/qdmt57EuL1SVgWL2Oq8+9JDo6Gps3b8bFixdRVFSEHj16YOnSpXBxcZErdkhICEJCQpCZmQkAMDMzg4+PDxwcHAAAFRUVWLVqFSIiIpCeng5NTU2MGjUKAQEB0NfXlzmOotrEPKm/Zo1dASIiIiIiIiIiIiKiGiUlJdi1axdmz54tbGvWrBmcnJxw/PhxpKSkIDQ0FKdOncK8efOEMkVFRbCzs4O+vj7i4uKwZcsWbNq0CUFBQXLFf/ToEczNzbF169Y6y2zcuBFBQUHYunUr4uLioKurCzs7Ozx48EDmOPHx8ejUqRO+//57JCUlYeXKlVixYoVEXGVrk4uLC2JjY3Ht2jW54pNiScuRJ82aNQv9+/evtV1R1xNQ3XGYk5Mj/CUkSM5WpYgcAao77d3c3DB58mSp+2Vpk7q6OpydnbFlyxa520lNV33vJefOnUP//v1x5MgRXL16FW5ubpg+fTp++uknueJ36dIFAQEBuHTpEi5duoQRI0bAyckJSUlJAIDi4mJcvnwZ3t7euHz5Mo4ePYqUlBSMGzdOrjiKahPzpP44jTxRA+F0nERERETUFPB7KxER0aurzqlRlXxk+9GjRzF37lzcuXPnmeW+/PJLBAYG4t9//wVQPYpwxYoVuH37NtTV1QEAAQEB2LJlC27duiXXyN0aKioqtUaBi8Vi6Ovrw8PDA8uWLQNQPcV6586dsWHDBsydO1fuODUWLFiAa9eu4fTp00rbpuHDh2PYsGHw9fWtdzuVnbKPbH9WjoSEhODAgQPw8fHByJEjJUa2K+p6WrNmDY4dOyZ1lDnQMDlSV0xZ2xQTEwN7e3vcv38fGhoacsen2pR9ZHt97yXSjB07Fp07d8bu3bvlrseTOnbsiMDAQMyaNUvq/ri4OFhZWeGff/6BkZFRvePUt02vY54oYhp5jmwnIiIiIiKilyL/t3xcX3IdSbOTcGP1DTy6/uiZ5cvKyrBy5Up07doV6urq6NatW60fN+7fv48FCxZAT08PLVu2hKmpKSIiIqSez9/fHyoqKvDw8JDY7urqChUVFYm/t99++4XaSkRERPV35swZWFpaPrNMdnY2jh49ChsbG2Hb+fPnYWNjI3S4AcDo0aORnZ0tTOOrCBkZGcjNzYW9vb2wTV1dHTY2Njh37twLnbuwsBAdO3YUXitjm6ysrBAb+/I62ai2unIkOTkZvr6+2Lt3L5o1q939o8jrKTU1Ffr6+jAxMcGUKVOQnp4u7GvIHHmarG2ytLRERUUFLl68qND4pLzqey+R5unPZnlVVlZi//79ePToEQYPHvzMOCoqKhJLP8jrRdrEPKkfdrYTERERERFRgyv8sxC5P+Sik2MndPPthtY9W+OfoH9Qnl9e5zGTJk3Cb7/9hl27duH69evYt28fevfuLewvLy+HnZ0dMjMzcfjwYVy/fh07d+6EgYFBrXPFxcVhx44dUqfTBIB33nlHYhrMujrsiYiIqOFlZmbWuV7t1KlT0apVKxgYGKBdu3b45ptvhH25ubno3LmzRPma10+uJf2ias4lLdaLxDl//jwOHjwoMepXGdtkYGCg0I5+kp+0HCkrK8PUqVMRGBhY54hYRV1PgwYNwt69e/Hrr79i586dyM3NhbW1NfLz8yXOpegckUbWNrVu3Rrt27fntfsaqe+95GmHDx9GXFwcZs6cKXcdEhIS0KZNG6irq2PevHkIDw9Hnz59pJYtLS3F8uXL4ezsXK+ZuRXRJuZJ/ag2dgWIiIiIXpZrvU0buwow/Zvr2hHR6+nur3fR4T8d0NGm+sl5PRc9PEx8iHun7wGetctHRkYiJiYG6enpwtP2xsbGEmV2796Ne/fu4dy5c2jRogUAoGvXrrXO9fDhQ7i4uGDnzp34/PPPpdZPXV0durq6L9BCIiIiUpSSkpJaU7nW2Lx5M1avXo3r16/Dy8sLnp6e2LZtm7D/6Wmwa1ZRrc90688jLVZ94yQlJcHJyQk+Pj6ws7N7bhxp2xVBljZpaGiguLhY4bFJdtJyZMWKFTA1NcWHH374zGMVcT05ODgI/+7Xrx8GDx6Mbt26Yc+ePfD0/N+Xe0XmyLPI2iZeu6+XF7mX1IiOjoarqyt27twJMzMzuevQq1cviEQi3L9/H0eOHMGMGTMQExNTq8O9oqICU6ZMQVVVldR6yEJRbWKeyI+d7URERESvmf9OfrdR43964ESjxieil6/qcRVKMkvQaWwnie1t+rZB8Q3p/4k/fvw4LC0tsXHjRnz33Xdo3bo1xo0bh3Xr1glrxx0/fhyDBw/GggUL8OOPP6JTp05wdnbGsmXL0Lx5c+FcCxYswNixYzFq1Kg6O9ujo6Oho6OD9u3bw8bGBuvXr4eOjo6C3gEiIiKSh7a2NgoKCqTu09XVha6uLnr37g0tLS0MGzYM3t7e0NPTg66ubq1Rs3l5eQBqj7B9ETUP6OXm5kJPT08iVn3iJCcnY8SIEZgzZw5WrVpVK5aytenevXvo1Enyex29XNJy5PTp00hISMDhw4cB/K/DWVtbGytXrsTatWsb7Hpq3bo1+vXrh9TUVACKz5FnkadNvHZfL/W9l9SIiYmBo6MjgoKCMH369HrVQU1NDd27dwdQPUV7XFwcvvjiC2zfvl0oU1FRgUmTJiEjIwOnT5+u16h2RbaJeSI/TiNPREREREREDaryQSVQBai2k3zeu3m75nhc+FjqMenp6Th79iwSExMRHh6O4OBgHD58GAsWLJAoc/jwYVRWViIiIgKrVq3Cf//7X6xfv14os3//fly+fBn+/v511s/BwQFhYWE4ffo0/vvf/yIuLg4jRoxAWVnZC7aciIiI6sPCwgLJycnPLVfTmVhzzx48eDDOnDmD8vL/LVNz8uRJ6Ovr15oh50WYmJhAV1cXUVFRwrby8nLExMTA2tparnMlJSVh+PDhmDFjhsR3mBrK2KbExERYWFgoLDbJT1qOHDlyBFeuXIFIJIJIJBKmkI6NjRW+QzfU9VRWVoZr164JnXqKzJHnkbVNaWlpKC0t5bX7GqnvvQSofhh77NixCAgIwEcffaSwOonFYok4NR3tqampOHXqFLS0tBQWB5C/TcyT+mFnOxEREREREb0cT88YKa67aFVVFVRUVBAWFgYrKyuMGTMGQUFBCA0NRUlJiVBGR0cHO3bswMCBAzFlyhSsXLkSISEhAIB///0XixYtwvfff1/n9IEAMHnyZIwdOxZ9+/aFo6MjfvnlF6SkpODnn39+0RYTERFRPYwePRpJSUkSIxIjIiLw7bffIjExEZmZmYiIiMD8+fMxZMgQoUPN2dkZ6urqcHV1FR7Y8/Pzg6enp1xTVz98+FDosASAjIwMiEQi3Lx5E0D11NQeHh7w8/NDeHg4EhMT4erqilatWsHZ2VnmODUd7XZ2dvD09ERubi5yc3Nx584doYwytik2Nhb29vYyxybFk5Yj3bp1Q9++fYU/ExMTAICpqakwY5OirqclS5YgJiYGGRkZ+PPPPzFx4kQUFRVhxowZABSXIwBw48YNiEQi5ObmoqSkRLiOazrXZW1TbGws3njjDXTr1k2u+NR01fdeUtMpvXDhQrz//vvCZ/O9e/fkiu/l5YXY2FhkZmYiISEBK1euRHR0NFxcXAAAjx8/xsSJE3Hp0iWEhYWhsrJSiPXkwyPPo8g2MU/qh53tRERERERE1KCat20ONEOtUeyVDyqhqil9dTM9PT0YGBhAU1NT2GZqagqxWIxbt24JZXr27CkxZbypqanw40R8fDzy8vIwcOBAqKqqQlVVFTExMfjyyy+hqqqKysrKOmN37dpVmAaTiIiIXq5+/frB0tISBw8eFLZpaGhg586dGDp0KExNTeHh4YF3330XJ078b5kqTU1NREVF4datW7C0tIS7uzs8PT0l1pDOzMyEiooKoqOj64x/6dIlWFhYCCP7PD09YWFhAR8fH6HMZ599Bg8PD7i7u8PS0hJZWVk4efIk2rZtK5RxdXWFra1tnXEOHTqEO3fuICwsDHp6esLfW2+9pbRtOn/+PAoLCzFx4sQ6Y1HDk5YjslDU9XTr1i1MnToVvXr1woQJE6CmpoYLFy6ga9euQhlF5AgAzJ49GxYWFti+fTtSUlKE6zg7O1vmNgHAvn37MGfOHDneLWrq6nsvCQ0NRXFxMfz9/SU+mydMmCCUkSVPbt++jWnTpqFXr14YOXIk/vzzT0RGRsLOzg5AdR4dP34ct27dwoABAyRinTt3TjjP8/JEUW0CmCf1pSKumUvgNVFUVARNTU0UFhbWe90DIln029OvsauAhBkJjV0FIiKlcq23aWNXAaZ/X2vsKnDNdiKS8LK+t6b5pkHDWAP60/WFbaleqWhr0RZ3TtypVX7Hjh3w8PBAXl4e2rRpAwD48ccfMWHCBDx8+BAaGhrw8vLCDz/8gPT0dDRrVv0s+RdffIENGzYgOzsbDx48wD///CNx3pkzZ6J3795YtmwZ+vbtK7Wu+fn5MDAwwI4dO+q9Nh8REZEyKC0tRUZGBkxMTJ45y4syioiIwJIlS5CYmCjc5xUhOjoa48ePR3p6Ojp06KCw80pja2sLW1tbrFmzpkHjvMw2ffDBB7CwsICXl1eDxqHnY47ILjExESNHjkRKSorEw7z06mOeyO51zZNnfVeStU9Z+hACIiIiIiIiIgXSHq2NWztuQcNYAxrdNVAQXYCK/Ap0HN4RALBixQpkZWVh7969AKqng1y3bh1mzpyJtWvX4u7du1i6dCnc3NygoaEBAJg/fz62bNmCRYsW4ZNPPkFqair8/PywcOFCAEDbtm1rdai3bt0aWlpawvaHDx9izZo1eP/996Gnp4fMzEx4eXlBW1sb48ePf1lvDxERET1lzJgxSE1NRVZWFgwNDRV23sjISHh5eTV458iDBw+QlpYmMbKwobysNpWVlcHc3ByLFy9u0DgkG+aI7LKzs7F3797XqgORqjFPZMc8qT+ObCdqIBzZTkSkfDiyvRpHthPRk17m99b83/JxN+IuHhc+hrqBOvSc9dC6V2skzEiAq6srMjMzJabh+/vvv/HJJ5/gjz/+gJaWFiZNmoTPP/9c6GwHqqcyXbx4MUQiEQwMDDBr1iwsW7ZMYmr5J9na2mLAgAEIDg4GAJSUlOC9997DX3/9hfv370NPTw/Dhw/HunXrFPpjDBERUWNoyiPbiYiIiBoaR7YTERERERFRk6E1UgtaI7Wk7gsNDa21rXfv3oiKinrmOQcPHowLFy7IXIen19TT0NDAr7/+KvPxREREREREREQ1FLdAARERERERERERERERERER0WuCne1ERERERERERERERERERERyYmc7ERERERERERERERERERGRnNjZTkREREREREREREREREREJCd2thMREREREREREREREREREclJtbErQERERERERK+3a71NGzW+6d/XGjU+ERERERERETVNHNlOREREREREREREREolPz8fOjo6yMzMbOyq0P8rKyuDkZER4uPjG7sqBOaIMmKOKB/miey2bt2KcePGNXY1miSObCciIiIiIiIiIiJ6jfTb0++lxkuYkSD3Mf7+/nB0dISxsTGA6g4TFxcXXL16Veg8cXJygp+fH9q1a/e/WAkJ+Pjjj3Hx4kV07NgRc+fOhbe3N1RUVGSOfebMGQQGBiI+Ph45OTkIDw/He++9J1FGLBZj7dq12LFjBwoKCjBo0CB89dVXMDMzkzlOU2uTuro6lixZgmXLluHUqVMyx26KjJf//FLjZQaMlfuYp3PkSfn5+TA3N0dWVhYKCgrQvn17YZ+yXE+yWrRoEc6ePYvExESYmppCJBLVKqMsbXqdcgQA1qxZo/Sx6nMviY6OxubNm3Hx4kUUFRWhR48eWLp0KVxcXOSKHRISgpCQEKGj38zMDD4+PnBwcAAAVFRUYNWqVYiIiEB6ejo0NTUxatQoBAQEQF9fX+62hoaGIigoCCkpKWjfvj0mTpyIrVu31ip348YNWFhYoHnz5rh//76wfc6cOVi/fj3Onj2LoUOHyh3/dcaR7URERERERERERESkNEpKSrBr1y7Mnj1b2NasWTM4OTnh+PHjSElJQWhoKE6dOoV58+YJZYqKimBnZwd9fX3ExcVhy5Yt2LRpE4KCguSK/+jRI5ibm0vtpKixceNGBAUFYevWrYiLi4Ouri7s7Ozw4MEDmeM0xTa5uLggNjYW165xGZ7GJC1HnjRr1iz079+/1nZlu55kIRaL4ebmhsmTJ0vdr2xtYo4oj/reS86dO4f+/fvjyJEjuHr1Ktzc3DB9+nT89NNPcsXv0qULAgICcOnSJVy6dAkjRoyAk5MTkpKSAADFxcW4fPkyvL29cfnyZRw9ehQpKSn1Gl0eFBSElStXYvny5UhKSsJvv/2G0aNH1ypXUVGBqVOnYtiwYbX2qaurw9nZGVu2bJE7/uuOI9uJiIiIiIhe0LZt2xAYGIicnByYmZkhODhY6n9ea5SVlcHX1xfff/89cnNz0aVLF6xcuRJubm61yu7fvx9Tp06Fk5MTjh07Jmz39/fH0aNH8ffff0NDQwPW1tbYsGEDevXqJZQ5evQotm/fjvj4eOTn5+Ovv/7CgAEDhP2ZmZlIdE2UWkdDd0NoWmkCAK5/eh0V+RUS+7XHaEN3kq4sbw8RERGRXH755Reoqqpi8ODBwrYOHTpg/vz5wuuuXbvC3d0dgYGBwrawsDCUlpYiNDQU6urq6Nu3L1JSUhAUFARPT0+ZR7k6ODgIIw+lEYvFCA4OxsqVKzFhwgQAwJ49e9C5c2f88MMPmDt3rkxxmmKbtLS0YG1tjX379sHX11em2KR40nKkRkhICO7fvw8fHx/88ssvEvuU7XqSxZdffgkAuHPnDq5evVprv7K1iTmiPOp7L/Hy8pI4z8KFC/Hrr78iPDwcjo6OMsd/uuz69esREhKCCxcuwMzMDJqamoiKipIos2XLFlhZWeHmzZswMjKSKU5BQQFWrVqFn376CSNHjhS2S5tFYtWqVejduzdGjhyJc+fO1do/btw42Nvbo6SkBBoaGjLFJ45sJyIiIiIieiEHDhyAh4cHVq5cib/++gvDhg2Dg4MDbt68WecxkyZNwm+//YZdu3bh+vXr2LdvH3r37l2r3D///IMlS5ZI7biPiYnBggULcOHCBURFReHx48ewt7fHo0ePhDKPHj3CkCFDEBAQILUehoaG6BXcS+JPZ7wOmqk3Q5v+bSTK6ozXkSjXaVwnWd8iIiIiIrmcOXMGlpaWzyyTnZ2No0ePwsbGRth2/vx52NjYQF1dXdg2evRoZGdnK3S93oyMDOTm5sLe3l7Ypq6uDhsbG6mdF7JqKm2ysrJCbGyswmKT/OrKkeTkZPj6+mLv3r1o1qx2948yXk8vShnbxBxRDvW9l0hTWFiIjh071rsulZWV2L9/Px49eiT1IZkn46ioqEgs/fA8UVFRqKqqQlZWFkxNTdGlSxdMmjQJ//77r0S506dP49ChQ/jqq6/qPJelpSUqKipw8eJFmeMTO9uJiIiIiIheSFBQEGbNmoXZs2fD1NQUwcHBMDQ0REhIiNTykZGRiImJQUREBEaNGgVjY2NYWVnB2tpaolxlZSVcXFywdu1avPHGG1LP4+rqCjMzM5ibm+Pbb7/FzZs3ER8fL5SZNm0afHx8MGrUKKl1ad68OVq0byHxVxRfhHZW7dC8ZXOJss1aNpMo9/R+IiIiIkXJzMysc73aqVOnolWrVjAwMEC7du3wzTffCPtyc3PRuXNnifI1r3NzcxVWv5pzSYtVnzhNrU0GBgYK7cQk+UnLkbKyMkydOhWBgYF1johVxutJEbGUrU3MEeVQ33vJ0w4fPoy4uDjMnDlT7jokJCSgTZs2UFdXx7x58xAeHo4+ffpILVtaWorly5fD2dlZWD9eFunp6aiqqoKfnx+Cg4Nx+PBh3Lt3D3Z2digvLwdQvVa9q6srQkNDn3nu1q1bo3379rx+5cTOdiIiIiIionoqLy9HfHy8xOgGALC3t69zxMbx48dhaWmJjRs3wsDAAD179sSSJUtQUlIiUc7X1xedOnXCrFmzZKpLYWEhALzQ0/YlmSUovVmKjv+pfY67EXdxbcE13PC+gbzjeah6XFXvOERERETPUlJSgpYtW0rdt3nzZly+fBnHjh1DWloaPD09JfY/PWW0WCyWul0RpMWqT5ym1iYNDQ0UFxcrPDbJTlqOrFixAqampvjwww+feayyXU8NFUfa9oaKxRxRTi9yL6kRHR0NV1dX7Ny5U+q07M/Tq1cviEQiXLhwAfPnz8eMGTOQnJxcq1xFRQWmTJmCqqoqbNu2Ta4YVVVVqKiowJdffonRo0fj7bffxr59+5Camorff/8dADBnzhw4OzvjP//5z3PPx+tXflyznYiIiIiIqJ7u3r2LyspKuUZspKen4+zZs2jZsiXCw8Nx9+5duLu74969e9i9ezcA4I8//sCuXbsgEolkqodYLIanpyeGDh2Kvn371rs9BWcKoK6vjlY9Wkls17LXgkZXDTRv3RzF6cW4ffg2Ku5WwMDNoN6xiIiIiOqira2NgoICqft0dXWhq6uL3r17Q0tLC8OGDYO3tzf09PSgq6tb6ztYXl4egNqjUV+Erq4ugOqRrnp6ehKx6hOnqbXp3r176NSJSwo1Jmk5cvr0aSQkJODw4cMA/tfhrK2tjZUrV2Lt2rVKeT0pIpaytYk5ohzqey+pERMTA0dHRwQFBWH69On1qoOamhq6d+8OoHqK9ri4OHzxxRfYvn27UKaiogKTJk1CRkYGTp8+LdeodgBCnZ8cMd+pUydoa2sLy9udPn0ax48fx6ZNmwBUfz5UVVVBVVUVO3bsgJubm3Asr1/5cWQ7ERERERHRC5JnxEZVVRVUVFQQFhYGKysrjBkzBkFBQQgNDUVJSQkePHiADz/8EDt37oS2trZM8T/++GNcvXoV+/btq3cbqsqrcP/8fXQY1qHWPu3R2mjduzVaGrZER5uO0J+uj4IzBXj88HG94xERERHVxcLCQurIv6fVdCaWlZUBAAYPHowzZ84I0+YCwMmTJ6Gvrw9jY2OF1c/ExAS6urqIiooStpWXlyMmJqbW0kDyagptSkxMhIWFhcJik/yk5ciRI0dw5coViEQiiEQiYVrs2NhYLFiwAIByXk8vShnbxBxRDvW9lwDVI9rHjh2LgIAAfPTRRwqrk1gslohT09GempqKU6dOQUtLS+5zDhkyBABw/fp1Ydu9e/dw9+5ddO3aFQBw/vx54bNBJBLB19cXbdu2hUgkwvjx44Xj0tLSUFpayutXThzZTkREREREVE/a2tpo3ry51JEUdY2i0NPTg4GBATQ1NYVtpqamEIvFuHXrFh49eoTMzEw4OjoK+6uqqqdsV1VVxfXr19GtWzdh3yeffILjx4/jzJkz6NKlS73bUhhXCHG5GO2HtH9u2Vbdq0e+l98uh2ob/reSiIiIFGv06NFYsWIFCgoK0KFD9YOAERERuH37Nt566y20adMGycnJ+OyzzzBkyBChQ83Z2Rlr166Fq6srvLy8kJqaCj8/P/j4+Mg1nfTDhw9x48YN4XVGRgZEIhE6duwIIyMjqKiowMPDA35+fujRowd69OgBPz8/tGrVCs7OzjLHaaptio2Nxbp162SOTYonLUee/D8CUD0LF1D9f4327dsDUM7r6Xlu3LiBhw8fIjc3FyUlJcLsX3369IGamppStok5ohzqey+p6WhftGgR3n//feH/+2pqanIt2+bl5QUHBwcYGhriwYMH2L9/P6KjoxEZGQkAePz4MSZOnIjLly/jxIkTqKysFGJ17NgRampqMsXp2bMnnJycsGjRIuzYsQPt2rXDihUr0Lt3bwwfPhxA9efAky5duoRmzZrVmhkvNjYWb7zxRq3PE3o2jmwnIiIiIiKqJzU1NQwcOFBidAMAREVF1TliY8iQIcjOzsbDhw+FbSkpKWjWrBm6dOmC3r17IyEhQeKp83HjxmH48OEQiUQwNDQEUP1E/Mcff4yjR4/i9OnTMDExeaG2FJwpQFuLtlBt9/zO85J/qteXV23PjnYiIiJSvH79+sHS0hIHDx4UtmloaGDnzp0YOnQoTE1N4eHhgXfffRcnTpwQymhqaiIqKgq3bt2CpaUl3N3d4enpKbEWb2ZmJlRUVBAdHV1n/EuXLsHCwkIY2efp6QkLCwv4+PgIZT777DN4eHjA3d0dlpaWyMrKwsmTJ9G2bVuhjKurK2xtbeuM0xTbdP78eRQWFmLixIl1xqKGJy1HZKFs19PzcgQAZs+eDQsLC2zfvh0pKSlC3OzsbKVsE3NEedT3XhIaGori4mL4+/tDT09P+JswYYJQRpZr6vbt25g2bRp69eqFkSNH4s8//0RkZCTs7OwAALdu3cLx48dx69YtDBgwQCLWuXPnhPPIkid79+7FoEGDMHbsWNjY2KBFixaIjIxEixYt5HrP9u3bhzlz5sh1DAEq4pr5EV4TRUVF0NTURGFhodzrHhDJo9+efo1dBSTMSGjsKhARKZVrvU2fX6iBmf59rbGrgP9OfrdR43964MTzCxE1IQcOHMC0adPw9ddfY/DgwdixYwd27tyJpKQkdO3aFStWrEBWVhb27t0LoHrEhKmpKd5++22sXbsWd+/exezZs2FjY4OdO3dKjeHq6or79+/j2LFjwjZ3d3f88MMP+PHHH9GrVy9hu6amJjQ0NABUTx138+ZNZGdnY+zYsdi/fz969eolrE8HVH9vLbtdhtTlqei6uCva9m8rEbv4RjGK04rRundrNG/VHCUZJcj5IQcaJhrouqirQt7Dg/6NOx29Mnw2ExERNYTS0lJkZGTAxMQELVu2bOzqyCUiIgJLlixBYmIimjVT3Jix6OhojB8/Hunp6cJIx4Zia2sLW1tbrFmzpkHjvMw2ffDBB7CwsICXl1eDxqHnY47Ijjny+mKeyC4xMREjR45ESkqKxEx8r7pnfVeStU+ZwxCIiIiIiIhewOTJk5Gfnw9fX1/k5OSgb9++iIiIENZGy8nJwc2bN4Xybdq0QVRUFD755BNYWlpCS0sLkyZNwueffy5X3JCQEACo9YT7t99+C1dXVwDA8ePHMXPmTGHflClTAACrV6+W+I96QWwBVDuook3fNrXiqKiqoPDPQuQdy4P4sRgttFqgg00HdBrTSa76EhEREcljzJgxSE1NRVZWljCzjyJERkbCy8urwTtHHjx4gLS0NInRkg3lZbWprKwM5ubmWLx4cYPGIdkwR2THHHl9MU9kl52djb17975WHe2KwpHtRA2EI9uJiJQPR7ZX48h2InqSMnxv5ch2IiKihtGUR7YTERERNTRFjGznmu1ERERERERERERERERERERyYmc7ERERERERERERERERERGRnNjZTkREREREREREREREREREJCd2thMREREREREREREREREREcmJne1ERERERERERERERERERERyUm3sChAREREREb221mg2dg0AE6PGrgGRwm3btg2BgYHIycmBmZkZgoODMWzYsDrLl5WVwdfXF99//z1yc3PRpUsXrFy5Em5ubgCAnTt3Yu/evUhMTAQADBw4EH5+frCyshLOERISgpCQEGRmZgIAzMzM4OPjAwcHB4lY165dw7JlyxATE4OqqiqYmZnh4MGDMDIyQmZmJkxMTKTW8eDBg/jggw8AAMbGxvjnn38k9i9btgwBAQHyvVFERERERET0QtjZTkRERERERESvjAMHDsDDwwPbtm3DkCFDsH37djg4OCA5ORlGRtIfLpk0aRJu376NXbt2oXv37sjLy8Pjx4+F/dHR0Zg6dSqsra3RsmVLbNy4Efb29khKSoKBgQEAoEuXLggICED37t0BAHv27IGTkxP++usvmJmZAQDS0tIwdOhQzJo1C2vXroWmpiauXbuGli1bAgAMDQ2Rk5MjUbcdO3Zg48aNtTrtfX19MWfOHOF1mzZtXvCdIyIiIiIiInlxGnkiIiIiIiIiemUEBQVh1qxZmD17NkxNTREcHAxDQ0OEhIRILR8ZGYmYmBhERERg1KhRMDY2hpWVFaytrYUyYWFhcHd3x4ABA9C7d2/s3LkTVVVV+O2334Qyjo6OGDNmDHr27ImePXti/fr1aNOmDS5cuCCUWblyJcaMGYONGzfCwsICb7zxBsaOHQsdHR0AQPPmzaGrqyvxFx4ejsmTJ9fqTG/btq1EOXa2E9GrJj8/Hzo6OsKMIdT4ysrKYGRkhPj4+MauCoE5ooyYI8qHeaJYS5YswcKFCxu7GkqHne1ERERERERE9EooLy9HfHw87O3tJbbb29vj3LlzUo85fvw4LC0tsXHjRhgYGKBnz55YsmQJSkpK6oxTXFyMiooKdOzYUer+yspK7N+/H48ePcLgwYMBAFVVVfj555/Rs2dPjB49Gjo6Ohg0aBCOHTtWZ5z4+HiIRCLMmjWr1r4NGzZAS0sLAwYMwPr161FeXl7neYiInnatt+lL/asPf39/ODo6wtjYGEB1h8k777wDfX19qKurw9DQEB9//DGKiookjktISICNjQ00NDRgYGAAX19fiMViuWKfOXMGjo6O0NfXh4qKitTParFYjDVr1kBfXx8aGhqwtbVFUlJSvdpa074uXbpARUUF9+/fV8o2qaurY8mSJVi2bFl9mti0rNF8uX/18HSOPKkpXE+yuHLlCqZOnQpDQ0NoaGjA1NQUX3zxRa1yytKm1ypHAPx2uttL+6uv+txLoqOj4eTkBD09PbRu3RoDBgxAWFiY3LFDQkLQv39/tGvXDu3atcPgwYPxyy+/CPsrKiqwbNky9OvXD61bt4a+vj6mT5+O7OxsuWMtWrQIAwcOhLq6OgYMGFBrv6xtCgsLg7m5OVq1agU9PT3MnDkT+fn5wv7PPvsM3377LTIyMuSu46uMne1ERERERERE9Eq4e/cuKisr0blzZ4ntnTt3Rm5urtRj0tPTcfbsWSQmJiI8PBzBwcE4fPgwFixYUGec5cuXw8DAAKNGjZLYnpCQgDZt2kBdXR3z5s1DeHg4+vTpAwDIy8vDw4cPERAQgHfeeQcnT57E+PHjMWHCBMTExEiNs2vXLpiamkqMsgeqf0zbv38/fv/9d3z88ccIDg6Gu7v7c98fIqKmoqSkBLt27cLs2bOFbc2aNYOTkxOOHz+OlJQUhIaG4tSpU5g3b55QpqioCHZ2dtDX10dcXBy2bNmCTZs2ISgoSK74jx49grm5ObZu3VpnmY0bNyIoKAhbt25FXFwcdHV1YWdnhwcPHsjfYACzZs1C//79a21Xtja5uLggNjYW165dkys+KZa0HHlSU7menic+Ph6dOnXC999/j6SkJKxcuRIrVqyQiKtsbWKOKI/63kvOnTuH/v3748iRI7h69Src3Nwwffp0/PTTT3LFr1lm6tKlS7h06RJGjBgBJycn4QGN4uJiXL58Gd7e3rh8+TKOHj2KlJQUjBs3Tu62isViuLm5YfLkyVL3y9Kms2fPYvr06Zg1axaSkpJw6NAhxMXFSbx/Ojo6sLe3x9dffy13HV9lXLOdiIiIiIiIiF4pKioqEq/FYnGtbTWqqqqgoqKCsLAwaGpWjywLCgrCxIkT8dVXX0FDQ0Oi/MaNG7Fv3z5ER0cLa63X6NWrF0QiEe7fv48jR45gxowZiImJQZ8+fVBVVQUAcHJywuLFiwEAAwYMwLlz5/D111/DxsZG4lwlJSX44Ycf4O3tXavONccDQP/+/dGhQwdMnDhRGO1ORNTU/fLLL1BVVRVmBwGADh06YP78+cLrrl27wt3dHYGBgcK2sLAwlJaWIjQ0FOrq6ujbty9SUlIQFBQET0/POu8FT3NwcICDg0Od+8ViMYKDg7Fy5UpMmDABALBnzx507twZP/zwA+bOnStXe0NCQnD//n34+PhIjHpUxjZpaWnB2toa+/btg6+vr1ztJMWRliM1mtL19Dxubm4Sr9944w2cP38eR48exccff6yUbWKOKI/63ku8vLwkzrNw4UL8+uuvCA8Ph6Ojo8zxny67fv16hISE4MKFCzAzM4OmpiaioqIkymzZsgVWVla4efMmjIyMZI715ZdfAgDu3LmDq1ev1tovS5suXLgAY2NjYZp4ExMTzJ07Fxs3bpQ4dty4cfD29saGDRtkrt+rjiPbiYiIiIiIiOiVoK2tjebNm9caxZ6Xl1drtHsNPT09GBgYCB3tAGBqagqxWIxbt25JlN20aRP8/Pxw8uRJqaPF1NTU0L17d1haWsLf3x/m5ubCVKfa2tpQVVUVRro/GevmzZu1znX48GEUFxdj+vTpz23322+/DQC4cePGc8sSETUFZ86cgaWl5TPLZGdn4+jRoxIPK50/fx42NjZQV1cXto0ePRrZ2dkKXa83IyMDubm5EsuWqKurw8bGps5lS+qSnJwMX19f7N27F82a1f65XhnbZGVlhdjYWIXFJvnVlSNN8XqSV2FhocRSPsrYJuaIcqjvvUSap687eUlbZqquOCoqKmjfvn29Y8nq6TZZW1vj1q1biIiIgFgsxu3bt3H48GGMHTtW4jgrKyv8+++/+Oeffxq8jk0FO9uJiIiIiIiI6JWgpqaGgQMH1hohEhUVVWsq9hpDhgxBdnY2Hj58KGxLSUlBs2bN0KVLF2FbYGAg1q1bh8jIyOf+aFdDLBajrKxMqNtbb72F69evS5RJSUlB165dax27a9cujBs3Dp06dXpunL/++gtA9YMDRESvgszMTOjr60vdN3XqVLRq1QoGBgZo164dvvnmG2Ffbm6u1KVEavYpSs255Fm2RJqysjJMnToVgYGBdY5gVMY2GRgYKLQTk+QnLUea6vUkj/Pnz+PgwYMSI+OVsU3MEeVQ33vJ0w4fPoy4uDjMnDlT7jo8a5mpp5WWlmL58uVwdnZGu3bt5I4lD2ltsra2RlhYGCZPngw1NTXo6uqiffv22LJli8SxBgYGAMBr/AnsbCciIiIiIiKiV4anpye++eYb7N69G9euXcPixYtx8+ZNYR3GFStWSIwWd3Z2hpaWFmbOnInk5GScOXMGS5cuhZubmzCF/MaNG7Fq1Srs3r0bxsbGyM3NRW5urkQHvZeXF2JjY5GZmYmEhASsXLkS0dHRcHFxEcosXboUBw4cwM6dO3Hjxg1s3boVP/30U6311m/cuIEzZ85IXYf1/Pnz2Lx5M0QiETIyMoQfnMeNGyfXVJNERMqspKSk1lIdNTZv3ozLly/j2LFjSEtLg6enp8R+aUuJSNuuCPIsWyLNihUrYGpqig8//FDuONK2K4IsbdLQ0EBxcbHCY5PspOVIU72eZJWUlAQnJyf4+PjAzs7uuXGkbVcE5kjT8SL3khrR0dFwdXXFzp07YWZmJncdapaZunDhAubPn48ZM2YgOTm5VrmKigpMmTIFVVVV2LZtm9xx5FFXm5KTk7Fw4UL4+PggPj4ekZGRyMjIkFjPHoDwfyRe4//DNduJiIiIiIiI6JUxefJk5Ofnw9fXFzk5Oejbty8iIiKE0eM5OTkS07a3adMGUVFR+OSTT2BpaQktLS1MmjQJn3/+uVBm27ZtKC8vx8SJEyVirV69GmvWrAEA3L59G9OmTUNOTg40NTXRv39/REZGSvwYPH78eHz99dfw9/fHwoUL0atXLxw5cgRDhw6VOO/u3bthYGAgMU1pDXV1dRw4cABr165FWVkZunbtijlz5uCzzz574feOiEhZaGtro6CgQOo+XV1d6Orqonfv3tDS0sKwYcPg7e0NPT096OrqSl1KBKg9GvVF6OrqAqge6frkrCLPWrZEmtOnTyMhIQGHDx8G8L8OQm1tbaxcuRJr165Vyjbdu3dPpplXqOFIy5Gmej3JIjk5GSNGjMCcOXOwatWqWrGUrU3MEeVQ33tJjZiYGDg6OiIoKEimpZ2kqVlmCgAsLS0RFxeHL774Atu3bxfKVFRUYNKkScjIyMDp06cbdFT7s9rk7++PIUOGYOnSpQCA/v37o3Xr1hg2bBg+//xz4b25d+8eAPAafwI724mIiIiIiIjoleLu7l5rtHiN0NDQWtt69+5da+r5J8kyReKuXbtkqpubmxvc3NyeWcbPzw9+fn5S97355pu4cOGCTLGIiJoqCwsLfP/9988tV9OZWLNkx+DBg+Hl5YXy8nKoqakBAE6ePAl9fX0YGxsrrH4mJibQ1dVFVFQULCwsAADl5eWIiYnBhg0bZD7PkSNHUFJSIryOi4uDm5sbYmNj0a1bN6VtU2JiolCGGoe0HGmq19PzJCUlYcSIEZgxYwbWr19fa78ytok5ohzqey8Bqkd/v/vuu9iwYQM++ugjhdXpyWWmgP91tKempuL333+HlpaWwmI97XltKi4uhqqqZLdx8+bNhXrXSExMRIsWLeo10v9VxWnkiYiIiIiIiIiIiEhpjB49GklJSRIjEiMiIvDtt98iMTERmZmZiIiIwPz58zFkyBChQ83Z2Rnq6upwdXVFYmIiwsPD4efnB09PT7mmk3748CFEIhFEIhEAICMjAyKRSJgZRUVFBR4eHvDz80N4eDgSExPh6uqKVq1awdnZWeY43bp1Q9++fYU/ExMTAICpqSl0dHSUtk2xsbFSZ1+hl0dajjTV6+lZkpKSMHz4cNjZ2cHT01NYyufOnTtCGWVsE3NEOdT3XhIdHY2xY8di4cKFeP/994XrrmZEt6yet8zU48ePMXHiRFy6dAlhYWGorKwUYpWXl8sV68aNGxCJRMjNzUVJSYlwLdecR5Y2OTo64ujRowgJCUF6ejr++OMPLFy4EFZWVtDX1xfKxcbGYtiwYcJ08sSR7URERERERERERESvFdO/rzV2FZ6pX79+sLS0xMGDBzF37lwA1WvE7ty5E4sXL0ZZWRkMDQ0xYcIELF++XDhOU1MTUVFRWLBgASwtLdGhQwd4enpKrMWbmZkJExMT/P7777C1tZUa/9KlSxg+fLjwuub4GTNmCDOkfPbZZygpKYG7uzsKCgowaNAgnDx5Em3bthWOc3V1RWZmJqKjo+v9Xihbm86fP4/CwsJaS6u8ctYUNnYNnklajshC2a6n5+XIoUOHcOfOHYSFhSEsLEzY3rVrV2HmIWVr02uTIwBGjkhr7Co8U33vJaGhoSguLoa/vz/8/f2F7TY2NsK1Kss19bxlpm7duoXjx48DAAYMGCBx7JPnleVeMnv2bMTExAiva2ZWyMjIgLGxsUxtcnV1xYMHD7B161Z8+umnaN++PUaMGFFr5oZ9+/Zh7dq1ddbldaQifnLs/2ugqKgImpqaKCwsbNB1D4j67enX2FVAwoyExq4CEZFSudbbtLGroBQ/av138ruNGv/TAycaNT6RUlmj2dg1QD8To8auAg76P27U+Mrw2UxERNQQSktLkZGRARMTE7Rs2bKxqyOXiIgILFmyBImJiWjWTHETtEZHR2P8+PFIT09Hhw4dFHZeaWxtbWFra4s1a9Y0aJyX2aYPPvgAFhYW8PLyatA49HzMEdkxR15fzBPF+vnnn7F06VJcvXq11pTzTdWzvivJ2qf8arwTRERERERERERERPTKGDNmDFJTU5GVlQVDQ0OFnTcyMhJeXl4N3jny4MEDpKWl4cSJhn/Y+GW1qaysDObm5li8eHGDxiHZMEdkxxx5fTFPFOvRo0f49ttvX5mOdkXhyHaiBsKR7UREyocj26txZDuREuHIdgAc2U5ERNRQmvLIdiIiIqKGxpHtREREREREREQK8tvpbo1dBaVf+5KIiIiIiIj+R3ELFBAREREREREREREREREREb0m2NlOREREREREREREREREREQkJ3a2ExERERERERERERERERERyYmd7URERERERERERERERERERHJiZzsREREREREREREREREREZGc2NlOREREREREREREREREREQkJ3a2ExEREREREREREZFSyc/Ph46ODjIzMxu7KvT/ysrKYGRkhPj4+MauCoE5ooyYI8qHedI05eXloVOnTsjKymrsqshEtbErQEREREREREREREQvz1fzTr/UeAu+HiH3Mf7+/nB0dISxsTGA6g4TFxcXXL16Veg8cXJygp+fH9q1ayccl5CQgI8//hgXL15Ex44dMXfuXHh7e0NFRUXm2GfOnEFgYCDi4+ORk5OD8PBwvPfeexJlxGIx1q5dix07dqCgoACDBg3CV199BTMzM7naKa1eISEhmDdvntK1SV1dHUuWLMGyZctw6tQpudrZ1PTb0++lxkuYkSD3MU/nyJPy8/Nhbm6OrKwsFBQUoH379v+LpSTXkyyaWt6/TjkCALq/i15arNzhA+p1XH3uJdHR0di8eTMuXryIoqIi9OjRA0uXLoWLi4tcsUNCQhASEiJ09JuZmcHHxwcODg4AgIqKCqxatQoRERFIT0+HpqYmRo0ahYCAAOjr68sVa9GiRTh79iwSExNhamoKkUgksV/WNoWFhWHjxo1ITU2FpqYm3nnnHWzatAlaWloy12XNmjXYv38//v33X6ipqWHgwIFYv349Bg0aBAC4d+8eVq9ejZMnT+Lff/+FtrY23nvvPaxbtw6ampoAAB0dHUybNg2rV6/GN998I9d70Rg4sp2IiIiIiIiIiIiIlEZJSQl27dqF2bNnC9uaNWsGJycnHD9+HCkpKQgNDcWpU6ckOqWLiopgZ2cHfX19xMXFYcuWLdi0aROCgoLkiv/o0SOYm5tj69atdZbZuHEjgoKCsHXrVsTFxUFXVxd2dnZ48OCB3O399ttvkZOTI/zNmDFDadvk4uKC2NhYXLt2Te52kuJIy5EnzZo1C/3796+1Xdmup+dpinnPHFEe9b2XnDt3Dv3798eRI0dw9epVuLm5Yfr06fjpp5/kit+lSxcEBATg0qVLuHTpEkaMGAEnJyckJSUBAIqLi3H58mV4e3vj8uXLOHr0KFJSUjBu3Di52yoWi+Hm5obJkydL3S9Lm86ePYvp06dj1qxZSEpKwqFDhxAXF1fn50xdevbsia1btyIhIQFnz56FsbEx7O3tcefOHQBAdnY2srOzsWnTJiQkJCA0NBSRkZGYNWuWxHlmzpyJsLAwFBQUyPluvHwqYrFY3NiVeJmKioqgqamJwsJCiSefiBTtZT/9KE19nogkInqVXett2thVgOnfjf+frf9OfrdR43964ESjxidSKms0G7sG6Gdi1NhVwEH/x40aXxk+m0k5/Ha6W2NXASNHpDV2FYjoFVJaWoqMjAyYmJigZcuWwnZlH9l+9OhRzJ07V/hhvi5ffvklAgMD8e+//wKoHkW4YsUK3L59G+rq6gCAgIAAbNmyBbdu3ZJrlGsNFRWVWiNcxWIx9PX14eHhgWXLlgGonj66c+fO2LBhA+bOnftC53+SMrZp+PDhGDZsGHx9feWO3VQo+8j2Z+VISEgIDhw4AB8fH4wcOVJiZLsyXk/yagp5/zrkCKD8I9vrey+RZuzYsejcuTN2794tdz2e1LFjRwQGBtbqWK4RFxcHKysr/PPPPzAykv//6mvWrMGxY8dqjWyX5uk2bdq0CSEhIUhL+9//R7Zs2YKNGzc+8715npp+2VOnTmHkyJFSyxw6dAgffvghHj16BFXV/03KbmJiAm9vb7i5udU7/vPU9V3pybo/r0+ZI9uJiIiIiIiIiIiISGmcOXMGlpaWzyyTnZ2No0ePwsbGRth2/vx52NjYCB1uADB69GhkZ2crdL3ejIwM5Obmwt7eXtimrq4OGxsbnDt3Tu7zffzxx9DW1sZbb72Fr7/+GlVVVcI+ZWyTlZUVYmNjFRab5FdXjiQnJ8PX1xd79+5Fs2a1u3+U8XqSR1PJe+aIcqjvvUSawsJCdOzYsd51qaysxP79+/Ho0SMMHjz4mXFUVFQkln5oKE+3ydraGrdu3UJERATEYjFu376Nw4cPY+zYsfWOUV5ejh07dkBTUxPm5ubPrEu7du0kOtqBppNL7GwnIiIiIiIiIiIiIqWRmZlZ53q1U6dORatWrWBgYIB27dpJrOWam5uLzp07S5SveZ2bm6uw+tWcS1oseeOsW7cOhw4dwqlTpzBlyhR8+umn8PPzk4ilbG0yMDBQaCcmyU9ajpSVlWHq1KkIDAysc0SsMl5Psmhqec8cUQ71vZc87fDhw4iLi8PMmTPlrkNCQgLatGkDdXV1zJs3D+Hh4ejTp4/UsqWlpVi+fDmcnZ0bfGZuaW2ytrZGWFgYJk+eDDU1Nejq6qJ9+/bYsmWL3Oc/ceIE2rRpg5YtW2Lz5s2IioqCtra21LL5+flYt26d1BkvmkousbOdiIiIiIiIiIiIiJRGSUlJralca2zevBmXL1/GsWPHkJaWBk9PT4n9T08ZXbOKan2mkn4eabHkjbNq1SoMHjwYAwYMwKeffgpfX18EBgY+N4607YogS5s0NDRQXFys8NgkO2k5smLFCpiamuLDDz985rHKdj3JoqnlPXNEObzIvaRGdHQ0XF1dsXPnTpiZmcldh169ekEkEuHChQuYP38+ZsyYgeTk5FrlKioqMGXKFFRVVWHbtm1yx5FHXW1KTk7GwoUL4ePjg/j4eERGRiIjI0NiPXtZDR8+HCKRCOfOncM777yDSZMmIS8vr1a5oqIijB07Fn369MHq1atr7W8qucTOdiIiIiIiIiIiIiJSGtra2igoKJC6T1dXF71794aTkxO2b9+OkJAQ5OTkCPueHmFa8+P+06NRX4Suri6A2qNm8/LyXjjO22+/jaKiIty+fVuIpWxtunfvHjp16qSw2CQ/aTly+vRpHDp0CKqqqlBVVRXWRtbW1hY6sZTxepL1fE0p75kjyqG+95IaMTExcHR0RFBQEKZPn16vOqipqaF79+6wtLSEv78/zM3N8cUXX0iUqaiowKRJk5CRkYGoqKgGHdX+rDb5+/tjyJAhWLp0Kfr374/Ro0dj27Zt2L17d6335nlat26N7t274+2338auXbugqqqKXbt2SZR58OAB3nnnHbRp0wbh4eFo0aJFrfM0lVxiZzsRERERERERETW4bdu2wcTEBC1btsTAgQOfuf5idHQ0VFRUav39/fffEuWCg4PRq1cvaGhowNDQEIsXL0Zpaamw//Hjx1i1ahVMTEygoaGBN954A76+vhLrIbu6utaK8/bbb0vEsbW1rVVmypQpCnpniOhpFhYWUkf+Pa1m9GpZWRkAYPDgwThz5gzKy8uFMidPnoS+vj6MjY0VVj8TExPo6uoiKipK2FZeXo6YmBhYW1u/0Ln/+usvtGzZUlivVxnblJiYCAsLC4XFJvlJy5EjR47gypUrEIlEEIlEwrTYsbGxWLBgAQDlvJ7k1RTynjmiHOp7LwGqv4uOHTsWAQEB+OijjxRWJ7FYLBGnpqM9NTUVp06dgpaWlsJiPe15bSouLkazZpLdxs2bNxfq/SKebndRURHs7e2hpqaG48eP1zkDQVPJJdXnFyEiIiIiIiIiIqq/AwcOwMPDA9u2bcOQIUOwfft2ODg4IDk5uc51ZQHg+vXrEqN7nhzZEhYWhuXLl2P37t2wtrZGSkoKXF1dAVRPDQoAGzZswNdff409e/bAzMwMly5dwsyZM6GpqYlFixYJ53rnnXfw7bffCq/V1NRq1WXOnDnw9fUVXmtoaMj/RhCRTEaPHo0VK1agoKAAHTp0AABERETg9u3beOutt9CmTRskJyfjs88+w5AhQ4QONWdnZ6xduxaurq7w8vJCamoq/Pz84OPjI9d00g8fPsSNGzeE1xkZGRCJROjYsSOMjIygoqICDw8P+Pn5oUePHujRowf8/PzQqlUrODs7yxznp59+Qm5uLgYPHgwNDQ38/vvvWLlyJT766COoq6srbZtiY2Oxbt06mWOT4knLkW7dukmUuXv3LgDA1NRUeHhDGa+nZ2mqec8cUQ71vZfUdEovWrQI77//vjCbgZqaGjp27ChzfC8vLzg4OMDQ0BAPHjzA/v37ER0djcjISADVD4VOnDgRly9fxokTJ1BZWSnE6tixo9Tvo3W5ceMGHj58iNzcXJSUlEAkEgEA+vTpAzU1NZna5OjoiDlz5iAkJASjR49GTk4OPDw8YGVlBX19fZnq8ejRI6xfvx7jxo2Dnp4e8vPzsW3bNty6dQsffPABgOoR7fb29iguLsb333+PoqIiFBUVAaj+rl/TwV9cXIz4+Hj4+fnJ/D40lkbvbN+2bRsCAwORk5MDMzMzBAcHY9iwYXWWLysrg6+vL77//nvk5uaiS5cuWLlyJdzc3F5irYmIiIiIiIiISFZBQUGYNWsWZs+eDaB6RPqvv/6KkJAQ+Pv713mcjo6O0EHwtPPnz2PIkCHCD9zGxsaYOnUqLl68KFHGyckJY8eOFcrs27cPly5dkjiXurq6MD1sXVq1avXcMkRNxYKvRzR2FZ6pX79+sLS0xMGDBzF37lwA1Q+47Ny5E4sXL0ZZWRkMDQ0xYcIELF++XDhOU1MTUVFRWLBgASwtLdGhQwd4enpKrMWbmZkJExMT/P7777C1tZUa/9KlSxg+fLjwuub4GTNmIDQ0FADw2WefoaSkBO7u7igoKMCgQYNw8uRJtG3bVjjO1dUVmZmZiI6OlhqnRYsW2LZtGzw9PVFVVSXMvlEzClkZ23T+/HkUFhZi4sSJUuO8KhJmJDR2FZ5JWo7IQtmup+flSFPM+9clRwAgd/iAxq7CM9X3XhIaGori4mL4+/tLfE+1sbERrlVZrqnbt29j2rRpyMnJgaamJvr374/IyEjY2dkBAG7duoXjx48DAAYMGCBx7JPnfV6eAMDs2bMRExMjvK4ZDZ6RkQFjY2OZ2uTq6ooHDx5g69at+PTTT9G+fXuMGDECGzZsEMpHR0dj+PDhwnmf1rx5c/z999/Ys2cP7t69Cy0tLbz11luIjY0V1oePj4/Hn3/+CQDo3r27xPFPnvfHH3+EkZHRM/uMlYWK+EXH/r+AAwcOYNq0aRJPNX/zzTfPfKrZyckJt2/fxueff47u3bsjLy8Pjx8/lnnqkaKiImhqaqKwsLBB1z0g6renX2NXQem/FBIRvWzXeps2dhVg+ve1xq4C/jv53UaN/+mBE40an0iprNFs7Bqgn0ndI0pfloP+jxs1vjJ8NpNy+O10t+cXamAjR6Q1dhUUrry8HK1atcKhQ4cwfvx4YfuiRYsgEokkfhisUfNDnrGxMUpLS9GnTx+sWrVK4kfw/fv3Y968eTh58iSsrKyQnp6OsWPHYsaMGcIPpgEBAfj6669x8uRJ9OzZE1euXIG9vT2Cg4MxdepUANU/LB47dgxqampo3749bGxssH79eujo6AixbG1tkZSUBLFYjM6dO8PBwQGrV6+W+HGdSBmVlpYiIyNDWMKhKYmIiMCSJUuQmJhYa1rbFxEdHY3x48cjPT1dGOnYUGxtbWFra4s1a9Y0aJyX2aYPPvgAFhYW8PLyatA49HzMEdkxR15fzBPFCg0Nxfr165GcnCx1jXVFsrKygoeHh1yzYdTHs74rydqn3Kgj2+V9qjkyMhIxMTFIT08XpjVQ5JobRERERERERESkWHfv3kVlZSU6d+4ssb1z587CFJZP09PTw44dOzBw4ECUlZXhu+++w8iRIxEdHY3//Oc/AIApU6bgzp07GDp0KMRiMR4/foz58+dLjExatmwZCgsL0bt3bzRv3hyVlZVYv3690NEOAA4ODvjggw/QtWtXZGRkwNvbGyNGjEB8fLwwjbOLi4uwVmtiYiJWrFiBK1euSKzbSkSKNWbMGKSmpiIrKwuGhoYKO29kZCS8vLwavHPkwYMHSEtLw4kTDf+w8ctqU1lZGczNzbF48eIGjUOyYY7Ijjny+mKeKFZkZCT8/PwavKM9Ly8PEydOlPjOrswabWR7fZ5qdnd3R0pKCiwtLfHdd9+hdevWGDduHNatW1fnOlllZWUoKysTXhcVFcHQ0JAj26nBcWQ7EZHy4cj2ahzZTqREOLIdAEe2k/LgyPaGkZ2dDQMDA5w7dw6DBw8Wtq9fvx7fffcd/v77b5nO4+joCBUVFWG6zejoaEyZMgWff/45Bg0ahBs3bmDRokWYM2cOvL29AVSPfl+6dCkCAwNhZmYGkUgEDw8PBAUFYcaMGVLj5OTkoGvXrti/fz8mTJggtUx8fDwsLS0RHx+PN998U563g+ilasoj24mIiIgaWpMe2V6fp5rT09Nx9uxZtGzZEuHh4bh79y7c3d1x79497N69W+ox/v7+WLt2rcLrT0REREREREREz6etrY3mzZvX+r0nLy+v1u9Cz/L222/j+++/F157e3tj2rRpwoyJ/fr1w6NHj/DRRx9h5cqVaNasGZYuXYrly5djypQpQpl//vkH/v7+dXa26+npoWvXrkhNTa2zLm+++SZatGiB1NRUdrYTEREREb3GFLdAQT2pqKhIvBaLxbW21aiqqoKKigrCwsJgZWWFMWPGICgoCKGhoSgpKZF6zIoVK1BYWCj8/fvvvwpvAxERERERERERSaempoaBAwfWmnI9KioK1tbWMp/nr7/+gp6envC6uLi41tqbzZs3h1gsRs1EjnWVqaqqqjNOfn4+/v33X4lYT0tKSkJFRcUzyxARERER0auv0Ua21+epZj09PRgYGEBT839TLZqamkIsFuPWrVvo0aNHrWPU1dWF9bWIiIiIiIiIiOjl8/T0xLRp02BpaYnBgwdjx44duHnzJubNmwegerBEVlYW9u7dCwAIDg6GsbExzMzMUF5eju+//x5HjhzBkSNHhHM6OjoiKCgIFhYWwjTy3t7eGDduHJo3by6UWb9+PYyMjGBmZoa//voLQUFBcHNzAwA8fPgQa9aswfvvvw89PT1kZmbCy8sL2trawrKHaWlpCAsLw5gxY6CtrY3k5GR8+umnsLCwwJAhQ17m20hEREREREqm0Trbn3yq+ck126OiouDk5CT1mCFDhuDQoUN4+PAh2rRpAwBISUlBs2bN0KVLl5dSbyIiIiIiIiIiks/kyZORn58PX19f5OTkoG/fvoiIiEDXrl0BVK+TfvPmTaF8eXk5lixZgqysLGhoaMDMzAw///wzxowZI5RZtWoVVFRUsGrVKmRlZaFTp05C53qNLVu2wNvbG+7u7sjLy4O+vj7mzp0LHx8fANWj3BMSErB3717cv38fenp6GD58OA4cOIC2bdsCqP4N67fffsMXX3yBhw8fwtDQEGPHjsXq1auFTn0iIiIiIno9qYhr5tVqBAcOHMC0adPw9ddfC08179y5E0lJSejatWutp5ofPnwIU1NTvP3221i7di3u3r2L2bNnw8bGBjt37pQppqyL2RO9qH57+jV2FZAwI6Gxq0BEpFSu9TZt7CrA9O9rjV0F/Hfyu40a/9MDJxo1PpFSWaP5/DINrJ+JUWNXAQf9HzdqfGX4bCbl8Nvpbo1dBYwckdbYVSCiV0hpaSkyMjJgYmKCli1bNnZ1iIiIiJTKs74rydqn3Ggj2wH5n2pu06YNoqKi8Mknn8DS0hJaWlqYNGkSPv/888ZqAhERERERERERERERERERvYYatbMdANzd3eHu7i51X2hoaK1tvXv3RlRUVAPXioiIiIiIiIiIiIiIiIiIqG7NGrsCRERERERERERERERPys/Ph46ODjIzMxu7KtQItm7dinHjxjV2NZQac0T5lJWVwcjICPHx8Y1dFfp/zJOmKS8vD506dUJWVlZjV0UmjT6ynYiIiIiIiIiIiIhenv9Ofvelxvv0wAm5j/H394ejoyOMjY0BVHeYuLi44OrVq0LniZOTE/z8/CTWUU1ISMDHH3+MixcvomPHjpg7dy68vb2hoqIic+wzZ84gMDAQ8fHxyMnJQXh4ON577z2JMmKxGGvXrsWOHTtQUFCAQYMG4auvvoKZmZlc7Vy0aBHOnj2LxMREmJqaQiQS1SrzOrZpzpw5WL9+Pc6ePYuhQ4fKFV8RrvU2fanxTP++JvcxT+fIk/Lz82Fubo6srCwUFBSgffv2wr6mdj01pTapq6tjyZIlWLZsGU6dOlWvdjYlxst/fmmxMgPG1uu4+txLoqOjsXnzZly8eBFFRUXo0aMHli5dChcXF7lih4SEICQkROjoNzMzg4+PDxwcHAAAFRUVWLVqFSIiIpCeng5NTU2MGjUKAQEB0NfXlyvW8z53ZW1TWFgYNm7ciNTUVGhqauKdd97Bpk2boKWlJVd9rl27hmXLliEmJgZVVVUwMzPDwYMHYWRkJFFOLBZjzJgxiIyMlMg5HR0dTJs2DatXr8Y333wjV+zGwJHtRERERERERERERKQ0SkpKsGvXLsyePVvY1qxZMzg5OeH48eNISUlBaGgoTp06hXnz5gllioqKYGdnB319fcTFxWHLli3YtGkTgoKC5Ir/6NEjmJubY+vWrXWW2bhxI4KCgrB161bExcVBV1cXdnZ2ePDggVyxxGIx3NzcMHnyZKn7X9c2qaurw9nZGVu2bJEr9utCWo48adasWejfv3+t7U3xeqrRVNrk4uKC2NhYXLsm/wMUpFj1vZecO3cO/fv3x5EjR3D16lW4ublh+vTp+Omnn+SK36VLFwQEBODSpUu4dOkSRowYAScnJyQlJQEAiouLcfnyZXh7e+Py5cs4evQoUlJS6jWrx/M+d2Vp09mzZzF9+nTMmjULSUlJOHToEOLi4ur8nKlLWloahg4dit69eyM6OhpXrlyBt7c3WrZsWatscHBwnQ/FzJw5E2FhYSgoKJArfmOQe2T7v//+CxUVFXTp0gUAcPHiRfzwww/o06cPPvroI4VXkIiIiIiIiIiIXiNrNBu7BsCawsauAdFr7ZdffoGqqioGDx4sbOvQoQPmz58vvO7atSvc3d0RGBgobAsLC0NpaSlCQ0Ohrq6Ovn37IiUlBUFBQfD09JR5lKuDg4Mw8lAasViM4OBgrFy5EhMmTAAA7NmzB507d8YPP/yAuXPnytzWL7/8EgBw584dXL16tdb+17lN48aNg729PUpKSqChoSFz/NeBtBypERISgvv378PHxwe//PKLxL6meD01tTZpaWnB2toa+/btg6+vr1ztJMWq773Ey8tL4jwLFy7Er7/+ivDwcDg6Osoc/+my69evR0hICC5cuAAzMzNoamoiKipKosyWLVtgZWWFmzdv1hoF/izP+9yVpU0XLlyAsbExFi5cCAAwMTHB3LlzsXHjRpnrAQArV67EmDFjJI574403apW7cuUKgoKCEBcXBz09vVr7+/XrB11dXYSHh8PNzU2uOrxsco9sd3Z2xu+//w4AyM3NhZ2dHS5evAgvLy9+cBARERERERERERHRCzlz5gwsLS2fWSY7OxtHjx6FjY2NsO38+fOwsbGBurq6sG306NHIzs5W6Hq9GRkZyM3Nhb29vbBNXV0dNjY2OHfunMLiAK93mywtLVFRUYGLFy8qNP6roK4cSU5Ohq+vL/bu3YtmzWp3/zTF66kptsnKygqxsbEKi031U997iTSFhYXo2LFjvetSWVmJ/fv349GjR1IfknkyjoqKisQyCQ3l6TZZW1vj1q1biIiIgFgsxu3bt3H48GGMHSv7FP5VVVX4+eef0bNnT4wePRo6OjoYNGgQjh07JlGuuLgYU6dOxdatW6Grq1vn+ZpKLsnd2Z6YmAgrKysAwMGDB9G3b1+cO3cOP/zwA0JDQxVdPyIiIiIiIiIiIiJ6jWRmZta5Xu3UqVPRqlUrGBgYoF27dhJruebm5qJz584S5Wte5+bmKqx+NeeSFkuRcWpiva5tat26Ndq3b6/QDtNXhbQcKSsrw9SpUxEYGFjniNimdj011TYZGBjwulUC9b2XPO3w4cOIi4vDzJkz5a5DQkIC2rRpA3V1dcybNw/h4eHo06eP1LKlpaVYvnw5nJ2dhfXjG4q0NllbWyMsLAyTJ0+GmpoadHV10b59e7mW88jLy8PDhw8REBCAd955BydPnsT48eMxYcIExMTECOUWL14Ma2trODk5PfN8TSWX5O5sr6ioEJ4QOnXqlLB2QO/evZGTk6PY2hERERERERERERHRa6WkpETq2q4AsHnzZly+fBnHjh1DWloaPD09JfY/PWW0WCyWul0RpMV6WXGkbW+oWI3ZJg0NDRQXFys8flMnLUdWrFgBU1NTfPjhh888tildT021TbxulcOL3EtqREdHw9XVFTt37oSZmZncdejVqxdEIhEuXLiA+fPnY8aMGUhOTq5VrqKiAlOmTEFVVRW2bdsmdxx51NWm5ORkLFy4ED4+PoiPj0dkZCQyMjIk1rN/nqqqKgCAk5MTFi9ejAEDBmD58uV499138fXXXwMAjh8/jtOnTyM4OPi552squSR3Z7uZmRm+/vprxMbGIioqCu+88w6A6qkWtLS0FF5BIiIiIiIiIiIiInp9aGtro6CgQOo+XV1d9O7dG05OTti+fTtCQkKEQWC6urq1Rpjm5eUBqD0a9UXUTHkrLZYi49TEep3bdO/ePXTq1Emh8V8F0nLk9OnTOHToEFRVVaGqqoqRI0cKZVevXg2g6V1PTbVNvG6VQ33vJTViYmLg6OiIoKAgTJ8+vV51UFNTQ/fu3WFpaQl/f3+Ym5vjiy++kChTUVGBSZMmISMjA1FRUQ06qv1ZbfL398eQIUOwdOlS9O/fH6NHj8a2bduwe/dumQdba2trQ1VVtdbofVNTU9y8eRNAdV6npaWhffv2Qm4DwPvvvw9bW1uJ45pKLsnd2b5hwwZs374dtra2mDp1KszNzQFUP4lQM708ERERERERkTLYV1AAu/Q0DEi5jomZGbj0jKfio6OjoaKiUuvv77//lih3//59LFiwAHp6emjZsiVMTU0REREh7A8JCUH//v3Rrl07tGvXDoMHD8Yvv/wicY7bt2/D1dUV+vr6aNWqFd555x2kpqZKlLG1ta1VlylTpijgXSEiIlJuFhYWUkf+Pa1m9GpZWRkAYPDgwThz5gzKy8uFMidPnoS+vj6MjY0VVj8TExPo6uoiKipK2FZeXo6YmBhYW1srLA7wercpLS0NpaWlsLCwUGj8V4G0HDly5AiuXLkCkUgEkUgkTIsdGxuLBQsWAGh611NTbVNiYiKvWyVQ33sJUP1/w7FjxyIgIAAfffSRwuokFosl4tR0tKempuLUqVMNOqj5eW0qLi5Gs2aS3cbNmzcX6i0LNTU1vPXWW7h+/brE9pSUFHTt2hUAsHz5cly9elXIa5FIBKB6toFvv/1W4rimkkuq8h5ga2uLu3fvoqioCB06dBC2f/TRR2jVqpVCK0dERERERERUX78UFcE/7zZ8OuvCQkMDBwvvY+6tf/GTyRvQb9GizuOuX78uMZrgySfpy8vLYWdnBx0dHRw+fBhdunTBv//+i7Zt2wplunTpgoCAAHTv3h0AsGfPHjg5OeGvv/6CmZkZxGIx3nvvPbRo0QI//vgj2rVrh6CgIIwaNQrJyclo3bq1cK45c+bA19dXeK2hoaGQ94aIiEiZjR49GitWrEBBQYHwG3RERARu376Nt956C23atEFycjI+++wzDBkyROhQc3Z2xtq1a+Hq6govLy+kpqbCz88PPj4+ck0n/fDhQ9y4cUN4nZGRAZFIhI4dO8LIyAgqKirw8PCAn58fevTogR49esDPzw+tWrWCs7OzXG29ceMGHj58iNzcXJSUlAidDn369IGamtpr3abY2Fi88cYb6Natm1zxXwfScuTp9+nu3bsAqkeUtm/fHkDTy5Gm2qbY2FisW7dO5tjUMOp7L6nplF60aBHef/99YTYDNTU1dOzYUeb4Xl5ecHBwgKGhIR48eID9+/cjOjoakZGRAIDHjx9j4sSJuHz5Mk6cOIHKykohVseOHaGmpiZzrOd97srSJkdHR8yZMwchISEYPXo0cnJy4OHhASsrK+jr68tcl6VLl2Ly5Mn4z3/+g+HDhyMyMhI//fQToqOjAVTPKlAzU8STjIyMYGJiIrwuLi5GfHw8/Pz8ZI7dWOTubAeqn2CIj49HWloanJ2d0bZtW6ipqbGznYiIiIiIiJRGaME9vK/ZHhP//4e4FTqd8cejR9h/vwCenXTqPE5HR0f48e5pu3fvxr1793Du3Dm0+P8O+5on9Gs4OjpKvF6/fj1CQkJw4cIFmJmZITU1FRcuXEBiYqKwRt62bdugo6ODffv2Yfbs2cKxrVq1kvpDBBER0Yv49MCJxq7CM/Xr1w+WlpY4ePAg5s6dC6D6gbOdO3di8eLFKCsrg6GhISZMmIDly5cLx2lqaiIqKgoLFiyApaUlOnToAE9PT4m1eDMzM2FiYoLff/+91nS1NS5duoThw4cLr2uOnzFjBkJDQwEAn332GUpKSuDu7o6CggIMGjQIJ0+elHgAz9XVFZmZmUIHgzSzZ89GTEyM8LpmBF9GRgaMjY1f2zYBwL59+zBnzpw64zQk07+vNUpcWUnLEVk0xeupqbXp/PnzKCwsxMSJE+vdpqYiM2BsY1fhmep7LwkNDUVxcTH8/f3h7+8vbLexsRGuVVmuqdu3b2PatGnIycmBpqYm+vfvj8jISNjZ2QEAbt26hePHjwMABgwYIHHsk+dVxOeuLG1ydXXFgwcPsHXrVnz66ado3749RowYgQ0bNgjlo6OjMXz4cOG80owfPx5ff/01/P39sXDhQvTq1QtHjhzB0KFD66y/ND/++COMjIwwbNgwuY5rDCpiWcf+/79//vkH77zzDm7evImysjKkpKTgjTfegIeHB0pLS4UF7pVVUVERNDU1UVhY2KDrHhD129OvsauAhBkJjV0FIiKlcq23aWNXQSn+w/7fye82anxl/2GP6KVao9nYNUA/E6PGrgIO+j9W+DnLxWIMTLmOzfoGGPXEj19+t2/j77JS7DX6Xwd5zWdzzQ8HxsbGKC0tRZ8+fbBq1SqJH93GjBmDjh07olWrVvjxxx/RqVMnODs7Y9myZcIUe0+qrKzEoUOHMGPGDPz111/o06cPEhIS0L9/f9y4cUNitI6enh5Gjx4t/Jhna2uLpKQkiMVidO7cGQ4ODli9erXEj3mkWL+dbvyRcyNHpDV2FRqfEnw2Yk1hY9eASCFKS0uRkZEBExMTtGzZsrGrI5eIiAgsWbIEiYmJtaa1fRHR0dEYP3480tPTJWZubQi2trawtbXFmjVrGjTOq9imxMREjBw5EikpKdDUVIL7ghJijsjuZbbpgw8+gIWFBby8vBo0DsmGeaJYoaGhWL9+PZKTk4WHzxuKlZUVPDw85J5dRV7P+q4ka5+y3CPbFy1aBEtLS1y5ckVi7YDx48dLPH1PRERERERE1FjuVz5GJQAtVckOcC3V5rj7qFLqMXp6etixYwcGDhyIsrIyfPfddxg5ciSio6Pxn//8BwCQnp6O06dPw8XFBREREUhNTcWCBQvw+PFj+Pj4COdKSEjA4MGDUVpaijZt2iA8PBx9+vQBAPTu3Rtdu3bFihUrsH37drRu3RpBQUHIzc1FTk6OcA4XFxdhbcjExESsWLECV65ckVgnkoiI6FU1ZswYpKamIisrC4aGhgo7b2RkJLy8vBq8c+TBgwdIS0vDiRMN/7Dxq9im7Oxs7N27lx3tz8Ackd3LalNZWRnMzc2xePHiBo1DsmOeKFZkZCT8/PwavKM9Ly8PEydOxNSpUxs0jqLIPbJdW1sbf/zxB3r16oW2bdviypUreOONN5CZmYk+ffqguLi4oeqqEBzZTi8LR7YTESkfjmyvxpHtREpECUZvvqoj2/MeV8A2LQ0/GHXFgCfWOf86/y5+KirCzyZvCNue9dns6OgIFRUVYXq/nj17Ck++14xkDwoKQmBgoERHeXl5OW7evIn79+/jyJEj+OabbxATEyN0uMfHx2PWrFm4cuUKmjdvjlGjRgkjLSIiIqTWJT4+HpaWloiPj8ebb75Zz3eGnoUj25WEEnw2cmQ7vSqa8sh2IiIiooamiJHtcs+ZUFVVhcrK2qMAbt26xansiIiIiIiISCm0b66K5gDuPpbsyL/3uBJaUqZ7r8vbb7+N1NRU4bWenh569uwpMWW8qakpcnNzUV5eLmxTU1ND9+7dYWlpCX9/f5ibm+OLL74Q9g8cOBAikQj3799HTk4OIiMjkZ+fDxMTkzrr8uabb6JFixYS9SEiIiIiIiKixiN3Z7udnR2Cg4OF1yoqKnj48CFWr16NMWPGKLJuRERERERERPWipqKCPi1b4lzxI4nt54ofSYx0f56//voLenp6wushQ4bgxo0bqKqqEralpKRAT08PampqdZ5HLBajrKys1nZNTU106tQJqampuHTpEpycnOo8R1JSEioqKiTqQ0RERERERESNR+4124OCgjBixAj06dMHpaWlcHZ2RmpqKrS1tbFv376GqCMRERERERGR3Fw7dMSynGyYtWyJAS01cKjwPnIqKjC5ffW6ekF38pD3+DF+/P/ywcHBMDY2hpmZGcrLy/H999/jyJEjOHLkiHDO+fPnY8uWLVi0aBE++eQTpKamws/PDwsXLhTKeHl5wcHBAYaGhnjw4AH279+P6OhoREZGCmUOHTqETp06wcjICAkJCVi0aBHee+892NvbAwDS0tIQFhaGMWPGQFtbG8nJyfj0009hYWGBIUOGNPybR0RERERERETPJXdnu4GBAUQiEfbv34/4+HhUVVVh1qxZcHFxgYYcowOIiIiIiIiIGpJDu3a4X1mJkLt3caeyEj3U1LC9iyEMWrQAUD3FfE5FhVC+vLwcS5YsQVZWFjQ0NGBmZoaff/5ZYhY3Q0NDnDx5EosXL0b//v1hYGCARYsWYdmyZUKZ27dvY9q0acjJyYGmpib69++PyMhI2NnZCWVycnLg6emJ27dvQ09PD9OnT4e3t7ewX01NDb/99hu++OILPHz4EIaGhhg7dixWr14tMYU9ERERERERETUeuTrbKyoq0KtXL5w4cQIzZ87EzJkzG6peRERERERERC9saocOmNqhg9R9fnr6Eq8/++wzfPbZZ8895+DBg3HhwoU69+/ateu551i4cKHEaPinGRoaIiYm5rnnISIiIiIiIqLGI9ea7S1atEBZWRlUVFQaqj5ERERERERERERERERERERKT67OdgD45JNPsGHDBjx+/Lgh6kNERERERERERERERERERKT05O5s//PPP3H06FEYGRlh9OjRmDBhgsQfEREREREREREREdGLyM/Ph46ODjIzMxu7KtQItm7dinHjxjV2NZQac0T5lJWVwcjICPHx8Y1dFfp/zJNXV0JCArp06YJHjx41dlXkW7MdANq3b4/333+/IepCRERERERERERERA3s1vLYlxqvS8AwuY/x9/eHo6MjjI2NAVR3mLi4uODq1atC54mTkxP8/PzQrl074biEhAR8/PHHuHjxIjp27Ii5c+fC29tbrqVRz5w5g8DAQMTHxyMnJwfh4eF47733JMqIxWKsXbsWO3bsQEFBAQYNGoSvvvoKZmZmcrVz0aJFOHv2LBITE2FqagqRSFSrjCLa5O/vj6NHj+Lvv/+GhoYGrK2tsWHDBvTq1Usp2zRnzhysX78eZ8+exdChQ+WKrwhfzTv9UuMt+HqE3Mc8nSNPys/Ph7m5ObKyslBQUID27dsL+5pajkirV0hICObNm6d0bVJXV8eSJUuwbNkynDp1Sq52NklrNF9irMJ6HVafe0l0dDQ2b96MixcvoqioCD169MDSpUvh4uIiV+yQkBCEhIQIHf1mZmbw8fGBg4MDAKCiogKrVq1CREQE0tPToampiVGjRiEgIAD6+vpyxXre566sbQoLC8PGjRuRmpoKTU1NvPPOO9i0aRO0tLRkrsvRo0exfft2xMfHIz8/H3/99RcGDBgg7L937x5Wr16NkydP4t9//4W2tjbee+89rFu3Dpqa/7umUlJSsHTpUvzxxx8oLy9Hv3798Pnnn2P48OEAgH79+sHKygqbN2/GqlWr5Hq/FE3uke3ffvvtM/+IiIiIiIiIiIiIiOqrpKQEu3btY6ynrwAA/FJJREFUwuzZs4VtzZo1g5OTE44fP46UlBSEhobi1KlTEh1uRUVFsLOzg76+PuLi4rBlyxZs2rQJQUFBcsV/9OgRzM3NsXXr1jrLbNy4EUFBQdi6dSvi4uKgq6sLOzs7PHjwQK5YYrEYbm5umDx5stT9impTTEwMFixYgAsXLiAqKgqPHz+Gvb29xIhAZWqTuro6nJ2dsWXLFrlivy6k5ciTZs2ahf79+9fa3hRzBKjul8rJyRH+ZsyYobRtcnFxQWxsLK5duyZ3O0mx6nsvOXfuHPr3748jR47g6tWrcHNzw/Tp0/HTTz/JFb9Lly4ICAjApUuXcOnSJYwYMQJOTk5ISkoCABQXF+Py5cvw9vbG5cuXcfToUaSkpNRrVo/nfe7K0qazZ89i+vTpmDVrFpKSknDo0CHExcXV+TlTl0ePHmHIkCEICAiQuj87OxvZ2dnYtGkTEhISEBoaisjISMyaNUui3NixY/H48WOcPn0a8fHxGDBgAN59913k5uYKZWbOnImQkBBUVlbKVUdFk3tke407d+7g+vXrUFFRQc+ePdGpUydF1ouIiIiIiIiIiIiIXkO//PILVFVVMXjwYGFbhw4dMH/+fOF1165d4e7ujsDAQGFbWFgYSktLERoaCnV1dfTt2xcpKSkICgqCp6enzKNcHRwchJGH0ojFYgQHB2PlypXC0qp79uxB586d8cMPP2Du3Lkyt/XLL78EUP17+9WrV2vtV1SbIiMjJV5/++230NHRQXx8PP7zn/8oZZvGjRsHe3t7lJSUQENDQ+b4rwNpOVIjJCQE9+/fh4+PD3755ReJfU0xR4DqGZd1dXWl7lO2NmlpacHa2hr79u2Dr6+vXO0kxarvvcTLy0viPAsXLsSvv/6K8PBwODo6yhz/6bLr169HSEgILly4ADMzM2hqaiIqKkqizJYtW2BlZYWbN2/CyMhI5ljP+9yVpU0XLlyAsbExFi5cCAAwMTHB3LlzsXHjRpnrAQDTpk0DgDqn7u/bty+OHDkivO7WrRvWr1+PDz/8EI8fP4aqqiru3r2LGzduYPfu3cKDQwEBAdi2bRuSkpKEz4PRo0cjPz8fMTExGDFC/hlCFEXuzvZHjx7hk08+wd69e1FVVQUAaN68OaZPn44tW7agVatWCq8kERERERERUUN52dOESlOfqUOJiIheVWfOnIGlpeUzy2RnZ+Po0aOwsbERtp0/fx42NjZQV1cXto0ePRorVqxAZmYmTExMFFK/jIwM5Obmwt7eXtimrq4OGxsbnDt3Tu6OxGf5P/buO67q+v///529RJwsB6CiKDhwZI5yY+rb1N6VirlyhpajMmcipZiV0cLUzJGmvnPlp0whTcWZC3OUEg7SQJyICxDO7w9/nm+nA8oxCLTb9XI5l7fnOR+v19sjJx6v5/NZWNeUlnZnS+YyZcpIKp7X1LBhQ2VlZemnn34y+f8ZeX9Gjh49qoiICO3evVsnTpwwq39YPyPDhw/XwIED5efnpwEDBmjw4MGytrYuttf02GOPKS7unz2uA+Ye9GdJbtLS0lSzZs0HjiU7O1tff/21rl+/nutDMn+ex8rKyuToh8Ly12tq2rSpJkyYoHXr1qlDhw5KTU3VihUr1KlTp38klpIlS8rW9k7aumzZsqpZs6YWLVqk+vXry8HBQbNnz5aHh4caNGhg7Gdvb6+6desqLi6uSJPtFm8jP3r0aG3ZskX/93//pytXrujKlSv65ptvtGXLFr366quFESMAAAAAAAAA4F/i1KlTeZ5X27NnTzk7O6tChQoqWbKkPv/8c2NdSkqKPDw8TNrfff/nbWf/rrtj5TZXQc5zd66CviaDwaDRo0erefPmCgoKMhmrOF2Ti4uLSpUqlefqyH+z3D4jGRkZ6tmzp9599908V8Q+jJ+Rt956S19//bV++OEH9ejRQ6+++qqmTZtmMldxu6YKFSrw97YYeNCfJX+1YsUK7dmzR/3797c4hkOHDqlEiRJycHDQ0KFDtXr1atWqVSvXtrdu3dLYsWMVGhpqPD++sOR2TU2bNtWSJUvUvXt32dvby9PTU6VKlSr04zwuXryot956y+SBFSsrK8XGxurAgQNydXWVo6OjPvjgA61fv97sQYTi8HmzONm+cuVKzZs3Tx06dFDJkiVVsmRJdezYUXPnztWKFSsKI0YAAAAAAAAAwL/EzZs35ejomGvdBx98oP3792vNmjVKTEzU6NGjTer/umW0wWDItbwg5DbXPzVPbuX5NXz4cP38889aunRpvuYqymtycnLSjRs3Cnz+h11un5Fx48apZs2aeuGFF+7Z92H7jEycOFFNmjRRvXr19OqrryoiIsJky++85smtvCDk55r4e1s8/J2fJXdt3rxZ/fr109y5cxUYGGhxDDVq1FB8fLx27dqll156SX379tXRo0fN2mVlZalHjx7KyclRdHS0xfNYIq9rOnr0qF555RW9+eab2rdvn9avX6+TJ0+anGdf0K5evapOnTqpVq1amjx5srHcYDAoLCxM7u7uiouL008//aQuXbroP//5j5KTk03GKA6fN4uT7Tdu3DB7ckeS3N3di/xiAAAAAAAAAAAPt3Llyuny5cu51nl6eiogIEBdunTR7NmzNWvWLOMv3j09Pc1WmKampkoyX436d9w9Kza3uQpynrtzFeQ1vfzyy1q7dq1+/PFHVaxY0WQeqfhd06VLl1S+fPkCnf9RkNtnZNOmTfr6669la2srW1tbtWnTxtj2bhLrUfiMPP7447p69arOnTtnnKu4XRN/b4uHB/1ZcteWLVvUuXNnzZw5U3369HmgGOzt7VWtWjU1bNhQkZGRqlu3rj788EOTNllZWXr++ed18uRJxcbGFuqq9ntdU2RkpJo1a6bXX39dderUUfv27RUdHa0vvvjC7N4UhPT0dD311FMqUaKEVq9eLTs7O2Pdpk2b9O2332rZsmVq1qyZ6tevr+joaDk5OWnhwoUm4xSHz5vFyfYmTZpo8uTJunXrlrHs5s2bmjJlyj3PGQAAAAAAAAAA4H6Cg4NzXfn3V3dXr2ZkZEi687vrrVu3KjMz09gmJiZG3t7e8vX1LbD4/Pz85OnpqdjYWGNZZmamtmzZoqZNmxbYPFLBXZPBYNDw4cO1atUqbdq0yewc6+J4TYmJibp165aCg4MLdP5HQW6fkZUrV+rgwYOKj49XfHy8cVvsuLg4DRs2TNKj8Rk5cOCAHB0djVtJF8drOnz4MH9vi4EH/Vki3Vn93alTJ02fPl2DBw8usJgMBoPJPHcT7QkJCfrhhx9UtmzZApvrr+53TTdu3JC1tWna2MbGxhh3Qbp69apCQkJkb2+vtWvXmu1AcHdx91/jsba2Vk5OjklZcfi8WZxs//DDD7Vjxw5VrFhRbdq0Udu2bVWpUiXt2LHD7GkMAAAAAAAAAAAs0b59ex05csRkReK6des0f/58HT58WKdOndK6dev00ksvqVmzZsaEWmhoqBwcHNSvXz8dPnxYq1ev1rRp0zR69GiLtpO+du2aMWEpSSdPnlR8fLySkpIk3dlGeuTIkZo2bZpWr16tw4cPq1+/fnJ2dlZoaKhF1/rbb78pPj5eKSkpunnzpnHeu4nDgrqmYcOGafHixfrqq6/k6uqqlJQU45zF9Zri4uJUpUoVVa1a1aL5/w1y+4xUrVpVQUFBxtfdBypq1qwpd3d3SQ/fZ+T//u//NHfuXB0+fFiJiYn6/PPPNWHCBA0ePFgODg7F9pri4uIUEhKS77lROB70Z8ndpPQrr7yi//73v8Z/Ly9dumTR/OPHj1dcXJxOnTqlQ4cOacKECdq8ebN69eolSbp9+7aeffZZ7d27V0uWLFF2drZxrj8/PJIf9/t3Nz/X1LlzZ61atUqzZs3SiRMntH37dr3yyit67LHH5O3tne9YLl26pPj4eOODDseOHTPGJt1Z0R4SEqLr169r3rx5unr1qjGe7OxsSXceoildurT69u2rgwcP6vjx43r99dd18uRJderUyTjXqVOndPbsWbVt29ai+1XQbC3tEBQUpISEBC1evFi//vqrDAaDevTooV69esnJyakwYgQAAAAAAAAAFJCK058o6hDuqXbt2mrYsKH+97//aciQIZLunMk6d+5cjRo1ShkZGapUqZKeeeYZjR071tjPzc1NsbGxGjZsmBo2bKjSpUtr9OjRJmfxnjp1Sn5+fvrxxx/VsmXLXOffu3evWrVqZXx/t3/fvn21YMECSdKYMWN08+ZNhYWF6fLly2rcuLFiYmLk6upq7NevXz+dOnVKmzdvzvNaBw4cqC1bthjf312dd/LkSfn6+hbYNc2aNUuSzOrnz5+vfv36FbtrkqSlS5dq0KBBec5TmIZ91rpI5s2v3D4j+fGwfUbs7OwUHR2t0aNHKycnR1WqVFFERIRxpX5xvKadO3cqLS1Nzz77bK7zPFLC04o6gnt60J8lCxYs0I0bNxQZGanIyEhjeYsWLYx/V/Pzd+rcuXPq3bu3kpOT5ebmpjp16mj9+vVq166dJOnMmTNau3atJKlevXomff88bkH8u5ufa+rXr5/S09P1ySef6NVXX1WpUqXUunVrvfPOO8b2mzdvVqtWrYzj5mbt2rXq37+/8X2PHj0kSZMnT1Z4eLj27dun3bt3S5KqVatm0vfuuOXKldP69es1YcIEtW7dWllZWQoMDNQ333yjunXrGtsvXbpUISEh8vHxyfPe/BOsDAW99r+Yu3r1qtzc3JSWllao5x4AtRfWLuoQdKjvoaIOAQCKlV8CahZ1CKr56y9FHYLe7/6fIp3/1eXfFun8QLES7lbUEai2X+WiDkH/i7xdpPNvavlpkc4vFf9f6P5bbNxU9Cvn2rROLOoQil4x+LexuP/yGMivW7du6eTJk/Lz8zPborW4W7dunV577TUdPnzYbBvZv2Pz5s3q1q2bTpw4odKlSxfYuLlp2bKlWrZsqfDw8EKd51G8psOHD6tNmzY6fvy43NyKwc+FYojPSP79k9f03HPPKTg4WOPHjy/UeZA/fE4K1oIFCzR16lQdPXrU5Iz1opCRkSF/f38tXbpUzZo1e+Bx7vVdKb85ZYtXtkdGRsrDw0MvvviiSfkXX3yh8+fP64033rB0SAAAAAAAAAAAjDp27KiEhASdPXtWlSpVKrBx169fr/Hjxxd6ciQ9PV2JiYn69tvCf9j4UbymP/74Q4sWLSLRfg98RvLvn7qmjIwM1a1bV6NGjSrUeZB/fE4K1vr16zVt2rQiT7RL0unTpzVhwoS/lWgvKBavbPf19dVXX32lpk2bmpTv3r1bPXr00MmTJws0wILGynb8U1jZDgDFDyvb72BlO1CMFIPVm6xsZ2U7/h9WthcTxeDfRla241HxMK9sBwAAKGwFsbLd4j0TUlJS5OXlZVZevnx5JScnWzocAAAAAAAAAAAAAAAPHYuT7ZUqVdL27dvNyrdv3y5vb+8CCQoAAAAAAAAAAAAAgOLM4jPbBw4cqJEjRyorK0utW9/Z5m7jxo0aM2aMXn311QIPEAAAAAAAAAAAAACA4sbiZPuYMWN06dIlhYWFKTMzU5Lk6OioN954Q+PGjSvwAAEAAAAAAAAAAAAAKG4sTrZbWVnpnXfe0aRJk/TLL7/IyclJ/v7+cnBwKIz4AAAAAAAAAAAAAAAodiw+s/2uEiVKqFGjRnJ1dVViYqJycnIKMi4AAAAAAAAAAAAAAIqtfCfbFy5cqKioKJOywYMHq0qVKqpdu7aCgoL0+++/F3R8AAAAAAAAAAAAAAAUO/lOtn/22Wdyc3Mzvl+/fr3mz5+vRYsWac+ePSpVqpSmTJlSKEECAAAAAAAAAP49Ll68KHd3d506daqoQ0ER+OSTT/T0008XdRgAANxXvs9sP378uBo2bGh8/8033+jpp59Wr169JEnTpk1T//79Cz5CAAAAAAAAAECBCQ8PL/bzRUZGqnPnzvL19ZV0J/neq1cv/fzzz8ZEfJcuXTRt2jSVLFnS2O/QoUMaPny4fvrpJ5UpU0ZDhgzRpEmTZGVlle+5t27dqnfffVf79u1TcnKyVq9era5du5q0MRgMmjJliubMmaPLly+rcePG+vTTTxUYGGjRdY4YMULbtm3T4cOHVbNmTcXHx5u1KYhrmjVrlmbNmmV8eCEwMFBvvvmmOnToUCyvadCgQZo6daq2bdum5s2bWzQ/AAD/pHyvbL9586bJl5YdO3boySefNL6vUqWKUlJSCjY6AAAAAAAAAMC/ys2bNzVv3jwNHDjQWGZtba0uXbpo7dq1On78uBYsWKAffvhBQ4cONba5evWq2rVrJ29vb+3Zs0cff/yx3nvvPc2cOdOi+a9fv666devqk08+ybPNjBkzNHPmTH3yySfas2ePPD091a5dO6Wnp1s0l8Fg0Isvvqju3bvnWl9Q11SxYkVNnz5de/fu1d69e9W6dWt16dJFR44cKZbX5ODgoNDQUH388ccWzQ0AwD8t3yvbfXx8tG/fPvn4+OjChQs6cuSIyRNlKSkpJtvMAwAAAAAAAABgqe+//162trZq0qSJsax06dJ66aWXjO99fHwUFhamd99911i2ZMkS3bp1SwsWLJCDg4OCgoJ0/PhxzZw5U6NHj873SvAOHTqYrPj+K4PBoKioKE2YMEHPPPOMJGnhwoXy8PDQV199pSFDhuT7Wj/66CNJ0vnz5/Xzzz+b1RfUNXXu3Nnk/dSpUzVr1izt2rVLgYGBxfKann76aYWEhOjmzZtycnLK9/wAAPyT8r2yvU+fPho2bJjeeustPffccwoICFCDBg2M9Tt27FBQUFChBAkAAAAAAAAA+HfYunWryZGmufnjjz+0atUqtWjRwli2c+dOtWjRQg4ODsay9u3b648//ijQs99PnjyplJQUhYSEGMscHBzUokUL7dixo8DmkQrnmrKzs7Vs2TJdv37d+EBDcbymhg0bKisrSz/99FOBzg8AQEHKd7L9jTfe0MCBA7Vq1So5Ojrq66+/Nqnfvn27evbsWeABAgAAAAAAAAD+PU6dOiVvb+9c63r27ClnZ2dVqFBBJUuW1Oeff26sS0lJkYeHh0n7u+8L8gjUu2PlNldBH7VakNd06NAhlShRQg4ODho6dKhWr16tWrVqmYxVnK7JxcVFpUqVKtAHJQAAKGj5TrZbW1vrrbfe0oEDB/T999+rZs2aJvVff/21BgwYUOABAgAAAAAAAAD+PW7evClHR8dc6z744APt379fa9asUWJiokaPHm1S/9dt1Q0GQ67lBSG3uf6peXIrv58aNWooPj5eu3bt0ksvvaS+ffvq6NGj952rKK/JyclJN27cKPD5AQAoKPk+sx0AAAAAAAAAgMJWrlw5Xb58Odc6T09PeXp6KiAgQGXLltUTTzyhSZMmycvLS56enmarsFNTUyWZr9j+Ozw9PSXdWYXt5eVlMldBznN3roK6Jnt7e1WrVk3SnS3a9+zZow8//FCzZ88uttd06dIllS9fvkDnBwCgIOV7ZTsAAAAAAAAAAIUtODjYbMV1bu6uhs7IyJAkNWnSRFu3blVmZqaxTUxMjLy9veXr61tg8fn5+cnT01OxsbHGsszMTG3ZskVNmzYtsHmkwr0mg8FgvHfF8ZoSExN169YtBQcHF+j8AAAUJJLtAAAAAAAAAIBio3379jpy5IjJ6vZ169Zp/vz5Onz4sE6dOqV169bppZdeUrNmzYwJ2tDQUDk4OKhfv346fPiwVq9erWnTpmn06NEWbYV+7do1xcfHKz4+XpJ08uRJxcfHKykpSdKdrc5HjhypadOmafXq1Tp8+LD69esnZ2dnhYaGWnStv/32m+Lj45WSkqKbN28a572biC6oaxo/frzi4uJ06tQpHTp0SBMmTNDmzZvVq1evYntNcXFxqlKliqpWrWrR/AAA/JPYRh7AIys6OlrvvvuukpOTFRgYqKioKD3xxBO5tt28ebNatWplVv7LL78oICBAknTkyBG9+eab2rdvn06fPq0PPvhAI0eOzHP+yMhIjR8/XiNGjFBUVFRBXBIAAAAAAMDfFh4eXtQh3FPt2rXVsGFD/e9//9OQIUMk3Tm7e+7cuRo1apQyMjJUqVIlPfPMMxo7dqyxn5ubm2JjYzVs2DA1bNhQpUuX1ujRo03OdT916pT8/Pz0448/qmXLlrnOv3fvXpPfE93t37dvXy1YsECSNGbMGN28eVNhYWG6fPmyGjdurJiYGLm6uhr79evXT6dOndLmzZvzvNaBAwdqy5Ytxvd3V3GfPHlSvr6+BXZN586dU+/evZWcnCw3NzfVqVNH69evV7t27YxtitM1SdLSpUs1aNCgPOcBAKA4eOBke2Zmpk6ePKmqVavK1pacPYDiZfny5Ro5cqSio6PVrFkzzZ49Wx06dNDRo0dVuXLlPPsdO3ZMJUuWNL7/85lQN27cUJUqVfTcc89p1KhR95x/z549mjNnjurUqfP3LwYWKYqHLGbNmqVZs2bp1KlTkqTAwEC9+eab6tChgyQpKytLEydO1Lp163TixAm5ubmpbdu2mj59ury9vQvu4gEAAAAAeERMmjRJr732mgYNGiRra2u1atVKO3bsuG+/2rVra+vWrXnWnzp1SqVKlVLdunXzbNOyZUvjFvV5sbKyUnh4+D0fXDh16lSeye+77pW0vqsgrmnevHn3nac4XdPhw4cVHx+v//3vf/cdCwCAomTxNvI3btzQgAED5OzsrMDAQOPWOa+88oqmT59e4AECwIOYOXOmBgwYoIEDB6pmzZqKiopSpUqVNGvWrHv2c3d3l6enp/FlY2NjrGvUqJHeffdd9ejRQw4ODnmOce3aNfXq1Utz585V6dKlC+yacH93H7KYMGGCDhw4oCeeeEIdOnQw/qzKy7Fjx5ScnGx8+fv7G+vuPmQxffp0eXp65tq/YsWKmj59uvbu3au9e/eqdevW6tKli44cOWIcY//+/Zo0aZL279+vVatW6fjx43r66acL7uIBAAAAAHiEdOzYUUOGDNHZs2cLdNz169dr/Pjxhf47m/T0dCUmJuq1114r1HmkR/Oa/vjjDy1atEhubm6FPhcAAH+HxUvSx40bp4MHD2rz5s166qmnjOVt27bV5MmTTbbtAYCikJmZqX379pn9exQSEnLfJ6CDg4N169Yt1apVSxMnTsx11fP9DBs2TJ06dVLbtm319ttvW9wfD+7PD1lIUlRUlDZs2KBZs2YpMjIyz37u7u4qVapUrnWNGjVSo0aNJCnPn3GdO3c2eT916lTNmjVLu3btUmBgoHF7tD/7+OOP9dhjjykpKemeuy0AAAAAAPBvNWLEiAIf859aMObq6qrff//9H5nrUbymkJCQf2QeAAD+LotXtq9Zs0affPKJmjdvLisrK2N5rVq1lJiYWKDBAcCDuHDhgrKzs+Xh4WFS7uHhoZSUlFz7eHl5ac6cOVq5cqVWrVqlGjVqqE2bNvfczio3y5Yt0/79+++Z2EXhuPuQxV//Yyy/D1l4eXmpTZs2+vHHH/9WHNnZ2Vq2bJmuX7+uJk2a5NkuLS1NVlZWeSb5AQAAAAAAAABA8Wbxyvbz58/L3d3drPz69esmyXcAKGp//TfJYDDk+e9UjRo1VKNGDeP7Jk2a6Pfff9d7772nJ598Ml/z/f777xoxYoRiYmLk6Oj44IHjgfydhywaNGigjIwMffnll2rTpo02b96c7//f7zp06JCaNGmiW7duqUSJElq9erVq1aqVa9tbt25p7NixCg0NVcmSJS2aBwAAAAAAAAAAFA8WJ9sbNWqk7777Ti+//LKk/5fMmjt37j1X8AHAP6VcuXKysbExS7CmpqaaJWLv5fHHH9fixYvz3X7fvn1KTU1VgwYNjGXZ2dnaunWrPvnkE2VkZJicAY/C8U8/ZPHnseLj43XlyhWtXLlSffv21ZYtW8wS7llZWerRo4dycnIUHR1t0RwAAAAAADwIg8FQ1CEAAAAUOwXxHcniZHtkZKSeeuopHT16VLdv39aHH36oI0eOaOfOndqyZcvfDggA/i57e3s1aNBAsbGx6tatm7E8NjZWXbp0yfc4Bw4ckJeXV77bt2nTRocOHTIp69+/vwICAvTGG2+QaC9kRfWQxV329vaqVq2aJKlhw4bas2ePPvzwQ82ePdvYJisrS88//7xOnjypTZs2saodAAAAAFCo7v4uIjMzU05OTkUcDQAAQPFy48YNSZKdnd0Dj2Fxsr1p06bavn273nvvPVWtWlUxMTGqX7++du7cqdq1az9wIABQkEaPHq3evXurYcOGatKkiebMmaOkpCQNHTpUkjRu3DidPXtWixYtkiRFRUXJ19dXgYGByszM1OLFi7Vy5UqtXLnSOGZmZqaOHj1q/PPZs2cVHx+vEiVKqFq1anJ1dVVQUJBJHC4uLipbtqxZOQpeUT1kkReDwaCMjAzj+7uJ9oSEBP34448qW7bs354DAAAAAIB7sbW1lbOzs86fPy87OztZW1sXdUgAAABFzmAw6MaNG0pNTVWpUqX+1mJJi5PtklS7dm0tXLjwgScFgMLWvXt3Xbx4UREREUpOTlZQUJDWrVsnHx8fSVJycrKSkpKM7TMzM/Xaa6/p7NmzcnJyUmBgoL777jt17NjR2OaPP/5QcHCw8f17772n9957Ty1atNDmzZv/sWtD3oriIQtJGj9+vDp06KBKlSopPT1dy5Yt0+bNm7V+/XpJ0u3bt/Xss89q//79+vbbb5WdnW1cgV+mTBnZ29v/Y/cIAAAAAPDvYWVlJS8vL508eVKnT58u6nAAAACKlVKlSsnT0/NvjWFxsv3q1au5lltZWcnBwYGEAYBiIywsTGFhYbnWLViwwOT9mDFjNGbMmHuO5+vra/H5HSTh/1lF9ZDFuXPn1Lt3byUnJ8vNzU116tTR+vXr1a5dO0nSmTNntHbtWklSvXr1TGL+8ccf1bJly0K4GwAAAAAA3NkJzt/fX5mZmUUdCgAAQLFhZ2dXIMf/WpxsL1WqlKysrPKsr1ixovr166fJkyezLREA4B9XFA9ZzJs372+PAQAAAABAYbG2tpajo2NRhwEAAPDIsTjZvmDBAk2YMEH9+vXTY489JoPBoD179mjhwoWaOHGizp8/r/fee08ODg4aP358YcQMAAAAAAAAAAAAAECRsnjp+cKFC/X+++/rrbfeUufOnfX000/rrbfe0nvvvafly5drwoQJ+uijj4zn4QIA/jnR0dHy8/OTo6OjGjRooLi4uHz12759u2xtbc22OM/KylJERISqVq0qR0dH1a1b13gO+V3p6ekaOXKkfHx85OTkpKZNm2rPnj0mbaysrHJ9vfvuu3/regEAAAAAAAAAAIqKxcn2nTt3mpxde1dwcLB27twpSWrevLnJmbgAgMK3fPlyjRw5UhMmTNCBAwf0xBNPqEOHDvf99zgtLU19+vRRmzZtzOomTpyo2bNn6+OPP9bRo0c1dOhQdevWTQcOHDC2GThwoGJjY/Xll1/q0KFDCgkJUdu2bXX27Fljm+TkZJPXF198ISsrK/33v/8tuBsAAAAAAAAAAADwD7I42V6xYsVcz6adN2+eKlWqJEm6ePGiSpcu/fejQ7FTFKtmJens2bN64YUXVLZsWTk7O6tevXrat2+fsf7cuXPq16+fvL295ezsrKeeekoJCQkmY7Rs2dJsVW2PHj0svwlAMTVz5kwNGDBAAwcOVM2aNRUVFaVKlSpp1qxZ9+w3ZMgQhYaGqkmTJmZ1X375pcaPH6+OHTuqSpUqeumll9S+fXu9//77kqSbN29q5cqVmjFjhp588klVq1ZN4eHh8vPzM5nX09PT5PXNN9+oVatWqlKlSsHeBAAAAAAAAAAAgH+Ixcn29957Tx988IHq1q2rgQMHatCgQapXr56ioqKMyZc9e/aoe/fuBR4silZRrZq9fPmymjVrJjs7O33//fc6evSo3n//fZUqVUqSZDAY1LVrV504cULffPONDhw4IB8fH7Vt21bXr183mW/QoEEmq2tnz579928MUAxkZmZq3759CgkJMSkPCQnRjh078uw3f/58JSYmavLkybnWZ2RkyNHR0aTMyclJ27ZtkyTdvn1b2dnZ92zzV+fOndN3332nAQMG3Pe6AADIr+L6UGh4eLgCAgLk4uKi0qVLq23bttq9e7ex/tSVHFlNuZrr6+sjWcZ2vlHpZvVjf7hl4V0CAAAAAABAQbK1tMPTTz+t48eP67PPPtOxY8dkMBjUoUMHrVmzRr6+vpKkl156qaDjRDHw51WzkhQVFaUNGzZo1qxZioyMzLPf3VWzNjY2WrNmjUndl19+qQkTJqhjx46S7vzd2bBhg95//30tXrxYkvTOO++oUqVKmj9/vrHf3b9rkpSQkKBdu3bp8OHDCgwMlHTnl63u7u5aunSpMV5JcnZ2lqen59+6D3i4vN/9P0Udgl5d/m2hz3HhwgVlZ2fLw8PDpNzDw0MpKSm59klISNDYsWMVFxcnW9vcfxy0b99eM2fO1JNPPqmqVatq48aN+uabb5SdnS1JcnV1VZMmTfTWW2+pZs2a8vDw0NKlS7V79275+/vnOubChQvl6uqqZ5555m9cseU8f4z/R+f7q5RW9Yp0fgB4lN19KDQ6OlrNmjXT7Nmz1aFDBx09elSVK1fOs9+fHwo9d+6cSd3EiRO1ePFizZ07VwEBAdqwYYO6deumHTt2GI/VuvtQaKtWrfT999/L3d1diYmJxodCJal69er65JNPVKVKFd28eVMffPCBQkJC9Ntvv6l8+fKqVNJKya+WMJl7zr4szdieoQ7+pj+fI1o6aFADO+P7EvZWD3rLAAAAAAAAUAAsXtkuST4+PoqMjNSqVau0evVqRUZGmiQ/8egpqlWzkrR27Vo1bNhQzz33nNzd3RUcHKy5c+eajCHJZBwbGxvZ29ubraxdsmSJypUrp8DAQL322mtKT0+/z5UDDxcrK9NfuhsMBrMyScrOzlZoaKimTJmi6tWr5znehx9+KH9/fwUEBMje3l7Dhw9X//79ZWNjY2zz5ZdfymAwqEKFCnJwcNBHH31kfMAmN1988YV69epl9tkHAOBBFcVRKpLpQ6GPPfaYfH191aZNG1WtWtXYJjQ0VG3btlWVKlUUGBiomTNn6urVq/r5558lSTbWVvIsYW3yWv1rlroH2pkl010dZNKOZDsAAAAAAEDReqBkuyTduHFDv/76q37++WeTFx5Nf2fV7JIlS+67ajYhIUE5OTmKjY3VN998o+TkZGObEydOaNasWfL399eGDRs0dOhQvfLKK1q0aJEkKSAgQD4+Pho3bpwuX76szMxMTZ8+XSkpKSbj9OrVS0uXLtXmzZs1adIkrVy58h9fWQsUlnLlysnGxsbs85iammr2uZWk9PR07d27V8OHD5etra1sbW0VERGhgwcPytbWVps2bZIklS9fXmvWrNH169d1+vRp/frrrypRooT8/PyMY1WtWlVbtmzRtWvX9Pvvv+unn35SVlaWSZu74uLidOzYMZMdJwAA+DuK80OhucU6Z84cubm5qW7durm22fdHtuJTcjSgvp1Z3TvbM1V2RrrqfXZNU7dmKDPbkOdcAAAAeDgUxXFIkZGRatSokVxdXeXu7q6uXbvq2LFjJm2uXbum4cOHq2LFinJyclLNmjVNHma9dOmSXn75ZdWoUUPOzs6qXLmyXnnlFaWlpT3YjQAA4CFl8Tby58+fV//+/fX999/nWn93a2E8mgpj1eygQYMUEBAgKysrVa1aVf379zfZMj4nJ0cNGzbUtGnTJEnBwcE6cuSIZs2apT59+sjOzk4rV67UgAEDVKZMGdnY2Kht27bq0KGDyVyDBg0y/jkoKEj+/v5q2LCh9u/fr/r16z/Q/QCKC3t7ezVo0ECxsbHq1q2bsTw2NlZdunQxa1+yZEkdOnTIpCw6OlqbNm3SihUrzBLljo6OqlChgrKysrRy5Uo9//zzZmO6uLjIxcVFly9f1oYNGzRjxgyzNvPmzVODBg3yTDAAAGCpojpKRfp/D4WOHj1a48eP108//aRXXnlFDg4O6tOnj7Hdt99+qx49eujGjRvy8vJSbGysypUrl+u88w5kqmY5azWtZBrXiMb2qu9lo9JOVvrpbLbGbczQySs5+vxpp3zdJwAAABQ/RXUc0pYtWzRs2DA1atRIt2/f1oQJExQSEqKjR4/KxcVFkjRq1Cj9+OOPWrx4sXx9fRUTE6OwsDB5e3urS5cu+uOPP/THH3/ovffeU61atXT69GkNHTpUf/zxh1asWFF4Nw0AgGLG4pXtI0eO1OXLl7Vr1y45OTlp/fr1Wrhwofz9/bV27drCiBHFQFGumvXy8lKtWrVMxq9Zs6aSkpKM7xs0aKD4+HhduXJFycnJWr9+vS5evJjrytq76tevLzs7OyUkJDzQPQGKm9GjR+vzzz/XF198oV9++UWjRo1SUlKShg4dKkkaN26c8Rf/1tbWCgoKMnm5u7vL0dFRQUFBxv+w2r17t1atWqUTJ04oLi5OTz31lHJycjRmzBjjvBs2bND69et18uRJxcbGqlWrVqpRo4b69+9vEt/Vq1f19ddfs6odAFAoiuIolZycHNWvX1/Tpk1TcHCwhgwZokGDBpltX9+qVSvFx8drx44deuqpp/T8888rNTXVbM6bWQZ9dShLA4LNV7WPauKgFr62quNho4H17fVZJ0fNO5Clizdy7ntvAAAAUDwV1XFI69evV79+/RQYGKi6detq/vz5SkpK0r59+4xtdu7cqb59+6ply5by9fXV4MGDVbduXe3du1fSncVMK1euVOfOnVW1alW1bt1aU6dO1f/93//p9u3bBXSHAAAo/ixOtm/atEkffPCBGjVqJGtra/n4+OiFF17QjBkzFBkZWRgxohj486rZP4uNjVXTpk3N2t9dNRsfH298DR06VDVq1FB8fLwaN25s0v7uqtnbt29r5cqVJitxmzVrZraN0fHjx+Xj42M2r5ubm8qXL6+EhATt3bs31xW9dx05ckRZWVny8vLK1z0Airvu3bsrKipKERERqlevnrZu3ap169YZPyvJyckmD6nkx61btzRx4kTVqlVL3bp1U4UKFbRt2zaVKlXK2CYtLU3Dhg1TQECA+vTpo+bNmysmJkZ2dqaJgmXLlslgMKhnz55/+1oBALiruD8UKt3Z/aVatWp6/PHHNW/ePNna2mrevHlmsa04mqUbWVKfuubJ9r96vOKdpP9vl0i2AwAAPIyK8jikv7q79XuZMmWMZc2bN9fatWt19uxZGQwG/fjjjzp+/Ljat29/z3FKliyZ5+5RAAA8iiz+qXf9+nW5u7tLuvPD9/z586pevbpq166t/fv3F3iAKD5Gjx6t3r17q2HDhmrSpInmzJljtmr27NmzWrRokXHV7J/9edXsXbt379bZs2dVr149nT17VuHh4WarZkeNGqWmTZtq2rRpev755/XTTz9pzpw5mjNnjrHN119/rfLly6ty5co6dOiQRowYoa5duxq/rCYmJmrJkiXq2LGjypUrp6NHj+rVV19VcHCwmjVrVpi3DfhHhYWFKSwsLNe6BQsW3LNveHi4wsPDTcpatGiho0eP3rPf888/n+u28n81ePBgDR48+L7tAACwRFEepWLJQ6F/ZjAYlJGRYVY+70CWnq5hq/Iu938m+kDKne3svVwtfn4aAAAAxUBRHof0ZwaDQaNHj1bz5s1Nfm/70UcfadCgQapYsaJsbW1lbW2tzz//XM2bN891nIsXL+qtt97SkCFD8nP5AAA8MixOtteoUUPHjh2Tr6+v6tWrp9mzZ8vX11efffYZK4Qfcd27d9fFixcVERGh5ORkBQUFFdiq2RMnTqhEiRLq2LGjvvzyS5NVs40aNdLq1as1btw4RUREyM/PT1FRUerVq5exTXJyskaPHq1z587Jy8tLffr00aRJk4z19vb22rhxoz788ENdu3ZNlSpVUqdOnTR58mSTrUABAADw8CmuD4Vev35dU6dO1dNPPy0vLy9dvHhR0dHROnPmjJ577jmTGH67lKOtp7O1rpez2fXt/P22dp3JVis/W7k5WGnPH9kateGWnq5hq8puJNsBAAAeZoVxHNKgQYMUEBAgKysrVa1aVf3799f8+fNzbT98+HD9/PPPZivfP/roI+3atUtr166Vj4+Ptm7dqrCwMHl5ealt27Ymba9evapOnTqpVq1aea64BwDgUWVxsn3kyJFKTk6WJE2ePFnt27fXkiVLZG9vf99Vk3j4FcWqWUn6z3/+o//85z951r/yyit65ZVX8qyvVKmStmzZct95AAAA8PAprg+F2tjY6Ndff9XChQt14cIFlS1bVo0aNVJcXJwCAwNN5vviQKYqlLRSSFXzB0EdbK20/MhtTdmSoYxsycfNWoPq22lMMwcL7xQAAACKiwc9DunAgQMaPny4JCknJ0cGg0G2traKiYlR69atjcch3bp1SxcvXpS3t7fGjh1rtoOTJL388stau3attm7dqooVKxrLb968qfHjx2v16tXq1KmTJKlOnTqKj4/Xe++9Z5JsT09P11NPPaUSJUpo9erVZscKAgDwqLM42f7n1cTBwcE6deqUfv31V1WuXFnlypUr0OAAAAAAID+K40Ohjo6OWrVq1X3HkKRpbRw1rY1jrnX1vWy0a6BLvsYBAADAw6Eoj0MyGAx6+eWXtXr1am3evNmsb1ZWlrKysmRtbbqLko2NjXJycozvr169qvbt28vBwUFr1641OyseAIB/A4uT7REREXrttdfk7Hxne0NnZ2fVr19fN2/eVEREhN58880CDxIAAAAAAAAAgEdJUR2HNGzYMH311Vf65ptv5Orqalxd7+bmJicnJ5UsWVItWrTQ66+/LicnJ/n4+GjLli1atGiRZs6cKenOivaQkBDduHFDixcv1tWrV3X16lVJUvny5Tm6EwDwr2Fxsn3KlCkaOnSoMdl+140bNzRlyhSS7QAAAAAAAAAA3EdRHYc0a9YsSVLLli1N+s6fP1/9+vWTJC1btkzjxo1Tr169dOnSJfn4+Gjq1KnGBwH27dun3bt3S5KqVatmMs7Jkyfl6+trUdwAADysLE62GwwGWVlZmZUfPHhQZcqUKZCgAAAF56/b4v5bYwAAAAAAAChuiuI4JIPBcN+4PD09NX/+/DzrW7Zsma9xAAB41OU72V66dGlZWVnJyspK1atXN0m4Z2dn69q1a8an2gAAAAAAAAAAAAAAeJTlO9keFRUlg8GgF198UVOmTJGbm5uxzt7eXr6+vmrSpEmhBAkAAAAAAAAAAAAAQHGS72R73759JUl+fn5q2rSp7OzsCi0oPPx8x35XpPOfmt6pSOcHAADAw6HIv7c6Fun0AAAAAIqJ6Ohovfvuu0pOTlZgYKCioqL0xBNP3Lff9u3b1aJFCwUFBSk+Pt5Y3rJlS23ZssWsfceOHfXdd3f+O8jX11enT582axMWFqZPP/1U0p3jCpYtW6bff/9d9vb2atCggaZOnarGjRvfc67u3btr2bJl+bp2AHiYWXxme4sWLZSTk6Pjx48rNTVVOTk5JvVPPvlkgQUHAAAAAAAAAADwKFu+fLlGjhyp6OhoNWvWTLNnz1aHDh109OhRVa5cOc9+aWlp6tOnj9q0aaNz586Z1K1atUqZmZnG9xcvXlTdunX13HPPGcv27Nmj7Oxs4/vDhw+rXbt2Jm2qV6+uTz75RFWqVNHNmzf1wQcfKCQkRL/99pvKly9vbDdo0CBFREQY3zs5OT3YzQCAh4zFyfZdu3YpNDRUp0+flsFgMKmzsrIy+YcZAAAAAAAAAAAAeZs5c6YGDBiggQMHSrpzrO+GDRs0a9YsRUZG5tlvyJAhCg0NlY2NjdasWWNSV6ZMGZP3y5Ytk7Ozs0ki/c/JckmaPn26qlatqhYtWhjLQkNDzWKdN2+efv75Z7Vp08ZY7uzsLE9Pz/xdMAA8QixOtg8dOlQNGzbUd999Jy8vL1lZWRVGXAAAAAAAAAAA/OsV9dFHEsd2FqbMzEzt27dPY8eONSkPCQnRjh078uw3f/58JSYmavHixXr77bfvO8+8efPUo0cPubi45BnH4sWLNXr06DzzPpmZmZozZ47c3NxUt25dk7olS5Zo8eLF8vDwUIcOHTR58mS5urreNy4AeNhZnGxPSEjQihUrVK1atcKIBwAAAAAAAAAA4F/hwoULys7OloeHh0m5h4eHUlJScu2TkJCgsWPHKi4uTra290/z/PTTTzp8+LDmzZuXZ5s1a9boypUr6tevn1ndt99+qx49eujGjRvy8vJSbGysypUrZ6zv1auX/Pz85OnpqcOHD2vcuHE6ePCgYmNj7xsbADzsLE62N27cWL/99hvJdgAAAAAAAAAAgALw19XkBoMh1xXm2dnZCg0N1ZQpU1S9evV8jT1v3jwFBQXpscceu2ebDh06yNvb26yuVatWio+P14ULFzR37lw9//zz2r17t9zd3SXdOa/9rqCgIPn7+6thw4bav3+/6tevn68YAeBhZXGy/eWXX9arr76qlJQU1a5dW3Z2dib1derUKbDgAAAAAAAAAAAAHlXlypWTjY2N2Sr21NRUs9XukpSenq69e/fqwIEDGj58uCQpJydHBoNBtra2iomJUevWrY3tb9y4oWXLlikiIiLPGE6fPq0ffvhBq1atyrXexcVF1apVU7Vq1fT444/L399f8+bN07hx43JtX79+fdnZ2SkhIYFkO4BHnsXJ9v/+97+SpBdffNFYZmVlZXzKKjs7u+CiAwAAAAAAAAAAeETZ29urQYMGio2NVbdu3YzlsbGx6tKli1n7kiVL6tChQyZl0dHR2rRpk1asWCE/Pz+Tuv/973/KyMjQCy+8kGcM8+fPl7u7uzp16pSvmA0GgzIyMvKsP3LkiLKysuTl5ZWv8QDgYWZtaYeTJ0+avU6cOGH8XwAAAAAAABSt6Oho+fn5ydHRUQ0aNFBcXFy++m3fvl22traqV6+eWd2VK1c0bNgweXl5ydHRUTVr1tS6deuM9bdv39bEiRPl5+cnJycnValSRREREcrJyTG2uXbtmoYPH66KFSvKyclJNWvW1KxZs4z1p06dktWUq7m+vj6SZWznG5VuVj/2h1sPcKcAACh6o0eP1ueff64vvvhCv/zyi0aNGqWkpCQNHTpUkjRu3Dj16dNHkmRtba2goCCTl7u7uxwdHRUUFCQXFxeTsefNm6euXbuqbNmyuc6dk5Oj+fPnq2/fvmbnv1+/fl3jx4/Xrl27dPr0ae3fv18DBw7UmTNn9Nxzz0mSEhMTFRERob179+rUqVNat26dnnvuOQUHB6tZs2YFfasAoNixeGW7j49PYcQBAAAAAACAArB8+XKNHDlS0dHRatasmWbPnq0OHTro6NGjqly5cp790tLS1KdPH7Vp00bnzp0zqcvMzFS7du3k7u6uFStWqGLFivr999/l6upqbPPOO+/os88+08KFCxUYGKi9e/eqf//+cnNz04gRIyRJo0aN0o8//qjFixfL19dXMTExCgsLk7e3t7p06aJKlSop+dUSJnPP2ZelGdsz1MHf9NdYES0dNKjB/zvesIS9+bm2AAA8DLp3766LFy8qIiJCycnJCgoK0rp164z5mOTkZCUlJVk87vHjx7Vt2zbFxMTk2eaHH35QUlKSyW7Gd9nY2OjXX3/VwoULdeHCBZUtW1aNGjVSXFycAgMDJd1Zmb9x40Z9+OGHunbtmipVqqROnTpp8uTJsrGxsThmAHjYWJxsl6Qvv/xSn332mU6ePKmdO3fKx8dHUVFR8vPzy3VbEwAAAAAAAPwzZs6cqQEDBmjgwIGSpKioKG3YsEGzZs1SZGRknv2GDBmi0NBQ2djYaM2aNSZ1X3zxhS5duqQdO3bIzu5OgvuvCzJ27typLl26GLeg9fX11dKlS7V3716TNn379lXLli0lSYMHD9bs2bO1d+9edenSRTY2NvIsYboR4+pfs9Q90M4sme7qILO2AAA8rMLCwhQWFpZr3YIFC+7ZNzw8XOHh4Wbl1atXl8FguGffkJCQPNs4OjrmeY77XZUqVdKWLVvu2QYAHmUW/xfJrFmzNHr0aHXs2FFXrlwxntFeqlQpRUVFFXR8AAAAAAAAyKfMzEzt27dPISEhJuUhISHasWNHnv3mz5+vxMRETZ48Odf6tWvXqkmTJho2bJg8PDwUFBSkadOmGX8vJEnNmzfXxo0bdfz4cUnSwYMHtW3bNnXs2NGkzdq1a3X27FkZDAb9+OOPOn78uNq3b5/rvPv+yFZ8So4G1Lczq3tne6bKzkhXvc+uaerWDGVm3zuZABQnRXHUw59FRkbKyspKI0eONCk/d+6c+vXrJ29vbzk7O+upp55SQkKCSZuWLVvKysrK5NWjR498xQ8AAPCosXhl+8cff6y5c+eqa9eumj59urG8YcOGeu211wo0OAAAAAAAAOTfhQsXlJ2dLQ8PD5NyDw8PpaSk5NonISFBY8eOVVxcnNlZrXedOHFCmzZtUq9evbRu3TolJCRo2LBhun37tt58801J0htvvKG0tDQFBATIxsZG2dnZmjp1qnr27Gkc56OPPtKgQYNUsWJF2draytraWp9//rmaN2+e67zzDmSqZjlrNa1kGteIxvaq72Wj0k5W+ulstsZtzNDJKzn6/GmnfN8roKgU1VEPd+3Zs0dz5sxRnTp1TMoNBoO6du0qOzs7ffPNNypZsqRmzpyptm3b6ujRoybnQA8aNEgRERHG905OfPYAAMC/k8XJ9pMnTyo4ONis3MHBQdevXy+QoAAAAAAAAPDgrKxMt1w3GAxmZZKUnZ2t0NBQTZkyRdWrV89zvJycHLm7u2vOnDmysbFRgwYN9Mcff+jdd981JtuXL1+uxYsX66uvvlJgYKDi4+M1cuRIeXt7q2/fvpLuJNt37dqltWvXysfHR1u3blVYWJi8vLzUtm1bkzlvZhn01aEsTXrSwSyeUU3+X1kdDxuVdrTSs1/f1DttHVTWma3lUbwV1VEPknTt2jX16tVLc+fO1dtvv21Sl5CQoF27dunw4cPGs5ijo6Pl7u6upUuXGuOVJGdnZ3l6ej7Q9QMAADxKLP6vDz8/P8XHx5uVf//996pVq1ZBxAQAAAAAAIAHUK5cOdnY2JitYk9NTTVb7S5J6enp2rt3r4YPHy5bW1vZ2toqIiJCBw8elK2trTZt2iRJ8vLyUvXq1WVjY2PsW7NmTaWkpCgzM1OS9Prrr2vs2LHq0aOHateurd69e2vUqFHG5OHNmzc1fvx4zZw5U507d1adOnU0fPhwde/eXe+9955ZbCuOZulGltSnrvkW8n/1eMU7cf12KSefdwooGkV51IMkDRs2TJ06dTJ7uEWSMjIyJN05o/kuGxsb2dvba9u2bSZtlyxZonLlyikwMFCvvfaa0tPT733hAAAAjyiLV7a//vrrGjZsmG7duiWDwaCffvpJS5cuVWRkpD7//PPCiBEAAAAAAAD5YG9vrwYNGig2NlbdunUzlsfGxqpLly5m7UuWLKlDhw6ZlEVHR2vTpk1asWKF/Pz8JEnNmjXTV199pZycHFlb31m7cfz4cXl5ecne3l6SdOPGDWPdXTY2NsrJuZMAz8rKUlZW1j3b/Nm8A1l6uoatyrvcf63IgZQ7CUUvV1a1o3gryqMeli1bpv3792vPnj25jhEQECAfHx+NGzdOs2fPlouLi2bOnKmUlBQlJycb2/Xq1Ut+fn7y9PTU4cOHNW7cOB08eFCxsbEPcksAAAAeahYn2/v376/bt29rzJgxunHjhkJDQ1WhQgV9+OGH6tGjR2HECAAAAAAAgHwaPXq0evfurYYNG6pJkyaaM2eOkpKSNHToUEnSuHHjdPbsWS1atEjW1tYKCgoy6e/u7i5HR0eT8pdeekkff/yxRowYoZdfflkJCQmaNm2aXnnlFWObzp07a+rUqapcubICAwN14MABzZw5Uy+++KKkO4n9Fi1a6PXXX5eTk5N8fHy0ZcsWLVq0SDNnzjSJ4bdLOdp6OlvrejmbXd/O329r15lstfKzlZuDlfb8ka1RG27p6Rq2quxGsh0Ph3/6qIfff/9dI0aMUExMjMnK9T+zs7PTypUrNWDAAJUpU0Y2NjZq27atOnToYNJu0KBBxj8HBQXJ399fDRs21P79+1W/fn1LbgMAAMBDz+Jku3TnC9WgQYN04cIF4xc5AAAAAAAAFL3u3bvr4sWLioiIUHJysoKCgrRu3Trj+c3JyclKSkqyaMxKlSopJiZGo0aNUp06dVShQgWNGDFCb7zxhrHNxx9/rEmTJiksLEypqany9vbWkCFDjKtqpTsra8eNG6devXrp0qVL8vHx0dSpU40PAtz1xYFMVShppZCqNvorB1srLT9yW1O2ZCgjW/Jxs9ag+nYa08z8bHeguHnQox4OHDig4cOHS7qTWDcYDLK1tVVMTIxat24tLy8v2dnZ5XnUw759+5SamqoGDRoY67Ozs7V161Z98sknysjIMCbp4+PjlZaWpszMTJUvX16NGzdWw4YN87ym+vXry87OTgkJCSTbgUL26dBNRR2Chn3WuqhDAIBixeJk+8mTJ3X79m35+/urXLlyxvKEhATZ2dnJ19e3IOMDAAAAAACAhcLCwhQWFpZr3YIFC+7ZNzw8XOHh4WblTZo00a5du/Ls5+rqqqioKEVFReXZxtPTU/Pnz7/n/JI0rY2jprXJffVtfS8b7Rroct8xgOKoqI56aNOmjdk4/fv3V0BAgN544w2TJL0kubm5SbrzO9+9e/fqrbfeyvOajhw5oqysLHl5eVlwJwAAAB4NFifb+/XrpxdffFH+/v4m5bt379bnn3+uzZs3F1RsAAAAAAAAAPBIKYqjHlxdXc3GcXFxUdmyZU3Kv/76a5UvX16VK1fWoUOHNGLECHXt2lUhISGSpMTERC1ZskQdO3ZUuXLldPToUb366qsKDg5Ws2bNCuV+AQAAFGcWJ9sPHDiQ6xenxx9/3LiVEQAAAAAAAADAXFEd9ZAfycnJGj16tM6dOycvLy/16dNHkyZNMtbb29tr48aN+vDDD3Xt2jVVqlRJnTp10uTJk81WxwMAAPwbWFvawcrKSunp6WblaWlpys7OLpCgAAAAAAAAAOBRFRYWplOnTikjI0P79u3Tk08+aaxbsGDBPXcPDQ8PV3x8vFn53aMebt26pcTERI0fP/6eCfDNmzebHfvwyiuv6Pfff1dmZqZOnz6tt956S/b29sb6SpUqacuWLbp48aIyMjL022+/6cMPP1SZMmXyfe3AX0VHR8vPz0+Ojo5q0KCB4uLi8tVv+/btsrW1Vb169UzKFyxYICsrK7PXrVu3ch0nMjJSVlZWGjlypEm5wWBQeHi4vL295eTkpJYtW+rIkSNm/Xfu3KnWrVvLxcVFpUqVUsuWLXXz5s18XQMA4OFncbL9iSeeUGRkpEliPTs7W5GRkWrevHmBBgfkR/r+73TmswE6/V43JS8YoVu/H85Xv7/zZSwyMlKNGjWSq6ur3N3d1bVrVx07dsxknMP9Duf6Or/u/N++ZgAAAAAAAAB42C1fvlwjR47UhAkTdODAAT3xxBPq0KHDfXd3SEtLU58+fdSmTZtc60uWLKnk5GSTl6Ojo1m7PXv2aM6cOapTp45Z3YwZMzRz5kx98skn2rNnjzw9PdWuXTuTxYg7d+7UU089pZCQEP3000/as2ePhg8fLmtri1MvAICHlMXbyL/zzjtq0aKFatSooSeeeEKSFBcXp6tXr2rTpk0FHiBwL9d/2apLG+eqTMhLcqxQS+nx3yv163AlRfZU5cqV8+z35y9j586dM6svWbKkWfL8z1/GtmzZomHDhqlRo0a6ffu2JkyYoJCQEB09elQuLi6SpBpRNUz6Xzt0TWe/OCu3hm5/55IBAAAAAAAA4JEwc+ZMDRgwQAMHDpQkRUVFacOGDZo1a5YiIyPz7DdkyBCFhobKxsZGa9asMau3srKSp6fnPee+du2aevXqpblz5+rtt982qTMYDIqKitKECRP0zDPPSJIWLlwoDw8PffXVVxoyZIgkadSoUXrllVc0duxYY19/f/98XTsA4NFgcbI9MDBQP//8sz755BMdPHhQTk5O6tOnj4YPH852QfjHXd2zRiXqtJNr3faSpDJtB+vmyf2F/mVs/fr1Ju/nz58vd3d3k22/7ErZmca6/6pcAlxk724vAAAAAACKI9+x3xV1CDplvvAQAPAIyszM1L59+0wS1ZIUEhKiHTt25Nlv/vz5SkxM1OLFi82S5Hddu3ZNPj4+ys7OVr169fTWW28pODjYpM2wYcPUqVMntW3b1myckydPKiUlRSEhIcYyBwcHtWjRQjt27NCQIUOUmpqq3bt3q1evXmratKkSExMVEBCgqVOnsgswAPyLWLSXSVZWllq1aqVr165p2rRp+u6777RixQq9+eabJNrxjzNkZykz5Tc5+Zl+SXLyC87Xl7HJkyfn2ebul7GKFSvqP//5jw4cOHDPWNLS0iQpz8/B7bTbSv85XaWfLH3PcQAAAAAAAADg3+DChQvKzs6Wh4eHSbmHh4dSUlJy7ZOQkKCxY8dqyZIlsrXNfS1hQECAFixYoLVr12rp0qVydHRUs2bNlJCQYGyzbNky7d+/P88FW3fnv1dsJ06ckCSFh4dr0KBBWr9+verXr682bdqYzAUAeLRZtLLdzs5Ohw8flpWVVWHFA+Rb9o2rkiFH1s6mCWwbl9JKSTmWa5+7X8bi4uLu+2Wsdu3aunr1qj788EM1a9ZMBw8ezHULIIPBoNGjR6t58+YKCgrKdczL2y/LxtFGJRuUtPAqAQAAAAAAAODR9dd8g8FgyDUHkZ2drdDQUE2ZMkXVq1fPc7zHH39cjz/+uPF9s2bNVL9+fX388cf66KOP9Pvvv2vEiBGKiYnJ9Rz3/MaWk5Mj6c4uqv3795ckBQcHa+PGjfriiy/uufMqAODRYdHKdknq06eP5s2bVxixAA/E7HtXAXwZe+GFF1S3bl098cQT+t///qfq1avr448/zrX98OHD9fPPP2vp0qV5jnl562W5Pe4ma3uLP3IAAAAA8LdER0fLz89Pjo6OatCggeLi4vLVb/v27bK1tVW9evXybLNs2TJZWVmpa9euJuW+vr6ysrIyew0bNszYxmAwKDw8XN7e3nJyclLLli115MgRk3FatmxpNkaPHj3yfe0AAKD4KleunGxsbMxWsaemppqtKJek9PR07d27V8OHD5etra1sbW0VERGhgwcPytbWVps2bcp1HmtrazVq1Mi42nzfvn1KTU1VgwYNjONs2bJFH330kWxtbZWdnW08YvResXl5eUmSatWqZdKmZs2aSkpKeoA7AgB4GFl8ZntmZqY+//xzxcbGqmHDhnJxcTGpnzlzZoEFB9yLjXNJycpa2dcvm5Rn37hyzy9jBw4c0PDhwyXdefrQYDDI1tZWMTExat26tVm/v34Z+7OXX35Za9eu1datW1WxYsVc47x+7LoyUzJVOowt5AEAAAD8s5YvX66RI0cqOjpazZo10+zZs9WhQwcdPXpUlStXzrNfWlqa+vTpozZt2ujcuXO5tjl9+rRee+01PfHEE2Z1e/bsUXZ2tvH94cOH1a5dOz333HPGshkzZmjmzJlasGCBqlevrrffflvt2rXTsWPH5Orqamw3aNAgRUREGN87OTlZdA8A4GEWHh5e1CEUixjwaLK3t1eDBg0UGxurbt26GctjY2PVpUsXs/YlS5bUoUOHTMqio6O1adMmrVixQn5+frnOYzAYFB8fr9q1a0uS2rRpYzZO//79FRAQoDfeeEM2Njby8/OTp6enYmNjjWe9Z2ZmasuWLXrnnXck3Xm40NvbW8eOme6yevz4cXXo0MHCuwEAeFhZnGw/fPiw6tevL+nOD40/Y3t5/JOsbOxk71lNN0/Fy7l6U2P5rVPxajqol1n7gvoydrfs5Zdf1urVq7V58+Y8+0p3VrU7+jrKqTK/EHrUbf/tlDYfO6H0mxnycCuhLvUCVaV8mfv3275dLVq0UFBQkOLj43Nts2zZMvXs2VNdunTRmjVrcm0TGRmp8ePHa8SIEYqKijKWr1mzRgcPHjRpW6FCBQ0cODC/lwYAAICH1MyZMzVgwADjd7+oqCht2LBBs2bNuufWpkOGDFFoaKhsbGxy/f6ZnZ2tXr16acqUKYqLi9OVK1dM6suXL2/yfvr06apatapatGgh6c5/U0VFRWnChAl65plnJEkLFy6Uh4eHvvrqKw0ZMsTY19nZ2bi6DAAAPFpGjx6t3r17q2HDhmrSpInmzJmjpKQkDR06VJI0btw4nT17VosWLZK1tbXZMZ7u7u5ydHQ0KZ8yZYoef/xx+fv76+rVq/roo48UHx+vTz/9VJLk6upqNo6Li4vKli1rLLeystLIkSM1bdo0+fv7y9/fX9OmTZOzs7NCQ0ONbV5//XVNnjxZdevWVb169bRw4UL9+uuvWrFiRaHdMwBA8WJxsv3HH38sjDiAB1KyUVdd+HamHDyrycG7ptIPrtftq+cL9cuYJA0bNkxfffWVvvnmG7m6uhq3E3JzczNZZZF9M1tpe9Lk1cOrMG8DioH4pD+0Nv6onqkfJN9ypbUrMUmfx/2k19u3uGe/v7ti6K49e/Zozpw5qlOnTq711apVM3ki2MbGJh9XBQAAgIdZZmam9u3bp7Fjx5qUh4SEaMeOHXn2mz9/vhITE7V48WK9/fbbubaJiIhQ+fLlNWDAgPtuS5+ZmanFixdr9OjRxof0T548qZSUFIWEhBjbOTg4qEWLFtqxY4dJsn3JkiVavHixPDw81KFDB02ePNlk5TsAAHh4de/eXRcvXlRERISSk5MVFBSkdevWycfHR5KUnJxs8ZbsV65c0eDBg5WSkiI3NzcFBwdr69ateuyxxywaZ8yYMbp586bCwsJ0+fJlNW7cWDExMSbfQ0aOHKlbt25p1KhRunTpkurWravY2FhVrVrVorkAAA8vi5Ptd/32229KTEzUk08+KScnJxnyOCcbKEwuNZ9Uzs10Xdm+TNnXL8m+nI/cnwsv9C9js2bNknTn/MA/mz9/vvr162d8n7Y7TZLk9rjbA1wdHiZbjp/UY36V1LjKna04uwQH6ljKee1MPH3Pfn93xZAkXbt2Tb169dLcuXPz/GWojY2NSpQoYfF1AQAA4OF14cIFZWdnmx2z5eHhYXb+6F0JCQkaO3as4uLiZGub+68Mtm/frnnz5uW5K9NfrVmzRleuXDH5b6W78+cW2+nT/+87dK9evYzbuB4+fFjjxo3TwYMHFRsbm6+5AQBA8RcWFqawsLBc6xYsWHDPvuHh4WZHHXzwwQf64IMPLIph8+bNZmVWVla5jv9XY8eONXu4EQDw72Fxsv3ixYt6/vnn9eOPP8rKykoJCQmqUqWKBg4cqFKlSun9998vjDiBPLnW7yTX+p1yrSusL2MGgyFfsZVpWUZlWt5/G3E83G5n5+js5TS1DjB9YrW6Z3mdung5z34FtWJo2LBh6tSpk9q2bZvnOKdOndK7774rR0dH+fj4qE2bNnJxccnnFQIAAOBh9tcH4/N6WD47O1uhoaGaMmWKqlevnutY6enpeuGFFzR37lyVK1cuX/PPmzdPHTp0kLe3t8WxDRo0yPjnoKAg+fv7q2HDhtq/f7/xiDsAAAAAAIqKxcn2UaNGyc7OTklJSapZs6axvHv37ho1ahTJdgD/OtczM5VjMMjV0d6k3NXBQem3MnLtU1ArhpYtW6b9+/drz549ebapVq2aatWqpVKlSuny5cv68ccftXDhQg0ePDjPuQEAAPDwK1eunGxsbMxWsaemppqtKJfuJNL37t2rAwcOaPjw4ZKknJwcGQwG2draKiYmRmXKlNGpU6fUuXNnY7+cnBxJkq2trY4dO2ayberp06f1ww8/aNWqVSZz3T2DPSUlRV5e/+/Yrbxiu6t+/fqys7NTQkICyXYAAAAAQJGzOMsSExOjDRs2qGLFiibl/v7+Jlu9AcC/z19W5Sj3HRAKasXQ77//rhEjRigmJkaOjo55RhUUFGT8s7u7u7y9vRUVFaWEhASTh6YAAADwaLG3t1eDBg0UGxurbt26GctjY2PVpUsXs/YlS5bUoUOHTMqio6O1adMmrVixQn5+frKxsTFrM3HiRKWnp+vDDz9UpUqVTOrmz58vd3d3depkuhvZ3a3hY2NjFRwcLOnO2e5btmzRO++8k+c1HTlyRFlZWSYJegAAAAAAiorFyfbr16/L2dnZrPzChQtycHAokKAA4GHiYm8vaysrs1Xs1zIy5epo/u9iQa0YOnTokFJTU9WgQQNjm+zsbG3dulWffPKJMjIyZGNjYza/q6urSpUqpYsXLxbI9QMAAKD4Gj16tHr37q2GDRuqSZMmmjNnjpKSkjR06FBJ0rhx43T27FktWrRI1tbWJg9qSnce1nR0dDQp/2ubUqVK5Vqek5Oj+fPnq2/fvmY7KllZWWnkyJGaNm2a/P395e/vr2nTpsnZ2VmhoaGSpMTERC1ZskQdO3ZUuXLldPToUb366qsKDg5Ws2bNCuT+AAAAAADwd1icbH/yySe1aNEivfXWW5Lu/AdyTk6O3n33XbVq1arAAwSA4s7WxloVSrvp+Lnzql3R01h+/NwFBXmbb4FZUCuG3N3dzdr0799fAQEBeuONN3JNtEvSjRs3lJaWJldX1we9ZAAAADwkunfvrosXLyoiIkLJyckKCgrSunXr5OPjI0lKTk5WUlJSocz9ww8/KCkpSS+++GKu9WPGjNHNmzcVFhamy5cvq3HjxoqJiTF+T7W3t9fGjRv14Ycf6tq1a6pUqZI6deqkyZMn5/ldFwAAAACAf5LFyfZ3331XLVu21N69e5WZmakxY8boyJEjunTpkrZv314YMQJAsdeiup+W/hSvSqVLyadcKe1K/F1XbtzU41UrSyqcFUP29vZmbVxcXFS2bFlj+bVr1xQTE6OaNWvK1dVVV65c0caNG+Xs7KyAgIACvQcAAAAonsLCwhQWFpZr3YIFC+7ZNzw8XOHh4fdsk9cYISEhMhhyP1pJuvPw/r3Gr1SpkrZs2XLPuQHcX3R0tN59910lJycrMDBQUVFReuKJJ+7bb/v27WrRooWCgoIUHx9vLF+1apWmTZum3377TVlZWfL399err76q3r17G9uEh4drypQpJuN5eHgoJSXF+N7KyvQotrtmzJih119/XadOnZKfn1+ubf73v//pueeekyT5+vqaHW35xhtvaPr06fe9RgAPl9oLaxfp/If6Hrp/IwDAv47FyfZatWrp559/1qxZs2RjY6Pr16/rmWee0bBhwzgzDcVHuFtRRyD5VS7qCPAPqlfZW9czMxV7NEFXb2XI062EBjzRSGVc7hy7UZgrhu7FxsZGqampOnjwoG7duiVXV1f5+vrq2Wef5egPAAAAAHjELV++XCNHjlR0dLSaNWum2bNnq0OHDjp69KgqV8779xZpaWnq06eP2rRpo3PnzpnUlSlTRhMmTFBAQIDs7e317bffqn///nJ3d1f79u2N7QIDA/XDDz8Y3/91R4rk5GST999//70GDBig//73v5LuPHDz1zZz5szRjBkz1KFDB5PyiIgIDRo0yPi+RIkS97otAAAAQIGxKNl++vRpxcTEKCsrS88//7zZE6oA8G/WrJqvmlXzzbWuMFcM/dnmzZtN3js5OemFF164bz8AAAAAwKNn5syZGjBggAYOHChJioqK0oYNGzRr1ixFRkbm2W/IkCEKDQ2VjY2N1qxZY1LXsmVLk/cjRozQwoULtW3bNpNku62trTw9PZWXv9Z98803atWqlapUqSLpTnL+r21Wr16t7t27myXTXV1d7zkXAAAAUFis89tw69atCgwM1JAhQzR8+HAFBwdr6dKlhRkbAAAAAAAAgAeQmZmpffv2KSQkxKQ8JCREO3bsyLPf/PnzlZiYqMmTJ993DoPBoI0bN+rYsWN68sknTeoSEhLk7e0tPz8/9ejRQydOnMhznHPnzum7777TgAED8myzb98+xcfH59rmnXfeUdmyZVWvXj1NnTpVmZmZ940dAAAAKAj5TrZPmjRJrVq10pkzZ3Tx4kW9+OKLGjNmTGHGBgAAAAAAAOABXLhwQdnZ2fLw8DAp/+vZ6X+WkJCgsWPHasmSJbK1zXtDzLS0NJUoUUL29vbq1KmTPv74Y7Vr185Y37hxYy1atEgbNmzQ3LlzlZKSoqZNm+rixYu5jrdw4UK5urrqmWeeyXPOefPmqWbNmmratKlJ+YgRI7Rs2TL9+OOPGj58uKKiohQWFpbnOAAAAEBByvc28ocOHdLWrVvl7e0tSXr//fc1d+5cXb58WaVLly60AAEAAAAAAAA8GCsrK5P3BoPBrEySsrOzFRoaqilTpqh69er3HNPV1VXx8fG6du2aNm7cqNGjR6tKlSrGLeb/fKZ67dq11aRJE1WtWlULFy7U6NGjzcb74osv1KtXLzk6OuY6382bN/XVV19p0qRJZnWjRo0y/rlOnToqXbq0nn32WeNqdwAAAKAw5TvZfuXKFbm7uxvfu7i4yNnZWVeuXCHZDgAAAADAQ+7M2Lginb/i9CeKdH7gUVOuXDnZ2NiYrWJPTU01W+0uSenp6dq7d68OHDig4cOHS5JycnJkMBhka2urmJgYtW7dWpJkbW2tatWqSZLq1aunX375RZGRkWbnud/l4uKi2rVrKyEhwawuLi5Ox44d0/Lly/O8lhUrVujGjRvq06fPfa/78ccflyT99ttvJNsBAABQ6PKdbJeko0ePmnxBNxgM+uWXX5Senm4sq1OnTsFFBwAAAAAAAMBi9vb2atCggWJjY9WtWzdjeWxsrLp06WLWvmTJkjp06JBJWXR0tDZt2qQVK1bIz88vz7kMBoMyMjLyrM/IyNAvv/yiJ54wf6hm3rx5atCggerWrZtn/3nz5unpp59W+fLl82xz14EDByRJXl5e920LAAAA/F0WJdvbtGkjg8FgUvaf//xHVlZWxi2osrOzCzRAAAAAAAAAAJYbPXq0evfurYYNG6pJkyaaM2eOkpKSNHToUEnSuHHjdPbsWS1atEjW1tYKCgoy6e/u7i5HR0eT8sjISDVs2FBVq1ZVZmam1q1bp0WLFmnWrFnGNq+99po6d+6sypUrKzU1VW+//bauXr2qvn37mox/9epVff3113r//ffzvIbffvtNW7du1bp168zqdu7cqV27dqlVq1Zyc3PTnj17NGrUKD399NOqXLnyA90zAAAAwBL5TrafPHmyMOMAgEdWUW/HqdyPvAMAAAAAPOK6d++uixcvKiIiQsnJyQoKCtK6devk4+MjSUpOTlZSUpJFY16/fl1hYWE6c+aMnJycFBAQoMWLF6t79+7GNmfOnFHPnj114cIFlS9fXo8//rh27dplnPeuZcuWyWAwqGfPnnnO98UXX6hChQoKCQkxq3NwcNDy5cs1ZcoUZWRkyMfHR4MGDdKYMWMsuiYAAADgQeU72f7XL8MAAAAAAAAAirewsDCFhYXlWrdgwYJ79g0PD1d4eLhJ2dtvv6233377nv2WLVuWr9gGDx6swYMH37PNtGnTNG3atFzr6tevr127duVrLgAAAKAwWBd1AAAAAAAAAAAAAAAAPGxItgMAAAAAAAAAAAAAYCGS7QAAAAAAAAAAAAAAWIhkOwAAAAAAAAAAAAAAFnqgZPvt27f1ww8/aPbs2UpPT5ck/fHHH7p27VqBBgcAAAAAAAAAAAAAQHFka2mH06dP66mnnlJSUpIyMjLUrl07ubq6asaMGbp165Y+++yzwogTAAAAAAAAwD/k06GbijoEDfusdVGHAAAAANyTxSvbR4wYoYYNG+ry5ctycnIylnfr1k0bN24s0OAAAAAAAAAAAAAAACiOLF7Zvm3bNm3fvl329vYm5T4+Pjp79myBBQYAAAAAAAAAAAAAQHFl8cr2nJwcZWdnm5WfOXNGrq6uBRIUAAAAAAAAAAAAAADFmcXJ9nbt2ikqKsr43srKSteuXdPkyZPVsWPHgowNAAAAAAAAAAAAAIBiyeJt5D/44AO1atVKtWrV0q1btxQaGqqEhASVK1dOS5cuLYwYAQAAAAAAAAAAAAAoVixOtnt7eys+Pl5Lly7V/v37lZOTowEDBqhXr15ycnIqjBgBAAAAAAAAAAAAAChWLE62S5KTk5NefPFFvfjiiwUdDwAAAAAAAAAAAAAAxZ7Fyfa1a9fmWm5lZSVHR0dVq1ZNfn5+fzswAAAAAAAAAAAAAACKK4uT7V27dpWVlZUMBoNJ+d0yKysrNW/eXGvWrFHp0qULLFAAAAAAAAAAAFCw0vd/p7SfVin72iXZl6us0m0GybFS0H37bd++XS1atFBQUJDi4+NN6lauXKlJkyYpMTFRVatW1dSpU9WtWzdjva+vr06fPm02ZlhYmD799FPj+19++UVvvPGGtmzZomuZ1+Tg7aBKwyrJvqy9Ms9n6vjrx3ONrVJYJbk95pbPOwAAwIOztrRDbGysGjVqpNjYWKWlpSktLU2xsbF67LHH9O2332rr1q26ePGiXnvttcKIFwAAAAAAAAAAFIDrv2zVpY1z5dbkeXn3+0gOFQOV+nW4bl9NvWe/tLQ09enTR23atDGr27lzp7p3767evXvr4MGD6t27t55//nnt3r3b2GbPnj1KTk42vmJjYyVJzz33nLFNYmKimjdvroCAAG3evFnVIqrJ/Wl3WdvdSWvYlbVTjagaJi/3bu6ydrBWiTolCuL2AABwXxavbB8xYoTmzJmjpk2bGsvatGkjR0dHDR48WEeOHFFUVBTnuQMAAAAAAAAAUIxd3bNGJeq0k2vd9pKkMm0H6+bJ/Uo/sE6lW/TLs9+QIUMUGhoqGxsbrVmzxqQuKipK7dq107hx4yRJ48aN05YtWxQVFaWlS5dKksqXL2/SZ/r06apatapatGhhLJswYYI6duyoGTNmSJLsf7aXvbu9sd7K2kp2pexMr2ffVZV8rKRsHG0suxEAADwgi1e2JyYmqmTJkmblJUuW1IkTJyRJ/v7+unDhwt+PDgAAAAAAAAAAFDhDdpYyU36Tk1+wSbmTX7Ayzv6aZ7/58+crMTFRkydPzrV+586dCgkJMSlr3769duzYkWv7zMxMLV68WC+++KKsrKwkSTk5Ofruu+9UvXp1tW/fXu7u7kqMSNTVfVfzjOvmqZu6lXRLZZ4sk2cbAAAKmsXJ9gYNGuj111/X+fPnjWXnz5/XmDFj1KhRI0lSQkKCKlasWHBRAgAAAAAAAACAApN946pkyJG1c2mTchuX0sq+fjnXPgkJCRo7dqyWLFkiW9vcN85NSUmRh4eHSZmHh4dSUlJybb9mzRpduXJF/fr1M5alpqbq2rVrmj59up566inFxMSoZP2SSvokSdd/vZ7rOJe3XpaDt4Oc/Z3zumQAAAqcxdvIz5s3T126dFHFihVVqVIlWVlZKSkpSVWqVNE333wjSbp27ZomTZpU4MECAAAAAAAAAICC8/8vJv9/DAZJfy2UsrOzFRoaqilTpqh69er3GdO0v8FgMCu7a968eerQoYO8vb2NZTk5OZKkLl26aNSoUZKk8v8prxu/3dClHy/JJcDFZIyczBxd2XlF7k+73zMuAAAKmsXJ9ho1auiXX37Rhg0bdPz4cRkMBgUEBKhdu3aytr6zUL5r164FHScAAAAAAAAAACggNs4lJStrs1Xs2TeuyMallFn79PR07d27VwcOHNDw4cMl3UmKGwwG2draKiYmRq1bt5anp6fZKvbU1FSz1e6SdPr0af3www9atWqVSXm5cuVka2urWrVqmZQ7eDvoxvEbZuOk7UmTIdOgUs3M4wYAoDBZnGyX7jyV9tRTT+mpp54q6HgAAAAAAAAAAEAhs7Kxk71nNd08FS/n6k2N5bdOxcvJv7FZ+5IlS+rQoUMmZdHR0dq0aZNWrFghPz8/SVKTJk0UGxtrXJEuSTExMWratKn+av78+XJ3d1enTp1Myu3t7dWoUSMdO3bMpDwjJUN25ezMxrm89bJcg11lW/KBUh66uPGiLnx/Qbev3JZDBQd5hXrJpYZLrm23bdumN954Q7/++qtu3LghHx8fDRkyxOR6s7KyFBkZqYULF+rs2bOqUaOG3nnnHZOcSnp6uiZNmqTVq1crNTVVwcHB+vDDD43H9UrmOwTc1bXxYLWt113Xb13Vd3sX6tcze3X5+nmVcHRTHd9m+k/DfnJyKPFA9wIAYJkH+slz/fp1bdmyRUlJScrMzDSpe+WVVwokMAAAAAAAAAAAUHhKNuqqC9/OlINnNTl411T6wfW6ffW8XOt1lCRd3rJA2ekXpemdZG1traCgIJP+7u7ucnR0NCkfMWKEnnzySb3zzjvq0qWLvvnmG/3www/atm2bSd+cnBzNnz9fffv2zfX899dff13du3fXk08+qVatWuniDxeVHp8uv7F+Ju0yzmXoxvEb8hnl80D3IG13mlK+SpFXHy85+zvr8o+XdXrmaVWbVk32Ze3N2ru4uGj48OGqU6eOXFxctG3bNg0ZMkQuLi4aPHiwJGnixIlavHix5s6dq4CAAG3YsEHdunXTjh07FBwcLEkaOHCgDh8+rC+//FLe3t5avHix2rZtq6NHj6pChQqSpOTkZJO5X+sxU19teU/1qjxxJ/YbF5V246K6PT5EnqV9denaOS2L+0Bp1y9oYEj4A90PAIBlLE62HzhwQB07dtSNGzd0/fp1lSlTRhcuXJCzs7Pc3d1JtgMAAAAAAAAA8BBwqfmkcm6m68r2Zcq+fkn25Xzk/ly4bN3unH2efe2ybl89b9GYTZs21bJlyzRx4kRNmjRJVatW1fLly9W4selq+R9++EFJSUl68cUXcx2nW7du+uyzzxQZGXkn71Beqjy8slyqm644vxx3WbalbVUi6MFWcl/YcEGlnyytMi3KSJK8ennp2uFrurTpkjyf8zRrHxwcbEyYS5Kvr69WrVqluLg4Y7L9yy+/1IQJE9Sx452HFl566SVt2LBB77//vhYvXqybN29q5cqV+uabb/Tkk09KksLDw7VmzRrNmjVLb7/9tiTJ09N0/kOnt8vfu57Klbxzvr13GT8N+lNSvbybtzo3GqBFmyKVnZMtG2ubB7onAID8szjZPmrUKHXu3FmzZs1SqVKltGvXLtnZ2emFF17QiBEjCiNGAAAAAAAAAABQCFzrd5Jr/U651pXrNCrX8rvCw8MVHh5uVv7ss8/q2WefvWffkJAQGQyGe7Z58cUXjcn42gtr59rG81lPeT5rnhTPj5zbObp56qbKdypvUl4iqIRu/GZ+NnxuDhw4oB07dhgT5JKUkZEhR0dHk3ZOTk7G1f23b99Wdnb2Pdv81blz53Q4abd6t3zjnvHcyrwmR3tnEu0A8A+xtrRDfHy8Xn31VdnY2MjGxkYZGRmqVKmSZsyYofHjxxdGjAAAAAAAAAAAAAUqOz1bypHZWe82JW10O+32PftWrFhRDg4OatiwoYYNG6aBAwca69q3b6+ZM2cqISFBOTk5io2N1TfffGPcFt7V1VVNmjTRW2+9pT/++EPZ2dlavHixdu/ebbZ1/F0LFy6Uo52z6vk9kWdM126l6fv9i9Ws5n/yewsAAH+Txcl2Ozs7WVlZSZI8PDyUlJQkSXJzczP+GQAAAAAAAAAA4KFg9Zf3915wL0mKi4vT3r179dlnnykqKkpLly411n344Yfy9/dXQECA7O3tNXz4cPXv3182Nv9vtfmXX34pg8GgChUqyMHBQR999JFCQ0NN2vzZF198oYbV2sjO1vwceUm6mXldn30/QV6lfdSxQZ/7XwAAoEBYvI18cHCw9u7dq+rVq6tVq1Z68803deHCBX355ZeqXTv3bVwAAAAAAAAAAACKExtXG8laZqvYs9OzZet27/SJn5+fJKl27do6d+6cwsPD1bNnT0lS+fLltWbNGt26dUsXL16Ut7e3xo4da+wjSVWrVtWWLVt0/fp1Xb16VV5eXurevbtJm7vi4uJ07NgxdXv21VxjuZV5Q9HrxsrBzkmDQiJkY2Nx6gcA8IAsXtk+bdo0eXl5SZLeeustlS1bVi+99JJSU1M1Z86cAg8QAAAAAAAAAACgoFnbWsvJ10nXjlwzKb925JqcqznnexyDwaCMjAyzckdHR1WoUEG3b9/WypUr1aVLF7M2Li4u8vLy0uXLl7Vhw4Zc28ybN08NGjRQxbJVzepuZl7XJ9+NkY21nYa0fyvPle8AgMJh0eNNBoNB5cuXV2BgoKQ7T2etW7euUAIDAAAAAAAAAAAoTOXal9OZOWfk5Oskp2pOurz5srIuZqlMqzKSpJSvU3T78m2p7532n376qSpXrqyAgABJ0rZt2/Tee+/p5ZdfNo65e/dunT17VvXq1dPZs2cVHh6unJwcjRkzxthmw4YNMhgMqlGjhn777Te9/vrrqlGjhvr3728S39WrV/X111/r/fffV3a8aey3Mm/o0+/eUObtW+rberxuZd3QrawbkqQSjm6yts59S3oAQMGxONnu7++vI0eOyN/fv7BiAgAAAAAAAAAAxUW4W1FHIPlVLpRh3Rq76fa120r9JlW3027LoYKDfEb7yL7cnRXit6/cVubFTGP7nJwcjRs3TidPnpStra2qVq2q6dOna8iQIcY2t27d0sSJE3XixAmVKFFCHTt21JdffqlSpUoZ26SlpWncuHE6c+aMypQpo//+97+aOnWq7OzsTOJbtmyZDAaDevbsqcXx+0zqki4c16nUXyRJU5b1NqmbErpEZV09C+QeAQDyZlGy3draWv7+/rp48SLJdgAAAAAAAAAA8NAr26asyrYpm2tdxUEVTd6//PLLJqvYc9OiRQsdPXr0nm2ef/55Pf/88/eNbfDgwRo8eHCuddW96+mTIRvvOwYAoPBYfGb7jBkz9Prrr+vw4cOFEQ8AAAAAAAAAAAAAAMWexcn2F154QT/99JPq1q0rJycnlSlTxuRlqejoaPn5+cnR0VENGjRQXFxcvvpt375dtra2qlevnsVzAgAAAAAAAAAA4I6tR77R5K96aeTnT+mdlUP1W/LPebbdtm2bmjVrprJly8rJyUkBAQH64IMPzNpFRUWpRo0acnJyUqVKlTRq1CjdunXLWB8eHi4rKyuTl6en6db3BoNB4eHh8vb2lpOTk1q2bKkjR46YzbVz5061bt1aLi4uKlWqlFq2bKmbN2/+jTsCAPlj0Tby0p1/HAvK8uXLNXLkSEVHR6tZs2aaPXu2OnTooKNHj6py5bzPX0lLS1OfPn3Upk0bnTt3rsDiAQAAAAAAAAAA+DfZ99uPWrkjWt2bv6IqnkHadvRbRa8bp4nPf6Eyrh5m7V1cXDR8+HDVqVNHLi4u2rZtm4YMGSIXFxfjlvdLlizR2LFj9cUXX6hp06Y6fvy4+vXrJ0kmifnAwED98MMPxvc2NjYmc82YMUMzZ87UggULVL16db399ttq166djh07JldXV0l3Eu1PPfWUxo0bp48//lj29vY6ePCgrK0tXm8KABazONnet2/fApt85syZGjBggAYOHCjpTiJ/w4YNmjVrliIjI/PsN2TIEIWGhsrGxkZr1qwpsHgAAAAAAAAAAAD+TTYdWqEmAR3UtGYnSdKzzYbplzN7FXf0/9Sl8UCz9sHBwQoODja+9/X11apVqxQXF2dMtu/cuVPNmjVTaGiosU3Pnj31008/mYxla2trtpr9LoPBoKioKE2YMEHPPPOMJGnhwoXy8PDQV199pSFDhkiSRo0apVdeeUVjx4419vX393/Q2wEAFnmgx3oSExM1ceJE9ezZU6mpqZKk9evX57p1R14yMzO1b98+hYSEmJSHhIRox44defabP3++EhMTNXny5HzNk5GRoatXr5q8AAAAAAAAAAAA/u1uZ2fp9/PHVbNiQ5PymhUb6OS5/OV8Dhw4oB07dqhFixbGsubNm2vfvn3G5PqJEye0bt06derUyaRvQkKCvL295efnpx49eujEiRPGupMnTyolJcUkj+Tg4KAWLVoY80ipqanavXu33N3d1bRpU3l4eKhFixbatm2bZTcCAB6QxSvbt2zZog4dOqhZs2baunWrpk6dKnd3d/3888/6/PPPtWLFinyNc+HCBWVnZ8vDw3QLEg8PD6WkpOTaJyEhQWPHjlVcXJxsbfMXemRkpKZMmZKvtgAAAAAAAAAAAH/1S0DNog5BavlpgQ957Vaacgw5cnUqbVLu6lRaV29cumffihUr6vz587p9+7bCw8ONuxhLUo8ePXT+/Hk1b95cBoNBt2/f1ksvvWSy+rxx48ZatGiRqlevrnPnzuntt99W06ZNdeTIEZUtW9aYK8otj3T69GlJMibnw8PD9d5776levXpatGiR2rRpo8OHD7PCHUChs3hl+9ixY/X2228rNjZW9vb2xvJWrVpp586dFgdgZWVl8t5gMJiVSVJ2drZCQ0M1ZcoUVa9ePd/jjxs3TmlpacbX77//bnGMAAAAAAAAAAAA/xYGmedv/iouLk579+7VZ599pqioKC1dutRYt3nzZk2dOlXR0dHav3+/Vq1apW+//VZvvfWWsU2HDh303//+V7Vr11bbtm313XffSbqzVfyf3SuPlJOTI+nO8cP9+/dXcHCwPvjgA9WoUUNffPHFA18/AOSXxSvbDx06pK+++sqsvHz58rp48WK+xylXrpxsbGzMVrGnpqaaPaUkSenp6dq7d68OHDig4cOHS7rzj6jBYJCtra1iYmLUunVrs34ODg5ycHDId1wAAAAAAAAAAAD/BiUc3WRtZa30m5dNyq/dvGy22v2v/Pz8JEm1a9fWuXPnFB4erp49e0qSJk2apN69extXu9euXVvXr1/X4MGDNWHCBFlbm68FdXFxUe3atZWQkCBJxrPcU1JS5OXlZWz35zzS3fJatWqZjFWzZk0lJSXl7yYAwN9g8cr2UqVKKTk52az8wIEDqlChQr7Hsbe3V4MGDRQbG2tSHhsbq6ZNm5q1L1mypA4dOqT4+Hjja+jQoapRo4bi4+PVuHFjSy8FAAAAAAAAAADgX8vWxk6VylfXr2f2mZT/emaf/DwC8z2OwWBQRkaG8f2NGzfMEuo2NjYyGAwyGAy5jpGRkaFffvnFmED38/OTp6enSR4pMzNTW7ZsMeaRfH195e3trWPHjpmMdfz4cfn4+OQ7fgB4UBavbA8NDdUbb7yhr7/+WlZWVsrJydH27dv12muvqU+fPhaNNXr0aPXu3VsNGzZUkyZNNGfOHCUlJWno0KGS7mwBf/bsWS1atEjW1tYKCgoy6e/u7i5HR0ezcgAAAAAAAAAAANxf69rPatGP01W5fHX5edTS9l++06VrqXqiVmdJ0je7P1fa9Qvq0/rOeeuffvqpKleurICAAEnStm3b9N577+nll182jtm5c2fNnDlTwcHBaty4sX777TdNmjRJTz/9tGxsbCRJr732mjp37qzKlSsrNTVVb7/9tq5evaq+fftKurN9/MiRIzVt2jT5+/vL399f06ZNk7Ozs0JDQ41tXn/9dU2ePFl169ZVvXr1tHDhQv36669asWLFP3YPAfx7WZxsnzp1qvr166cKFSrIYDCoVq1axvPUJ06caNFY3bt318WLFxUREaHk5GQFBQVp3bp1xqeNkpOT2eYDAAAAAAAAAACgkDSo1krXM67q+31f6uqNS/Iq46uwDpEq43pnq/arNy7q0rVUY/ucnByNGzdOJ0+elK2trapWrarp06dryJAhxjYTJ06UlZWVJk6cqLNnz6p8+fLq3Lmzpk6damxz5swZ9ezZUxcuXFD58uX1+OOPa9euXSYr0seMGaObN28qLCxMly9fVuPGjRUTEyNXV1djm5EjR+rWrVsaNWqULl26pLp16yo2NlZVq1YtzNsGAJIeINluZ2enJUuWKCIiQgcOHFBOTo6Cg4Pl7+//QAGEhYUpLCws17oFCxbcs294eLjCw8MfaF4AAAAAAAAAAABITwZ20ZOBXXKt693qDZP3L7/8sskq9tzY2tpq8uTJmjx5cp5tli1bdt+4rKys8pULGjt2rMaOHXvf8QCgoFmcbN+yZYtatGihqlWr8lQQAAAAAAAAAAAAAOBfydrSDu3atVPlypU1duxYHT58uDBiAgAAAAAAAAAAAACgWLM42f7HH39ozJgxiouLU506dVSnTh3NmDFDZ86cKYz4AAAAAAAAAAAAAAAodixOtpcrV07Dhw/X9u3blZiYqO7du2vRokXy9fVV69atCyNGAAAAAAAAAAAAAACKFYuT7X/m5+ensWPHavr06apdu7a2bNlSUHEBAAAAAAAAAAAAAFBsPXCyffv27QoLC5OXl5dCQ0MVGBiob7/9tiBjAwAAAAAAAAAAAACgWLK1tMP48eO1dOlS/fHHH2rbtq2ioqLUtWtXOTs7F0Z8AAAAAAAAAAAAKAbe7/6fIp3/1eUs+gRQvFicbN+8ebNee+01de/eXeXKlTOpi4+PV7169QoqNgAAAAAAAAAAAAAAiiWLk+07duwweZ+WlqYlS5bo888/18GDB5WdnV1gwQEAAAAAAAAAAAAAUBw98JntmzZt0gsvvCAvLy99/PHH6tixo/bu3VuQsQEAAAAAAAAAAAAAUCxZtLL9zJkzWrBggb744gtdv35dzz//vLKysrRy5UrVqlWrsGIEAAAAAAAAAAAAAKBYyffK9o4dO6pWrVo6evSoPv74Y/3xxx/6+OOPCzM2AAAAAAAAAAAAAACKpXyvbI+JidErr7yil156Sf7+/oUZEwAAAAAAAAAAAAAAxVq+V7bHxcUpPT1dDRs2VOPGjfXJJ5/o/PnzhRkbAAAAAAAAAAAAAADFUr6T7U2aNNHcuXOVnJysIUOGaNmyZapQoYJycnIUGxur9PT0wowTAAAAAAAAAAAAAIBiI9/J9rucnZ314osvatu2bTp06JBeffVVTZ8+Xe7u7nr66acLI0YAAAAAAAAAAAAAAIoVi5Ptf1ajRg3NmDFDZ86c0dKlSwsqJgAAAAAAAAAAAAAAirW/lWy/y8bGRl27dtXatWsLYjgAAAAAAAAAAAAAAIq1Akm2AwAAAAAAAAAAAADwb0KyHQAAAAAAAAAAAAAAC5FsBwAAAAAAAAAAAADAQiTbAQAAAAAAAAAAAACwEMl2AAAAAAAAAP8q0dHR8vPzk6Ojoxo0aKC4uLg8227btk3NmjVT2bJl5eTkpICAAH3wwQd5tl+2bJmsrKzUtWtXk/Lbt29r4sSJ8vPzk5OTk6pUqaKIiAjl5OQY2xgMBoWHh8vb21tOTk5asGCBUlNTTcZZsGCBpkyZYvJasWLFg90IAAAA/C22RR0AAAAAAAAAAPxTli9frpEjRyo6OlrNmjXT7Nmz1aFDBx09elSVK1c2a+/i4qLhw4erTp06cnFx0bZt2zRkyBC5uLho8ODBJm1Pnz6t1157TU888YTZOO+8844+++wzLVy4UIGBgdq7d6/69+8vNzc3jRgxQpI0Y8YMzZw5UwsWLFD16tXVo0cPffnllxo+fLgcHByMY9WvX1+tWrUyvre15de8AAAARYGV7QAAAAAAAAD+NWbOnKkBAwZo4MCBqlmzpqKiolSpUiXNmjUr1/bBwcHq2bOnAgMD5evrqxdeeEHt27c3Ww2fnZ2tXr16acqUKapSpYrZODt37lSXLl3UqVMn+fr66tlnn1VISIj27t0r6c6q9qioKE2YMEHPPPOMgoKC1LVrV2VlZenQoUMmY9nZ2alEiRLGl6OjYwHdHQAAAFiCZDsAAAAAAACAf4XMzEzt27dPISEhJuUhISHasWNHvsY4cOCAduzYoRYtWpiUR0REqHz58howYECu/Zo3b66NGzfq+PHjkqSDBw9q27Zt6tixoyTp5MmTSklJMYnN1tZWvr6+OnPmjMlYhw4d0owZMxQdHa2YmBhlZGTkK3YAAAAULPYXAgAAAAAAAPCvcOHCBWVnZ8vDw8Ok3MPDQykpKffsW7FiRZ0/f163b99WeHi4Bg4caKzbvn275s2bp/j4+Dz7v/HGG0pLS1NAQIBsbGyUnZ2tqVOnqmfPnpJknP+vsbm4uCgtLc34vnbt2ipdurRKlCih1NRUbdy4UefOnVPv3r3zdQ8AAABQcEi2AwAAAAAAAPhXsbKyMnlvMBjMyv4qLi5O165d065duzR27FhVq1ZNPXv2VHp6ul544QXNnTtX5cqVy7P/8uXLtXjxYn311VcKDAxUfHy8Ro4cKW9vb/Xt2/eesf1ZgwYNjH92d3dXmTJlNHfuXCUnJ8vLy+u+1w4AAICCQ7IdAAAAAAAAwL9CuXLlZGNjY7aKPTU11WxF+V/5+flJurOy/Ny5cwoPD1fPnj2VmJioU6dOqXPnzsa2OTk5ku5sA3/s2DFVrVpVr7/+usaOHasePXoYxzl9+rQiIyPVt29feXp6Srqzwv3PSfMbN26oRIkSecbl5eUla2trXbx4kWQ7AADAP4wz2wEAAAAAAAD8K9jb26tBgwaKjY01KY+NjVXTpk3zPY7BYDCekx4QEKBDhw4pPj7e+Hr66afVqlUrxcfHq1KlSpLuJM2trU1/HWtjY2NMzPv5+cnT09MktuzsbJ06dUoVK1bMM5bz588rJydHrq6u+Y4fAAAABYOV7QAAAAAAAAD+NUaPHq3evXurYcOGatKkiebMmaOkpCQNHTpUkjRu3DidPXtWixYtkiR9+umnqly5sgICAiRJ27Zt03vvvaeXX35ZkuTo6KigoCCTOUqVKiVJJuWdO3fW1KlTVblyZQUGBurAgQOaOXOmXnzxRUl3to8fOXKkpk2bJn9/f/n7+2vNmjWys7NT7dq1JUmXLl3SoUOH5O/vL2dnZ50/f14xMTHy9PQ0JvUBAADwzyHZDgAAAAAAAOBfo3v37rp48aIiIiKUnJysoKAgrVu3Tj4+PpKk5ORkJSUlGdvn5ORo3LhxOnnypGxtbVW1alVNnz5dQ4YMsWjejz/+WJMmTVJYWJhSU1Pl7e2tIUOG6M033zS2GTNmjG7evKmwsDBdvnxZnp6e6t27txwcHCTdWQl/8uRJ7d69W5mZmSpZsqT8/f3VsmVLs1XzAAAAKHwk2wEAAAAAAAD8q4SFhSksLCzXugULFpi8f/nll42r2PPrr2NIkqurq6KiohQVFZVnPysrK4WHhys8PFySjP97l5ubm/r162dRLAAAACg8PO4IAAAAAAAAAAAAAICFSLYDAAAAAAAAAAAAAGAhku0AAAAAAAAAAAAAAFiIZDsAAAAAAAAAAAAAABYi2Q4AAAAAAAAAAAAAgIVsizoAAAAAAAAAAChuzoyNK+oQJMeiDgAAAAD3wsp2AAAAAAAAAAAAAAAsRLIdAAAAAAAAAAAAAAALkWwHAAAAAAAAAAAAAMBCJNsBAAAAAAAAAAAAALAQyXYAAAAAAAAAAAAAACxEsh0AAAAAAAAAAAAAAAuRbAcAAAAAAAAAAAAAwEIk2wEAAAAAAAAAAAAAsBDJdgAAAAAAAAAAAAAALESyHQAAAAAAAAAAAAAAC5FsBwAAAAAAAAAAAADAQiTbAQAAAAAAAAAAAACwEMl2AAAAAAAAAAAAAAAsRLIdAAAAAPD/sXfn8TXd+R/H38mNLLIREokiiZ3E1lCxFS3R0FRV7SWKthrtWIZB0UlNLe2g0UUstVZFaqpVpVSpbSytVFottVQjFYkQZCXr/f3h57bXTXCJZtTr+XjcxyPnez7n+/3eo3Mnue/zPQcAAAAAAABWImwHAAAAAAClZvl3n6j1/N6qPauTui4bpv2/fV9i7Tenf1CPlRFqNPdxOTk5qX79+nrrrbfMan766Sf17NlTfn5+srGxUVRUlEU/M2bMUIsWLeTq6iovLy89+eSTOnr0qFlNZGSk6tevL2dnZ1WsWFGdOnXS/v37TfsTEhLU6dGTxb527Mgy1Q3on2ixf9GitNs8WwAAAACAexlhO4BSMW/ePPn7+8vR0VFBQUHatWtXibW7d+9WmzZtVKlSpTv6Qm3nzp0KCwtT1apVZWNjo08//dSiZvU332vsRxvMXm9/9d87fbsAAAAAivHZka16bes7ernVIH0x+H09VK2xBq35h5IyzhZbX76co8IffEr/6f+Ojhw5osmTJ2vy5MlauHChqSYnJ0c1a9bUzJkz5e3tXWw/O3bs0IgRI7Rv3z5t2bJFBQUFCgkJUXZ2tqmmbt26evfdd3Xo0CHt3r1bfn5+CgkJ0blz5yRJ1atX10drapi9wsMrytHRRg89VN5svMGDK5rVPfNMxTs9dQAAAACAe5BdWU8AwL0vNjZWo0aN0rx589SmTRstWLBAoaGhOnz4sGrUqGFR7+zsrJdeekmNGzeWs7Ozdu/erRdeeEHOzs56/vnnJf3+hVqvXr00evToYsfNzs5WkyZN9Oyzz6pnz54lzq+et6f6tGhs2raz5TojAAAA4G5Y9O1H6tO4m/o1eVySFNnpb9rx6zf64OCnmtD+BYv6wCp1FVilriSpmp+f/Pz8tHbtWu3atcv0t0GLFi3UokULSdKECROKHXfTpk1m20uXLpWXl5fi4uL08MMPS5L69+9vVjNnzhwtXrxYP/zwgx599FEZDAZ5eJh/TbL7v9nq0MFFTk7mf0M4lbe1qAUAAAAA3H/4yxDAHZszZ46GDh2qYcOGSZKioqK0efNmRUdHa8aMGRb1zZo1U7NmzUzbt/uFWmhoqEJDQ286PztbW7k5OVr9vgAAAADcurzCfB1KOaaI4AFm7Q/7t9CBpB9vqY+DBw9qz549ev311+9oLunp6ZIkDw+P4ueal6eFCxfK3d1dTZo0Kbbm2LFc/XIiT3/7W2WLfbGrL2nlBxfl5WWnh9s7q3fvCipXzuaO5gwAAAAAuPcQtgO4I3l5eYqLi7MIxENCQrRnz55b6qO0vlAryS/n0vTPdVvkVK6canl66LFG9eTq6HBXxgIAAADuVxdy0lVoLJRnefNbqld29tC57As3PLbFez114a10FRQUKDIy0nQh7+0wGo0aM2aM2rZtq8DAQLN9n3/+ufr27aucnBz5+Phoy5YtqlzZMkyXpC++yFSNGuUUEGB+4W6Pp9xUp46DXF1s9fPPuVq8+IJSkgv097Getz1nAAAAAMC9ibAdwB05f/68CgsLVaVKFbP2KlWqKCUl5YbHVqtWTefOnSuVL9RKUt/bU42reauic3ldyM7R5h+Paf72fRrdua3sDIZSHw8AAAC439nYmK/wNhqNstGNV31/POAduY5oqH379mnChAmqXbu2+vXrd1vjv/TSS/rhhx+0e/dui30dO3ZUfHy8zp8/r0WLFql3797av3+/vLy8zOpyc4u0bWuWnnmmgkUfTz/9e1vNWg5ycbXV1NdSNew5D7m78zcGAAAAANxPCNsBlIpiv1CzufEXart27VJWVlapfKFWkqY1qpp+9nF3VfWK7pq2YZuOJKeqUTWfUh0LAAAAuJ95lHeXwcag1OtWsaflXFRl54olHHVVjQpVVa1RIzVq1Ehnz55VZGTkbf1t8PLLL+uzzz7Tzp07Va1aNYv9zs7Oql27tmrXrq3g4GDVqVNHixcv1sSJE83qdu7MVm5ukTqHuN50zIYNrq58P3Mmn7AdAAAAAO4zhO0A7kjlypVlMBgsVrGnpqZarHa/nr+/vyTd8Rdq1nBzclTF8k46l5lzV8cBAAAA7jf2hnJq5F1XuxIOKLTuw6b2XQkHFFKn7S33YzQalZuba9XYRqNRL7/8sj755BNt377d9LfG7Y71xReZatXKWRUq3Dw8P3Hi6vEeHnzFAgAAAAD3G/4SBHBH7O3tFRQUpC1btqhHjx6m9i1btqh79+633M/tfKF2O7Jz83Qp54rcnHhmOwAAAFDanmvRW6M+n6bG3vUUVDVAH36/XkkZqXqm6dW/DWbuWKCUzPOKenySJGnZd2v1gFsV1faoocvHj2v37t2aNWuWXn75ZVOfeXl5Onz4sOnnpKQkxcfHy8XFRbVr15YkjRgxQqtWrdK6devk6upquhjY3d1dTk5Oys7O1rRp0/TEE0/Ix8dHaWlpmjdvnk6fPq1evXqZvYekpHwd+uGKpk33tnh/h3+6osNHrqhpUyc5O9vq6NFcRc9LU6vW5VWlCl+xAAAAAMD9hr8EAdyxMWPGaODAgWrevLlatWqlhQsXKjExUcOHD5ckTZw4UUlJSVqxYoUk6b333lONGjVUv359SbrtL9SysrJ04sQJ0zG//vqr4uPj5eHhoRo1aigrK0vr4w+rUTUfuTk56EL2ZX1x6Gc5O9gr8AHLL84AAAAA3JknGjyqi5czNPe/y5WanaZ6lf21vNcbquZ+9ffvs1lpSso4a6o3Go2auWOhfktPVrlYe9WqVUszZ87UCy+8YKo5c+aMmjVrZtqeNWuWZs2apfbt22v79u2SpOjoaElShw4dzOazdOlSDR48WAaDQT///LOWL1+u8+fPq1KlSmrRooV27dqlgIAAs2M2fZGpypUNat7cyeL9lStno+3bs/XBikvKzzeqShU7de3mqj59KtzJaQMAAAAA3KMI2wHcsT59+igtLU1Tp05VcnKyAgMDtXHjRvn6+kqSkpOTlZiYaKovKirSxIkT9euvv8rOzu62v1A7cOCAOnbsaKoZM2aMJCk8PFzLli2TwWBQcnqmDpxK0pX8fLk6Oqq2VyUNbPWgHMvx8QcAAADcDeEP9lD4gz2K3fdWt1fMtp8N6qlng3pKkqrNbFfsMX5+fjIajTcc82b7HR0dtXbt2hvWXDN0mIeGDvModl+dug56990HbqkfAAAAAMBfH2kTgFIRERGhiIiIYvctW7bMbPvll182W8VenFv5Qq1Dhw43rHFyctLz7VvesA8AAAAAAAAAAADgdtiW9QQAAAAAAAAAAAAAALjXELYDAAAAAAAAAAAAAGAlwnYAAAAAAAAAAAAAAKxE2A4AAAAAAAAAAAAAgJUI2wEAAAAAAAAAAAAAsJJdWU8AAAAAAAAgMjKyrKegdg+X9QzwZ5v3bZ7+vSdXyZlGBXjZKqqLo9r5Fv912dq1axUdHa34+Hjl5uYqICBAkZGR6tKli6kmPz9fM2bM0PLly5WUlKR69erpjTfe0GOPPWaqiY6OVnR0tBISEiRJAQEBevXVVxUaGlrsuC+88IIWLlyot956S6NGjSq19w4AAADgzhG2A7hr3hu+raynAAAAAABAsWJ/zNeoTVc0r5uj2lQ3aEFcvkI/zNHhES6qUUz9zp071blzZ02fPl0VKlTQ0qVLFRYWpv3796tZs2aSpMmTJ2vlypVatGiR6tevr82bN6tHjx7as2ePqaZatWqaOXOmateuLUlavny5unfvroMHDyogIMBszE8//VT79+9X1apV7+q5AAAAAHB7uI08AAAAAAAA7jtz9uVqaLNyGvagvRp4GhT1mKOqu9sq+tu8YuujoqL0j3/8Qy1atFCdOnU0ffp01alTR+vXrzfVfPDBB3rllVfUtWtX1axZUy+++KK6dOmi2bNnm2rCwsLUtWtX1a1bV3Xr1tW0adPk4uKiffv2mY2XlJSkl156SR9++KHKlSt3d04CAAAAgDtC2A4AAAAAAID7Sl6hUXFnihRSy/ymjyE17bTndOEt9VFUVKTMzEx5eHiY2nJzc+Xo6GhW5+TkpN27dxfbR2FhoVavXq3s7Gy1atXKrO+BAwdq3LhxFqvdAQAAAPzv4DbyAAAAAAAAuK+czzGq0ChVcbExa6/iYqOUX4y31Mfs2bOVnZ2t3r17m9q6dOmiOXPm6OGHH1atWrW0detWrVu3ToWF5gH+oUOH1KpVK125ckUuLi765JNP1LBhQ9P+N954Q3Z2dvrb3/52B+8SAAAAwN3GynYAAAAAAADcl2yu2zYaLduKExMTo8jISMXGxsrLy8vUPnfuXNWpU0f169eXvb29XnrpJT377LMyGAxmx9erV0/x8fHat2+fXnzxRYWHh+vw4cOSpLi4OM2dO1fLli2Tjc2tzAYAAABAWSFsBwAAAAAAwH2lcnkbGWyklCzzVeyp2UaL1e7Xi42N1dChQ/XRRx+pU6dOZvs8PT316aefKjs7W6dOndLPP/8sFxcX+fv7m9XZ29urdu3aat68uWbMmKEmTZpo7ty5kqRdu3YpNTVVNWrUkJ2dnezs7HTq1Cn9/e9/l5+f352/eQAAAAClhrAdAAAAAAAA9xV7g42Cqtpqy8kCs/YtJwvUupqhhKOurmgfPHiwVq1apW7dupVY5+joqAceeEAFBQX6+OOP1b179xvOx2g0Kjc3V5I0cOBA/fDDD4qPjze9qlatqnHjxmnz5s1WvEsAAIA7N2/ePPn7+8vR0VFBQUHatWtXibVr165V586d5enpKTc3N7Vq1arY31+ioqJUr149OTk5qXr16ho9erSuXLli2h8ZGSkbGxuzl7e3t1kfgwcPtqgJDg4uvTcO3CKe2Q4AAAAAAID7zphgBw385LKaVzWoVTWDFsblKzG9SMOb20uSJk6cqKSkJK1YsULS1aB90KBBmjt3roKDg5WSkiJJcnJykru7uyRp//79SkpKUtOmTZWUlKTIyEgVFRXpH//4h2ncV155RaGhoapevboyMzO1evVqbd++XZs2bZIkVapUSZUqVTKba7ly5eTt7a169erd9fMCAABwTWxsrEaNGqV58+apTZs2WrBggUJDQ3X48GHVqFHDon7nzp3q3Lmzpk+frgoVKmjp0qUKCwvT/v371axZM0nShx9+qAkTJmjJkiVq3bq1jh07psGDB0uS3nrrLVNfAQEB+uqrr0zb1z+WR5Iee+wxLV261LRtb29fWm8duGWE7QAAAAAAALjv9Aksp7TLRk3dkavkLKMCvWy1cUB5+Va4eiPI5ORkJSYmmuoXLFiggoICjRgxQiNGjDC1h4eHa9myZZKkK1euaPLkyTp58qRcXFzUtWtXffDBB6pQoYKp/uzZsxo4cKCSk5Pl7u6uxo0ba9OmTercufOf8r4BAABu1Zw5czR06FANGzZM0tUV6Zs3b1Z0dLRmzJhhUR8VFWW2PX36dK1bt07r1683he179+5VmzZt1L9/f0mSn5+f+vXrp2+++cbsWDs7O4vV7NdzcHC4aQ1wtxG2AwAAAAAA4L4U0cJeES2KXwF1LUC/Zvv27Tftr3379jp8+PANaxYvXnyr0zNJSEiw+hgAAIA7kZeXp7i4OE2YMMGsPSQkRHv27LmlPoqKipSZmSkPDw9TW9u2bbVy5Up98803euihh3Ty5Elt3LhR4eHhZsceP35cVatWlYODg1q2bKnp06erZs2aZjXbt2+Xl5eXKlSooPbt22vatGny8vK6zXcM3B7CdgAAAAAAAAAAAAAm58+fV2FhoapUqWLWXqVKFdPjdG5m9uzZys7OVu/evU1tffv21blz59S2bVsZjUYVFBToxRdfNAv1W7ZsqRUrVqhu3bo6e/asXn/9dbVu3Vo//fST6XE7oaGh6tWrl3x9ffXrr79qypQpeuSRRxQXFycHB4dSOAPArSFsBwAAAAAAAAAAAGDBxsbGbNtoNFq0FScmJkaRkZFat26d2Wrz7du3a9q0aZo3b55atmypEydOaOTIkfLx8dGUKVMkXQ3Sr2nUqJFatWqlWrVqafny5RozZowkqU+fPqaawMBANW/eXL6+vtqwYYOeeuqpO3rPgDUI2wEAAAAAAAAAAACYVK5cWQaDwWIVe2pqqsVq9+vFxsZq6NChWrNmjTp16mS2b8qUKRo4cKDpOfCNGjVSdna2nn/+eU2aNEm2trYW/Tk7O6tRo0Y6fvx4iWP6+PjI19f3hjXA3WD5XywAAAAAAAAAAACA+5a9vb2CgoK0ZcsWs/YtW7aodevWJR4XExOjwYMHa9WqVerWrZvF/pycHItA3WAwyGg0ymg0Fttnbm6ujhw5Ih8fnxLHTUtL02+//XbDGuBuYGU7AAAAAAAAAAAAADNjxozRwIED1bx5c7Vq1UoLFy5UYmKihg8fLkmaOHGikpKStGLFCklXg/ZBgwZp7ty5Cg4ONq2Kd3Jykru7uyQpLCxMc+bMUbNmzUy3kZ8yZYqeeOIJGQwGSdLYsWMVFhamGjVqKDU1Va+//royMjIUHh4uScrKylJkZKR69uwpHx8fJSQk6JVXXlHlypXVo0ePP/s04T5H2A4AAAAAAAD8QaPljcp0/EPhh8p0fAAAAOnqc9HT0tI0depUJScnKzAwUBs3bpSvr68kKTk5WYmJiab6BQsWqKCgQCNGjNCIESNM7eHh4Vq2bJkkafLkybKxsdHkyZOVlJQkT09PhYWFadq0aab606dPq1+/fjp//rw8PT0VHBysffv2mcY1GAw6dOiQVqxYoUuXLsnHx0cdO3ZUbGysXF1d/4QzA/yOsB0AAAAAAAAAAACAhYiICEVERBS771qAfs327dtv2p+dnZ3++c9/6p///GeJNatXr75hH05OTtq8efNNxwL+DITtAAAAAAAApWjdunSt+ShdaWmF8vMrp4iISmrU2KnY2l27srX+swz98kuu8vONaty4lSIjI9WlSxdTTYcOHbRjxw6LY7t27aoNGzZIkmbMmKG1a9fq559/lpOTk1q3bq033nhD9erVM9Xb2NgUO4c333xT48aN04ULF3Rhy3xdTjiowozzsnVyU/m6warQ7hnZOjib6k9HD1FhRqpZH24tn1bFDoNv+RwBAAAAwF8BYTsAAAAAAEAp+frrLEXPS9Pf/lZZAYGO2vB5hiZOTNHiJdVVpYrl1zCHfrisoCAnDRlaUS4uBh39uaPCwsK0f/9+NWvWTJK0du1a5eXlmY5JS0tTkyZN1KtXL1Pbjh07NGLECLVo0UIFBQWaNGmSQkJCdPjwYTk7Xw3Kk5OTzcb+4osvNHToUPXs2VOSdObMGRVmXVDFjkNUrlINFWSk6sLm91SYmSbPHq+YHevedoBcmzxm2raxd7zDMwcAAAAA9x7CdgAAAAAAgFLy8X/S9Vioq7p2c5MkRYyorAMHLmv9+gwNG+ZhUR8xorLZdvig6Vq3bp3Wr19vCts9PMyPW716tcqXL28Wtm/atMmsZunSpfLy8lJcXJwefvhhSZK3t7dZzbp169SxY0fVrFlTkhQYGGgWqper6KMKDw/S+c9nyVhUKBtbg2mfrX15GVwq3tpJAQAAAIC/KMJ2AAAAAACAUpCfb9SxY7nq26+CWXtQkJMO/3TllvooKipSZmamRcD+R4sXL1bfvn1NK9aLk56eLskyqL/m7Nmz2rBhg5YvX37j+eRmy9a+vFnQLkkZ+/+j9D2rZXCrLOd6beXW8inZGMrdsC8AAAAA+KshbAcAAAAAACgF6emFKiqSKlY0D6YrVjTowoXCW+pj9uzZys7OVu/evYvd/8033+jHH3/U4sWLS+zDaDRqzJgxatu2rQIDA4utWb58uVxdXfXUU0+V2E/h5Qyl71ktl6ahZu1uzZ+QfZVasnV0UW7yMV3asVwF6WdVKfRvt/AOAQAAAOCvg7AdAAAAAACgFNlct22UZHN9YzG2bctS1FuRWrdunby8vIqtWbx4sQIDA/XQQw+V2M9LL72kH374Qbt37y6xZsmSJRowYIAcHYt/1npRbo5S17ymcpVqqEKbfmb73Fo8afrZ3stfto4uOv/pDFXoMFgGJ7cbvEMAAAD8FXh/HV/WU1BKx6ZlPQVAkmRb1hMAAAAAAAD4K3B3N8jWVrpw0XwV+6WLhRar3a/39ddZmj3rnD766CN16tSp2JqcnBytXr1aw4YNK7Gfl19+WZ999pm+/vprVatWrdiaXbt26ejRoyX2U5Sbo9SPXpWtvaO8npokG8ON12o4VK0nSSq4mHzDOgAAAAD4qyFsBwAAAAAAKAXlytmobl0HxcVdNmuPi7ushgHFryCXrq5o//eb5/TKK17q1q1biXUfffSRcnNz9cwzz1jsMxqNeumll7R27Vpt27ZN/v7+JfazePFiBQUFqUmTJhb7inJzdPajKZLBTp49p8jGzr7Efq7JO3tSkmRwqXjTWgAAAAD4K+E28gAAAAAAAKWk59PuemNmqurWtVfDho7asCFDqakFCgtzlSS9//4FnT9foAkTrt4mftu2LL0xM1URIyqpQUMHpaSkSJKcnJzk7u5u1vfixYv15JNPqlKlShbjjhgxQqtWrdK6devk6upq6sfd3V1OTk6muoyMDK1Zs0azZ8+26CMzM1NnY6fIWJCryo+PlTH3sgpzr144YFveTTa2BuUmHVHumaNyrNFYNg7llZd8XBe3LZJT7Zaycyv+1vcAAAAA8FdF2A4AAAAAAFBKOnZ0UUZGoVZ+cEkXLhTIz89e02d4q0qVcpKkC2kFSk0tMNV//nmGCguld95O0ztvp0nykSSFh4dr2bJlprpjx45p9+7d+vLLL4sdNzo6WpLUoUMHs/alS5dq8ODBpu3Vq1fLaDSqXz/z57BLUlxcnPKSj0qSzix8zmzfA8MXy869imQop+wju3TpvzFSYb4Mbl5yadJFbi173tL5AQAAAIC/EsJ2AAAAAACAUtS9u7u6d3cvdt8/xpuv/p4zp6rZ9qOP/FLscXXr1pXRaCxxzBvt+6Pnn39ezz//fLH7OnToIN/xn9/weAfv2vIZZLkqHgAAAADuRzyzHQAAAAAAAAAAAAAAK7GyHQAAAAAAALgL0ram6fwX51VwqUAODzjIp7+PnOs5F1ubfiBdF76+oCuJV+T2spsCAgIUGRmpLl26mGqWLVumZ5991uLYy5cvy9HRUZJUUFCgyMhIffjhh0pJSZGPj48GDx6syZMny9b26robGxubYufw5ptvaty4cUpISNBLCx4ttmZIp1f1YK32kqRXP+yvC1lnzfZ3btpX3Vs+V9yhAAAAwF8OK9sBAAAAAACAUpa+P10pq1LkGeapWlNrybmus07NOaW8tLxi63OO5sglwEW+o30VFxenjh07KiwsTAcPHjSrc3NzU3JystnrWtAuSW+88Ybmz5+vd999V0eOHNGbb76pf//733rnnXdMNdcfv2TJEtnY2Khnz56SpOrVq2v6wDVmr27Nw2Vv56iAGg+Zzadb88FmdY89+ExpnUIAAO478+bNk7+/vxwdHRUUFKRdu3aVWLt27Vp17txZnp6ecnNzU6tWrbR582azmmXLlsnGxsbideXKFVNNQUGBJk+eLH9/fzk5OalmzZqaOnWqioqKTDWDBw82O/7sI810YcQgs7FyPv9YF0YPU+rjbXX2kWYqysospbMC/G9jZTsAAAAAAABQys5vPq+KD1eUR3sPSZLPAB9l/ZilC9suyLuXt0W9zwAf08916tTR9OnTtW7dOq1fv17NmjUz7bOxsZG3t+Xx1+zdu1fdu3dXt27dJEl+fn6KiYnRgQMHTDXXH79u3Tp17NhRNWvWlCQZDAa5lfcwq/n+1/8qqFYHOZRzMmt3LFfeohYAAFgvNjZWo0aN0rx589SmTRstWLBAoaGhOnz4sGrUqGFRv3PnTnXu3FnTp09XhQoVtHTpUoWFhWn//v1mvzu4ubnp6NGjZscWd6He8uXLFRAQoAMHDujZZ5+Vu7u7Ro4caap77LHHtHTpUklSo//+KBu7cmZ9Gq9ckUOL1nJo0VpZ778j4H5B2A4AAAAAAACUoqKCIl1OuCzPbp5m7S6BLso5kXNrfRQVKTMzUx4e5kF2VlaWfH19VVhYqKZNm+pf//qX2Rfqbdu21fz583Xs2DHVrVtX33//vXbv3q2oqKhixzl79qw2bNig5cuXlziXxHPHdDrthHq3/ZvFvi3fr9YX332gii5ealazvTo16S07Q7liegEAADcyZ84cDR06VMOGDZMkRUVFafPmzYqOjtaMGTMs6q////a7eaGeJDk4OJj6MXikWPTj/PQASVJe/AGLfcBfGWE7AAAAAAClZOdP67T1+4+UnpMmn4p+6tk6QrV9GhdbG39yl3Yd/kxJab9o4qqi234+8x/NmDFDr7zyikaOHGn25dutPJ/Z/42Hi62J7v6aHq/fUZLUKrq3TmeYf7EW0bK/JnYYXuyxwP2qMLNQKpLs3My/ejO4GVSQXnBLfcyePVvZ2dnq3bu3qa1+/fpatmyZGjVqpIyMDM2dO1dt2rTR999/rzp16kiSxo8fr/T0dNWvX18Gg0GFhYWaNm2a+vXrV+w4y5cvl6urq5566qkS57L35y/kXaGGanoHmLV3aPSUqleuo/IOLjqV+rM++2ax0jKTNaD92Ft6jwAA4Kq8vDzFxcVpwoQJZu0hISHas2fPLfVxty/U2759u7y8vFShQgVl1AmUy9CXZFuRu9sAhO0AAAAAAJSCuBNf6+M989Sn7d9U0ztQuw9/rnkbJ2py7yXycK1iUX8i+QfVrxakJx4aqufe6Hzbt3285ttvv9XChQvVuLFluJ+cnGy2/cUXX2jo0KFmz2eOG/GJWc2q79cren+MOtZsadb+97ZD1b/J46ZtZ3vzW0rjznh/HV+m41v+l4U7cv11LsZbOywmJkaRkZFat26dvLy8TO3BwcEKDg42bbdp00YPPvig3nnnHb399tuSrt6CduXKlVq1apUCAgIUHx+vUaNGqWrVqgoPD7cYa8mSJRowYECxnyuSlFeQqwMnthb7LPZHGj9t+vmBSrXk5OCqxVteU/eWz8nF0V3S7V+EVFCYr5XfN76ti5Cio6MVHR2thIQESVJAQIBeffVVhYaGSpLy8/M1efJkbdy4USdPnpS7u7s6deqkmTNnqmrVqpKkCxcuaMqWKO1M+FZnMlLl4eSuLnXbaWy7oXJzcDGNy0VIAIDScP78eRUWFqpKFfO/G6pUqaKUFMtV5MW5mxfqhYaGqlevXvL19dWvv/6qp0aP1YW/P69K81fJxt6+FM4AcO8ibAcAAAAAoBRsO/QftaofqtYNrt5+8ek2I3Tk9AHtOrxe3VsOs6h/us0I08938nxm6epqlQEDBmjRokV6/fXXLfbfyvOZvVwqmdVsOrZLYfU7ytm+vFm7i72TRS0AcwZXg2Qri1XshZmFsnO/8ddx6fvTNXTZUK1Zs0adOnW6Ya2tra1atGih48ePm9rGjRunCRMmqG/fvpKkRo0a6dSpU5oxY4ZF2L5r1y4dPXpUsbGxJY4Rf3Kn8gpy9VDdkBvORZL8qzSQJJ1PPyMXR/c7ugjJycFF+f4/39ZFSNWqVdPMmTNVu3ZtSVdX73fv3l0HDx5UQECAcnJy9N1332nKlClq0qSJLl68qFGjRumJJ54w3TL3zJkzOpt1XpM7RqhOJT8lZaRo4ubZOpt5Xgt6/MtsbC5CAgCUluvvSGU0Gku8S9Uf3e0L9fr06WPqIzAwUBVyHXW+X1fl7tslx4cfvaP3DNzrCNsBAAAAALhDBYX5+u3cMYU0Nb9Nc4NqQfr17E+31Mft3vZRkkaMGKFu3bqpU6dOxYbtf3Qrz2f+IeWofko9rtc7j7LYF71/lebuWaGqbl7qVq+DhrfsJ3uezwyYsbWzlZOfk7J+ypJbkJupPeunLLk2cy3xuEv7LilpcZLWfrTW9NzUGzEajYqPj1ejRo1MbTk5ObK1tTWrMxgMKioqsjh+8eLFCgoKUpMmTUocY8/PX6iRbyu5OlW46Xx+O39CkuRW/urn2J1chCRJI6YPuq2LkMLCwsy2p02bpujoaO3bt08BAQFyd3fXli1bzGreeecdPfTQQ0pMTFSNGjUUGBiohT1+/zz1q/iA/vHwcxr5+esqKCqQne3vX6tyERIA4E5VrlxZBoPBYhV7amqqxWr368XGxmro0D/nQr1rDJU8Zajio8KkxFt5e8BfGmE7AAAAAAB3KOtKuoqMRXJ1qmjW7upUURk5F26pj9u97ePq1av13Xff6dtvv72lcW7l+cyrf9igOpV81bxaI7P2Ic2fVmCVuqrg6Kr45COauWOBfktP1r9Dx9/S2MD9pHKXyjq98LSc/JzkVNtJF7dfVH5avjw6Xg2iU9akqOBigao9X03S1aD99KLT8unvo+DgYNOX7U5OTnJ3v3pL9tdee03BwcGqU6eOMjIy9Pbbbys+Pl7vvfeeadywsDBNmzZNNWrUUEBAgA4ePKg5c+ZoyJAhZvPLyMjQmjVrNHv27BLfw7n0JP2S/INeDJ1use9kyk9KSD2iulWbytHeWYnnjurjPfPUyLe1PFyrlPlFSNcUFhZqzZo1ys7OVqtWrUocKz09XTY2NqpQoUKJNZm52XKxL28WtEtchAQAuHP29vYKCgrSli1b1KNHD1P7li1b1L179xKPi4mJ0ZAhQxQTE/OnXKh3TVH6JRWmnpVtpco3HRP4q7O9eQkAAAAAALgdRlneCrI41277GBsba3Hbx2eeeUZNmjRRu3bt9NFHH6lu3bp65513JEm//fabRo4cqZUrV5b4vOXr3ez5zJfzc7Xu8Ffq09jyy7rnWvRWqxpN1cCrlvo1eVwzuvxdq3/YoIuX029pbOB+4t7SXd79vZW6LlW/vPqLso9my3eMr+wrX32uacGlAuWl5ZnqL3x9QSqUkj9Ilo+Pj+k1cuRIU82lS5f0/PPPq0GDBgoJCVFSUpJ27typhx56yFTzzjvv6Omnn1ZERIQaNGigsWPH6oUXXtC//mV+6/PVq1fLaDSaPY/1ent//kLuzpVVv3pzi33lDOX03S/bNXf9GE37aIg2HFim1g266dlHJ0m6+xchffbZZ4qJiZGjo6PatGljtkJPkg4dOiQXFxc5ODho+PDh+uSTT9SwYcNix7ly5YomTJig/v37y83Nrdiai5fTNXfPcg1o+oRZ+5DmT+vdJyL1Ub+5GvzgU1p8YI0mfTnnlt4fAAB/NGbMGL3//vtasmSJjhw5otGjRysxMVHDhw+XJE2cOFGDBg0y1cfExGjQoEGaPXu26UK9lJQUpaf//rv5a6+9ps2bN+vkyZOKj4/X0KFDFR8fb+pT+v1CvQ0bNighIUGffPKJ5syZYwr9s7KyNHbsWO3du1cJCQnavn27Lk0aKVv3CnJo+4ipn8IL55V/4qgK/n+1e8HJ48o/cVRFGdb/rZCz7iOd699NZ7u0VNoL/ZX3w3cl1q5du1adO3eWp6en3Nzc1KpVK23evLnE+tWrV8vGxkZPPvmkWXtBQYEmT54sf39/OTk5qWbNmpo6darZRQdZWVl66aWXVK1aNTk5OalBgwaKjo426+eXX35Rjx49TPPp3bu3zp49a/U5wL2Dle0AAAAAANwhF0d32drYKvPyRbP2rMsXLYKm68Wd+FqrV8y+rds+xsXFKTU1VUFBQaaawsJC7dy5U++++65yc3NlMBhM+27l+cwbj27X5fwrejrwsRvORZKaVQ2QJCVcTFJFJ/eb1gP3m0qPVlKlR4u/vXi156qZbdecWNP086HwQ8Ue89Zbb+mtt9664Ziurq6KiopSVFTUDeuef/55Pf/88zeseaLlMD1RzO3eJam6Z12N7fHuDY8vzq1ehHTgxDZ99MFbt/XsWUmqV6+e4uPjdenSJX388ccKDw/Xjh07LAL3/Px89e3bV0VFRZo3b16xc8nMzVb4mvGqU8lPo9s8a7bvuRa/XwjQwKuW3B1d9MKnr+qVDsP5XAQAWKVPnz5KS0vT1KlTlZycrMDAQG3cuFG+vr6SpOTkZCUm/n7b9gULFqigoEAjRozQiBG/P4olPDxcy5Ytk/T7hXopKSlyd3dXs2bNir1Qb8qUKYqIiFBqaqqqVq2qF154Qa+++qqkq6vcDx06pBUrVujSpUvy8fGRoUETub/6hmzLO5v6ufzZf5S9YoFp++KooZIkt3+8JqfHzC9Wu5ErX29W5nv/luvIibIPbKrL6z/WpQkvqdLSj2Wo4mNRv3PnTnXu3FnTp09XhQoVtHTpUoWFhWn//v0Wd745deqUxo4dq3bt2ln088Ybb2j+/Plavny5AgICdODAAT377LNyd3c3Xfw4evRoff3111q5cqX8/Pz05ZdfKiIiQlWrVlX37t2VnZ2tkJAQNWnSRNu2bZMkTZkyRWFhYdq3b5/FHQTw10DYDgAAAADAHbIzlFN1z7r6+XScmvi3NbX/fDpOjfzalHjcgRPb9OH2f+uj/8Te1m0fH330UR06ZB7KPfvss6pfv77Gjx9vFrRLt/Z85tU/bFDn2m1UqXyFm87np7NXQ3+eVQzgend6EdKHO2bpk08/vq1nz0pXb8dbu3ZtSVLz5s317bffau7cuVqw4PcQID8/X71799avv/6qbdu2FbuqPSs3RwM/Gitneycteup1lTPc+OtULkICANyJiIgIRUREFLvvWoB+zfbt22/aX2lcqOfk5GSxUtz763iLOpfBw+UyeLhFu7Wy16yUU+iTKt/t6mOvXF8ap9wDe5Xz2Rq5Pvc3i/rr5z19+nStW7dO69evNwvbCwsLNWDAAL322mvatWuXLl26ZHbc3r171b17d9PfZX5+foqJidGBAwfMasLDw9WhQwdJVy9eXLBggQ4cOKDu3bvrv//9rxISEnTw4EHT7xVLly6Vh4eHtm3bdtPfa3Bv4hIKAAAAAABKwSONntaenzdq789fKOXiKX28Z54uZKWqXcMwSdK6/e9rxbaZpvoDJ7Zpxdcz1aPV8Nu+7aOrq6sCAwPNXs7OzqpUqZICAwPN5nft+czDhhW/SlWSfr14Wvt/+159mzxusS8u6Uct+vYj/XT2uBIvndH6I9s0YfMsda7dRg+4Vbmjcwfgr+ePFyH90c+n4+RfJaDE4w6c2KaV29/U4EdeseoiJB8fy5Vu19fl5uaatq8F7cePH9dXX32lSpUsLxrKzM3WgI/+rnKGclrSc4Yc7RxuOh8uQgIA4PYZ8/NVcOyI7Ju3Mmu3bx6s/J++v6U+ioqKlJmZKQ8PD7P2qVOnytPTU0OHDi32uLZt22rr1q06duyYJOn777/X7t271bVrV7Oazz77TElJSTIajfr666917NgxdenSRZKUm5srGxsbOTj8/juDo6OjbG1ttXv37luaP+49rGwHAAAAAKAUBNXuqOzcDH0R94Eyci7Ix8NPEaEz5OF6NYjOyEnThaxUU/3uw5+rqKhQH+1+Wx/5/H7rY2tv+3irbuX5zLE/bJS3a2W1929hsc/eUE7rj2xT1H+XKbcwT9XcvNW/yeN6sWV/q+cC4P7wSKOnteLrmarhWVf+VRrqv0c2WFyElJ59XoMemSDp94uQnm49Qv5VGiolJUXS1RV17u5XV4m/9tprCg4OVp06dZSRkaG3335b8fHxeu+990zjvvLKKwoNDVX16tWVmZmp1atXa/v27dq0aZOkq89kffrpp/Xdd9/p888/V2FhoWksDw8P2dvbKzMzUwNi/67LBVc09/HJyszNVmZutiSpUvkKMtgaFJf0o747c1itazSTq4Ozvk/+Wa9te5eLkAAAuE1F6RelokLZVjQPyg0VKynvQtot9TF79mxlZ2erd+/fH/Xy3//+V4sXL1Z8fHyJx40fP17p6emqX7++DAaDCgsLNW3aNLO/n95++20999xzqlatmuzs7GRra6v3339fbdtevbtZcHCwnJ2dNX78eE2fPl1Go1Hjx49XUVGRkpOTrTgTuJcQtgMAAAAAUEoeDuiuhwO6F7tvYMfxZtujnphj+nnE/EeKPeZWbvt4vZJuJ3krz2ee0P55TWhffE0j73r6bNB8q+YC4P52Rxch7X5br3xwtd3ai5DOnj2rgQMHKjk5We7u7mrcuLE2bdqkzp07S5JOnz6tzz77TJLUtGlTszl//fXX6tChg+Li4nQw+bAkqd1C84uU9gyPVXV3Hy5CAgDgbrGxMds0Go0WbcWJiYlRZGSk1q1bJy8vL0lSZmamnnnmGS1atEiVK1cu8djY2FitXLlSq1atUkBAgOLj4zVq1ChVrVpV4eHhkq6G7fv27dNnn30mX19f7dy5UxEREfLx8VGnTp3k6empNWvW6MUXX9Tbb78tW1tb9evXTw8++KDFI77w10HYDgAAAAAAAOCuuN2LkKTiL0S6lYuQFi9efMP9fn5+V7+0v4EOHTrot/E7b1jDRUgAAJQuW/eKkq1BRdetYi+6dMFitfv1YmNjNXToUK1Zs8bs2ei//PKLEhISFBYW9nt/RUWSJDs7Ox09elS1atXSuHHjNGHCBPXt21eS1KhRI506dUozZsxQeHi4Ll++rFdeeUWffPKJ6VE3jRs3Vnx8vGbNmmUaMyQkRL/88ovOnz8vOzs7VahQQd7e3vL397/zE4T/SYTtAAAAAAAAAAAAAMqUTblysqvbQHlx++TY7veL7vLi9smhdYcSj4uJidGQIUMUExNjCsKvqV+/vg4dOmTWNnnyZGVmZmru3LmqXr26JCknJ0e2trZmdQaDwRTM5+fnKz8//4Y1f3RtFf22bduUmpqqJ5544ibvHvcqwnYAAAAAAADgf8iR+g3KegpSh/duXgMAAMrE1m21ynYCNh/fta6dez2j9BmTVa5eQ5Vr2FiXP1+rorMpKh/2tCQpc9HbKjqfKnX8XNLVoH3QoEGaO3eugoODlZKSIklycnKSu7u7HB0dFRgYaDZGhQoVJMmsPSwsTNOmTVONGjUUEBCggwcPas6cORoyZIgkyc3NTe3bt9e4cePk5OQkX19f7dixQytWrNCcOb/fnWfp0qVq0KCBPD09tXfvXo0cOVKjR49WvXr17to5Q9myvXkJAAC418ybN0/+/v5ydHRUUFCQdu3aVWLt2rVr1blzZ3l6esrNzU2tWrXS5s2bLWqaN2+uChUqyNnZWU2bNtUHH3xgVlNQUKDJkyfL399fTk5OqlmzpqZOnWp2ZefgwYNlY2Nj9goODi7dNw8AAAAAAADgnuTYsYtcR4xT1oqFSnu+r/J++E4VZrwjg3dVSVLRhfMqTE0x1S9YsEAFBQUaMWKEfHx8TK+RI0daNe4777yjp59+WhEREWrQoIHGjh2rF154Qf/6179MNatXr1aLFi00YMAANWzYUDNnztS0adM0fPhwU83Ro0f15JNPqkGDBpo6daomTZqkWbNm3eFZwf8yVrYDAPAXExsbq1GjRmnevHlq06aNFixYoNDQUB0+fFg1atSwqN+5c6c6d+6s6dOnq0KFClq6dKnCwsK0f/9+NWvWTJLk4eGhSZMmqX79+rK3t9fnn3+uZ599Vl5eXurSpYsk6Y033tD8+fO1fPlyBQQE6MCBA3r22Wfl7u5u9svtY489pqVLl5q27e3t7/IZAQAAAAAAAHCvKN+9t8p3713sPvfxU822t2/fbnX/y5Yts2hzdXVVVFSUoqKiSjzO29vb7HvN4sycOVMzZ860ek64dxG2AwDwFzNnzhwNHTpUw4YNkyRFRUVp8+bNio6O1owZMyzqr/8Fcvr06Vq3bp3Wr19vCts7dOhgVjNy5EgtX75cu3fvNoXte/fuVffu3U3PRfLz81NMTIwOHDhgdqyDg4O8vb1L460CAPCXMbvP42U9BfXxH1/WUwAAAAAA4J7CbeQBAPgLycvLU1xcnEJCQszaQ0JCtGfPnlvqo6ioSJmZmfLw8Ch2v9Fo1NatW3X06FE9/PDDpva2bdtq69atOnbsmCTp+++/1+7du9W1a1ez47dv3y4vLy/VrVtXzz33nFJTU615iwAAAAAAAAAA/E9gZTsAAKUsZ91Hyo5drqK087LzqyXXEWOljk2LrV27dq2io6MVHx+v3NxcBQQEKDIy0rRa/FrN9OnTdeLECeXn56tOnTr6+9//roEDB1r0d/78eRUWFuqpp57SyJEjTavWq1SpopSUFB05ckTjx4/Xjh07VFRUpICAAH300Udmt5efPXu2srOz1bu3+a2a0tPT9cADDyg3N1cGg0Hz5s1T586dTfvHjx+v9PR01a9fXwaDQYWFhZo2bZr69etnqgkNDVWvXr3k6+urX3/9VVOmTNEjjzyiuLg4OTg43M7pBgAAAPAXVdZ3/eCOHwAAALgZwnYAAErRla83K/O9f8t15ETZBzbV5fUf69KEl5TYo9NdfV76NfHx8ZKkWrVqmbUbjUYVFBSobdu2Gjp0qF577TW5u7vryJEjcnR0NNXFxMQoMjJS69atk5eXl1kfrq6uio+PV1ZWlrZu3aoxY8aoZs2aplvMx8bGauXKlVq1apUCAgIUHx+vUaNGqWrVqgoPD5ck9enTx9RfYGCgmjdvLl9fX23YsEFPPfXU7Z10AAAAAAAAAADKAGE7AAClKHvNSjmFPqny3a4Gx64vjVPugb13/XnpkpSVlaWRI0fK1tbWLECXpNTUVGVmZqpr16568803Te01a9Y0/RwbG6uhQ4dqzZo16tSpk8VcbW1tVbt2bUlS06ZNdeTIEc2YMcM0v3HjxmnChAnq27evJKlRo0Y6deqUZsyYYQrbr+fj4yNfX18dP3682P0AAAAAAAAAAPyv4pntAACUEmN+vgqOHZF981Zm7fbNg+/689IlacSIEXr88cfVvHlzXbx40Wzfli1bdOnSJdWtW1ddunSRl5eXWrZsqU8//VTS1RXtgwcP1qpVq9StW7dbe79Go3Jzc03bOTk5srU1/9XCYDCoqKioxD7S0tL022+/ycfH55bGBAAAAAAAAADgfwUr2wEAKCVF6RelokLZVjQPyg0VKynlx7hb6uN2n5e+evVqfffdd/r222+1bt069evXTz/99JOOHDmihQsX6tSpU8rPz9fMmTMVHByshx56SG3bttVTTz2lyZMna8aMGZo7d66Cg4OVkpIiSXJycpK7u7skacaMGWrevLlq1aqlvLw8bdy4UStWrFB0dLRpDmFhYZo2bZpq1KihgIAAHTx4UHPmzNGQIUMkXV15HxkZqZ49e8rHx0cJCQl65ZVXVLlyZfXo0cP6Ew4AAAAAAADgvuQ3YUNZT0EJM29t0RL+2gjbAQAobTY2ZptGo1E217UV53afl/7bb79p5MiR+vLLL+Xo6Kg+ffpo8uTJ2r9/v5o2barAwEB98MEH6tmzp7p37y57e3slJCRowoQJ2rNnj+bPn6+CggKNGDFCI0aMMI0ZHh6uZcuWSZKys7MVERGh06dPy8nJSfXr19fKlSvNnsH+zjvvaMqUKYqIiFBqaqqqVq2qF154Qa+++qqkq6vcDx06pBUrVujSpUvy8fFRx44dFRsbK1dX19s92wAAAAAAAAAAlAnCdgAASomte0XJ1qCiC2lm7UWXLqhKlSo3PPZOnpceFxen1NRUBQUFmeoLCwtlY2MjW1tbffPNNyosLJSdnZ0aNmyoyZMnm+oaNGigtLQ0paam3nB+r7/+ul5//fUb1ri6uioqKsriOfTXODk5afPmzTfsAwAAAAAAAACAewVhOwAApcSmXDnZ1W2gvLh9cmz3iKk9L26fWvfrU+JxMTExGjJkiGJiYm7reemPPvqoDh06ZLb/2WefVf369TV+/HgZDAYZDAa1aNFCR48eNas7duyYfH19b/UtAgAAAAAAAACA/2db1hMAAOCvxLnXM7q88RNd/uJTFZw6qcz3ZqnobIqGDx8uSZo4caIGDRpkqo+JidGgQYM0e/Zs0/PSU1JSlJ6ebqqZMWOGtmzZopMnT+rnn3/WnDlztGLFCj3zzDOSrq4oDwwMNHs5OzurUqVKCgwMNPUzbtw4xcbGatGiRTpx4oTeffddrV+/XhEREX/S2QEAAAAAAAD+d82bN0/+/v5ydHRUUFCQdu3aVWLt2rVr1blzZ3l6esrNzU2tWrWyuKvjokWL1K5dO1WsWFEVK1ZUp06d9M0335jVREZGysbGxuzl7e1tVjN48GCz/Z0ePamXXkoyq/n88wyNGXNGT4T9qk6PnlRWVuEdng0At4KwHQCAUuTYsYtcR4xT1oqFSnu+r/J++E4VZrxjWj2enJysxMREU/2CBQtMz0v38fExvUaOHGmqufa89ICAALVu3Vr/+c9/tHLlSg0bNsyqufXo0UPz58/Xm2++qUaNGun999/Xxx9/rLZt25bOmwcAAAAAAADuUbGxsRo1apQmTZqkgwcPql27dgoNDTX7Lu+Pdu7cqc6dO2vjxo2Ki4tTx44dFRYWpoMHD5pqtm/frn79+unrr7/W3r17VaNGDYWEhCgpyTwoDwgIUHJysul1/V0sJemxxx4z7f9oTQ1Nn24eyOfmGtWiRXn161+xFM4GgFvFbeQBAChl5bv3VvnuvYvdt2zZMrPt7du337S/W3le+vVK6nfIkCEaMmSIVX0BAAAAAAAAf3Vz5szR0KFDTQtcoqKitHnzZkVHR2vGjBkW9VFRUWbb06dP17p167R+/Xo1a9ZMkvThhx+a1SxatEj/+c9/tHXrVrO7X9rZ2VmsZr+eg4ODqcbDwzLe69nTXZIUH3/5Ju8UQGliZTsAAAAAAAAAAADuW3l5eYqLi1NISIhZe0hIiPbs2XNLfRQVFSkzM1MeHh4l1uTk5Cg/P9+i5vjx46patar8/f3Vt29fnTx50uLY7du3y8vLS3Xr1tXs2ed08SK3iQf+FxC2AwAAAAAAAAAA4L51/vx5FRYWqkqVKmbtVapUUUpKyi31MXv2bGVnZ6t37+LveClJEyZM0AMPPKBOnTqZ2lq2bKkVK1Zo8+bNWrRokVJSUtS6dWulpaWZakJDQ/Xhhx9q27Ztmj17to4dzdW4sWeUl2e08p0CKG3cRh4AgPtFpHtZz0CKTC/rGQAAAAAAAADFsrGxMds2Go0WbcWJiYlRZGSk1q1bJy8vr2Jr3nzzTcXExGj79u1ydHQ0tYeGhpp+btSokVq1aqVatWpp+fLlGjNmjCSpT58+pprAwEBdvuKtAf0TtX9/jtq1c7bqPQIoXaxsBwAAAAAAAAAAQJn674kETduwTRP+84Xe2rJLJ89dKLF27dq16ty5szw9PeXm5qZWrVpp8+bNZjU//fSTevbsKT8/P9nY2Fg8Y12Sad8DDzwgSWrdurVsbGw0YsQISVJqaqoqV66sl156SdWqVZOTk5MaNGig6OhoUx/vvvuu+vfvr5ycHHXu3Fk2Njam15o1ayRJs2bN0vTp0/Xll1+qcePGNzwPzs7OatSokY4fP15iTaVKdqpSxU5Jp/Nv2BeAu4+wHQAAAAAAAAAAAGUmPvGMPos/rE4Namt0SFvVrOyh93d9o4vZl4ut37lzpzp37qyNGzcqLi5OHTt2VFhYmA4ePGiqycnJUc2aNTVz5kx5e3sX28+3336r5ORkJScnq1mzZgoLC5Mk9erVS5K0ZcsWZWVladOmTVq5cqWOHDmi0aNH6+WXX9a6desUExOjcePGacmSJaZ+kpOT9dprr8nZ2VmhoaH697//rX/961/atGmTmjdvftNzkZubqyNHjsjHx6fEmvT0QqWmFsqjkuGm/QG4u7iNPAAAfwK/CRvKegpKcLx5DQAAAAAAAPBn23HsVz3kX10ta9aQJHVvFqCjKee095dT6tq4vkX99avUp0+frnXr1mn9+vVq1qyZJKlFixZq0aKFpKvPSi+Op6en6efx48erf//+8vT0lJeXl0aPHq3ExERVrVpV4eHh2rx5s5KSkrRixQotWLBAy5Yt0+eff665c+ea3QreyclJn3zyifr06aN58+ZpypQpWrVqlfz8/EzPf3dxcZGLi4skaezYsQoLC1ONGjWUmpqq119/XRkZGQoPD5ckZWVlKTIyUj179pSPj48SEhI0ZXKK3N1t1bbt77eQv3ChQBcuFOpM0tXV7r+ezJNTeVt5ednJzY1QHrhbCNsBAAAAAAAAAABQJgoKi5R0MV2P1K9l1l7X21MJaRdvqY+ioiJlZmbKw8PjtufRo0cPOTk5KT8/X82aNVNgYKA2btyoVatW6bPPPpO/v79SU1P19ddf69ixYyoqKlJBQYFGjBhhuu28JD3++OOKj4/Xe++9p/79+ysvL09PP/202Vj//Oc/FRkZKUk6ffq0+vXrp/Pnz8vT01PBwcHat2+ffH19JUkGg0GHDh3SihUrdOnSJfn4+Kh+/XKaPKWKypf//QbW69dn6IMVl0zbo0cnS5LGjfNUl8dcb/u8ALgxwnYAAAAAAAAAAACUiey8PBUZjXJ1tDdrd3VwUOaV3FvqY/bs2crOzlbv3r1vex6ffvqprly5omPHjqlq1aqm9uDgYD333HNasWKF7Ozs9Nhjj+n999/XwIEDi+0nIiJCDRo0UOvWrZWQkHDTcVevXn3D/U5OThbPo9+6rZZFXXi4h8LDb/9iAwC3h7AdAAAAAAAAAAAAZczGbMso4y0dFRMTo8jISK1bt05eXl63PfrixYsVGhpqFrRL0ttvv619+/bps88+k6+vr3bu3KmIiAj5+PioU6dOZrWXL1/WqlWrNGXKlNueB4B7C2E7AAAAAAAAAAAAyoSzvb1sbWwsVrFn5ebJ1dHhhsfGxsZq6NChWrNmjUXwbY1Tp07pq6++0tq1a83aL1++rFdeeUWffPKJunXrJklq3Lix4uPjNWvWLIsx//Of/ygnJ0eDBg267bkAuLfY3rwEAAAAAAAAAAAAKH12Bls9UNFdx86eM2s/dva8/CpVLPG4mJgYDR48WKtWrTIF4bdr6dKl8vLysugnPz9f+fn5srU1j9MMBoOKioos+lm8eLGeeOIJeXp63tF8ANw7WNkOAAAAAAAAAACAMtO+rr9ivolX9YoV5Fu5gvb98psu5VxWcK0akqSNP/ys9MtX9Pf/r4+JidGgQYM0d+5cBQcHKyUlRdLV55u7u7tLkvLy8nT48GHTz0lJSYqPj5eLi4tq165tGruoqEhLly5VeHi47OzMYzM3Nze1b99e48aNk5OTk3x9fbVjxw6tWLFCc+bMMas9ceKEdu7cqY0bN96NUwTgfxRhOwAAAAAAAAAAAMpM0xpVlZ2Xpy2HjyvjSq683V00tF0LeTiXlyRlXMnVxZzLpvoFCxaooKBAI0aM0IgRI0zt4eHhWrZsmSTpzJkzatasmWnfrFmzNGvWLLVv317bt283tX/11VdKTEzUkCFDip3b6tWrNXHiRA0YMEAXLlyQr6+vpk2bpuHDh5vVLVmyRA888IBCQkLu9HQAuIcQtgMAAAAAAAAAAKBMtantpza1/Yrd1/ehJmbbfwzLS+Ln5yej0XjTupCQkBvWeXt7a+nSpabtyMhIZWRk6LXXXjOrs7e319ChQzV16tSbjnmn2j1814cAcIt4ZjsAAAAAAAAAAAAAAFYibAcAAAAAAAAAAAAAwEqE7QAAAAAAAAAAAAAAWImwHQAAAAAAAAAAAAAAKxG2AwAAAAAAAAAAAABgJcJ2AAAAAAAAAAAAAACsZFfWEwAAAAAAAAAAAABu5vSEXWU9BcmxrCcA4H8JK9sBAAAAAAAAAAAAALASYTsAAAAAAAAAAAAAAFYibAcAAAAAAAAAAAAAwEqE7QAAAAAAAAAAAAAAWImwHQAAAAAAAAAAAAAAKxG2AwAAAAAAAAAAAABgJcJ2AAAAAAAAAAAAAACsRNgOAAAAAAAAAAAAAICVCNsBAAAAAAAAAAAAALASYTsAAAAAAAAAAAAAAFYibAcAAAAAAAAAAAAAwEplHrbPmzdP/v7+cnR0VFBQkHbt2lVi7dq1a9W5c2d5enrKzc1NrVq10ubNm//E2QIAAAAAAAAAAAAAUMZhe2xsrEaNGqVJkybp4MGDateunUJDQ5WYmFhs/c6dO9W5c2dt3LhRcXFx6tixo8LCwnTw4ME/eeYAAAAAAAAAAAAAgPtZmYbtc+bM0dChQzVs2DA1aNBAUVFRql69uqKjo4utj4qK0j/+8Q+1aNFCderU0fTp01WnTh2tX7/+T545AAAAAAAAAAAAAOB+VmZhe15enuLi4hQSEmLWHhISoj179txSH0VFRcrMzJSHh0eJNbm5ucrIyDB7AQAAAAAAAAAAAABwJ8osbD9//rwKCwtVpUoVs/YqVaooJSXllvqYPXu2srOz1bt37xJrZsyYIXd3d9OrevXqdzRvAAAAAAAAAAAAAADK9DbykmRjY2O2bTQaLdqKExMTo8jISMXGxsrLy6vEuokTJyo9Pd30+u233+54zgAAAAAAAAAAAACA+5tdWQ1cuXJlGQwGi1XsqampFqvdrxcbG6uhQ4dqzZo16tSp0w1rHRwc5ODgcMfzBQAAAAAAAAAAAADgmjJb2W5vb6+goCBt2bLFrH3Lli1q3bp1icfFxMRo8ODBWrVqlbp163a3pwkAAAAAAAAAAAAAgIUyW9kuSWPGjNHAgQPVvHlztWrVSgsXLlRiYqKGDx8u6eot4JOSkrRixQpJV4P2QYMGae7cuQoODjatindycpK7u3uZvQ8AAAAAAAAAAAAAwP2lTMP2Pn36KC0tTVOnTlVycrICAwO1ceNG+fr6SpKSk5OVmJhoql+wYIEKCgo0YsQIjRgxwtQeHh6uZcuW/dnTBwAAAAAAAAAAAADcp8o0bJekiIgIRUREFLvv+gB9+/btd39CAAAAAAAAAAAAAADcRJk9sx0AAAAAAAAAAAAAgHsVYTsAAAAAAAAAAAAAAFYibAcAAAAAAAAAAAAAwEqE7QAAAAAAAAAAAAAAWImwHQAAAAAAAAAAAAAAKxG2AwAAAAAAAAAAAABgJcJ2AAAAAAAAAAAAAACsRNgOAAAAAAAAAAAAAICVCNsBAAAAAAAAAAAAALASYTsAAAAAAAAAAAAAAFYibAcAAAAAAAAAAAAAwEqE7QAAAAAAAAAAAAAAWImwHbgHpW1N09GxR/XTsJ904p8nlH00u8Ta5ORk9e/fX/Xq1ZOtra1GjRplUbNs2TLZ2NhYvK5cuWKqKSgo0OTJk+Xv7y8nJyfVrFlTU6dOVVFRkalm8ODBZse/tOBRzfrkJbOxoj4bo5cWPGr2WvLVv+78pAAAAAAAAAAAAAB/IruyngAA66TvT1fKqhT5DPJR+TrldfHrizo155RqT68t+0r2FvW5ubny9PTUpEmT9NZbb5XYr5ubm44ePWrW5ujoaPr5jTfe0Pz587V8+XIFBATowIEDevbZZ+Xu7q6RI0ea6h577DEtXbpUkrT0H7tlsLX8mGldv5sebzHYtF3OYDlvAAAAAAAAAAAA4H8ZYTtwjzm/+bwqPlxRHu09JEk+A3yU9WOWLmy7IO9e3hb1fn5+mjt3riRpyZIlJfZrY2Mjb2/L46/Zu3evunfvrm7dupn6jYmJ0YEDB8zqHBwcTP24lfcoti97O4cS9wEAAAAAAAAAAAD3Am4jD9xDigqKdDnhslwCXczaXQJdlHMi5476zsrKkq+vr6pVq6bHH39cBw8eNNvftm1bbd26VceOHZMkff/999q9e7e6du1qVrd9+3Z5eXmpbt26WrVjtjIvX7QY68CJrRq/vIde/2iI1u6dryt5dzZ3AAAAAAAAAAAA4M/GynbgHlKYWSgVSXZu5v/TNbgZVJBecNv91q9fX8uWLVOjRo2UkZGhuXPnqk2bNvr+++9Vp04dSdL48eOVnp6u+vXry2AwqLCwUNOmTVO/fv1M/YSGhqpXr17y9fXVr7/+qhefHa2314/VP3pGm24V36LOo6rk6i238h46c+FXrf9msZLSftHLj//7tucPAAAAAAAAAAAA/NkI24F7kc1128Y76y44OFjBwcGm7TZt2ujBBx/UO++8o7fffluSFBsbq5UrV2rVqlUKCAhQfHy8Ro0apapVqyo8PFyS1KdPH1MfgYGBOhR6Ra+u6q+fTu1X05rtrvbdoJuppqqHv7zcq+nNtS/qt3PHVN2z7p29EQAAAAAAAAAAAOBPQtgO3EMMrgbJVhar2AszC2XnXnr/c7a1tVWLFi10/PhxU9u4ceM0YcIE9e3bV5LUqFEjnTp1SjNmzDCF7ddzd64kD5cqOpdxusSxqleuI4OtnVLTkwjbAQAAAAAAAAAAcM/gme3APcTWzlZOfk7K+inLrD3rpyyVr12+1MYxGo2Kj4+Xj4+PqS0nJ0e2tuYfGQaDQUVFRSX2k3UlXRezU+VWvlKJNckXE1RYVCD3G9QAAAAAAAAAAAAA/2tY2Q7cYyp3qazTC0/Lyc9JTrWddHH7ReWn5cujo4ckKWVNigouFqja89VMx8THx0uSsrKydO7cOcXHx8ve3l4NGzaUJL322msKDg5WnTp1lJGRobffflvx8fF67733TH2EhYVp2rRpqlGjhgICAnTw4EHNmTNHQ4YMMfUdGRmpnj17ysfHRwkJCVqwabJcHN3VxK+tJOlc+hkdOPGVGtZoKRdHd6VcPKW1e+erWuXaqukd8GecPgAAAAAAAAAAAKBUELYD9xj3lu4qyCpQ6rpUFaQXyOEBB/mO8ZV9ZXtJUsGlAuWl5Zkd06xZM9PPcXFxWrVqlXx9fZWQkCBJunTpkp5//nmlpKTI3d1dzZo1086dO/XQQw+ZjnvnnXc0ZcoURUREKDU1VVWrVtULL7ygV199VdLVVe6HDh3SihUrdOnSJfn4+MjHvb6GdJoiR/urq+7tDHY6mnRQXx9aq7z8K6rg4qnAGi0VGjRItraGu3naAAAAAAAAAAAAgFJF2A7cgyo9WkmVHi3+tuvVnqtm0WY0Gm/Y31tvvaW33nrrhjWurq6KiopSVFRUsfudnJy0efNms7b3hm8z267o4qVRT9x4HAAAAAAAAAAAAOBewDPbAQAAAAAAAAAAAACwEmE7AAAAAAAAAAAAAABWImwHAAAAAAAAAAAAAEmZ323Q6flDdWpWDyUvG6krv/1YYm1ycrL69++vevXqydbWVqNGjbph36tXr5aNjY2efPLJEmtmzJghGxubYvs6cuSInnjiCbm7u8vV1VXBwcFKTEw07e/QoYNsbGzMXn379r3ZW8YdIGwHAAAAAAAAAAAAcN/LPrJTF7Yuknur3qo6+G05VAtQ6ppIFWSkFlufm5srT09PTZo0SU2aNLlh36dOndLYsWPVrl27Emu+/fZbLVy4UI0bN7bY98svv6ht27aqX7++tm/fru+//15TpkyRo6OjWd1zzz2n5ORk02vBggW38M5xu+zKegIAAAAAAAAAAAAAUNYyvv1ULo07y7VJF0mSR6fndfnX75R5cKMqth9sUe/n56e5c+dKkpYsWVJiv4WFhRowYIBee+017dq1S5cuXbKoycrK0oABA7Ro0SK9/vrrFvsnTZqkrl276s033zS11axZ06KufPny8vb2vtlbRSlhZTsAAAAAAAAAAACA+5qxMF95KSfk5N/MrN3Jv5lyk36+o76nTp0qT09PDR06tMSaESNGqFu3burUqZPFvqKiIm3YsEF169ZVly5d5OXlpZYtW+rTTz+1qP3www9VuXJlBQQEaOzYscrMzLyjuePGWNkO/IUdqd+gbCfQ4b2yHR8AAAAAAAAAAOAWFOZkSMYi2ZavaNZucK6owuzvbrvf//73v1q8eLHi4+NLrFm9erW+++47ffvtt8XuT01NVVZWlmbOnKnXX39db7zxhjZt2qSnnnpKX3/9tdq3by9JGjBggPz9/eXt7a0ff/xREydO1Pfff68tW7bc9vxxY4TtAAAAAAAAAAAAACDJxua6BqNR0vWNtyYzM1PPPPOMFi1apMqVKxdb89tvv2nkyJH68ssvLZ6/fk1RUZEkqXv37ho9erQkqWnTptqzZ4/mz59vCtufe+450zGBgYGqU6eOmjdvru+++04PPvjgbb0H3BhhOwAAAAAAAAAAAID7mqG8m2Rjq8Lsi2bthTmXZHCucFt9/vLLL0pISFBYWJip7Vpwbmdnp6NHj+rQoUNKTU1VUFDQ72MWFmrnzp169913lZubq8qVK8vOzk4NGzY0679BgwbavXt3ieM/+OCDKleunI4fP07YfpcQtgMAAAAAAAAAAAC4r9kYysneu7YuJ8SrfN3WpvYrCfFyqtPytvqsX7++Dh06ZNY2efJkZWZmau7cuapevbq8vLwsap599lnVr19f48ePl8FgkMFgUIsWLXT06FGzumPHjsnX17fE8X/66Sfl5+fLx8fntuaPmyNsBwAAAAAAAAAAAHDfc2vxpM5/PkcO3rXlULWBMr/fpIKMc3Jt2lWSdHHHMhVmpqny4383HXPtWexZWVk6d+6c4uPjZW9vr4YNG8rR0VGBgYFmY1SoUEGSTO329vYWNc7OzqpUqZJZ+7hx49SnTx89/PDD6tixozZt2qT169dr+/btkq6uov/www/VtWtXVa5cWYcPH9bf//53NWvWTG3atCnN04Q/IGwHAAAAAAAAAAAAcN9zbvCwii5n6tJ/V6sw+4LsK/vKq1ek7Ny9JEmFWRdVkHHO7JhmzZqZfo6Li9OqVavk6+urhISEUp1bjx49NH/+fM2YMUN/+9vfVK9ePX388cdq27atpKuh/datWzV37lxlZWWpevXq6tatm/75z3/KYDCU6lzwO8J2AAAAAAAAAAAAAJDk+mA3uT7Yrdh9lbuNtmgzGo1W9b9s2bKb1lxbrX69IUOGaMiQIcXuq169unbs2GHVXHDnbMt6AgAAAAAAAAAAAAAA3GsI2wEAAAAAAAAAAAAAsBJhOwAAAAAAAAAAAAAAViJsBwAAAAAAAAAAAADASoTtAAAAAAAAAAAAAABYya6sJwAAAAAAAAAAAAAA95RI9zIeP71sx4ckVrYDAAAAAAAAAAAAAGA1wnYAAAAAAAAAAAAAAKxE2A4AAAAAAAAAAAAAgJUI2wEAAAAAAAAAAAAAsBJhOwAAAAAAAAAAAAAAViJsBwAAAAAAAAAAAADASoTtAAAAAAAAAAAAAABYibAdAAAAAAAAAAAAAAArEbYDAAAAAAAAAAAAAGAlwnYAAAAAAAAAAAAAAKxE2A4AAIC75r8nEjRtwzZN+M8XemvLLp08d6HE2uTkZPXv31/16tWTra2tRo0aZVHz008/qWfPnvLz85ONjY2ioqIsaqKjo9W4cWO5ubnJzc1N/v7+8vb2lqOjo4KCgrRr1y4NHjxYNjY2Zq/g4OC7NodWrVrpiy++MKspaQ53w7x58+Tv7292DkryVz0HAAAAAAAAQGkjbAcAAMBdEZ94Rp/FH1anBrU1OqStalb20Pu7vlFiYmKx9bm5ufL09NSkSZPUpEmTYmtycnJUs2ZNzZw5U97e3sXWVKtWTTNnztSBAwc0depUJSYm6vz58/roo4/Url07hYaGKisrS4899piSk5NNr40bN96VORw4cECPPPKIunfvrp9++smsrrg5lLbY2FiNGjVKkyZN0sGDB03n4M/8dyjrcwAAAAAAAADcDXZlPQEAAAD8Ne049qse8q+uljVrSJK6NwvQ0ZRzio6O1owZMyzq/fz8NHfuXEnSkiVLiu2zRYsWatGihSRpwoQJxdaEhYWZfh44cKCef/55xcbG6ty5c4qKitLmzZt19OhR1apVyyIo9vDwKPU5SNK0adMUHR2tffv2KSAgwNTu4OBQYlhdWubMmaOhQ4dq2LBhkmQ6B3/mv4NUtucAAAAAAAAAuBtY2Q4AAIBSV1BYpKSL6apbxdOsva63p/bs2fOnzCEvL09xcXEqX768srOz1apVK0lSSEiIzp07p+3bt8vLy0t169bVc889p9TU1Lsyj8LCQq1evdpsDtfc7TlcOwchISFm7SEhIX/av4NUtucAAAAAAAAAuFtY2Q4AAIBSl52XpyKjUa6O9mbtrg4O+iUl5a6Pf+jQIbVs2VKFhYVasGCBPvnkEzVs2FCSVKVKFdna2urDDz+Ur6+vfv31V02ZMkWPPPKI4uLi5ODgUGpzaNWqla5cuSIXFxezOUhSaGioevXqdVfncP78eRUWFqpKlSpm7VWqVFHKn/TvUNbnAAAAAAAAALhbWNkOALht3377rebOnavXX39dCxcu1KlTp0qsTU5OVv/+/VWvXj3Z2tpq1KhRxdZ9/PHHatiwoRwcHNSwYUN98sknZvsLCgo0efJk+fv7y8nJSTVr1tTUqVNVVFRkqomMjFT9+vXl7OysihUrqlOnTso/csisnwujh+nsI83MXpf+Nf72TwaAEtiYbRlllI2NTQm1padevXrasmWLJOmpp55SeHi4Dh8+fHUORqPc3NzUrVs3BQYGKiwsTF988YWOHTumDRs23PHY8+bNk7+/v1q0aCE/Pz9FR0frxRdfNJuDJPXp00cPPvigpk+frrFjx+qHH37QkSNHip3DnX422tjYmH02/utf/9Lp06e1f/9+s346dOggGxsb02vHjh139Az1evXqKT4+Xvv27SvxHNytfwcAAAAAAADgbiNsBwDclh9//FGbNm1Su3bt9MILL6hGjRr68MMPlZ6eXmx9bm6uPD09NWnSJDVp0qTYmr1796pPnz4aOHCgvv/+ew0cOFC9e/c2C4PeeOMNzZ8/X++++66OHDmiN998U//+97/1zjvvmGrq1q2rd999V4cOHdLu3bvl5+eni/+IUNGlC2bjOXV7SpX/s8X0chs9uRTODPC7mIsX1fnkL2p67KieTvhVB3JySqwtrQtS/Pz8zMLSa68RI0ZIkvLz8/X590c0a/NOTfx4k6Z+9pVi9scr/fKVUnvfkuRsby9bGxtlXsk1a8/KzbNYZX032Nvbq0WLFjIYDOrRo4eaNGlieg55amqqxRx8fHzk6+ur48eP39G4sbGxGjVqlCZNmqSDBw+qU6dOGj16tF588UWzOVxz/Wejq6urxRzu5LMxNjZWBoNBKSkpZp+NPXv2lJubm+mW+n/03HPPKTk5WcnJyWrVqpUeffTR2z4f9vb2ql27tpo3b64ZM2YUew7+qLT+HQAAAAAAAIA/A2E7AOC27Nu3T82aNdODDz4oT09PPfbYY3J3d9e3335bbL2fn5/mzp2rQYMGyd3dvdiaqKgode7cWRMnTlT9+vU1ceJEPfroo4qKijLV7N27V927d1e3bt3k5+enp59+WiEhITpw4ICppn///urUqZNq1qypgIAAzZkzR8bsLOWfNA9vbBwcZfCobHrZurje+Ym5T837Nk/+czPl+HqGghZmadepghJrSytUjoyMtAiUvb29zWqu39/w6M9qePRnLb6QZqoJTzxlar/2+vuZpNs/Gf/vi4wMzUg9qxc8KuljXz8FlS+vF07/psTExGLrS+uClG+//dYUlCYnJ5tWd/fq1UuSlJOTo6RLGerUsLZGd26r8DZBOpeZraW7DxQ75u2yM9jqgYruOnbWPMg9dva8WrduXapjlcTe3l5BQUHasmWLjEajcnOvBv9btmyxmENaWpp+++03+fj43NGYc+bM0dChQzVs2DA1aNBAUVFRql69uqKjo83mcM0fPxvLly+vrKwsizncyWdjfHy86Rz88bPxu+++U79+/ZSRkaEffvjBbLzy5cvL29tb3t7esre3L9XbuRd3Dv6otP4dAAAAAAAAgD8DYTsAwGqFhYU6c+aMatWqZdZes2ZNnT59+rb73bt3r0JCQszaunTpoj179pi227Ztq61bt+rYsWOSpO+//167d+9W165di+0zLy9PCxculI2zi8rVqmu27/LWjUp9sqPOP9tTmdFzVJSTfdtzv5/F/pivUZuuaFI7Bx18wVntatgp9MMcJaYXFVtfWqGyJAUEBJgFy4cOmT8u4I/7kpOT9bq3t2wkhVx3YUUvd3ftqFXb9IqsYh7a345lFy+op3sFPV2hgmo5OGiiVxX5lCun6OjoYutL64IUT09PU1Dq7e2tzz//XLVq1VL79u0lSe7u7nqhfUs1rV5VXm4u8q1UUU8+GKDTF9N1MfvyHb/vP2pf11/f/Pqbvjn5m85mZGrdwcO6lHNZw4cPlyRNnDhRgwYNMjsmPj5e8fHxysrK0rlz5xQfH2922/G8vDxTTV5enpKSkhQfH68TJ06Yal555RXt2rVLCQkJevrpp7VgwQJ9/fXXateunUaPHq3ExESlpaVp7969ioiIUEhIiMLCwlS5cmX16NHjtudw+PBhxcXFKSQkxGwODz74oD788ENt375dAwYMkCRlZWVp7Nix2rt3rxISErR9+3b9+OOPcnR0VI8ePczOyZ1+No4ZM0bvv/++lixZoiNHjpjOgZOTk9zd3fXZZ5+Z/Tt8+OGHqlChgmrVqqWff/5ZZ86cueN/h0OHDmnSpEk3PQd//HcAAAAAAAAA/tfZlfUEAAD3npycHBmNRrm4uJi1u7i46JdffrntflNSUixu7VylShWlpKSYtsePH6/09HTVr19fBoNBhYWFmjZtmvr162d23Oeff66+ffsqJydHPj4+qvjv+bJ1r2ja7/hoVxl8qsrgUVkFv55Q5vvvqODkMVX89/zbnv/9as6+XA1tVk7DHrSXJEU9ZtDmXwoU/W2eZnRytKi/FipL0pIlS4rt84+hsnQ1lN2xY4eioqIUExNjqrOzs7NYzf5H1+/blpWlh8qXV3V7e7N2R1tbedqV3q9FeUajDl+5ouc8Kpm1ty7vbBaQWmvv3r0aPXq0WVuXLl3MwnazeeTlaeXKlRozZswNn5N+Jb9ANpKc7Ev3V8OmNaoqOy9PWw4fV8aVXHm7u2houxby9fWVdPViiOtX+jdr1sz0c1xcnFatWiVfX18lJCRIks6cOWNWM2vWLM2aNUvt27fX9u3bJUlnz57VwIEDlZycLHd3d9WpU0eXLl1SRESEAgMD9cknn2jWrFnq3r27zp8/L3t7e/Xt21exsbFydXW97TkEBwersLBQVapUMZuD3f//t7Vp0yZ17txZkmQwGHTo0CGtWLFCly5dko+Pj5ycnNStWze5uppfDFIan41paWmaOnWqkpKSVFRUpKKiIi1btkxbtmzRe++9Z/p3GDBggPz9/dW5c2fTY0HWrFmjNWvW3PG/Q+PGjW96Djp27Gj6dwAAAAAAAAD+1xG2AwBKjdFovOM+rg8EjUajWVtsbKxWrlypVatWKSAgQPHx8Ro1apSqVq2q8PBwU13Hjh0VHx+v8+fPa9GiRVo29R+q9N4Hsq3oIUkq//hTplo7/9oyVKuhC8MHKP/YEZWr2+CO38f9Iq/QqLgzRZrQxvw20yE17bTndOFt93urofLx48dVtWpVOTg4qGXLlpo+fbpq1qxZbJ9nz57VzqwsTS/m9tSfZ2RofUaGKhkMaufsohGVK8nZ1nDb879UWKBCSZXszPuoZGfQN38ISK11K6HrH3366ae6dOmSBg8eXGKf+YWF2vjDz2pWo6ocy5W77bmVpE1tP7Wp7VfsvmXLllm03exzxM/P76Y1ixcvvum8rgW+xbndOZw5c0YPPPCAbGxszOYwbdo0ffDBB2ZjOjk5afPmzWbHd+jQocSQ+U4/GyMiIhQREaHs7GwlJyebPhuv3THCy8tL0tXntf/xHMTFxal58+aKi4vTgw8+eNNz8Ec3+3co7hwAAAAAAAAA9xJuIw8AsFr58uVlY2OjrKwss/bs7GyL1e7W8Pb2tggNU1NTzcLFcePGacKECerbt68aNWqkgQMHavTo0ZoxY4bZcc7Ozqpdu7aCg4O1ePFi2RgMuvyF+TO//8iuTgPJzk6FScU/TxvFO59jVKFRquJiHgRWcbFRStbtX3xxK6Fyy5YttWLFCm3evFmLFi1SSkqKWrdurbS0tOu7kyQtX75c5W1t1fm6W8g/7uauf/tU1fLqNfRipcrakpWpvyXd+TPbJclG1wWksgxNre7zJqHrHy1evFihoaGqWrVqsfsLi4q0cu9BGY1GPRUUeEfzglS5cmUZDIabfo5Z625+NtrZ2d0wFH/wwQdVrlw5HT9+/LbnDwAAAAAAAPxVEbYDAKxmMBhUtWpVnTx50qz95MmTqlat2m3326pVK23ZssWs7csvv1Tr1q1N2zk5ObK1Nf+/L4PBoKKi4p8PbmKUjHn5Je4uTPhFKiiQrUdl6ycOXR/1Go2WbVb3eZNQOTQ0VD179lSjRo3UqVMnbdiwQdLVUL04S5Ys0eNubnK47r+fXhUqqLWzs+o4OKirm5uiqj6gvTk5Onzlym3PvYLBTgZJ5wsKzNovFBTe9dD1mlOnTumrr77SsGHDiu2rsKhIH+z9Theyc/R8+5Z3ZVX7/cbe3l5BQUEWn2Nbtmwx+xyz1t38bDQajcrNzS1x/08//aT8/Hz5FHNHCAAAAECS5s2bJ39/fzk6OiooKEi7du0qsTY5OVn9+/dXvXr1ZGtrq1GjRt3z4wMAgPsbYTsA4LYEBwfru+++08GDB3Xu3Dlt2rRJ6enpat68uSTpq6++0iefmK8kj4+PV3x8vLKysnTu3DnFx8fr8OHDpv0jR47Ul19+qTfeeEM///yz3njjDX311Vdmf/yGhYVp2rRp2rBhgxISEvTJJ59ozpw56tGjh6Srq+tfeeUV7du3T6dOndJ3332nYcOGqfDcWTm2v3oL54Kk35S1YoHyj/6kwpQzyt23S5de+4fsatdXucCmd/fE/cVULm8jg40sVrGnZhstVrtbw5pQ+RpnZ2c1atSo2BW4u3bt0tGjR/W0e4Wbjt3QwUF2kk7l5Vk7bRN7Gxs1dHTUnpxss/Y9Odl3PXS9ZunSpfLy8lK3bt0s9l0L2s9lZuuF9i3l7GBvUYPbM2bMGL3//vtasmSJjhw5otGjRysxMVHDhw+XJE2cOFGDBg0yO6YsPxtPnz6tXr16SZJ++eUXTZ06VQcOHFBCQoI2btyoXr16qVmzZmrTps1dPnMAAAC4F8XGxmrUqFGaNGmSDh48qHbt2ik0NFSJicXfNS43N1eenp6aNGmSmjRpcs+PDwAAwDPbAQC3JTAwUJcvX9aOHTuUlZUlLy8vDRgwQBUqVJAkZWVlKT093eyYZs2amX6Oi4vTqlWr5Ovrq4SEBElS69attXr1ak2ePFlTpkxRrVq1FBsbq5YtW5qOe+eddzRlyhRFREQoNTVVVatW1QsvvKBXX31V0tWVnD///LOWL1+u8+fPq1KlSmrRooU85i6RnX8tSZJNuXLK++4b5ayNkfFyjgye3rIPbiuXQS/IxnD7z+m+H9kbbBRU1VZbThaoR4PfV0ZvOVmg7vVu/9eMa6HyH5/bXlKofE1ubq6OHDmidu3aWexbvHixgoKCVD8ru5gjzZ3Iy1OBJE+7O/s1aXBFD41PPqMAR0c1dXTSmvRLSs7PNwtdk5KStGLFCtMx8fHxkmQWutrb26thw4aSroauDz/8sN544w11795d69at01dffaXdu3ebjV1UVKSlS5cqPDxcdte9j4KCAq3Y851OX0zX0HYtVGQ0KuPy1VX85e3tZWfgWsw70adPH6WlpWnq1KlKTk5WYGCgNm7cKF9fX0lXV9Jc/8VfWX427tq1SwEBAZKurszfunWr5s6dq6ysLFWvXl3dunXTP//5Txn4bAQAAEAx5syZo6FDh5ruqBUVFaXNmzcrOjra4pFGkuTn56e5c+dKunr3sXt9fAAAAMJ2AMBta9GihVq0aFHsvieffNKizWi8+TO8n376aT399NMl7nd1dVVUVJSioqKK3e/o6Ki1a9datHt/HW/62eDlLY+okp9RDOuMCXbQwE8uq3lVg1pVM2hhXL4S04s0vPnV1dITv7qipEyjVvRwMh1TGqHy2LFjFRYWpho1aig1NVWvv/66MjIyFB4ebja/jIwMrVmzRrNnz5ai5prtS8zL0+cZGXrYxVkVDQadyM3Tv8+lqoGDg5o5OelOhLq56VJhoaLPn9e5wkLVsbfXgmrV73roKl29s0RiYqKGDBliMa/Tp0/rpzNnJUlzvjS/veLwDsGq7VXpjt43pIiICEVERBS7b9myZRZtZfnZ+EfVq1fXjh07bjoXAAAAQJLy8vIUFxenCRMmmLWHhIRoz549f/nxAQAAJMJ2AABwh/oEllPaZaOm7shVcpZRgV622jigvHwrXF0hnZxlVGK6+XOjSyNUPn36tPr166fz58/L09NTwcHB2rdvnynMvmb16tUyGo3q16+fzlwXtpezsdG+nGx9cPGCcoxGedvZqb2ziyIqV5bB5k6fOi/1q1hR/SpWLHbf3QpdpatfLpXUl5+fn2b1try1PAAAAABY4/z58yosLLR43FeVKlUsHgv2VxwfAABAImwHAAClIKKFvSJaFP/c72VPWq4QL41QefXq1bc0t+eff17PP/+8JOnMdft8ypXTihq+lgfhrjo9YdfNi+6y9x23lvUUFBkZWabj//GOH2UlpWPTsp4CAAAA7pDNdRcqG41Gi7a/8vgAAOD+xkM5AQAAAAAAAABWqVy5sgwGg8Uq8tTUVIvV5n/F8QEAACTCdgAAAAAAAACAlezt7RUUFKQtW7aYtW/ZskWtW7f+y48PAAAgcRt5AAAAAAAAAMBtGDNmjAYOHKjmzZurVatWWrhwoRITEzV8+HBJ0sSJE5WUlKQVK1aYjomPj5ckZWVl6dy5c4qPj5e9vb0aNmx4z40PAABA2A4AuOu2bqtV1lOQbD4u6xkAgJky/2zkcxEAAAB3qE+fPkpLS9PUqVOVnJyswMBAbdy4Ub6+vpKk5ORkJSYmmh3TrFkz089xcXFatWqVfH19lZCQcM+NDwAAQNgOAADwJ3pv+LayngIAAAAAlJqIiAhFREQUu2/ZsmUWbUaj8S81PgAAuL8RtgMAgD9No+WNynT8j8p0dAAAAAAAAADAX4ltWU8AAAAAAAAAAAAAAIB7DWE7AAAAAAAAAAAAAABWImwHAAAAAAAAAAAAAMBKhO0AAAAAAAAAAAAAAFjJrqwnAAAAAAAAAAD437R1W60yHX+AzcdlOr4kpXRsWtZTAAAA/6NY2Q4AAAAAAAAAAAAAgJUI2wEAAAAAAAAAAAAAsBJhOwAAAAAAAAAAAAAAViJsBwAAAAAAAAAAAADASoTtAAAAAAAAAAAAAABYibAdAAAAAAAAAAAAAAArEbYDAAAAAAAAAAAAAGAlwnYAAAAAAAAAAAAAAKxE2A4AAAAAAAAAAAAAgJUI2wEAAAAAAAAAAAAAsBJhOwAAAAAAAAAAAAAAViJsBwAAAAAAAAAAAADASoTtAAAAAAAAAAAAAABYibAdAAAAAAAAAAAAAAArEbYDAAAAAAAAAAAAAGAlwnYAAAAAAAAAAAAAAKxE2A4AAAAAAAAAAAAAgJUI2wEAAAAAAAAAAAAAsBJhOwAAAAAAAAAAAAAAViJsBwAAAAAAAAAAAADASoTtAAAAAAAAAAAAAABYibAdAAAAAAAAAAAAAAArEbYDAAAAAAAAAAAAAGAlwnYAAAAAAAAAAAAAAKxE2A4AAAAAAAAAAAAAgJUI2wEAAAAAAAAAAAAAsBJhOwAAAAAAAAAAAAAAViJsBwAAAAAAAAAAAADASoTtAAAAAAAAAAAAAABYibAdAAAAAAAAAAAAAAArEbYDAAAAAAAAAAAAAGAlwnYAAAAAAAAAAAAAAKxE2A4AAAAAAAAAAAAAgJUI2wEAAAAAAAAAAAAAsBJhOwAAAAAAAAAAAAAAViJsBwAAAAAAAAAAAADASoTtAAAAAAAAAAAAAABYibAdAAAAAAAAAAAAAAArEbYDAAAAAAAAAAAAAGAlwnYAAAAAAAAAAAAAAKxE2A4AAAAAAAAAAAAAgJUI2wEAAAAAAAAAAAAAsBJhOwAAAAAAAAAAAAAAViJsBwAAAAAAAAAAAADASoTtAAAAAAAAAAAAAABYibAdAAAAAAAAAAAAAAArEbYDAAAAAAAAAAAAAGAlwnYAAAAAAAAAAAAAAKxE2A4AAAAAAAAAAAAAgJUI2wEAAAAAAAAAAAAAsBJhOwAAAAAAAAAAAAAAViJsBwAAAAAAAAAAAADASoTtAAAAAAAAAAAAAABYibAdAAAAAAAAAAAAAAArEbYDAAAAAAAAAAAAAGAlwnYAAAAAAAAAAAAAAKxE2A4AAAAAAAAAAAAAgJUI2wEAAAAAAAAAAAAAsBJhOwAAAAAAAAAAAAAAViJsBwAAAAAAAAAAAADASoTtAAAAAAAAAAAAAABYibAdAAAAAAAAAAAAAAArEbYDAAAAAAAAAAAAAGAlwnYAAAAAAAAAAAAAAKxE2A4AAAAAAAAAAAAAgJUI2wEAAAAAAAAAAAAAsBJhOwAAAAAAAAAAAAAAViJsBwAAAAAAAAAAAADASoTtAAAAAAAAAAAAAABYibAdAAAAAAAAAAAAAAArEbYDAAAAAAAAAAAAAGAlwnYAAAAAAAAAAAAAAKxE2A4AAAAAAAAAAAAAgJUI2wEAAAAAAAAAAAAAsBJhOwAAAAAAAAAAAAAAViJsBwAAAAAAAAAAAADASoTtAAAAAAAAAAAAAABYibAdAAAAAAAAAAAAAAArEbYDAAAAAAAAAAAAAGAlwnYAAAAAAAAAAAAAAKxE2A4AAAAAAAAAAAAAgJUI2wEAAAAAAAAAAAAAsBJhOwAAAAAAAAAAAAAAVirzsH3evHny9/eXo6OjgoKCtGvXrhvW79ixQ0FBQXJ0dFTNmjU1f/78P2mmAAAAAAAAAAAAAABcVaZhe2xsrEaNGqVJkybp4MGDateunUJDQ5WYmFhs/a+//qquXbuqXbt2OnjwoF555RX97W9/08cff/wnzxwAAAAAAAAAAAAAcD8r07B9zpw5Gjp0qIYNG6YGDRooKipK1atXV3R0dLH18+fPV40aNRQVFaUGDRpo2LBhGjJkiGbNmvUnzxwAAAAAAAAAAAAAcD+zK6uB8/LyFBcXpwkTJpi1h4SEaM+ePcUes3fvXoWEhJi1denSRYsXL1Z+fr7KlStncUxubq5yc3NN2+np6ZKkjIyMO30LuIGi3JwyHT/Dxlim40tS4f+1d++hTd1vHMc/p82aSCWma+tqt2qVUUUr6i6Km7dKwUL3zyhWpJ1G/cNaFbEb26xCL5ZqnXipiBQ1XmBYZBSxQ0Qns2Brxf9UDG4irUoVcdOpZbabze+P0mBML0nU31mS9wsCPd8853yfU+HxIU+a/P3C7BT07IW5Ofzd3Wnq/pLU9c8/Zqegp13m/h66jK6hg96yzs4es1NQj/HM3P1NrosStVEyvy5K1EbJ/LooURsl8+ui9N/oyc2uz9TmXmbXZ2pzL7PrM7W5l9n12ey6KFEbJfProkRtlMyvixK1UTK/Lkrm963U5l7UZmqzRG3uQ22mNvYmYP7rKpGs7/9/j2fwf2fDM1TEW9LR0aH3339fzc3N+uyzz7zr1dXVOnLkiG7cuOF3TkZGhpxOp0pLS71rLS0t+vzzz9XR0aFRo0b5nVNeXq6Kioq3cxMAAAAAAAAAAAAAgIh0584dffDBBwM+b9pftvcxDMPn2OPx+K0NFd/fep8NGzaopKTEe9zT06M///xTiYmJg+4DAOj15MkTpaWl6c6dO7Lb7WanAwAAgP8gekYAAAAMhZ4RQDjxeDx6+vSpUlNTB40zbdielJSk2NhY3b9/32f9wYMHeu+99/o9JyUlpd94i8WixMTEfs+xWq2yWq0+aw6HI/TEASBK2e12mmAAAAAMip4RAAAAQ6FnBBAuRowYMWRMzP8hj37FxcXp448/1tmzZ33Wz5496/Ox8i+bOXOmX/yZM2f0ySef9Pt97QAAAAAAAAAAAAAAvA2mDdslqaSkRAcOHJDL5ZLb7db69et1+/ZtFRUVSer9CPglS5Z444uKitTe3q6SkhK53W65XC4dPHhQ33zzjVm3AAAAAAAAAAAAAACIQqZ+Z/uiRYv0xx9/qLKyUvfu3VNmZqZOnTqlMWPGSJLu3bun27dve+PHjh2rU6dOaf369dq7d69SU1NVW1urvLw8s24BACKe1WpVWVmZ31dyAAAAAH3oGQEAADAUekYAkcjweDwes5MAAAAAAAAAAAAAACCcmPox8gAAAAAAAAAAAAAAhCOG7QAAAAAAAAAAAAAABIlhOwAAAAAAAAAAAAAAQWLYDgAAAAAAAAAAAABAkBi2AwAAAAAAAAAAAAAQJIbtABAGnE6nDMNQUVGR33PFxcUyDENOp9MvfuvWrT6xJ06ckGEY3uPz58/LMAw9fvzYu1ZXV6cpU6YoPj5eDodD06ZNU01NjSQpPT1dhmEM+Jg3b16/+ZeXl/cb/8svv3hjnjx5oo0bN2rChAmy2WxKSUlRdna2Ghoa5PF4QvitAQAARLa+nu/Vx82bN32ep4cEAACIfJHeG0qB9X7z5s2TYRiqr6/3OXfXrl1KT0/3Hh8+fFiGYSgnJ8cn7vHjxzIMQ+fPnx8wDwB4mcXsBAAAgUlLS1N9fb127typYcOGSZKeP3+uY8eOafTo0X7xNptNNTU1WrlypRISEgLa4+DBgyopKVFtba3mzp2rrq4uXblyRdevX5ckXb58WS9evJAktbS0KC8vTzdu3JDdbpckxcXFDXjtSZMm+bwwKknvvvuupN4mdtasWfrrr79UVVWlTz/9VBaLRU1NTfr22281f/58ORyOgO4BAAAgmuTk5OjQoUM+a8nJyd6f6SEdAd0DAABAJIjk3jCY3s9ms2nTpk3Ky8vTO++8M2CuFotF586d06+//qqsrKyA7g8AXsWwHQDCxEcffaRbt26poaFBBQUFkqSGhgalpaVp3LhxfvHZ2dm6efOmtmzZom3btgW0R2Njo/Lz87VixQrv2qRJk7w/v9yc973IOXLkyIBexLRYLEpJSen3udLSUrW1tem3335Tamqqdz0jI0OLFy+WzWYLKH8AAIBoY7VaB+yxJHpIAACAaBLJvWEwvd/ixYvV2Nio/fv3q7i4eMBrxsfHKz8/X99//70uXboU0P0BwKv4GHkACCPLli3zeXeqy+XS8uXL+42NjY1VdXW19uzZo7t37wZ0/ZSUFLW2tqq9vf2N5BuInp4e1dfXq6CgwKdR7jN8+HBZLLw3DAAAIFT0kAAAAOgTjr1hsL2f3W5XaWmpKisr1dnZOei1y8vLdfXqVf30009vLF8A0YVhOwCEka+++koXLlxQW1ub2tvb1dzcrMLCwgHjv/zyS02dOlVlZWUBXb+srEwOh0Pp6ekaP368nE6njh8/rp6entfO/erVqxo+fLj3MX36dEnSw4cP9ejRI02YMOG19wAAAIg2P//8s0+PtXDhQr8YekgAAIDoEKm9YSi9X3FxsWw2m3bs2DFoXGpqqtatW6eNGzfq33//DTlHANGLt3kDQBhJSkpSbm6ujhw5Io/Ho9zcXCUlJQ16Tk1NjebPn6+vv/56yOuPGjVKFy9e1LVr19TU1KSWlhYtXbpUBw4c0OnTpxUTE/p7tMaPH6+TJ096j61WqyTJ4/FIkgzDCPnaAAAA0SorK0v79u3zHsfHx/vF0EMCAABEh0jtDUPp/axWqyorK7VmzRqtWrVq0NjvvvtOdXV1crlcys/PDzo/ANGNYTsAhJnly5drzZo1kqS9e/cOGT9nzhwtWLBApaWlcjqdAe2RmZmpzMxMrV69WhcuXNDs2bPV1NSkrKyskPOOi4vThx9+6LeenJyshIQEud3ukK8NAAAQreLj4/vtsV5FDwkAABD5IrU3DLX3Kyws1Pbt21VVVaX09PQB4xwOhzZs2KCKigp98cUXQecHILrxMfIAEGZycnLU3d2t7u5uLViwIKBztm7dqsbGRrW0tAS938SJEyVpyO83ClVMTIwWLVqkH3/8UR0dHX7Pd3Z28hFOAAAAr4keEgAAAH3CrTcMtfeLiYnRli1btG/fPrW1tQ26x9q1axUTE6Pdu3eHlCOA6MWwHQDCTGxsrNxut9xut2JjYwM6Z/LkySooKNCePXsGjVu1apU2b96s5uZmtbe3q7W1VUuWLFFycrJmzpz5JtLvV3V1tdLS0jRjxgwdPXpU169f1++//y6Xy6WpU6fq2bNnb21vAACAaEAPCQAAgD7h2BuG2vvl5uZqxowZqqurG/T6NptNFRUVqq2tDTlHANGJYTsAhCG73S673R7UOZs3b/Z+v9FAsrOz1draqoULFyojI0N5eXmy2Ww6d+6cEhMTXyflQSUkJKi1tVWFhYWqqqrStGnTNHv2bB07dkw//PCDRowY8db2BgAAiBb0kAAAAOgTbr3h6/R+NTU1ev78+ZB7LF26VOPGjQs5RwDRyfAMVRkBAAAAAAAAAAAAAIAP/rIdAAAAAAAAAAAAAIAgMWwHAAAAAAAAAAAAACBIDNsBAAAAAAAAAAAAAAgSw3YAAAAAAAAAAAAAAILEsB0AAAAAAAAAAAAAgCAxbAcAAAAAAAAAAAAAIEgM2wEAAAAAAAAAAAAACBLDdgAAAAAAAAAAAAAAgsSwHQAAAAAAAAAAAACAIDFsBwAAAAAAAAAAAAAgSAzbAQAAAAAAAAAAAAAI0v8AWzqRhPWfqgQAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 2000x900 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x = np.arange(len(width_scores.keys()))\n",
    "width = 0.25  # the width of the bars\n",
    "multiplier = 0\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(20,9),layout='constrained')\n",
    "\n",
    "for model in complexity_scores:\n",
    "    for performance in width_scores[model].index:\n",
    "        offset = width * multiplier\n",
    "        rects = ax.bar(offset, width_scores[model][performance], width, label=f'{performance[1]}')\n",
    "        ax.bar_label(rects, padding=3)\n",
    "        multiplier += 1  \n",
    "    multiplier += 1\n",
    "\n",
    "# Add some text for labels, title and custom x-axis tick labels, etc.\n",
    "ax.set_ylabel('Average Percentage Scores')\n",
    "ax.set_title(f'Layer Depth Model Scores')\n",
    "ax.set_xticks((x+width)*5 + width+0.25, width_scores.keys())\n",
    "ax.legend(loc='upper right', ncols=3)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Disregard the x-ticks and you get the idea. While the result is hideous and the average scores are pretty bad all around (due to the random sampling of most hyperparameters), there is still a relatively significant trend that networks with less layers and less neurons _generally_ perform slightly better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_fc_width_by_optimizer = mnist_fixed_fc_df.groupby(['optimizers'])['score'].mean()\n",
    "fmnist_fc_width_by_optimizer = fmnist_fixed_fc_df.groupby(['optimizers'])['score'].mean()\n",
    "fmnist_cnn_width_by_optimizer = fmnist_fixed_cnn_df.groupby(['optimizers'])['accuracy'].mean()\n",
    "optimizer_scores = {'MNIST FC':mnist_fc_width_by_optimizer,'FMNIST FC':fmnist_fc_width_by_optimizer,'FMNIST CNN':fmnist_cnn_width_by_optimizer}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 2000x900 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x = np.arange(len(optimizer_scores.keys()))\n",
    "width = 0.25  # the width of the bars\n",
    "multiplier = 0\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(20,9),layout='constrained')\n",
    "\n",
    "for model in optimizer_scores:\n",
    "    for performance, score in optimizer_scores[model].items():\n",
    "        offset = width * multiplier\n",
    "        rects = ax.bar(offset, score, width, label=performance)\n",
    "        ax.bar_label(rects, padding=3)\n",
    "        multiplier += 1  \n",
    "    multiplier += 1\n",
    "\n",
    "# Add some text for labels, title and custom x-axis tick labels, etc.\n",
    "ax.set_ylabel('Average Percentage Scores')\n",
    "ax.set_title(f'Optimizer Model Scores')\n",
    "ax.set_xticks((x+width)/1.5, optimizer_scores.keys())\n",
    "ax.legend(loc='upper right', ncols=3)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_fc_width_by_batch = mnist_fixed_fc_df.groupby(['batches'])['score'].mean()\n",
    "fmnist_fc_width_by_batch = fmnist_fixed_fc_df.groupby(['batches'])['score'].mean()\n",
    "fmnist_cnn_width_by_batch = fmnist_fixed_cnn_df.groupby(['batches'])['accuracy'].mean()\n",
    "batch_scores = {'MNIST FC':mnist_fc_width_by_batch,'FMNIST FC':fmnist_fc_width_by_batch,'FMNIST CNN':fmnist_cnn_width_by_batch}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 2000x900 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x = np.arange(len(batch_scores.keys()))\n",
    "width = 0.25  # the width of the bars\n",
    "multiplier = 0\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(20,9),layout='constrained')\n",
    "\n",
    "for model in batch_scores:\n",
    "    for performance, score in batch_scores[model].items():\n",
    "        offset = width * multiplier\n",
    "        rects = ax.bar(offset, score, width, label=performance)\n",
    "        ax.bar_label(rects, padding=3)\n",
    "        multiplier += 1  \n",
    "    multiplier += 1\n",
    "\n",
    "# Add some text for labels, title and custom x-axis tick labels, etc.\n",
    "ax.set_ylabel('Average Percentage Scores')\n",
    "ax.set_title(f'Batch Size Model Scores')\n",
    "ax.set_xticks(x + width+0.25, batch_scores.keys())\n",
    "ax.legend(loc='upper right', ncols=3)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's make some tables for the report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_fc_min = mnist_fixed_fc_df.sort_values(by='score').T\n",
    "fmnist_fc_min = fmnist_fixed_fc_df.sort_values(by='score').T\n",
    "fmnist_cnn_min = fmnist_fixed_cnn_df.sort_values(by='accuracy').T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>2</th>\n",
       "      <th>14</th>\n",
       "      <th>12</th>\n",
       "      <th>0</th>\n",
       "      <th>24</th>\n",
       "      <th>28</th>\n",
       "      <th>9</th>\n",
       "      <th>22</th>\n",
       "      <th>10</th>\n",
       "      <th>37</th>\n",
       "      <th>...</th>\n",
       "      <th>13</th>\n",
       "      <th>3</th>\n",
       "      <th>18</th>\n",
       "      <th>30</th>\n",
       "      <th>1</th>\n",
       "      <th>26</th>\n",
       "      <th>5</th>\n",
       "      <th>15</th>\n",
       "      <th>11</th>\n",
       "      <th>4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>inputs</th>\n",
       "      <td>784</td>\n",
       "      <td>784</td>\n",
       "      <td>784</td>\n",
       "      <td>784</td>\n",
       "      <td>784</td>\n",
       "      <td>784</td>\n",
       "      <td>784</td>\n",
       "      <td>784</td>\n",
       "      <td>784</td>\n",
       "      <td>784</td>\n",
       "      <td>...</td>\n",
       "      <td>784</td>\n",
       "      <td>784</td>\n",
       "      <td>784</td>\n",
       "      <td>784</td>\n",
       "      <td>784</td>\n",
       "      <td>784</td>\n",
       "      <td>784</td>\n",
       "      <td>784</td>\n",
       "      <td>784</td>\n",
       "      <td>784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>number_of_layers</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>outputs</th>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>...</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>neurons_per_layer</th>\n",
       "      <td>[20, 10]</td>\n",
       "      <td>[30, 30, 10]</td>\n",
       "      <td>[30, 20, 10]</td>\n",
       "      <td>[10, 10]</td>\n",
       "      <td>[30, 10, 30, 10]</td>\n",
       "      <td>[30, 10, 50, 10]</td>\n",
       "      <td>[50, 10]</td>\n",
       "      <td>[30, 10, 20, 10]</td>\n",
       "      <td>[30, 10, 10]</td>\n",
       "      <td>[30, 10, 10, 40, 10]</td>\n",
       "      <td>...</td>\n",
       "      <td>[30, 20, 10]</td>\n",
       "      <td>[20, 10]</td>\n",
       "      <td>[30, 50, 10]</td>\n",
       "      <td>[30, 10, 10, 10, 10]</td>\n",
       "      <td>[10, 10]</td>\n",
       "      <td>[30, 10, 40, 10]</td>\n",
       "      <td>[30, 10]</td>\n",
       "      <td>[30, 30, 10]</td>\n",
       "      <td>[30, 10, 10]</td>\n",
       "      <td>[30, 10]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dropout_layers</th>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>...</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>activation_functions</th>\n",
       "      <td>[sigmoid]</td>\n",
       "      <td>[tanh, sigmoid]</td>\n",
       "      <td>[tanh, relu]</td>\n",
       "      <td>[sigmoid]</td>\n",
       "      <td>[tanh, sigmoid, tanh]</td>\n",
       "      <td>[tanh, sigmoid, relu]</td>\n",
       "      <td>[relu]</td>\n",
       "      <td>[tanh, sigmoid, tanh]</td>\n",
       "      <td>[tanh, relu]</td>\n",
       "      <td>[tanh, sigmoid, relu, sigmoid]</td>\n",
       "      <td>...</td>\n",
       "      <td>[tanh, sigmoid]</td>\n",
       "      <td>[relu]</td>\n",
       "      <td>[tanh, sigmoid]</td>\n",
       "      <td>[tanh, sigmoid, relu, relu]</td>\n",
       "      <td>[relu]</td>\n",
       "      <td>[tanh, sigmoid, relu]</td>\n",
       "      <td>[sigmoid]</td>\n",
       "      <td>[tanh, sigmoid]</td>\n",
       "      <td>[tanh, relu]</td>\n",
       "      <td>[tanh]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>optimizers</th>\n",
       "      <td>Adam</td>\n",
       "      <td>Adam</td>\n",
       "      <td>Adam</td>\n",
       "      <td>Adam</td>\n",
       "      <td>Adam</td>\n",
       "      <td>Adam</td>\n",
       "      <td>SGD</td>\n",
       "      <td>Adam</td>\n",
       "      <td>Adam</td>\n",
       "      <td>SGD</td>\n",
       "      <td>...</td>\n",
       "      <td>SGD</td>\n",
       "      <td>SGD</td>\n",
       "      <td>Adam</td>\n",
       "      <td>Adam</td>\n",
       "      <td>SGD</td>\n",
       "      <td>Adam</td>\n",
       "      <td>SGD</td>\n",
       "      <td>SGD</td>\n",
       "      <td>SGD</td>\n",
       "      <td>Adam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>learning_rates</th>\n",
       "      <td>0.3</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.3</td>\n",
       "      <td>...</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>weight_decays</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>loss_functions</th>\n",
       "      <td>CrossEntropyLoss</td>\n",
       "      <td>CrossEntropyLoss</td>\n",
       "      <td>CrossEntropyLoss</td>\n",
       "      <td>CrossEntropyLoss</td>\n",
       "      <td>CrossEntropyLoss</td>\n",
       "      <td>CrossEntropyLoss</td>\n",
       "      <td>CrossEntropyLoss</td>\n",
       "      <td>CrossEntropyLoss</td>\n",
       "      <td>CrossEntropyLoss</td>\n",
       "      <td>CrossEntropyLoss</td>\n",
       "      <td>...</td>\n",
       "      <td>CrossEntropyLoss</td>\n",
       "      <td>CrossEntropyLoss</td>\n",
       "      <td>CrossEntropyLoss</td>\n",
       "      <td>CrossEntropyLoss</td>\n",
       "      <td>CrossEntropyLoss</td>\n",
       "      <td>CrossEntropyLoss</td>\n",
       "      <td>CrossEntropyLoss</td>\n",
       "      <td>CrossEntropyLoss</td>\n",
       "      <td>CrossEntropyLoss</td>\n",
       "      <td>CrossEntropyLoss</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>batches</th>\n",
       "      <td>100</td>\n",
       "      <td>200</td>\n",
       "      <td>300</td>\n",
       "      <td>300</td>\n",
       "      <td>100</td>\n",
       "      <td>300</td>\n",
       "      <td>300</td>\n",
       "      <td>100</td>\n",
       "      <td>300</td>\n",
       "      <td>100</td>\n",
       "      <td>...</td>\n",
       "      <td>100</td>\n",
       "      <td>200</td>\n",
       "      <td>200</td>\n",
       "      <td>200</td>\n",
       "      <td>300</td>\n",
       "      <td>100</td>\n",
       "      <td>300</td>\n",
       "      <td>100</td>\n",
       "      <td>200</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>epochs</th>\n",
       "      <td>25</td>\n",
       "      <td>25</td>\n",
       "      <td>25</td>\n",
       "      <td>25</td>\n",
       "      <td>25</td>\n",
       "      <td>25</td>\n",
       "      <td>25</td>\n",
       "      <td>25</td>\n",
       "      <td>25</td>\n",
       "      <td>25</td>\n",
       "      <td>...</td>\n",
       "      <td>25</td>\n",
       "      <td>25</td>\n",
       "      <td>25</td>\n",
       "      <td>25</td>\n",
       "      <td>25</td>\n",
       "      <td>25</td>\n",
       "      <td>25</td>\n",
       "      <td>25</td>\n",
       "      <td>25</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>score</th>\n",
       "      <td>0.0892</td>\n",
       "      <td>0.0892</td>\n",
       "      <td>0.0974</td>\n",
       "      <td>0.098</td>\n",
       "      <td>0.098</td>\n",
       "      <td>0.1009</td>\n",
       "      <td>0.101</td>\n",
       "      <td>0.1028</td>\n",
       "      <td>0.1028</td>\n",
       "      <td>0.1032</td>\n",
       "      <td>...</td>\n",
       "      <td>0.7916</td>\n",
       "      <td>0.8162</td>\n",
       "      <td>0.8334</td>\n",
       "      <td>0.8953</td>\n",
       "      <td>0.8959</td>\n",
       "      <td>0.9038</td>\n",
       "      <td>0.9059</td>\n",
       "      <td>0.9102</td>\n",
       "      <td>0.9252</td>\n",
       "      <td>0.9315</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>13 rows × 40 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    2                 14                12  \\\n",
       "inputs                             784               784               784   \n",
       "number_of_layers                     1                 2                 2   \n",
       "outputs                             10                10                10   \n",
       "neurons_per_layer             [20, 10]      [30, 30, 10]      [30, 20, 10]   \n",
       "dropout_layers                   False              True             False   \n",
       "activation_functions         [sigmoid]   [tanh, sigmoid]      [tanh, relu]   \n",
       "optimizers                        Adam              Adam              Adam   \n",
       "learning_rates                     0.3               0.1               0.1   \n",
       "weight_decays                      1.0               0.3               0.1   \n",
       "loss_functions        CrossEntropyLoss  CrossEntropyLoss  CrossEntropyLoss   \n",
       "batches                            100               200               300   \n",
       "epochs                              25                25                25   \n",
       "score                           0.0892            0.0892            0.0974   \n",
       "\n",
       "                                    0                      24  \\\n",
       "inputs                             784                    784   \n",
       "number_of_layers                     1                      3   \n",
       "outputs                             10                     10   \n",
       "neurons_per_layer             [10, 10]       [30, 10, 30, 10]   \n",
       "dropout_layers                   False                  False   \n",
       "activation_functions         [sigmoid]  [tanh, sigmoid, tanh]   \n",
       "optimizers                        Adam                   Adam   \n",
       "learning_rates                     0.3                    0.1   \n",
       "weight_decays                      1.0                    1.0   \n",
       "loss_functions        CrossEntropyLoss       CrossEntropyLoss   \n",
       "batches                            300                    100   \n",
       "epochs                              25                     25   \n",
       "score                            0.098                  0.098   \n",
       "\n",
       "                                         28                9   \\\n",
       "inputs                                  784               784   \n",
       "number_of_layers                          3                 1   \n",
       "outputs                                  10                10   \n",
       "neurons_per_layer          [30, 10, 50, 10]          [50, 10]   \n",
       "dropout_layers                         True              True   \n",
       "activation_functions  [tanh, sigmoid, relu]            [relu]   \n",
       "optimizers                             Adam               SGD   \n",
       "learning_rates                          0.1               0.3   \n",
       "weight_decays                          0.03               0.1   \n",
       "loss_functions             CrossEntropyLoss  CrossEntropyLoss   \n",
       "batches                                 300               300   \n",
       "epochs                                   25                25   \n",
       "score                                0.1009             0.101   \n",
       "\n",
       "                                         22                10  \\\n",
       "inputs                                  784               784   \n",
       "number_of_layers                          3                 2   \n",
       "outputs                                  10                10   \n",
       "neurons_per_layer          [30, 10, 20, 10]      [30, 10, 10]   \n",
       "dropout_layers                        False             False   \n",
       "activation_functions  [tanh, sigmoid, tanh]      [tanh, relu]   \n",
       "optimizers                             Adam              Adam   \n",
       "learning_rates                         0.01              0.03   \n",
       "weight_decays                          0.03               0.3   \n",
       "loss_functions             CrossEntropyLoss  CrossEntropyLoss   \n",
       "batches                                 100               300   \n",
       "epochs                                   25                25   \n",
       "score                                0.1028            0.1028   \n",
       "\n",
       "                                                  37  ...                13  \\\n",
       "inputs                                           784  ...               784   \n",
       "number_of_layers                                   4  ...                 2   \n",
       "outputs                                           10  ...                10   \n",
       "neurons_per_layer               [30, 10, 10, 40, 10]  ...      [30, 20, 10]   \n",
       "dropout_layers                                  True  ...              True   \n",
       "activation_functions  [tanh, sigmoid, relu, sigmoid]  ...   [tanh, sigmoid]   \n",
       "optimizers                                       SGD  ...               SGD   \n",
       "learning_rates                                   0.3  ...              0.03   \n",
       "weight_decays                                    1.0  ...              0.03   \n",
       "loss_functions                      CrossEntropyLoss  ...  CrossEntropyLoss   \n",
       "batches                                          100  ...               100   \n",
       "epochs                                            25  ...                25   \n",
       "score                                         0.1032  ...            0.7916   \n",
       "\n",
       "                                    3                 18  \\\n",
       "inputs                             784               784   \n",
       "number_of_layers                     1                 2   \n",
       "outputs                             10                10   \n",
       "neurons_per_layer             [20, 10]      [30, 50, 10]   \n",
       "dropout_layers                    True              True   \n",
       "activation_functions            [relu]   [tanh, sigmoid]   \n",
       "optimizers                         SGD              Adam   \n",
       "learning_rates                   0.003              0.01   \n",
       "weight_decays                      0.3              0.01   \n",
       "loss_functions        CrossEntropyLoss  CrossEntropyLoss   \n",
       "batches                            200               200   \n",
       "epochs                              25                25   \n",
       "score                           0.8162            0.8334   \n",
       "\n",
       "                                               30                1   \\\n",
       "inputs                                        784               784   \n",
       "number_of_layers                                4                 1   \n",
       "outputs                                        10                10   \n",
       "neurons_per_layer            [30, 10, 10, 10, 10]          [10, 10]   \n",
       "dropout_layers                               True             False   \n",
       "activation_functions  [tanh, sigmoid, relu, relu]            [relu]   \n",
       "optimizers                                   Adam               SGD   \n",
       "learning_rates                              0.001             0.001   \n",
       "weight_decays                                0.01              0.03   \n",
       "loss_functions                   CrossEntropyLoss  CrossEntropyLoss   \n",
       "batches                                       200               300   \n",
       "epochs                                         25                25   \n",
       "score                                      0.8953            0.8959   \n",
       "\n",
       "                                         26                5   \\\n",
       "inputs                                  784               784   \n",
       "number_of_layers                          3                 1   \n",
       "outputs                                  10                10   \n",
       "neurons_per_layer          [30, 10, 40, 10]          [30, 10]   \n",
       "dropout_layers                        False              True   \n",
       "activation_functions  [tanh, sigmoid, relu]         [sigmoid]   \n",
       "optimizers                             Adam               SGD   \n",
       "learning_rates                        0.003              0.01   \n",
       "weight_decays                          0.01              0.01   \n",
       "loss_functions             CrossEntropyLoss  CrossEntropyLoss   \n",
       "batches                                 100               300   \n",
       "epochs                                   25                25   \n",
       "score                                0.9038            0.9059   \n",
       "\n",
       "                                    15                11                4   \n",
       "inputs                             784               784               784  \n",
       "number_of_layers                     2                 2                 1  \n",
       "outputs                             10                10                10  \n",
       "neurons_per_layer         [30, 30, 10]      [30, 10, 10]          [30, 10]  \n",
       "dropout_layers                   False              True             False  \n",
       "activation_functions   [tanh, sigmoid]      [tanh, relu]            [tanh]  \n",
       "optimizers                         SGD               SGD              Adam  \n",
       "learning_rates                    0.03             0.003             0.001  \n",
       "weight_decays                     0.01              0.01              0.01  \n",
       "loss_functions        CrossEntropyLoss  CrossEntropyLoss  CrossEntropyLoss  \n",
       "batches                            100               200               100  \n",
       "epochs                              25                25                25  \n",
       "score                           0.9102            0.9252            0.9315  \n",
       "\n",
       "[13 rows x 40 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist_fc_min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>28</th>\n",
       "      <th>22</th>\n",
       "      <th>79</th>\n",
       "      <th>78</th>\n",
       "      <th>40</th>\n",
       "      <th>24</th>\n",
       "      <th>42</th>\n",
       "      <th>46</th>\n",
       "      <th>20</th>\n",
       "      <th>48</th>\n",
       "      <th>...</th>\n",
       "      <th>9</th>\n",
       "      <th>1</th>\n",
       "      <th>29</th>\n",
       "      <th>19</th>\n",
       "      <th>47</th>\n",
       "      <th>43</th>\n",
       "      <th>11</th>\n",
       "      <th>65</th>\n",
       "      <th>49</th>\n",
       "      <th>15</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>inputs</th>\n",
       "      <td>784</td>\n",
       "      <td>784</td>\n",
       "      <td>784</td>\n",
       "      <td>784</td>\n",
       "      <td>784</td>\n",
       "      <td>784</td>\n",
       "      <td>784</td>\n",
       "      <td>784</td>\n",
       "      <td>784</td>\n",
       "      <td>784</td>\n",
       "      <td>...</td>\n",
       "      <td>784</td>\n",
       "      <td>784</td>\n",
       "      <td>784</td>\n",
       "      <td>784</td>\n",
       "      <td>784</td>\n",
       "      <td>784</td>\n",
       "      <td>784</td>\n",
       "      <td>784</td>\n",
       "      <td>784</td>\n",
       "      <td>784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>number_of_layers</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>outputs</th>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>...</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>neurons_per_layer</th>\n",
       "      <td>[40, 30, 10]</td>\n",
       "      <td>[40, 10, 10]</td>\n",
       "      <td>[40, 10, 10, 50, 10]</td>\n",
       "      <td>[40, 10, 10, 50, 10]</td>\n",
       "      <td>[40, 10, 10, 10]</td>\n",
       "      <td>[40, 20, 10]</td>\n",
       "      <td>[40, 10, 10, 10]</td>\n",
       "      <td>[40, 10, 20, 10]</td>\n",
       "      <td>[40, 10, 10]</td>\n",
       "      <td>[40, 10, 30, 10]</td>\n",
       "      <td>...</td>\n",
       "      <td>[30, 10]</td>\n",
       "      <td>[10, 10]</td>\n",
       "      <td>[40, 30, 10]</td>\n",
       "      <td>[50, 10]</td>\n",
       "      <td>[40, 10, 20, 10]</td>\n",
       "      <td>[40, 10, 10, 10]</td>\n",
       "      <td>[30, 10]</td>\n",
       "      <td>[40, 10, 10, 20, 10]</td>\n",
       "      <td>[40, 10, 30, 10]</td>\n",
       "      <td>[40, 10]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dropout_layers</th>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>activation_functions</th>\n",
       "      <td>[relu, tanh]</td>\n",
       "      <td>[relu, relu]</td>\n",
       "      <td>[relu, sigmoid, relu, tanh]</td>\n",
       "      <td>[relu, sigmoid, relu, tanh]</td>\n",
       "      <td>[relu, sigmoid, sigmoid]</td>\n",
       "      <td>[relu, tanh]</td>\n",
       "      <td>[relu, sigmoid, tanh]</td>\n",
       "      <td>[relu, sigmoid, tanh]</td>\n",
       "      <td>[relu, sigmoid]</td>\n",
       "      <td>[relu, sigmoid, sigmoid]</td>\n",
       "      <td>...</td>\n",
       "      <td>[relu]</td>\n",
       "      <td>[tanh]</td>\n",
       "      <td>[relu, tanh]</td>\n",
       "      <td>[relu]</td>\n",
       "      <td>[relu, sigmoid, sigmoid]</td>\n",
       "      <td>[relu, sigmoid, tanh]</td>\n",
       "      <td>[sigmoid]</td>\n",
       "      <td>[relu, sigmoid, relu, tanh]</td>\n",
       "      <td>[relu, sigmoid, sigmoid]</td>\n",
       "      <td>[relu]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>optimizers</th>\n",
       "      <td>Adam</td>\n",
       "      <td>Adam</td>\n",
       "      <td>SGD</td>\n",
       "      <td>Adam</td>\n",
       "      <td>Adam</td>\n",
       "      <td>Adam</td>\n",
       "      <td>Adam</td>\n",
       "      <td>Adam</td>\n",
       "      <td>Adam</td>\n",
       "      <td>Adam</td>\n",
       "      <td>...</td>\n",
       "      <td>SGD</td>\n",
       "      <td>SGD</td>\n",
       "      <td>SGD</td>\n",
       "      <td>SGD</td>\n",
       "      <td>SGD</td>\n",
       "      <td>SGD</td>\n",
       "      <td>SGD</td>\n",
       "      <td>SGD</td>\n",
       "      <td>SGD</td>\n",
       "      <td>SGD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>learning_rates</th>\n",
       "      <td>0.3</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.01</td>\n",
       "      <td>...</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>weight_decays</th>\n",
       "      <td>0.01</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.03</td>\n",
       "      <td>...</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>loss_functions</th>\n",
       "      <td>CrossEntropyLoss</td>\n",
       "      <td>CrossEntropyLoss</td>\n",
       "      <td>CrossEntropyLoss</td>\n",
       "      <td>CrossEntropyLoss</td>\n",
       "      <td>CrossEntropyLoss</td>\n",
       "      <td>CrossEntropyLoss</td>\n",
       "      <td>CrossEntropyLoss</td>\n",
       "      <td>CrossEntropyLoss</td>\n",
       "      <td>CrossEntropyLoss</td>\n",
       "      <td>CrossEntropyLoss</td>\n",
       "      <td>...</td>\n",
       "      <td>CrossEntropyLoss</td>\n",
       "      <td>CrossEntropyLoss</td>\n",
       "      <td>CrossEntropyLoss</td>\n",
       "      <td>CrossEntropyLoss</td>\n",
       "      <td>CrossEntropyLoss</td>\n",
       "      <td>CrossEntropyLoss</td>\n",
       "      <td>CrossEntropyLoss</td>\n",
       "      <td>CrossEntropyLoss</td>\n",
       "      <td>CrossEntropyLoss</td>\n",
       "      <td>CrossEntropyLoss</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>batches</th>\n",
       "      <td>200</td>\n",
       "      <td>200</td>\n",
       "      <td>200</td>\n",
       "      <td>100</td>\n",
       "      <td>50</td>\n",
       "      <td>200</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>200</td>\n",
       "      <td>200</td>\n",
       "      <td>...</td>\n",
       "      <td>100</td>\n",
       "      <td>200</td>\n",
       "      <td>50</td>\n",
       "      <td>100</td>\n",
       "      <td>50</td>\n",
       "      <td>50</td>\n",
       "      <td>200</td>\n",
       "      <td>50</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>epochs</th>\n",
       "      <td>25</td>\n",
       "      <td>25</td>\n",
       "      <td>25</td>\n",
       "      <td>25</td>\n",
       "      <td>25</td>\n",
       "      <td>25</td>\n",
       "      <td>25</td>\n",
       "      <td>25</td>\n",
       "      <td>25</td>\n",
       "      <td>25</td>\n",
       "      <td>...</td>\n",
       "      <td>25</td>\n",
       "      <td>25</td>\n",
       "      <td>25</td>\n",
       "      <td>25</td>\n",
       "      <td>25</td>\n",
       "      <td>25</td>\n",
       "      <td>25</td>\n",
       "      <td>25</td>\n",
       "      <td>25</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>score</th>\n",
       "      <td>0.0902</td>\n",
       "      <td>0.0992</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.8417</td>\n",
       "      <td>0.8457</td>\n",
       "      <td>0.8458</td>\n",
       "      <td>0.8477</td>\n",
       "      <td>0.8494</td>\n",
       "      <td>0.8569</td>\n",
       "      <td>0.8579</td>\n",
       "      <td>0.8621</td>\n",
       "      <td>0.8736</td>\n",
       "      <td>0.8762</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>13 rows × 80 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    28                22  \\\n",
       "inputs                             784               784   \n",
       "number_of_layers                     2                 2   \n",
       "outputs                             10                10   \n",
       "neurons_per_layer         [40, 30, 10]      [40, 10, 10]   \n",
       "dropout_layers                    True             False   \n",
       "activation_functions      [relu, tanh]      [relu, relu]   \n",
       "optimizers                        Adam              Adam   \n",
       "learning_rates                     0.3               0.3   \n",
       "weight_decays                     0.01               0.1   \n",
       "loss_functions        CrossEntropyLoss  CrossEntropyLoss   \n",
       "batches                            200               200   \n",
       "epochs                              25                25   \n",
       "score                           0.0902            0.0992   \n",
       "\n",
       "                                               79  \\\n",
       "inputs                                        784   \n",
       "number_of_layers                                4   \n",
       "outputs                                        10   \n",
       "neurons_per_layer            [40, 10, 10, 50, 10]   \n",
       "dropout_layers                              False   \n",
       "activation_functions  [relu, sigmoid, relu, tanh]   \n",
       "optimizers                                    SGD   \n",
       "learning_rates                                0.3   \n",
       "weight_decays                                 0.1   \n",
       "loss_functions                   CrossEntropyLoss   \n",
       "batches                                       200   \n",
       "epochs                                         25   \n",
       "score                                         0.1   \n",
       "\n",
       "                                               78                        40  \\\n",
       "inputs                                        784                       784   \n",
       "number_of_layers                                4                         3   \n",
       "outputs                                        10                        10   \n",
       "neurons_per_layer            [40, 10, 10, 50, 10]          [40, 10, 10, 10]   \n",
       "dropout_layers                              False                     False   \n",
       "activation_functions  [relu, sigmoid, relu, tanh]  [relu, sigmoid, sigmoid]   \n",
       "optimizers                                   Adam                      Adam   \n",
       "learning_rates                                0.1                       0.3   \n",
       "weight_decays                                 0.1                      0.01   \n",
       "loss_functions                   CrossEntropyLoss          CrossEntropyLoss   \n",
       "batches                                       100                        50   \n",
       "epochs                                         25                        25   \n",
       "score                                         0.1                       0.1   \n",
       "\n",
       "                                    24                     42  \\\n",
       "inputs                             784                    784   \n",
       "number_of_layers                     2                      3   \n",
       "outputs                             10                     10   \n",
       "neurons_per_layer         [40, 20, 10]       [40, 10, 10, 10]   \n",
       "dropout_layers                    True                   True   \n",
       "activation_functions      [relu, tanh]  [relu, sigmoid, tanh]   \n",
       "optimizers                        Adam                   Adam   \n",
       "learning_rates                     0.3                    0.3   \n",
       "weight_decays                      0.1                   0.03   \n",
       "loss_functions        CrossEntropyLoss       CrossEntropyLoss   \n",
       "batches                            200                    100   \n",
       "epochs                              25                     25   \n",
       "score                              0.1                    0.1   \n",
       "\n",
       "                                         46                20  \\\n",
       "inputs                                  784               784   \n",
       "number_of_layers                          3                 2   \n",
       "outputs                                  10                10   \n",
       "neurons_per_layer          [40, 10, 20, 10]      [40, 10, 10]   \n",
       "dropout_layers                        False             False   \n",
       "activation_functions  [relu, sigmoid, tanh]   [relu, sigmoid]   \n",
       "optimizers                             Adam              Adam   \n",
       "learning_rates                         0.01               0.1   \n",
       "weight_decays                           0.3               0.3   \n",
       "loss_functions             CrossEntropyLoss  CrossEntropyLoss   \n",
       "batches                                 100               200   \n",
       "epochs                                   25                25   \n",
       "score                                   0.1               0.1   \n",
       "\n",
       "                                            48  ...                9   \\\n",
       "inputs                                     784  ...               784   \n",
       "number_of_layers                             3  ...                 1   \n",
       "outputs                                     10  ...                10   \n",
       "neurons_per_layer             [40, 10, 30, 10]  ...          [30, 10]   \n",
       "dropout_layers                           False  ...             False   \n",
       "activation_functions  [relu, sigmoid, sigmoid]  ...            [relu]   \n",
       "optimizers                                Adam  ...               SGD   \n",
       "learning_rates                            0.01  ...              0.01   \n",
       "weight_decays                             0.03  ...              0.01   \n",
       "loss_functions                CrossEntropyLoss  ...  CrossEntropyLoss   \n",
       "batches                                    200  ...               100   \n",
       "epochs                                      25  ...                25   \n",
       "score                                      0.1  ...            0.8417   \n",
       "\n",
       "                                    1                 29                19  \\\n",
       "inputs                             784               784               784   \n",
       "number_of_layers                     1                 2                 1   \n",
       "outputs                             10                10                10   \n",
       "neurons_per_layer             [10, 10]      [40, 30, 10]          [50, 10]   \n",
       "dropout_layers                    True             False              True   \n",
       "activation_functions            [tanh]      [relu, tanh]            [relu]   \n",
       "optimizers                         SGD               SGD               SGD   \n",
       "learning_rates                    0.01              0.03               0.3   \n",
       "weight_decays                      0.0              0.01               0.0   \n",
       "loss_functions        CrossEntropyLoss  CrossEntropyLoss  CrossEntropyLoss   \n",
       "batches                            200                50               100   \n",
       "epochs                              25                25                25   \n",
       "score                           0.8457            0.8458            0.8477   \n",
       "\n",
       "                                            47                     43  \\\n",
       "inputs                                     784                    784   \n",
       "number_of_layers                             3                      3   \n",
       "outputs                                     10                     10   \n",
       "neurons_per_layer             [40, 10, 20, 10]       [40, 10, 10, 10]   \n",
       "dropout_layers                            True                  False   \n",
       "activation_functions  [relu, sigmoid, sigmoid]  [relu, sigmoid, tanh]   \n",
       "optimizers                                 SGD                    SGD   \n",
       "learning_rates                            0.03                   0.03   \n",
       "weight_decays                              0.0                    0.0   \n",
       "loss_functions                CrossEntropyLoss       CrossEntropyLoss   \n",
       "batches                                     50                     50   \n",
       "epochs                                      25                     25   \n",
       "score                                   0.8494                 0.8569   \n",
       "\n",
       "                                    11                           65  \\\n",
       "inputs                             784                          784   \n",
       "number_of_layers                     1                            4   \n",
       "outputs                             10                           10   \n",
       "neurons_per_layer             [30, 10]         [40, 10, 10, 20, 10]   \n",
       "dropout_layers                   False                        False   \n",
       "activation_functions         [sigmoid]  [relu, sigmoid, relu, tanh]   \n",
       "optimizers                         SGD                          SGD   \n",
       "learning_rates                    0.03                          0.3   \n",
       "weight_decays                      0.0                          0.0   \n",
       "loss_functions        CrossEntropyLoss             CrossEntropyLoss   \n",
       "batches                            200                           50   \n",
       "epochs                              25                           25   \n",
       "score                           0.8579                       0.8621   \n",
       "\n",
       "                                            49                15  \n",
       "inputs                                     784               784  \n",
       "number_of_layers                             3                 1  \n",
       "outputs                                     10                10  \n",
       "neurons_per_layer             [40, 10, 30, 10]          [40, 10]  \n",
       "dropout_layers                           False              True  \n",
       "activation_functions  [relu, sigmoid, sigmoid]            [relu]  \n",
       "optimizers                                 SGD               SGD  \n",
       "learning_rates                             0.1              0.03  \n",
       "weight_decays                              0.0               0.0  \n",
       "loss_functions                CrossEntropyLoss  CrossEntropyLoss  \n",
       "batches                                    100               100  \n",
       "epochs                                      25                25  \n",
       "score                                   0.8736            0.8762  \n",
       "\n",
       "[13 rows x 80 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fmnist_fc_min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>51</th>\n",
       "      <th>72</th>\n",
       "      <th>71</th>\n",
       "      <th>70</th>\n",
       "      <th>67</th>\n",
       "      <th>66</th>\n",
       "      <th>65</th>\n",
       "      <th>64</th>\n",
       "      <th>63</th>\n",
       "      <th>73</th>\n",
       "      <th>...</th>\n",
       "      <th>29</th>\n",
       "      <th>25</th>\n",
       "      <th>24</th>\n",
       "      <th>47</th>\n",
       "      <th>45</th>\n",
       "      <th>99</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>69</th>\n",
       "      <th>86</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>network_inputs</th>\n",
       "      <td>(28, 28, 1)</td>\n",
       "      <td>(28, 28, 1)</td>\n",
       "      <td>(28, 28, 1)</td>\n",
       "      <td>(28, 28, 1)</td>\n",
       "      <td>(28, 28, 1)</td>\n",
       "      <td>(28, 28, 1)</td>\n",
       "      <td>(28, 28, 1)</td>\n",
       "      <td>(28, 28, 1)</td>\n",
       "      <td>(28, 28, 1)</td>\n",
       "      <td>(28, 28, 1)</td>\n",
       "      <td>...</td>\n",
       "      <td>(28, 28, 1)</td>\n",
       "      <td>(28, 28, 1)</td>\n",
       "      <td>(28, 28, 1)</td>\n",
       "      <td>(28, 28, 1)</td>\n",
       "      <td>(28, 28, 1)</td>\n",
       "      <td>(28, 28, 1)</td>\n",
       "      <td>(28, 28, 1)</td>\n",
       "      <td>(28, 28, 1)</td>\n",
       "      <td>(28, 28, 1)</td>\n",
       "      <td>(28, 28, 1)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>conv2d_layers</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>conv2d_outputs</th>\n",
       "      <td>[32, 128]</td>\n",
       "      <td>[32, 32, 64]</td>\n",
       "      <td>[32, 32, 64]</td>\n",
       "      <td>[32, 32, 64]</td>\n",
       "      <td>[32, 32, 32]</td>\n",
       "      <td>[32, 32, 32]</td>\n",
       "      <td>[32, 32, 32]</td>\n",
       "      <td>[32, 32, 32]</td>\n",
       "      <td>[32, 32, 32]</td>\n",
       "      <td>[32, 32, 64]</td>\n",
       "      <td>...</td>\n",
       "      <td>[128]</td>\n",
       "      <td>[128]</td>\n",
       "      <td>[128]</td>\n",
       "      <td>[32, 64]</td>\n",
       "      <td>[32, 64]</td>\n",
       "      <td>[32, 32, 128, 32]</td>\n",
       "      <td>[32]</td>\n",
       "      <td>[32]</td>\n",
       "      <td>[32, 32, 32]</td>\n",
       "      <td>[32, 32, 128]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>kernels</th>\n",
       "      <td>[(2, 2), (3, 3)]</td>\n",
       "      <td>[(2, 2), (2, 2), (4, 4)]</td>\n",
       "      <td>[(2, 2), (2, 2), (5, 5)]</td>\n",
       "      <td>[(2, 2), (2, 2), (3, 3)]</td>\n",
       "      <td>[(2, 2), (2, 2), (4, 4)]</td>\n",
       "      <td>[(2, 2), (2, 2), (4, 4)]</td>\n",
       "      <td>[(2, 2), (2, 2), (3, 3)]</td>\n",
       "      <td>[(2, 2), (2, 2), (4, 4)]</td>\n",
       "      <td>[(2, 2), (2, 2), (2, 2)]</td>\n",
       "      <td>[(2, 2), (2, 2), (5, 5)]</td>\n",
       "      <td>...</td>\n",
       "      <td>[(2, 2)]</td>\n",
       "      <td>[(5, 5)]</td>\n",
       "      <td>[(4, 4)]</td>\n",
       "      <td>[(2, 2), (5, 5)]</td>\n",
       "      <td>[(2, 2), (4, 4)]</td>\n",
       "      <td>[(2, 2), (2, 2), (2, 2), (4, 4)]</td>\n",
       "      <td>[(2, 2)]</td>\n",
       "      <td>[(2, 2), (2, 2), (5, 5)]</td>\n",
       "      <td>[(2, 2), (2, 2), (3, 3)]</td>\n",
       "      <td>[(2, 2), (2, 2), (2, 2)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dense_layers</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dense_outputs</th>\n",
       "      <td>[64, 64, 10]</td>\n",
       "      <td>[32, 10]</td>\n",
       "      <td>[128, 16, 32, 10]</td>\n",
       "      <td>[64, 10]</td>\n",
       "      <td>[128, 64, 128, 10]</td>\n",
       "      <td>[128, 32, 64, 10]</td>\n",
       "      <td>[16, 16, 32, 10]</td>\n",
       "      <td>[64, 128, 16, 10]</td>\n",
       "      <td>[128, 32, 10]</td>\n",
       "      <td>[128, 32, 128, 10]</td>\n",
       "      <td>...</td>\n",
       "      <td>[32, 64, 10]</td>\n",
       "      <td>[32, 64, 10]</td>\n",
       "      <td>[32, 64, 64, 10]</td>\n",
       "      <td>[128, 128, 32, 10]</td>\n",
       "      <td>[64, 128, 10]</td>\n",
       "      <td>[64, 128, 10]</td>\n",
       "      <td>[128, 10]</td>\n",
       "      <td>[32, 10]</td>\n",
       "      <td>[16, 32, 10]</td>\n",
       "      <td>[16, 32, 10]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>network_output</th>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>...</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dropout_layers</th>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>conv2d_activations</th>\n",
       "      <td>[relu, tanh]</td>\n",
       "      <td>[relu, sigmoid, tanh]</td>\n",
       "      <td>[relu, sigmoid, sigmoid]</td>\n",
       "      <td>[relu, sigmoid, tanh]</td>\n",
       "      <td>[relu, sigmoid, relu]</td>\n",
       "      <td>[relu, sigmoid, tanh]</td>\n",
       "      <td>[relu, sigmoid, tanh]</td>\n",
       "      <td>[relu, sigmoid, sigmoid]</td>\n",
       "      <td>[relu, sigmoid, tanh]</td>\n",
       "      <td>[relu, sigmoid, relu]</td>\n",
       "      <td>...</td>\n",
       "      <td>[relu]</td>\n",
       "      <td>[relu]</td>\n",
       "      <td>[relu]</td>\n",
       "      <td>[relu, relu]</td>\n",
       "      <td>[relu, sigmoid]</td>\n",
       "      <td>[relu, sigmoid, relu, relu]</td>\n",
       "      <td>[relu]</td>\n",
       "      <td>[relu]</td>\n",
       "      <td>[relu, sigmoid, relu]</td>\n",
       "      <td>[relu, sigmoid, relu]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dense_activations</th>\n",
       "      <td>[tanh, relu, softmax]</td>\n",
       "      <td>[sigmoid, softmax]</td>\n",
       "      <td>[relu, tanh, sigmoid, softmax]</td>\n",
       "      <td>[sigmoid, softmax]</td>\n",
       "      <td>[tanh, sigmoid, tanh, softmax]</td>\n",
       "      <td>[relu, relu, tanh, softmax]</td>\n",
       "      <td>[tanh, sigmoid, tanh, softmax]</td>\n",
       "      <td>[tanh, relu, sigmoid, softmax]</td>\n",
       "      <td>[relu, sigmoid, softmax]</td>\n",
       "      <td>[sigmoid, relu, sigmoid, softmax]</td>\n",
       "      <td>...</td>\n",
       "      <td>[tanh, tanh, softmax]</td>\n",
       "      <td>[sigmoid, relu, softmax]</td>\n",
       "      <td>[sigmoid, relu, sigmoid, softmax]</td>\n",
       "      <td>[tanh, sigmoid, tanh, softmax]</td>\n",
       "      <td>[sigmoid, tanh, softmax]</td>\n",
       "      <td>[relu, sigmoid, softmax]</td>\n",
       "      <td>[relu, softmax]</td>\n",
       "      <td>[tanh, softmax]</td>\n",
       "      <td>[sigmoid, tanh, softmax]</td>\n",
       "      <td>[tanh, sigmoid, softmax]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>optimizers</th>\n",
       "      <td>Adam</td>\n",
       "      <td>Adam</td>\n",
       "      <td>Adam</td>\n",
       "      <td>Adam</td>\n",
       "      <td>SGD</td>\n",
       "      <td>SGD</td>\n",
       "      <td>SGD</td>\n",
       "      <td>Adam</td>\n",
       "      <td>Adam</td>\n",
       "      <td>Adam</td>\n",
       "      <td>...</td>\n",
       "      <td>SGD</td>\n",
       "      <td>SGD</td>\n",
       "      <td>Adam</td>\n",
       "      <td>SGD</td>\n",
       "      <td>SGD</td>\n",
       "      <td>SGD</td>\n",
       "      <td>Adam</td>\n",
       "      <td>Adam</td>\n",
       "      <td>SGD</td>\n",
       "      <td>SGD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>learning_rates</th>\n",
       "      <td>0.1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.01</td>\n",
       "      <td>...</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>weight_decays</th>\n",
       "      <td>0.01</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.03</td>\n",
       "      <td>...</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>loss_functions</th>\n",
       "      <td>categorical_crossentropy</td>\n",
       "      <td>categorical_crossentropy</td>\n",
       "      <td>categorical_crossentropy</td>\n",
       "      <td>categorical_crossentropy</td>\n",
       "      <td>categorical_crossentropy</td>\n",
       "      <td>categorical_crossentropy</td>\n",
       "      <td>categorical_crossentropy</td>\n",
       "      <td>categorical_crossentropy</td>\n",
       "      <td>categorical_crossentropy</td>\n",
       "      <td>categorical_crossentropy</td>\n",
       "      <td>...</td>\n",
       "      <td>categorical_crossentropy</td>\n",
       "      <td>categorical_crossentropy</td>\n",
       "      <td>categorical_crossentropy</td>\n",
       "      <td>categorical_crossentropy</td>\n",
       "      <td>categorical_crossentropy</td>\n",
       "      <td>categorical_crossentropy</td>\n",
       "      <td>categorical_crossentropy</td>\n",
       "      <td>categorical_crossentropy</td>\n",
       "      <td>categorical_crossentropy</td>\n",
       "      <td>categorical_crossentropy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>batches</th>\n",
       "      <td>150</td>\n",
       "      <td>200</td>\n",
       "      <td>100</td>\n",
       "      <td>150</td>\n",
       "      <td>100</td>\n",
       "      <td>300</td>\n",
       "      <td>50</td>\n",
       "      <td>300</td>\n",
       "      <td>300</td>\n",
       "      <td>300</td>\n",
       "      <td>...</td>\n",
       "      <td>100</td>\n",
       "      <td>50</td>\n",
       "      <td>150</td>\n",
       "      <td>200</td>\n",
       "      <td>50</td>\n",
       "      <td>200</td>\n",
       "      <td>300</td>\n",
       "      <td>300</td>\n",
       "      <td>50</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>epochs</th>\n",
       "      <td>25</td>\n",
       "      <td>25</td>\n",
       "      <td>25</td>\n",
       "      <td>25</td>\n",
       "      <td>25</td>\n",
       "      <td>25</td>\n",
       "      <td>25</td>\n",
       "      <td>25</td>\n",
       "      <td>25</td>\n",
       "      <td>25</td>\n",
       "      <td>...</td>\n",
       "      <td>25</td>\n",
       "      <td>25</td>\n",
       "      <td>25</td>\n",
       "      <td>25</td>\n",
       "      <td>25</td>\n",
       "      <td>25</td>\n",
       "      <td>25</td>\n",
       "      <td>25</td>\n",
       "      <td>25</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>accuracy</th>\n",
       "      <td>0.1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.8632</td>\n",
       "      <td>0.8657</td>\n",
       "      <td>0.8704</td>\n",
       "      <td>0.8715</td>\n",
       "      <td>0.8732</td>\n",
       "      <td>0.8861</td>\n",
       "      <td>0.8992</td>\n",
       "      <td>0.9029</td>\n",
       "      <td>0.9032</td>\n",
       "      <td>0.9074</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>17 rows × 120 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          51                        72  \\\n",
       "network_inputs                   (28, 28, 1)               (28, 28, 1)   \n",
       "conv2d_layers                              2                         3   \n",
       "conv2d_outputs                     [32, 128]              [32, 32, 64]   \n",
       "kernels                     [(2, 2), (3, 3)]  [(2, 2), (2, 2), (4, 4)]   \n",
       "dense_layers                               2                         1   \n",
       "dense_outputs                   [64, 64, 10]                  [32, 10]   \n",
       "network_output                            10                        10   \n",
       "dropout_layers                          True                     False   \n",
       "conv2d_activations              [relu, tanh]     [relu, sigmoid, tanh]   \n",
       "dense_activations      [tanh, relu, softmax]        [sigmoid, softmax]   \n",
       "optimizers                              Adam                      Adam   \n",
       "learning_rates                           0.1                       0.1   \n",
       "weight_decays                           0.01                      0.01   \n",
       "loss_functions      categorical_crossentropy  categorical_crossentropy   \n",
       "batches                                  150                       200   \n",
       "epochs                                    25                        25   \n",
       "accuracy                                 0.1                       0.1   \n",
       "\n",
       "                                                71                        70  \\\n",
       "network_inputs                         (28, 28, 1)               (28, 28, 1)   \n",
       "conv2d_layers                                    3                         3   \n",
       "conv2d_outputs                        [32, 32, 64]              [32, 32, 64]   \n",
       "kernels                   [(2, 2), (2, 2), (5, 5)]  [(2, 2), (2, 2), (3, 3)]   \n",
       "dense_layers                                     3                         1   \n",
       "dense_outputs                    [128, 16, 32, 10]                  [64, 10]   \n",
       "network_output                                  10                        10   \n",
       "dropout_layers                               False                      True   \n",
       "conv2d_activations        [relu, sigmoid, sigmoid]     [relu, sigmoid, tanh]   \n",
       "dense_activations   [relu, tanh, sigmoid, softmax]        [sigmoid, softmax]   \n",
       "optimizers                                    Adam                      Adam   \n",
       "learning_rates                                0.03                       0.3   \n",
       "weight_decays                                 0.03                      0.03   \n",
       "loss_functions            categorical_crossentropy  categorical_crossentropy   \n",
       "batches                                        100                       150   \n",
       "epochs                                          25                        25   \n",
       "accuracy                                       0.1                       0.1   \n",
       "\n",
       "                                                67  \\\n",
       "network_inputs                         (28, 28, 1)   \n",
       "conv2d_layers                                    3   \n",
       "conv2d_outputs                        [32, 32, 32]   \n",
       "kernels                   [(2, 2), (2, 2), (4, 4)]   \n",
       "dense_layers                                     3   \n",
       "dense_outputs                   [128, 64, 128, 10]   \n",
       "network_output                                  10   \n",
       "dropout_layers                                True   \n",
       "conv2d_activations           [relu, sigmoid, relu]   \n",
       "dense_activations   [tanh, sigmoid, tanh, softmax]   \n",
       "optimizers                                     SGD   \n",
       "learning_rates                                 0.1   \n",
       "weight_decays                                 0.03   \n",
       "loss_functions            categorical_crossentropy   \n",
       "batches                                        100   \n",
       "epochs                                          25   \n",
       "accuracy                                       0.1   \n",
       "\n",
       "                                             66  \\\n",
       "network_inputs                      (28, 28, 1)   \n",
       "conv2d_layers                                 3   \n",
       "conv2d_outputs                     [32, 32, 32]   \n",
       "kernels                [(2, 2), (2, 2), (4, 4)]   \n",
       "dense_layers                                  3   \n",
       "dense_outputs                 [128, 32, 64, 10]   \n",
       "network_output                               10   \n",
       "dropout_layers                             True   \n",
       "conv2d_activations        [relu, sigmoid, tanh]   \n",
       "dense_activations   [relu, relu, tanh, softmax]   \n",
       "optimizers                                  SGD   \n",
       "learning_rates                              0.3   \n",
       "weight_decays                               0.0   \n",
       "loss_functions         categorical_crossentropy   \n",
       "batches                                     300   \n",
       "epochs                                       25   \n",
       "accuracy                                    0.1   \n",
       "\n",
       "                                                65  \\\n",
       "network_inputs                         (28, 28, 1)   \n",
       "conv2d_layers                                    3   \n",
       "conv2d_outputs                        [32, 32, 32]   \n",
       "kernels                   [(2, 2), (2, 2), (3, 3)]   \n",
       "dense_layers                                     3   \n",
       "dense_outputs                     [16, 16, 32, 10]   \n",
       "network_output                                  10   \n",
       "dropout_layers                               False   \n",
       "conv2d_activations           [relu, sigmoid, tanh]   \n",
       "dense_activations   [tanh, sigmoid, tanh, softmax]   \n",
       "optimizers                                     SGD   \n",
       "learning_rates                                0.01   \n",
       "weight_decays                                 0.03   \n",
       "loss_functions            categorical_crossentropy   \n",
       "batches                                         50   \n",
       "epochs                                          25   \n",
       "accuracy                                       0.1   \n",
       "\n",
       "                                                64                        63  \\\n",
       "network_inputs                         (28, 28, 1)               (28, 28, 1)   \n",
       "conv2d_layers                                    3                         3   \n",
       "conv2d_outputs                        [32, 32, 32]              [32, 32, 32]   \n",
       "kernels                   [(2, 2), (2, 2), (4, 4)]  [(2, 2), (2, 2), (2, 2)]   \n",
       "dense_layers                                     3                         2   \n",
       "dense_outputs                    [64, 128, 16, 10]             [128, 32, 10]   \n",
       "network_output                                  10                        10   \n",
       "dropout_layers                                True                      True   \n",
       "conv2d_activations        [relu, sigmoid, sigmoid]     [relu, sigmoid, tanh]   \n",
       "dense_activations   [tanh, relu, sigmoid, softmax]  [relu, sigmoid, softmax]   \n",
       "optimizers                                    Adam                      Adam   \n",
       "learning_rates                                0.01                       0.3   \n",
       "weight_decays                                  0.0                      0.03   \n",
       "loss_functions            categorical_crossentropy  categorical_crossentropy   \n",
       "batches                                        300                       300   \n",
       "epochs                                          25                        25   \n",
       "accuracy                                       0.1                       0.1   \n",
       "\n",
       "                                                   73  ...  \\\n",
       "network_inputs                            (28, 28, 1)  ...   \n",
       "conv2d_layers                                       3  ...   \n",
       "conv2d_outputs                           [32, 32, 64]  ...   \n",
       "kernels                      [(2, 2), (2, 2), (5, 5)]  ...   \n",
       "dense_layers                                        3  ...   \n",
       "dense_outputs                      [128, 32, 128, 10]  ...   \n",
       "network_output                                     10  ...   \n",
       "dropout_layers                                  False  ...   \n",
       "conv2d_activations              [relu, sigmoid, relu]  ...   \n",
       "dense_activations   [sigmoid, relu, sigmoid, softmax]  ...   \n",
       "optimizers                                       Adam  ...   \n",
       "learning_rates                                   0.01  ...   \n",
       "weight_decays                                    0.03  ...   \n",
       "loss_functions               categorical_crossentropy  ...   \n",
       "batches                                           300  ...   \n",
       "epochs                                             25  ...   \n",
       "accuracy                                          0.1  ...   \n",
       "\n",
       "                                          29                        25  \\\n",
       "network_inputs                   (28, 28, 1)               (28, 28, 1)   \n",
       "conv2d_layers                              1                         1   \n",
       "conv2d_outputs                         [128]                     [128]   \n",
       "kernels                             [(2, 2)]                  [(5, 5)]   \n",
       "dense_layers                               2                         2   \n",
       "dense_outputs                   [32, 64, 10]              [32, 64, 10]   \n",
       "network_output                            10                        10   \n",
       "dropout_layers                         False                     False   \n",
       "conv2d_activations                    [relu]                    [relu]   \n",
       "dense_activations      [tanh, tanh, softmax]  [sigmoid, relu, softmax]   \n",
       "optimizers                               SGD                       SGD   \n",
       "learning_rates                           0.1                       0.3   \n",
       "weight_decays                           0.01                      0.01   \n",
       "loss_functions      categorical_crossentropy  categorical_crossentropy   \n",
       "batches                                  100                        50   \n",
       "epochs                                    25                        25   \n",
       "accuracy                              0.8632                    0.8657   \n",
       "\n",
       "                                                   24  \\\n",
       "network_inputs                            (28, 28, 1)   \n",
       "conv2d_layers                                       1   \n",
       "conv2d_outputs                                  [128]   \n",
       "kernels                                      [(4, 4)]   \n",
       "dense_layers                                        3   \n",
       "dense_outputs                        [32, 64, 64, 10]   \n",
       "network_output                                     10   \n",
       "dropout_layers                                  False   \n",
       "conv2d_activations                             [relu]   \n",
       "dense_activations   [sigmoid, relu, sigmoid, softmax]   \n",
       "optimizers                                       Adam   \n",
       "learning_rates                                   0.01   \n",
       "weight_decays                                     0.0   \n",
       "loss_functions               categorical_crossentropy   \n",
       "batches                                           150   \n",
       "epochs                                             25   \n",
       "accuracy                                       0.8704   \n",
       "\n",
       "                                                47                        45  \\\n",
       "network_inputs                         (28, 28, 1)               (28, 28, 1)   \n",
       "conv2d_layers                                    2                         2   \n",
       "conv2d_outputs                            [32, 64]                  [32, 64]   \n",
       "kernels                           [(2, 2), (5, 5)]          [(2, 2), (4, 4)]   \n",
       "dense_layers                                     3                         2   \n",
       "dense_outputs                   [128, 128, 32, 10]             [64, 128, 10]   \n",
       "network_output                                  10                        10   \n",
       "dropout_layers                                True                     False   \n",
       "conv2d_activations                    [relu, relu]           [relu, sigmoid]   \n",
       "dense_activations   [tanh, sigmoid, tanh, softmax]  [sigmoid, tanh, softmax]   \n",
       "optimizers                                     SGD                       SGD   \n",
       "learning_rates                                 0.1                      0.01   \n",
       "weight_decays                                  0.0                       0.0   \n",
       "loss_functions            categorical_crossentropy  categorical_crossentropy   \n",
       "batches                                        200                        50   \n",
       "epochs                                          25                        25   \n",
       "accuracy                                    0.8715                    0.8732   \n",
       "\n",
       "                                                  99  \\\n",
       "network_inputs                           (28, 28, 1)   \n",
       "conv2d_layers                                      4   \n",
       "conv2d_outputs                     [32, 32, 128, 32]   \n",
       "kernels             [(2, 2), (2, 2), (2, 2), (4, 4)]   \n",
       "dense_layers                                       2   \n",
       "dense_outputs                          [64, 128, 10]   \n",
       "network_output                                    10   \n",
       "dropout_layers                                 False   \n",
       "conv2d_activations       [relu, sigmoid, relu, relu]   \n",
       "dense_activations           [relu, sigmoid, softmax]   \n",
       "optimizers                                       SGD   \n",
       "learning_rates                                   0.3   \n",
       "weight_decays                                    0.0   \n",
       "loss_functions              categorical_crossentropy   \n",
       "batches                                          200   \n",
       "epochs                                            25   \n",
       "accuracy                                      0.8861   \n",
       "\n",
       "                                          0                         1   \\\n",
       "network_inputs                   (28, 28, 1)               (28, 28, 1)   \n",
       "conv2d_layers                              1                         1   \n",
       "conv2d_outputs                          [32]                      [32]   \n",
       "kernels                             [(2, 2)]  [(2, 2), (2, 2), (5, 5)]   \n",
       "dense_layers                               1                         1   \n",
       "dense_outputs                      [128, 10]                  [32, 10]   \n",
       "network_output                            10                        10   \n",
       "dropout_layers                          True                     False   \n",
       "conv2d_activations                    [relu]                    [relu]   \n",
       "dense_activations            [relu, softmax]           [tanh, softmax]   \n",
       "optimizers                              Adam                      Adam   \n",
       "learning_rates                          0.03                      0.01   \n",
       "weight_decays                           0.03                      0.03   \n",
       "loss_functions      categorical_crossentropy  categorical_crossentropy   \n",
       "batches                                  300                       300   \n",
       "epochs                                    25                        25   \n",
       "accuracy                              0.8992                    0.9029   \n",
       "\n",
       "                                          69                        86  \n",
       "network_inputs                   (28, 28, 1)               (28, 28, 1)  \n",
       "conv2d_layers                              3                         3  \n",
       "conv2d_outputs                  [32, 32, 32]             [32, 32, 128]  \n",
       "kernels             [(2, 2), (2, 2), (3, 3)]  [(2, 2), (2, 2), (2, 2)]  \n",
       "dense_layers                               2                         2  \n",
       "dense_outputs                   [16, 32, 10]              [16, 32, 10]  \n",
       "network_output                            10                        10  \n",
       "dropout_layers                         False                     False  \n",
       "conv2d_activations     [relu, sigmoid, relu]     [relu, sigmoid, relu]  \n",
       "dense_activations   [sigmoid, tanh, softmax]  [tanh, sigmoid, softmax]  \n",
       "optimizers                               SGD                       SGD  \n",
       "learning_rates                           0.1                       0.1  \n",
       "weight_decays                            0.0                       0.0  \n",
       "loss_functions      categorical_crossentropy  categorical_crossentropy  \n",
       "batches                                   50                        50  \n",
       "epochs                                    25                        25  \n",
       "accuracy                              0.9032                    0.9074  \n",
       "\n",
       "[17 rows x 120 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fmnist_cnn_min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
